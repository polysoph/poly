{"comments":[{"username":"gowers","timestamp":"2009-02-02T01:59:00.000Z","contents":"A quick question. Furstenberg and Katznelson used the Carlson-Simpson theorem in their proof. Does anyone know that proof well enough to know whether the Carlson-Simpson theorem might play a role here? If so, I could add it to the background-knowledge post. (But I’m sort of hoping it won’t be needed.)"},{"username":"jozsef","timestamp":"2009-02-02T02:08:00.000Z","contents":"In this note I will try to argue that we should consider a variant of the original problem first. If the removal technique doesn’t work here, then it won’t work in the more difficult setting. If it works, then we have a nice result! Consider the Cartesian product of an IP_d set. (An IP_d set is generated by d numbers by taking all the 2^d possible sums. So, if the n numbers are independent then the size of the IP_d set is 2^d. In the following statements we will suppose that our IP_d sets have size 2^n.) Prove that for any c>0 there is a d, such that any c-dense subset of the Cartesian product of an IP_d set (it is a two dimensional pointset) has a corner. The statement is true. One can even prove that the dense subset of a Cartesian product contains a square, by using the density HJ for k=4\\. (I will sketch the simple proof later) What is promising here is that one can build a not-very-large tripartite graph where we can try to prove a removal lemma. The vertex sets are the vertical, horizontal, and slope -1 lines, having intersection with the Cartesian product. Two vertices are connected by an edge if the corresponding lines meet in a point of our c-dense subset. Every point defines a triangle, and if you can find another, non-degenerate, triangle then we are done. This graph is still sparse, but maybe it is well-structured for a removal lemma.  \nFinally, let me prove that there is square if d is large enough compare to c. Every point of the Cartesian product has two coordinates, a 0,1 sequence of length d. It has a one to one mapping to [4]^d; Given a point ((x_1,…,x_d),(y_1,…,y_d)) where x_i,y_j are 0 or 1, it maps to (z_1,…,z_d), where z_i=0 if x_i=y_i=0, z_i=1 if x_i=1 and y_i=0, z_i=2 if x_i=0 and y_i=1, and finally z_i=3 if x_i=y_i=1\\. Any combinatorial line in [4]^d defines a square in the Cartesian product, so the density HJ implies the statement.  \n(I should learn how to use Latex in blogs…)"},{"username":"jason-dyer","timestamp":"2009-02-02T02:23:00.000Z","contents":"I find it reassuring the first thing I thought of when I read comment D was what turned out to be comment N. Would it help to have a graph where each node was a set consisting of a combinatorial line, and the edges are the sets that share a sequence in common? That is, {(1,2,3,3), (2,2,3,3), (3,2,3,3)} would link to {(1,2,3,3), (1,2,3,2), (1,2,3,1)}. I really do wonder if there’s a more direct approach to the problem, simply working out for %\\null [3]^n% what the minimal number of nodes one can remove before no combinatorial lines remain and then applying induction. Just fiddling with actual examples by hand a greedy approach works well. I’m sure this is horribly naive given the past solution had to apply ergodic theory."},{"username":"terence-tao","timestamp":"2009-02-02T02:26:00.000Z","contents":"As Gil pointed out in his post on this project, the k=2 case of density Hales-Jewett is Sperner’s theorem, [http://en.wikipedia.org/wiki/Sperner_family](http://en.wikipedia.org/wiki/Sperner_family) Now, of course, this theorem already has a combinatorial proof, but this proof has a rather different flavour than the triangle removal lemma (or its k=2 counterpart, which is the trivial statement that if a subset of a set V of vertices is small, then it can be removed entirely by removing a small number of vertices.) But perhaps a subproblem would be to find an alternate combinatorial proof of Sperner’s theorem. As pointed out to me by my student, Le Thai Hoang, another difference between DHJ and, say, the corners problem, is that one does not expect a positive density of all combinatorial lines to lie in the set. For instance, in the k=2 case, and identifying %{}[2]^n% with the set of subsets of {1,2,…,n}, Hoang observed that the set of subsets of {1,…,n} of cardinality %n/2 + O( \\sqrt{n} )% has positive density, but only about %(2+o(1))^n% of the %3^n% combinatorial lines here actually lie in the set. This seems to fit well with your observations that some weighting of the vertices etc. is needed. At the ergodic theory level, there is also a distinction between the ergodic theory proofs of, say, the corners theorem, and of density Hales-Jewett, which may be an indication that something different happens at the combinatorial level too. Namely, in the ergodic proof of corners (or Roth, or Szemeredi, etc.) one has a single dynamical system (with the action of a single group G), and one works with that system (and its factors) throughout the proof. But for DHJ, one has a system of measurable sets indexed by words, and one repeatedly refines the set of words to much smaller subsets in order to obtain good “stationarity” properties. (My student, Tim Austin, could explain this much better than I could.) It is only once one has all this stationarity that the proof can really begin in earnest. This is also consistent with proofs of the colouring Hales-Jewett theorem, which also massively restrict the space of lines being considered, and with the above observation that we do not expect a positive density of the full space of lines to be contained in the set. So perhaps some preliminary “regularisation” of the problem is needed before doing anything triangle-removal-like."},{"username":"terence-tao","timestamp":"2009-02-02T02:30:00.000Z","contents":"Incidentally, I only learned in the process of writing up my own post on this problem that Furstenberg-Katzelson have _two_ papers on Density Hales-Jewett: the “big one”, [http://www.ams.org/mathscinet-getitem?mr=1191743](http://www.ams.org/mathscinet-getitem?mr=1191743) that does the full k case, but also an earlier paper, [http://www.ams.org/mathscinet-getitem?mr=1001397](http://www.ams.org/mathscinet-getitem?mr=1001397) that just does the k=3 case. I’ve only looked at the former. An unrelated thing: there is an annoying wordpress latex bug concerning latex that begins with a bracket [, which may cause some difficulty in this context. One can fix it by prefacing any latex string that begins with a bracket with some filler (I use {})."},{"username":"tim-austin","timestamp":"2009-02-02T03:16:00.000Z","contents":"Terry’s description of the difference between the `ergodic theoretic’ approaches to the corners theorem and DHJ is well-put: at no point does an action of an easy-to-work-with _group_ of measure-preserving transformations emerge during the DHJ proof. Furstenberg and Katznelson introduce an enormous family of measure-preserving transformations indexed by all finite-length words in an alphabet of size k (and having some algebraic structure in terms of that family of words) in order to encode their notion of stationarity, but they neither use nor prove anything about the group generated by these transformations. Rather, they identify within their collection a great many `IP systems’ of transformations, a rather weaker sort of algebraic structure, and exploit that. In order to reach this point, they first obtain a kind of `stationarity’ for a family of positive-measure sets indexed by these words before introducing the transformations, and then expressing that stationarity in terms of these transformations in effect locks that notion of stationarity in place, and also gives a way to transport various mult-correlations between functions around on the underlying probability space. In this connexion, it’s in obtaining this notion of stationarity that Furstenberg and Katznelson first use the Carlson-Simpson Theorem, and they then use it repeatedly throughout the paper in order to recover some similar kind of symmetry among a collection of objects indexed by the finite-length words. At this point, I can’t see where the same need would arise in the approach to DHJ under discussion here."},{"username":"gowers","timestamp":"2009-02-02T06:25:00.000Z","contents":"With reference to Jozsef’s comment, if we suppose that the %d% numbers used to generate the set are indeed independent, then it’s natural to label a typical point of the Cartesian product as %(\\epsilon,\\eta)%, where each of %\\epsilon% and %\\eta% is a %01%-sequence of length %d%. Then a corner is a triple of the form %(\\epsilon,\\eta)%, %(\\epsilon,\\eta+\\delta)%, %(\\epsilon+\\delta,\\eta)%, where %\\delta% is a %\\{-1,0,1\\}%-valued sequence of length %d% with the property that both %\\epsilon+\\delta% and %\\eta+\\delta% are %01%-sequences. So the question is whether corners exist in every dense subset of the original Cartesian product. This is simpler than the density Hales-Jewett problem in at least one respect: it involves %01%-sequences rather than %012%-sequences. But that simplicity may be slightly misleading because we are looking for corners in the Cartesian product. A possible disadvantage is that in this formulation we lose the symmetry of the corners: the horizontal and vertical lines will intersect this set in a different way from how the lines of slope -1 do. I feel that this is a promising avenue to explore, but I would also like a little more justification of the suggestion that this variant is likely to be simpler."},{"username":"terence-tao","timestamp":"2009-02-02T06:31:00.000Z","contents":"The following questions are perhaps somewhat tangential to the main project, but have the advantage of requiring less combinatorial expertise than the main question, and so might possibly take better advantage of this format. Given any n, let %c_n% be the largest subset of %{}[3]^n% that does not contain a combinatorial line. Thus for instance %c_0=1%, %c_1 = 2%, %c_2 = 6% (for the latter, consider deleting the antidiagonal %(3,1), (2,2), (1,3)% from %{}[3]^2%). The k=3 density Hales-Jewett theorem is the statement that %c_n = o(3^n)% as %n \\to \\infty%. The first question, which seems very amenable to this medium, is to compute %c_3% exactly, and to get good upper and lower bounds on %c_4% (it seems unlikely that %c_4% will be easy to compute on the nose). This of course does not say too much about the asymptotic question, but one might at least try one’s luck with the OEIS once one has enough data (note that this would have worked with the k=2 theory). The other question is to get as good a _lower_ bound as one can for %c_n%. Of course, the trivial example %{}[2]^n% gives the lower bound %c_n \\geq 2^n%, but one can clearly do better than this. Writing the Behrend example in base 3 gives something like %c_n \\geq 3^{n - O(\\sqrt{n})}%. But presumably one can do better still."},{"username":"gowers","timestamp":"2009-02-02T06:36:00.000Z","contents":"Terry’s comment about Le Thai Hoang’s observation seems to me to be an important one, so let me spell out why, even though it’s basically there in what Terry says. If you follow the triangle-removal proof of the corners result, you find that you can prove something stronger: in a set of density %\\delta% you don’t just get one corner, but you get %c(\\delta)n^2% corners. (The proof is that if there are very few corners then there are very few triangles in the associated tripartite graph, and then you can apply triangle removal just as before to get a contradiction.) If one is going to find an analogous proof here, it is essential that it should not try to prove a lemma that has false consequences. In our case, we must be careful that we don’t accidentally try to develop an approach that would also imply that a positive density of all possible combinatorial lines lie in the set. I haven’t checked, but I think that weighting the vertices as suggested in KK avoids this pitfall, because now the analogous set to the one used by Le Thai Hoang is concentrated around sets of size %n/3%. Probably this is exactly what Terry meant at the end of his second paragraph."},{"username":"gowers","timestamp":"2009-02-02T06:45:00.000Z","contents":"In response to Terry and Tim’s comments about regularization and the ergodic-theory proof, my feeling is that we should bear in mind that something like this could well turn out to be necessary (on the grounds that genuine difficulties for one approach often have counterparts in other approaches), but should perhaps not try to guess in advance what form any regularization might take. (In case anyone’s wondering what I mean by “regularization”, a rough description would be that you have a subset %A% of a structure %X% and you find a subset %Y% of %X% of a similar form, such that %A\\cap Y% is an easier subset of %Y% to handle than %A% is of %X%.) Instead, we should press ahead, try to prove some kind of triangle-removal statement, find that it fails for some reason, try to work out a nice property of %A% that would get round the problem, prove that one can regularize to obtain that property, etc. etc. But this process will still involve guesswork, and people who know the ergodic-theoretic proof are likely to be able to make better guesses."},{"username":"jason-dyer","timestamp":"2009-02-02T07:17:00.000Z","contents":"_This of course does not say too much about the asymptotic question, but one might at least try one’s luck with the OEIS once one has enough data (note that this would have worked with the k=2 theory)._ Terry, has anyone tried to collected said data for k=3?"},{"username":"terence-tao","timestamp":"2009-02-02T07:27:00.000Z","contents":"Dear Jason, I don’t know, but I just emailed someone who I think would. My guess, though, is that density Hales-Jewett is probably not as well known as its cousins such as van der Waerden’s theorem to have much data already worked out. If so, then presumably we can start collecting data here. For instance, it is not hard to see that %c_{n+1} \\leq 3c_n% and that %c_{n+m} \\geq c_n c_m% for any n, m, so this already gives the bounds %12 \\leq c_3 \\leq 18% for %c_3%. Presumably one can do better."},{"username":"terence-tao","timestamp":"2009-02-02T09:08:00.000Z","contents":"Perhaps in order to understand the issue coming from Hoang’s observation a bit better, one can pose the following subproblem: what is the natural analogue of Varnavides theorem for DHJ? Varnavides used Roth’s theorem as a black box to show that any subset of %{}[n]% of density %\\delta% contained %\\geq c(\\delta) n^2% 3-term arithmetic progressions when n was large; a similar argument using the Ajtai-Szemeredi theorem as a black box also shows that any subset of %{}[n]^2% of density %\\delta% contains %\\geq c(\\delta) n^3% corners. So, using DHJ as a black box, what is the natural implication as to how “numerous” combinatorial lines are in a dense set? It seems that direct cardinality of such lines may not be the best measure of being “numerous”, but there should presumably still be some Varnavides-like theorem out there."},{"username":"jozsef","timestamp":"2009-02-02T10:26:00.000Z","contents":"Regarding Terry’s last comment about a Varnavides-like theorem, I’m a bit skeptical. Even for the k=2 case (Sperner’s thm) if we take points of the d dimensional cube where the difference between the 0-s and 1-s is at most %\\sqrt{d}% then we have a positive fraction of the elements and there are less than %\\binom{d/2}{\\sqrt{d}}2^d% combinatorial lines. Probably this should be the right magnitude for k=2 as a Varvadines-like result and it shouldn’t be difficult to prove. (But I don’t have the proof …)"},{"username":"gil","timestamp":"2009-02-02T12:32:00.000Z","contents":"Regarding lower bounds, fixing the numbers of ‘1’ ‘2’ and ‘3’ coordinates or even just fixing the number of ‘1’ coordinates gives better lower bounds, right? (Just like for Sperner’s theorem.)"},{"username":"terence-tao","timestamp":"2009-02-02T13:00:00.000Z","contents":"Hmm, Gil, you’re right; so we get a bound of %c_n \\gg 3^n / \\sqrt{n}% asymptotically. Dear Jozsef, yes, we will have to normalise things appropriately to get a Varnavides type theorem. For the k=3 problem, for instance, one might try to shoot for a lower bound for the number of combinatorial lines which is something like (3+o(1))^n rather than 4^n. Alternatively, we could somehow weight each line, perhaps by some factor depending on how many wildcards it contains. (Note that any set of density %\\delta% in a large %{}[3]^n% will contain at least one combinatorial line in which the number of wildcards is at most %C(\\delta)%, since one can partition %{}[3]^n% into copies of %{}[3]^{n_0}%, where %n_0% is the first index for which DHJ holds at density %\\delta/2%, and observe that the original set will have density at least %\\delta/2% in at least %\\delta/2% of these copies. So maybe one has to weight things to heavily favour lines with very few wildcards.)"},{"username":"terence-tao","timestamp":"2009-02-02T13:46:00.000Z","contents":"It just occurred to me that if a subset of %{}[3]^n% has positive density, then by the central limit theorem we may restrict attention to those strings which have %n/3+O(\\sqrt{n})% 1’s, 2’s, and 3’s, and still have a positive density of strings remaining. So without loss of generality we may assume that all strings in the set are “balanced” in this manner. Hence, the only combinatorial lines which really matter are those which have %O(\\sqrt{n})% wildcards in them, rather than %O(n)%. So the number of combinatorial lines that are genuinely “in play” is just %(3+o(1))^n% rather than %4^n%. I wonder if DHJ implies the existence of a combinatorial line with %\\Theta(\\sqrt{n})% wildcards. In the k=2 case it seems that a double counting argument should be able to establish something like this, though I haven’t checked it thoroughly. If so, then there seems to be a good shot at getting the “right” Varnavides type theorem."},{"username":"terence-tao","timestamp":"2009-02-02T14:01:00.000Z","contents":"Two more thoughts… firstly, a sufficiently good Varnavides type theorem for DHJ may have a separate application from the one in this project, namely to obtain a “relative” DHJ for dense subsets of a sufficiently pseudorandom subset of %{}[3]^n%, much as I did with Ben Green for the primes (and which now has a significantly simpler proof by Gowers and by Reingold-Trevisan-Tulsiani-Vadhan). There are other obstacles though to that task (e.g. understanding the analogue of “dual functions” for Hales-Jewett), and so this is probably a bit off-topic. Another thought is the following. For any a,b,c adding up to n, let %\\Gamma_{a,b,c}% be the subset of %{}[3]^n% consisting of those strings with a 1s, b 2s, and c 3s. If A is a dense subset of %{}[3]^n%, then %A \\cap \\Gamma_{a,b,c}% should be a dense subset of %\\Gamma_{a,b,c}% for many a, b, c; call a triple (a,b,c) _rich_ if this is the case. By applying the corners theorem (!) we should then be able to find (a+r,b,c), (a,b+r,c), (a,b,c+r) which are simultaneously rich. But now this might be a good place with which to try a triangle removal argument, converting the lines of r wildcards connecting %\\Gamma_{a+r,b,c}, \\Gamma_{a,b+r,c}, \\Gamma_{a,b,c+r}% into triangles as Tim suggests in the main post?"},{"username":"gowers","timestamp":"2009-02-02T14:13:00.000Z","contents":"Terry, I once tried something like what you suggest in the second paragraph of your last comment. There I was trying to do something different: come up with a definition of “quasirandom” that would guarantee that if you had quasirandom subsets of %\\Gamma_{a+r,b,c}%, %\\Gamma_{a,b+r,c}% and %\\Gamma_{a,b,c+r}% then you would get a combinatorial line. I failed because I kept thinking of more and more obstructions and it seemed hard to find a definition that would deal with them all. This might be another project worth pursuing. A problem here, which I feel is loosely related, is that you can’t apply triangle removal if you restrict to %\\Gamma_{a+r,b,c}%, %\\Gamma_{a,b+r,c}% and %\\Gamma_{a,b,c+r}% because you don’t get the degenerate triangles. This is similar to the problem that I had in comment CC above. (Incidentally, can anyone think of a better way of referring to comments than saying things like, “In Terry’s third last comment, paragraph 2”? Added later: I’ve gone back and numbered the comments.)"},{"username":"gowers","timestamp":"2009-02-02T14:17:00.000Z","contents":"A related observation is that if you’ve got three rich triples in the right configuration, you aren’t guaranteed a combinatorial line. For example, in two of the sets the first coordinate could always be 1 and in the third it could always be 2."},{"username":"gowers","timestamp":"2009-02-02T14:46:00.000Z","contents":"This is a response to the first paragraph of Terry’s first comment (comment number 4), in which he asks for another proof of Sperner’s theorem. I like this idea: in one sense one could say that the difficulty with density Hales-Jewett is that the proof of Sperner’s theorem doesn’t generalize in an obvious way. (I don’t necessarily mean directly, but one might try to find an argument that has the same relationship to the corners problem as a proof of Sperner’s theorem has to the trivial one-dimensional analogue of the corners theorem, which says that a dense subset of %\\null [n]% contains a pair of distinct points. But even this doesn’t seem to be at all easy.) A minor point of disagreement with what Terry says (again in comment 4). I think the correct analogue of triangle removal (in the tripartite case) one level down is the following statement: given two subsets %A\\subset[n]% and %B\\subset[n]% such that the number of pairs %(a,b)% with %a\\in A% and %b\\in B% is very small, then it is possible to remove a small number of elements from %A\\cup B% and end up with no pairs. The proof of course is that one of %A% and %B% must be small. The reason I call this the analogue is that I think the analogue of a tripartite graph is a pair of subsets of the vertex sets, and the analogue of a triangle (a triple of edges) is a pair of vertices. Now let me use this “pair-removal lemma” to prove the “one-dimensional corners” theorem. Suppose, then, that we have a subset %A\\subset [n]% of size at least %cn%. I’m going to define two vertex sets %X% and %Y% as follows. I let %X% be the set of all ordered singletons %x% such that %x\\in A%, and I let %Y% be the set of all ordered singletons %y% such that the sum of all the terms equals an element of %A%. (In other words, both %X% and %Y% are equal to %A%, but I want to demonstrate that I’m doing exactly what one does in higher dimensions.) If we can find a pair %(x,y)\\in X\\times Y%, then we are done (that is, we’ve found a configuration of the form %a, a+d% in the set %A%), except in the degenerate case that %x=y%. So if we can’t find a one-dimensional corner, then the only pairs in %X\\times Y% are the degenerate ones, which implies that there are only %cn% pairs. But then we can apply the pair-removal lemma and remove a few elements from %X\\cup Y% and end up with _no_ pairs. But the degenerate pairs are vertex-disjoint, so we have to remove at least %cn% elements to get rid of all those, so we have reached a contradiction. The next question might be whether we can use the “pair-removal lemma” to give a different proof of Sperner’s theorem. Or rather, we don’t actually want to prove the full strength of Sperner’s theorem but just the weaker statement that if you have at least %c2^n% subsets of %\\null [n]% then you can find one that’s a proper subset of another. Up to now what I’ve done is silly, but this project would not be silly: it might tell us exactly what the difficulty is with this approach to density Hales-Jewett, but in a much simpler context."},{"username":"gowers","timestamp":"2009-02-02T15:13:00.000Z","contents":"Continuing the same theme, there is a natural pair of vertex sets to define in the Sperner case. For all higher dimensions, one takes the vertex sets to be copies of the power set of %\\null[n]%, so one should surely do so here. Now in a certain sense the %j%th vertex set stands for the set of places where a sequence equals %j%. So here we would let %X% and %Y% be two copies of the power set of %\\null [n]%, with sets in %X% representing the set of places where a sequence equals 0 and sets in %Y% representing the set of places where a sequence equals 1\\. Then we take as our subset %\\mathcal{A}\\subset X% the set of all sets %U% such that the sequence that’s 0 on %U% and 1 on the complement of %U% belongs to %A%, and we take %\\mathcal{B}\\subset Y% to be the set of all sets %V% such that the sequence that’s 1 on %V% and 0 on the complement of %V% belongs to %A%. In set theoretic terms, %\\mathcal{A}% consists of sets that belong to %A% (which itself, just to be clear, is our given collection of %c2^n% subsets of %\\null [n]%), and %\\mathcal{B}% is consists of all _complements_ of sets that belong to %A%. At this point it is clear that what we are looking for is a pair %(U,V)% with %U\\in\\mathcal{A}%, %V\\in\\mathcal{B}%, and %U\\cap V=\\emptyset%. This will be a degenerate pair if %U\\cup V=[n]%, but otherwise will imply that %A% contains two sets with one a proper subset of the other. So this is the natural analogue of the triangles discussed in comments N and P. Note that in this situation we seem to be forced to put a graph structure on the two vertex sets %X% and %Y%, since we cannot just take any old pair %(U,V)% but have to make them disjoint. At first this looks bad, but I actually think it’s OK because the same disjointness condition occurs in all dimensions. In other words, you get a graph in higher dimensions and not a hypergraph. The next step, I think, would be to try to find a suitable “pair-removal lemma” for this more complicated situation where the pairs have to be disjoint sets. Is the following true (or even the right question)? Suppose you have two collections %\\mathcal{A}% and %\\mathcal{B}% of subsets of %\\null [n]%, and there are very few pairs %(A,B)% such that %A\\in\\mathcal{A}%, %B\\in\\mathcal{B}%, and %A\\cap B=\\emptyset%. Then you can remove a small fraction of the sets from %\\mathcal{A}\\cup\\mathcal{B}% and end up with _no_ such pairs."},{"username":"gowers","timestamp":"2009-02-02T16:02:00.000Z","contents":"This comment is a further response to Jozsef Solymosi’s comment (number 2). A slight variant of the problem you propose is this. Let’s take as our ground set the set of all pairs %(U,V)% of subsets of %\\null [n]%, and let’s take as our definition of a corner a triple of the form %(U,V)%, %(U\\cup D,V)%, %(U,V\\cup D)%, where both the unions must be disjoint unions. This is asking for more than you asked for because I insist that the difference %D% is positive, so to speak. It seems to be a nice combination of Sperner’s theorem and the usual corners result. But perhaps it would be more sensible not to insist on that positivity and instead ask for a triple of the form %(U,V)%, %((U\\cup D)\\setminus C,V)%, %(U, (V\\cup D)\\setminus C%, where %D% is disjoint from both %U% and %V% and %C% is contained in both %U% and %V%. That is your original problem I think. I think I now understand better why your problem could be a good toy problem to look at first. Let’s quickly work out what triangle-removal statement would be needed to solve it. (You’ve already done that, so I just want to reformulate it in set-theoretic language, which I find easier to understand.) We let all of %X%, %Y% and %Z% equal the power set of %\\null [n]%. We join %U\\in X% to %V\\in Y% if %(U,V)\\in A%. Ah, I see now that there’s a problem with what I’m suggesting, which is that in the normal corners problem we say that %(x,y+d)% and %(x+d,y)% lie in a line because both points have the same coordinate sum. When should we say that %(U,V\\cup D)% and %(U\\cup D,V)% lie in a line? It looks to me as though we have to treat the sets as %01%-sequences and take the sum again. So it’s not really a set-theoretic reformulation after all. That suggests a wild question, which might have an easy counterexample (or an easy proof using density Hales-Jewett, which would be useful just to see whether it is true). If you have a dense subset %A% of the set of all pairs %(U,V)% of subsets of %\\null [n]%, can you find a triple %(U,V)%, %(U,V\\cup D)% and %(U\\cup D,V)% in %A% such that the three sets %U%, %V% and %D% are all disjoint?"},{"username":"gowers","timestamp":"2009-02-02T17:45:00.000Z","contents":"I want to continue the discussion in 13, 14, 16, 17 and 18 concerning the appropriate Varnavides-type theorem for density Hales-Jewett. The basic difficulty appears to be that if you know that a point lies in a combinatorial line, then it affects the shape of the point. Let’s see this first in a very simple context, where we just put the uniform distribution on the set of all points and the uniform distribution on the set of all lines. We can represent a combinatorial line as a point in the space %\\{1,2,3,*\\}^n%, where the asterisks represent the varying coordinates. In this way we see easily that there are %4^n% combinatorial lines and that the set of fixed coordinates that take a given one of the values 1, 2 or 3 will typically have size about %n/4%. (For anyone who isn’t familiar with this kind of argument, if you choose a random combinatorial line, regarded as a sequence that consists of 1s, 2s, 3s and asterisks, then the set of places where you choose 1 is a random subset of %\\null [n]% where each point is chosen with probability %1/4%. With very high probability such a set has size close to %n/4%.) So we see that while a typical point has roughly equal numbers of 1s, 2s and 3s, a typical combinatorial line contains points that have around %n/4% of two values and around %n/2% of the third value. Suppose we try to remedy this by putting a different distribution on to the set of lines. Is there some natural way to do this that encourages the set of variable coordinates to be much smaller than the sets of fixed coordinates? A rather unpleasant approach would be simply to put some upper bound on the size of the set of variable coordinates and otherwise choose uniformly. The most natural thing I can think of to do is not all that much nicer, but at least it smooths the cutoff. It’s to choose the variable set of coordinates by picking each point independently with probability %p%, where %p% is something like %n^{-1/2}%, and then choose the fixed coordinates randomly to be 1, 2 or 3\\. Actually, I can already see that this doesn’t work. The size of the resulting set of variable coordinates will be strongly concentrated around %n^{1/2}%. But it’s not hard to choose a dense set of sequences such that if you choose any two of them, %x% and %y%, then the number of 1s in %x% and the number of 1s in %y% do not differ by approximately %n^{1/2}%. The number of combinatorial lines in such a set would not be dense. The fact that the natural measure on the set of sequences does not project properly to the natural measure on the set of triples %(r,s,t)% of integers such that %r+s+t=n% seems to be a fundamental difficulty. Another thought one might have is to put a different measure on the set of points, so as to encourage points where we get two values roughly %n/4% times and one value roughly %n/2% times. But the trouble with this is that it leads to the no-degenerate-triangles problem. To be precise, it would lead to three sets of sequences (according to whether 1, 2 or 3 was the preferred value) and no sequence would belong to all three sets. So it really does seem essential to force the variable set to be small somehow. And all I have to suggest so far is to do something very unnatural like first randomly choosing the cardinality according to some distribution that isn’t too concentrated about any value — the most natural one I can think of is a gentle exponential decay — then choosing a set of that cardinality, and then choosing the fixed coordinates randomly. But I have no reason to suppose that that leads to a Varnavides-type theorem."},{"username":"gowers","timestamp":"2009-02-02T18:44:00.000Z","contents":"I see now that there’s quite a substantial overlap between my last comment and Terry’s comment 16\\. But one can view my last comment as an elaboration of his, perhaps. Here I just want to make the quick suggestion that one way to work out what the appropriate Varnavides theorem should say is to prove it first. How would this strategy work? Well, we assume the density Hales-Jewett theorem in the following form: for every %\\delta>0% there exists %k% such that every subset of %\\null [3]^k% of size at least %(\\delta/2)3^k% contains a combinatorial line. (The %\\delta/2% is convenient in just a moment.) Now let %A% be a subset of %\\null [3]^n% of size at least %\\delta 3^n%. Here, %n% is an integer that is much bigger than %k%. The set %\\null [3]^n% can be covered uniformly by structures that are isomorphic to %\\null [3]^k%, and a positive proportion (in fact, %\\delta/2% at least) of these structures intersect %A% in a subset of density at least %\\delta/2%. So a positive proportion of these structures contain a combinatorial line. And now a double-counting argument gives us lots of combinatorial lines. Now there are several choices to make if one wants to pursue this line of attack. The main one is what set of %k%-dimensional substructures of %\\null [3]^n% to average over. For the benefit of people who haven’t thought too hard about Hales-Jewett, let me give a pretty general class of such structures. You choose %k% disjoint sets %V_1,V_2,\\dots,V_k%, and you choose fixed values in %\\{1,2,3\\}% for all elements of %\\null [n]% that do not belong to one of the %V_i%. The %k%-dimensional structure consists of the %3^k% sequences that take the fixed values outside the %V_i% and are constant on each %V_i%. What can we deduce from the fact that there exists %k% that depends on %\\delta% only such that if %A% intersects any %k%-dimensional set of this kind in at least %(\\delta/2)3^k% places then that %k%-dimensional set contains a combinatorial line in %A%? The choice one has is this: over what set of structures of this kind (or with what weighting on the structures) do we do an averaging argument? And what can we expect to get out of it at the end? Is it likely to give anything useful? One other question: is there a reasonably precise argument to the effect that if the triangle-removal approach worked, then it would imply a stronger Varnavides-type result? And if so, what would that result be? This could be another way to get a handle on the problem."},{"username":"gowers","timestamp":"2009-02-02T20:18:00.000Z","contents":"Another small thought in response to Terry’s comment number 18, paragraph 2, is this. Even if that idea runs into the difficulty I mentioned in comment 19, there might be a chance of getting something out of a small variant of the idea. Terry suggested using the corners theorem to find a triple %\\Gamma_{a+r,b,c}%, %\\Gamma_{a,b+r,c}%, %\\Gamma_{a,b,r+c}% in each part of which %A% is “rich”. The problem was that if one then used just these sets to form a tripartite graph, one would end up with no degenerate triangles. But what if instead one used the full 2-dimensional Szemer\\’edi theorem (which can be proved using the simplex-removal lemma in sufficiently high dimension, so it doesn’t depend on ergodic theory) to obtain a big network of sets %\\Gamma_{a,b,c}% in which %A% was rich? They could be organized into a triangular lattice: they would consist of all sets of the form %\\Gamma_{a+ir,b+jr,c+kr}% such that %i%, %j% and %k% are non-negative integers that sum to %m% for some reasonably large %m%. It’s not obvious that this would be any help, but it’s also not obvious that it wouldn’t be any help."},{"username":"gowers","timestamp":"2009-02-02T20:33:00.000Z","contents":"I was rather pleased with the “conjecture” in the final paragraph of comment 22, but have just noticed that it is completely false, at least if you interpret it in the most obvious way. Indeed, if you take both %\\mathcal{A}% and %\\mathcal{B}% to consist of _all_ subsets of %\\null [n]%, then you find that almost every set in %\\mathcal{A}% intersects almost every set in %\\mathcal{B}%. And it’s clear that if you just remove a small fraction of the sets from %\\mathcal{A}% and %\\mathcal{B}% you aren’t going to be able to ensure that _every_ set left in %\\mathcal{A}% intersects _every_ set left in %\\mathcal{B}%. (Proof: for each pair %(U,U^c)% you will either have to remove %U% from %\\mathcal{A}% or %U^c% from %\\mathcal{B}%.) This leaves me with two questions. (i) Can this pair-removal approach to Sperner be rescued somehow? (ii) If not, can one use its failure as the basis of a rigorous proof that no triangle-removal approach can work for density Hales-Jewett?"},{"username":"gowers","timestamp":"2009-02-02T20:47:00.000Z","contents":"Perhaps the answer to (i) is rather simple. First, we see how many disjoint pairs there are in the case where both %\\mathcal{A}% and %\\mathcal{B}% are the full power set of %\\null [n]%. The answer is %3^n%, because for each element of %\\null [n]% you get to choose whether it belongs to the first set, the second set, or neither. So now we would deem a pair of set systems %(\\mathcal{A},\\mathcal{B})% to have “few” disjoint pairs if the number of pairs %(U,V)% with %U\\in\\mathcal{A}%, %V\\in\\mathcal{B}% and %U\\cap V=\\emptyset% is at most %c3^n% for a tiny positive constant %c%. Under that assumption, does it follow that we can remove a small number of sets and end up with no disjoint pairs? And if so, would that prove Sperner’s theorem (in the weak form where we assume that a set system has positive density)?"},{"username":"gil-kalai","timestamp":"2009-02-02T21:16:00.000Z","contents":"Several remarks which are not directly towrds the specific project but related. 1\\. With all due respect to Sperner’s theorem it is hard to believe that the %3^n/\\sqrt{n}% lower bound cannot be improved. Right? Maybe as a side project we should come up with a %3^n/o(\\sqrt {n})% example for k=3 density HJ or ask around if this is known? 2\\. A sort of generic attack one can try with Sperner is to look at %f=1_A% and express using the Fourier expansion of %f% the expression %\\int f(x)f(y)1_{x where x<y is the partial order (=containment) for 0-1 vectors. Then one may hope that if f does not have a large Fourier coefficient then the expression above is similar to what we get when A is random and otherwise we can raise the density for subspaces. (OK, you can try it directly for the k=3 density HJ problem too but Sperner would be easier;)  \nThis is not unrealeted to the regularity philosophy. 3\\. Reductions: the works of Furstenberg and Katznelson went gradually from multidimensional Szemeredi to density theorems for affine subspaces to density HJ. (Maybe there were more stops on the way)  \nBeside the obvious reduction – combinatorial lines imply long AP (and also high dimension Szemeredi if I remember correctly) reductions in the opposite directions are not known (to me, at least). It is tempting to think that the when we find high dmensionall affine space perhaps in a much much much larger alphabet we can somehow get a little combinatorial line for the small alphabet. 4\\. There is an analogous for Sperner but with high dimensional combinatorial spaces instead of \"lines\" but I do not remember the details (Kleitman(?) Katona(?) those are ususal suspects.)"},{"username":"ryan-odonnell","timestamp":"2009-02-02T21:35:00.000Z","contents":"As a person who writes the phrase “choose the sequence x at random; then form y by rerandomizing each coordinate of x with probability p” in nearly half his papers, I’m naturally drawn to the 16-17-24-etc. comment thread. In #24, Tim suggests (and I agree) that the most natural way to pick a line is to form three sequences (x,y,z) by choosing (x_i,y_i,z_i) uniformly from [3]^3, with probability 1-p,  \n(x_i,y_i,z_i) = (1,2,3) with probability p, independently for each i = 1…n, with p taken to be perhaps 1/sqrt(n). But Tim also makes the good — and worrisome — point that this distribution has the “wrong” marginal on x (and on y and on z). For example, the marginal on x is the (1/3+2p/3, 1/3-p/3, 1/3-p/3) product distribution on [3]^n, whereas it “ought to be” the uniform distribution, because we are measuring the density of A with respect to the uniform distribution. It’s not completely clear to me that this is an insuperable problem though. For example, in #24 Tim wrote,  \n“But it’s not hard to choose a dense set of sequences such that if you choose any two of them, x and y, then the number of 1s in x and the number of 1s in y do not differ by approximately n^{1/2}.” I don’t quite see it… Isn’t it the case, as Terry pointed out in #17, that almost all strings in A have 1/3 +/- O(sqrt(n)) 1’s? In fact, shouldn’t a random pair x, y in A have a 1-count differing by Theta(sqrt(n)) with high probability? [All hidden constants here depend on delta.] Anyway, I agree it’s worrisome. To check whether it is indeed insuperable, I wonder if we could try “practising” this idea in the weak-Sperner’s setting Tim suggested at the end of #21\\. The analogous question there would be: Suppose A contains at least a delta-fraction of the strings in {0,1}^n (w.r.t. the uniform distribution). Suppose we choose to strings (x,y) as follows: (x_i,y_i) is uniform from {0,1}^2 with probability 1 – 1/sqrt(n)  \n(x_i,y_i) is (0,1) with probability 1/sqrt(n), independently for each i = 1…n. Is there a positive probability that both x and y are in A?"},{"username":"gowers","timestamp":"2009-02-02T21:45:00.000Z","contents":"Gil, a quick remark about Fourier expansions and the %k=3% case. I want to explain why I got stuck several years ago when I was trying to develop some kind of Fourier approach. Maybe with your deep knowledge of this kind of thing you can get me unstuck again. The problem was that the natural Fourier basis in %\\null [3]^n% was the basis you get by thinking of %\\null [3]^n% as the group %\\mathbb{Z}_3^n%. And if that’s what you do, then there appear to be examples that do not behave quasirandomly, but which do not have large Fourier coefficients either. For example, suppose that %n% is a multiple of 7, and you look at the set %A% of all sequences where the numbers of 1s, 2s and 3s are all multiples of 7\\. If two such sequences lie in a combinatorial line, then the set of variable coordinates for that line must have cardinality that’s a multiple of 7, from which it follows that the third point automatically lies in the line. So this set %A% has too many combinatorial lines. But I’m fairly sure — perhaps you can confirm this — that %A% has no large Fourier coefficient. You can use this idea to produce lots more examples. Obviously you can replace 7 by some other small number. But you can also pick some arbitrary subset %W% of %\\null[n]% and just ask that the numbers of 0s, 1s and 2s inside %W% are multiples of 7. If these observations are correct, then they suggest another very interesting subproblem: describe all obstructions to randomness. That is, find a nice explicit set %\\mathcal{F}% of functions defined on %\\null [3]^n% such that (the characteristic function of) any set with the wrong number of combinatorial lines correlates with a function in %\\mathcal{F}%. Ideally, correlation with a function in %\\mathcal{F}% would imply a density increase on some subset. Of course, this would be aiming for a Roth-type proof rather than a Ruzsa-Szemerédi-Solymosi-type proof."},{"username":"ryan-odonnell","timestamp":"2009-02-02T21:45:00.000Z","contents":"[Calling Gil’s post 29 and my earlier one 30.] Sorry for the lengthiness of the posts, but I wanted to throw out one more idea and one more question. Suppose the issue discussed in my #30 is indeed a problem. Here’s a possible (perhaps far-fetched) way around it: First, find the simultaneously “rich” triples (a+r,b,c), (a,b+r,c), (a,b,c+r) as in Terry’s post #18\\. Now take a new distribution on combinatorial lines: Uniform on lines which have exactly r wildcards. This may improve things because its marginals on x, y, and z are precisely the uniform distributions on Gamma_{a+r,b,c}, Gamma_{a,b+r,c}, and Gamma_{a,b,c+r}. And we know A is dense under these distributions, by the definition of richness. Working “slicewise” in this way has some intuitive appeal to me because it seems compatible with the prospect of getting the “correct” Sperner by these ideas, as opposed to just the “weak” Sperner."},{"username":"gowers","timestamp":"2009-02-02T21:52:00.000Z","contents":"In response to Ryan’s question in 30, I see that I didn’t explain myself clearly enough. Here’s what I meant. Suppose you have a distribution on combinatorial lines with the property that the size of the variable set has a very high probability of being very close to %\\sqrt n%. (NB, I don’t just mean order %\\sqrt n% but between %(1-\\epsilon)\\sqrt n% and %(1+\\epsilon)\\sqrt n%. This is I think the point of misunderstanding.) What I was claiming was that it was quite easy to choose a dense set of sequences such that if %x% and %y% are two of your sequences then the number of 1s in %x% and the number of 1s in %y% never differ by almost exactly %\\sqrt n%."},{"username":"ryan-odonnell","timestamp":"2009-02-02T21:55:00.000Z","contents":"Re Tim’s #33: Ah, I see. Right. So perhaps the thing to do is not to choose the number of wildcards to be Binomial(n,1/sqrt(n)), but, uh, I dunno, perhaps Poisson(sqrt(n))??"},{"username":"jason-dyer","timestamp":"2009-02-02T21:56:00.000Z","contents":"We’ve already got a number of threads developing. Here is my best effort at sorting them out. Discussion of original Furstenburg-Katzelson proof (#1, #5, #6, #10)  \n%IP_d% set problem (#2, #7, #23)  \nFinding lower and upper bounds (#3, #8, #11, #12, #15, #16)  \nSperner’s theorem (#4, #21, #22, #27, #28, Gil #29)  \nCreating a quasirandom definition (#18, #19, #26)  \nPositive density (#4, #9, #17)  \nAnalogue of Varnavides theorem (#13, #14, #16, #17, #18, #24, #25) The latter two threads might be combinable. _[I haven’t numbered this comment as it’s a metacontribution rather than a contribution — but thanks for it.]_"},{"username":"ryan-odonnell","timestamp":"2009-02-02T21:58:00.000Z","contents":"Now the question, following up on the separate thread initiated by Jozsef (specifically, #2 and Tim’s #7), just to confirm I have the question right… There is a dense subset A of {0,1}^n x {0,1}^n. Is it true that it must contain three nonidentical strings (x,x’), (y,y’), (z,z’) such that for each i = 1…n, the 6 bits [ x_i x’_i ]  \n[ y_i y’_i ]  \n[ z_i z’_i ] are equal to one of the following: [ 0 0 ] [ 0 0 ] [ 0, 1 ] [ 1 0 ] [ 1 1 ] [ 1 1 ]  \n[ 0 0 ], [ 0 1 ], [ 0, 1 ], [ 1 0 ], [ 1 0 ], [ 1 1 ],  \n[ 0 0 ] [ 1 0 ] [ 0, 1 ] [ 1 0 ] [ 0 1 ] [ 1 1 ] ?"},{"username":"gowers","timestamp":"2009-02-02T21:59:00.000Z","contents":"(Responding to 34) Unfortunately, Poisson(root n) is just as concentrated around root n as Binomial (n,1/sqrt n) (as one would expect since binomial tends to Poisson). That’s what slightly bothers me — the most natural distributions have that awkward property."},{"username":"ryan-odonnell","timestamp":"2009-02-02T22:11:00.000Z","contents":"(Re 36.) Hmm. But there has to be *some* correct distribution for the wildcards. After all, what *is* the distribution on the difference in 1-counts between independent and uniform x and y? It’s distributed as the sum of n i.i.d. r.v.’s which are +1 with probability 2/9, -1 with probability 2/9, and 0 with probability 5/9\\. So it’s like a Gaussian with standard deviation sigma = 2/3sqrt(n). So we could put in a number of wildcards equal to round(|N(0,4/9n)|)… This looks weird enough that it’s probably a wrong idea, but I still feel there may be a “natural” probabilistic way to do it. Or maybe I revert to the suggestion of thinking about distributions on fixed numbers of 1’s, 2’s, 3’s as in #32… <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span>"},{"username":"gowers","timestamp":"2009-02-02T22:29:00.000Z","contents":"Here’s an attempt to throw the spanner in the works of #32\\. Let’s define %A% to be the set of all sequences %x% with the following property: if the number of 1s in %x% is even then %x_n=2%, and if the number of 1s in %x% is odd then %x_n=3%. I first claim that virtually all slices are rich. Indeed, take a typical slice defined by the triple %r+s+t=n.% Then just choose %r% 1s, taking care not to choose %x_n%, choose the appropriate value of %x_n% according to whether %r% is even or odd, and then fill in the rest with the appropriate number of 2s and 3s. The only thing that can go wrong is that one of %s% and %t% is equal to 0, but for all normal slices we’re OK. Now I claim that we can’t have a combinatorial line with an odd number of wildcards. The reason is that when the wildcards change from 1 to 2, the final coordinate %x_n% is forced to change from 2 to 3, which is not allowed. So it looks to me as though it would be disastrous to take the uniform distribution over lines with some fixed number of wildcards (unless, perhaps, one had done some more preprocessing to get a stronger property than mere richness)."},{"username":"jason-dyer","timestamp":"2009-02-02T22:30:00.000Z","contents":"(Another metacomment.) Tim, now may be the right time to create multiple posts so people can sort into the discussion they want to focus on. Given the rate posts are piling up it will get too hard to do it later. In fact, by the time I hit “submit” I’m guessing 3 more comments will have come up. <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> #30, #32, #33, #34, #36, #37 spawned off Varnavides theorem thread, although tilting over to general randomness  \n#31 (off of #29) Sperner, sort of — Fourier approach might spawn its own thread  \n#35 IP_d problem thread _[Jason—I’m going to answer this question over on the Questions of Procedure post. Or rather, I’m going to ask what others think. I agree that it would, for instance, make it easier for a newcomer if they had a more organized structure of comments.]_"},{"username":"terence-tao","timestamp":"2009-02-02T22:56:00.000Z","contents":"[Incidentally, in other comment threads, it is often customary to refer to a specific comment by a timestamp, e.g. “Gowers Feb 2 3:18 pm”. But I will adhere to the numbering convention also…] 39\\. Wow, this thing is evolving rapidly… I will need some time to catch up, but I wanted to contribute two thoughts I had this morning… with regard to Gil’s 29 (Feb 2 4:16 pm), observe that the union of all %\\Gamma_{a,b,c}% where b+2c lies in a set that has no length 3 progressions, will have no combinatorial lines. So by plugging in the Behrend example it now seems that we have the lower bound %c_n \\geq 3^{n - O( \\sqrt{\\log n} )}% (if I haven’t made a miscomputation somewhere). Secondly, if one applies the corners argument locally instead of globally, one can make the r in the rich triple %(a+r,b,c), (a,b+r,c), (a,b,c+r)% to be of bounded size, r=O(1). I don’t know if this is helpful though. But it begins to remind me of the isoperimetric inequality for the cube (which would correspond to the case k=2, r=1). _[Morning Terry! Yes, that time stamp idea would be good, and would save me having to do a lot of editing to number comments. (Even if everyone remembers to number their comments, they don’t always know what number their comment will actually turn out to be.)]_"},{"username":"terence-tao","timestamp":"2009-02-02T23:28:00.000Z","contents":"Two more thoughts. Firstly, I can now show that %c_3 = 18%, by taking the set %\\{ (x,y,z) \\in {\\Bbb Z}_3^3: x+y+z \\neq 0 \\hbox{ mod } 3 \\}%, or equivalently the union of all the %\\Gamma_{a,b,c}% such that %a+2b% is not divisible by 3\\. (The upper bound %c_3 \\leq 18% follows from the general inequality %c_{n+1} \\leq 3c_n%.) So maybe %c_4% is computable also, for what it’s worth. Secondly, one may need some sort of “hereditary” richness, in which one considers not only the global counts a, b, c of 1s, 2s, and 3s in [n], but also considers all subsets of [n] and looks for richness there too. If there is one subset of [n] in which the richness fluctuates much more than in the original index set [n], then we pass to that set (incrementing a certain “richness energy” along the way) and keep going until one obtains “richness regularity” (by a standard “energy increment argument”), which should mean something like that the “richness statistics” of any “non-negligible” subindex set of the index set should be close to the statistics of the original index set. This may help avoid the kind of counterexamples that appear for instance in Gowers 20 (9.17am), or Gowers 38 (5.29 pm). It also reminds me of the stationarity reduction used in the ergodic theory world (Terry 4 9.26pm, Tim Austin 16 10:16pm)."},{"username":"terence-tao","timestamp":"2009-02-02T23:32:00.000Z","contents":"Incidentally, given that we are trying to find triangles in sparse (but now constant degree) graphs, we may eventually have to use the type of regularity lemmas associated with bounded degree graphs, as for instance studied by Elek and Lippner, [http://arxiv.org/abs/0809.2879](http://arxiv.org/abs/0809.2879)"},{"username":"jason-dyer","timestamp":"2009-02-03T00:49:00.000Z","contents":"I need to break out the compiler, but for what it’s worth, here’s my brute force data structure I’m going to use to try to get (at the least lower bound, and if they match upper bound exact) values of %c_n%: One set of nodes (call them “red”) correspond to the elements of %{}[3]^n%  \nAnother set of nodes (call them “blue”) correspond to the specific combinatorial lines. They each have degree 3 and are linked to by the blue nodes. Greedy algorithm:  \n1.) Look at the red nodes; out of the set of red nodes with highest degree, pick one at random.  \n2.) Remove that node, and all blue nodes linking to that node, and all edges corresponding to the red node and the blue nodes.  \n3.) If there are any blue nodes remaining, return to 1\\. Otherwise, all combinatorial lines have been removed."},{"username":"jason-dyer","timestamp":"2009-02-03T00:50:00.000Z","contents":"(42 fix) blue nodes are linked to by the red nodes."},{"username":"christian-elsholtz","timestamp":"2009-02-03T01:27:00.000Z","contents":"(43) Let me consider a related problem, which can be thought of as a  \ngeneralization:  \nwe consider sets of [3]^n where all lines, not just combinatorial lines,  \nare forbidden. This problem is also known as Moser’s cube problem.  \nIt is known that the extremal configurations have, 2,6,16,43 points in  \ndimensions n=1,2,3,4 respectively.  \nThe bounds in this problem were 3^n/sqrt(n) and o(3^n) respectively, as  \nwell. These general lines can be modeled by two different wild cards.  \nSay (1,x,3,y,x) is a line, if x runs from 1 to 3, and y runs from 3 to 1. Here is a sketch of an improvement of the lower bound, in the spirit of  \nTerry’s idea (#39). The sets Gamma_{a,b,c} do not contain any line, so taking unions of  \nseveral Gammas, one would need to study (with r wild cards x  \nand s wild cards y) Gammas with indices  \n(a+r,b,c+s), (a, b+r+s, c), (a+s, b, c+r).  \nThen take those Gamma_{a’,b’,c’}, where  \na’+2b’+3c’ is taken from a set without a 3 progression, and one gets  \na lower bound of 3^n/exp(c sqrt(log n)) Since the Hales-Jewett theorem is on combinatorial lines: some of the ideas discussed before might work for general lines,  \ni.e. Moser’s cube problem as well. Is there any general lower bound  \nconstruction that works for combinatorial lines only?  \n(Terry’s example with 18 ponts in dimension 3 has lines, so what is the right generalization?). What is genuinely different for combinatorial lines?"},{"username":"boris","timestamp":"2009-02-03T01:43:00.000Z","contents":"As far as I know describing obstructions to pseudorandomness is open even for Sperner’s theorem or Kruskal-Katona theorem (the latter corresponds to dealing with sets %\\Gamma_{a,b,c}%). Consider the following simple problem: Let %A% be a dense subset of %\\binom{[n]}{n/2}% and %B% be a dense subset of %\\binom{[n]}{n/2+1}%. When is the number of pairs %(a,b)\\in A\\times B% such that %a\\subset b% close to expected? Besides the obvious obstruction when all elements %a\\in A% contain %i\\in[n]%, but none of elements %b\\in B% do, there is also a more subtle obstruction. Let %s% be any subset of size %n/2% in %\\null [n]%. Choose any %4\\leq t=o(\\sqrt{n})% Let %a% belong to %A% iff %|a\\cap s|\\bmod t% is in the interval %\\null [0..t/2]% and %b% belong to %B% iff %|a\\cap s|\\bmod t% is in the interval %\\null [(t/2+2)...t-1]%. The sets %A% and %B% are of density close to %1/2%, but their union is an antichain. Of course, such examples can be made more complicated by taking several sets %s% for different moduli %t%. I suspect that there might a Freiman-type theorem buried here. There is even a weak connection to sumset addition: a shadow of a set %S% is just %S+{e_1,...,e_n}% where %e%‘s are the one-element sets, and the addition occurs in appropriate semigroup (it is easy to see that a few results from additive combinatorics do carry over to that setting)."},{"username":"terence-tao","timestamp":"2009-02-03T01:47:00.000Z","contents":"(45) Not a major concern at the moment, but if at some point we write up the precise bounds obtained from Behrend set constructions, we may as well use the best known Behrend set bound, due to Elkin, see e.g. this exposition of Green and Wolf: [http://arxiv.org/abs/0810.0732](http://arxiv.org/abs/0810.0732)"},{"username":"william-gasarch","timestamp":"2009-02-03T02:42:00.000Z","contents":"45.5\\. In the paper Some Lower Bounds for the Hales-Jewitt Numbers  \nby Eric Tressler he has the following: HJ(3,3)>6  \nHJ(4,3)>6"},{"username":"terence-tao","timestamp":"2009-02-03T02:45:00.000Z","contents":"(46) [Numbering of the previous two posts needs to be changed] @Jason (42): in view of existing examples, it may be worth restricting one’s search space to those sets that are unions of %\\Gamma_{a,b,c}%‘s, rather than arbitrary subsets of %{}[3]^n%. The problem is now some sort of weighted corner-avoidance problem, but one which may be more tractable to brute force computer search. @Boris: The problem you describe sounds very close to the machinery of influence, hypercontractivity, isoperimetric inequality, etc., but Gil is of course the one to ask regarding these things."},{"username":"jozsef","timestamp":"2009-02-03T02:52:00.000Z","contents":"Comment on the lower bound. (Gil 15, Terry 16, and Terry 39&44) Without loosing anything, we can suppose that the number of 0-s,1-s, and 2-s are %d\\pm \\sqrt{d}%. Then the range where a+2b (the sum of 0-s plust twice the sum of 1-s) is triangle free has length about %\\sqrt{d}.% If we care about the details, like we want to use Elkin’s improvement, then we should note this one as well. It changes the constant multiplier in the exponent."},{"username":"jozsef","timestamp":"2009-02-03T03:09:00.000Z","contents":"(47) contd. I think that we should get a better lower bound by using bounds on corners than on 3-term arithmetic progressions, however I’m not sure what is the best known lower bound on corners."},{"username":"tyler-neylon","timestamp":"2009-02-03T03:39:00.000Z","contents":"(48) @Jason (42): The structure you point out – a bipartite graph with one set of vertices representing combinatorial lines $$L$$, and the other representing elements of $${[3]}^n$$ – can be used to algorithmically find $$c_n$$ exactly (it sounds like you’re going for a probabilistic approach), although the code is more involved, and a bit slow (but polynomial in the size of the graph). I think we can reduce this to a min-cost flow problem. Basically, we want the smallest set of edges which covers $$L$$ — this corresponds to the smallest subset of $${[3]}^n$$ which can be removed to get a line-free set. Create the following directed graph: add vertex $$s$$ with edges to everything in $$L$$. Every line in $$L$$ has an edge to all three points it contains. Every point in $${[3]}^n$$ points to a new vertex $$t$$. Every edge has unit cost, and unit capacity, except the edges between $$L$$ and $${[3]}^n$$, which have unlimited capacity. Then a min-cost flow from $$s$$ to $$t$$ with at least $$|L|$$ capacity is what we want. This can be found in time $$O(n^2m^3\\log n)$$ where $$n$$ is number of nodes, $$m$$ is number of edges [see Schrijver’s Combinatorial Optimization, ch 12]. Not super fast, but maybe do-able for small values of our $$n$$. Also possibly of theoretical interest? Side note: In case my latex notation doesn’t turn out correctly, could someone mention the proper syntax for these comments? And in case it does look right (for others), I’m just surrounding the math parts in double dollar signs."},{"username":"tyler-neylon","timestamp":"2009-02-03T03:40:00.000Z","contents":"oops, that cool-looking face is supposed to be a forty-eight ( 48 )"},{"username":"jozsef","timestamp":"2009-02-03T03:46:00.000Z","contents":"(48) To Christian (43) If r=s=1 then you get 3,3,3 as an arithmetic progression. I don’t see a simple way to resolve the problem of degenerate arithmetic progressions in the general setting."},{"username":"jozsef","timestamp":"2009-02-03T03:48:00.000Z","contents":"oops twice! (as ( 48 ) was used twice)"},{"username":"terence-tao","timestamp":"2009-02-03T04:14:00.000Z","contents":"(50) [I don’t envy Tim’s job to clean up the numbering system… but one can of course use fractions or something…] @ Joszef (47), the best lower bound for corners is necessarily between that for 3-term progressions and for triangle removal, since triangle removal implies corners implies 3-term progressions. But ever since the original Ruzsa-Szemeredi paper, the best lower bounds for triangle removal come from… the Behrend construction for 3-term progressions. This is not a satisfactory state of affairs, but I don’t think there has been much progress in this regard. (Although I could imagine that the Elkin refinement could be pushed a little further for corners instead of 3-term progressions.) [Of course, Gowers has tower-type lower bounds for the regularity lemma used to prove triangle removal, but this does not directly seem to lead to any new lower bounds on the latter.]"},{"username":"terence-tao","timestamp":"2009-02-03T04:17:00.000Z","contents":"(51) Incidentally, to make latex comments, use “%_[Your LaTeX code]_%“. There are some quirks to the code, see the bottom of my post [http://terrytao.wordpress.com/about/](http://terrytao.wordpress.com/about/) for details."},{"username":"terence-tao","timestamp":"2009-02-03T04:19:00.000Z","contents":"Well, that failed horribly… I meant “_$_latex _[Your LaTeX code]_$” (where I artificially italicised the first dollar sign)."},{"username":"jason-dyer","timestamp":"2009-02-03T06:08:00.000Z","contents":"(52) Something that might help me out — could someone produce a counterexample where the greedy algorithm I outlined above in (42) *doesn’t* work?"},{"username":"ryan-odonnell","timestamp":"2009-02-03T06:15:00.000Z","contents":"( 53 ). Hi Boris! My student Karl Wimmer and I have some recent results about obstructions to pseudorandomness for Kruskal-Katona. Karl’s still writing them up for his thesis but he’s currently sidetracked with job interviews etc. The results say what you might guess: Roughly, if A is a subset of {[n] choose n/2} with constant density mu, then the density of the upper shadow of A is at least mu + const. log(n)/n… unless A is very noticeably correlated with a single coordinate. As Terry guesses, it’s “influences”, “hypercontractivity”, “Gil stuff” etc. Karl was planning to finish writing it and its applications in Learning Theory within two months. But if this polymath project veers too close I guess we’ll have to try to “rush to print” <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> I currently don’t see connections to the analogous Sperner problem; however I’ve only thought about it for like 5 minutes…"},{"username":"terence-tao","timestamp":"2009-02-03T06:33:00.000Z","contents":"(54) Tim Austin had a small comment which may be a clue of some sort: the ergodic theory proof of DHJ is heavily reliant on the ordering of the index set [n], even though the conclusion of DHJ is of course invariant under permutations of the index set. (For instance, it is convenient to divide [n] into [n-r] and [r] for some moderately large r, and re-interpret subsets of %{}[3]^n% instead as a family of subsets of %{}[3]^r%, indexed by elements of %{}[3]^{n-r}%.) The standard proof of the colouring HJ theorem also has this sort of feature, in that one must arbitrarily order [n], or at least partition it into components, in some fashion. So one may have to “break symmetry” here and not rely on a purely graph-theoretic argument."},{"username":"gowers","timestamp":"2009-02-03T06:52:00.000Z","contents":"I’ve been busy this evening and am about to go to bed, so only time for a quick remark at this stage, which again may be implicit in previous remarks. Let me call a set of the form %\\{(a+x,b+y,c+z):x,y,z\\geq 0, x+y+z=m\\}% a _triangular grid_ of sidelength %m%. The remark is that by means of an averaging argument, there is no problem in finding a triangular grid of sidelength %n^{1/3}%, say, such that for every element of the grid all of %(a+x,b+y,c+z)% is close to %(n/3,n/3,n/3)%, and such that a positive proportion of the sets %\\Gamma_{a+x,b+y,c+z}% are rich in elements of %A%. If we restrict to such a set, then we can hope for a Varnavides-type theorem because now a typical combinatorial line will have the same typical size of variable set as the sidelength of a typical equilateral triangle in the grid. In other words, in this set we no longer have the problem that the natural weights differ from the natural weights for corners. I’ll try to make this point more precisely tomorrow."},{"username":"terence-tao","timestamp":"2009-02-03T08:04:00.000Z","contents":"I thought it might also be worth reporting here on some things I tried that _don’t_ seem to work, though perhaps I am missing something. Firstly, I tried improving the lower bound we now have on %c_n% by not taking the global statistics a, b, c of 1s, 2s, and 3s (and their resulting sets %\\Gamma_{a,b,c}%, but instead partitioning [n] into subsets, using the local statistics for each subsets to create local versions of the %\\Gamma%‘s, and then somehow gluing it all back together again. This failed to improve upon what we already have, and I think the reason is while the global %\\Gamma_{a,b,c}% have no combinatorial lines, the same is not true of their local analogues (unless one takes a Cartesian product of several local %\\Gamma%‘s, but this seems to be rather lossy.) Indeed, I still don’t know of any good example of a really large set with no combinatorial lines that isn’t somehow built out of the %\\Gamma%‘s. The other thing I toyed with was trying to use the machinery of shifting and compression (in the spirit of Frankl, or Bollobas-Leader, or one of my papers with Ben Green) to try to get good answers to the k=2 problems (e.g. finding a good “robust” version of Sperner’s theorem, e.g. a Varnavides version). But this seems not to work too well. The problem seems to be that the extremisers for Sperner’s theorem don’t look much like downsets (they seem to be more like the boundary of a downset, though I don’t see how to exploit this). In any event, these techniques are quite specific to k=2 and would be unlikely to shed much light on k=3 except perhaps by analogy."},{"username":"ryan-odonnell","timestamp":"2009-02-03T10:35:00.000Z","contents":"( 57 ). On the topic of more analytic proofs of Sperner: I think I now see why Boris mentioned Kruskal-Katona in the same breath as Sperner. Let me try to sketch the connection I see. I’m sure it’s well known. Let %n% be odd for simplicity and write %B_j% for the set of strings in %\\{0,1\\}^n% with exactly %j% %1%‘s. Let %k = \\lfloor n/2 \\rfloor%. Let %A_j% denote %A \\cap B_j% and let %\\mu_j% denote %A_j%‘s density within %B_j%. One particular thing Sperner tells us is that if %\\mu_k + \\mu_{k+1} > 1% then %A% contains a “combinatorial line” (because the overall density of %A% is at least that of %B_{k}%‘s density within %\\{0,1\\}^n%). I.e., there is %x \\in A_k, y \\in A_{k+1}% such that %x \\prec y%. Here is a way to see that with Kruskal-Katona: K-K implies that the upper-shadow %\\partial^+ A_k% of %A_k% has density at least %\\mu_k% within %B_{k+1}%. Now together %\\partial^+ A_k% and %A_{k+1}% have density exceeding %1% within %B_{k+1}%, hence they overlap. — Indeed, I think you can get most of Sperner out of extending this argument. Let me be uncareful. Suppose that %A%‘s overall density exceeds that of %B_{k+1}% within %\\{0,1\\}^n%. As an example, perhaps %A% has %40\\%% of the strings in %B_{k}%, %40\\%% of the strings in %B_{k+5}% and %40\\%% of the strings in %B_{k+7}%. Now K-K implies that %A_{k}%‘s upper shadow is also at least %40\\%% of %B_{k+1}%. And the upper shaddow of that is at least %40\\%% (basically) of %B_{k+2}%. Etc. When you get up to %B_{k+5}%, you either get a collision and are done, or you have at least %80\\%% of %B_{k+5}%. Then you get at least %80\\%% of %B_{k+6}%, and then you have %120\\%% of %B_{k+7}% and you get a collision and are done. — The reason I kind of like this proof is that there is an “analytic” proof of Kruskal-Katona that seems amenable to “robustification”. Although to be honest, I can’t quite remember how we got talking about this subject just now…"},{"username":"boris","timestamp":"2009-02-03T11:14:00.000Z","contents":"(58) The reason I mentioned Kruskal-Katona is that Sperner’s original proof of Sperner’s theorem involved pushing shadows towards the center level like Ryan indicated. He used simple double-counting argument, of which Kruskal-Katona is the modern descendant. The point of the example I gave is to show that even if the shadow grows by only 1+o(1), it does not imply correlation with a single coordinate or even a small set of them. The version of Kruskal-Katona qualitatively similar to what Karl Wimmer and Ryan proved was proved recently by Peter Keevash [http://arxiv.org/abs/0806.2023](http://arxiv.org/abs/0806.2023)"},{"username":"sune-kristian-jakobsen","timestamp":"2009-02-03T16:48:00.000Z","contents":"59.  \nIt is possible to show that %c_4\\geq 49%. Look at the set %\\{(x,y,z,t)\\in\\mathbb{Z}_3^4: x+y+z+t\\neq 0 \\hbox{mod} 3\\}\\backslash\\{(1,1,1,1),(1,1,1,2),(1,1,2,1),(1,2,1,1),(2,1,1,1)\\}%"},{"username":"gowers","timestamp":"2009-02-03T16:50:00.000Z","contents":"Thread title: Jozsef’s alternative problem. I’m going to start by adding a few small thoughts on things I was writing about yesterday, and after that I’ll read more carefully the recent batch of comments and see if I have any reactions to them. Yesterday (comment 23) I suggested the following variant of Jozsef’s alternative problem (comment 2). Suppose %A% is a dense subset of the set of all pairs %(U,V)% of subsets of %\\null[n]%. Can you find three disjoint sets %U%, %V% and %D% such that %(U,V)%, %(U,V\\cup D)% and %(U\\cup D,V)% all belong to %A%? This question can’t have grabbed people’s attention much, since the answer is a trivial no. Just let %A% be the set of all pairs %(U,V)% such that %U\\cap V\\ne\\emptyset%. That’s definitely almost all pairs. However, can we make a sensible question out of this? A sensible question would be some density-type condition on %A% that would guarantee a triple of the given kind. An obvious candidate would be to insist from the start that %A% is a set of _disjoint_ pairs of sets. Note that there are %3^n% of these, since each element can choose whether to go into %U%, %V% or neither. Just for fun, let’s represent each pair %(U,V)% of disjoint sets as a sequence of 1s, 2s and 3s. Then a configuration of the desired kind is … a combinatorial line. So this rather nice new question was in fact the question we started with. But at least this thought process clarifies the relationship between Jozsef’s problem and density Hales-Jewett."},{"username":"gowers","timestamp":"2009-02-03T18:34:00.000Z","contents":"Thread title: Varnavides. (This is the thread devoted to thinking about how to find a suitable statement that says that a dense set doesn’t just contain one combinatorial line but contains “a dense subset of all possible combinatorial lines”. The problem is that the most natural interpretation of this is false. See Jason Dyer’s comment just after 38 for numbers of other comments that belong to this thread.) I want to continue with what I was talking about in 55\\. At the moment I am in the happy state of not knowing why we are not already done. Of course, I don’t for a moment think that we _are_ done, but being in this position is always nice because you know that by the time you see properly why you aren’t done you are guaranteed to understand the problem better. The grounds for this unrealistic optimism are that we now seem to have an easy solution to the Varnavides problem. Just to recap, the problem is that one would expect any triangle-removal argument to lead not just to the conclusion that a dense set contains one combinatorial line, but to the conclusion that the number of combinatorial lines it contains is within a constant of the trivial maximum (which is what you get when your set has density 1). To recap further, this statement is false for density Hales-Jewett, because you can take as your set the set of all sequences %x% that take each value roughly %n/3% times.  \nStandard probabilistic arguments show that almost all sequences have this form, but the size of the set of variable coordinates in any combinatorial line in this set is very small, and this can be used to show that the set of combinatorial lines has cardinality far smaller than %4^n%. (This is the number of combinatorial lines, because each point chooses whether to be 1, 2, 3 or variable.) Let’s regard this as a Varnavides-summary comment. I’ll continue this line of thinking in my next comment."},{"username":"gowers","timestamp":"2009-02-03T18:47:00.000Z","contents":"Thread title: Varnavides. A problem we were grappling with yesterday was how to put a natural measure on the set of combinatorial lines in such a way that the variable sets were forced to be small. Here is a proposal for a solution to that problem. I’ve already hinted at it in 55, and I think Terry may have had it in the back of his mind at around 39 and 40\\. Briefly, the suggestion is to restrict to a _large triangular grid of slices_ . Let me elaborate on what I mean by this. First, a _slice_ is a set of the form %\\Gamma_{r,s,t}%, where that is the set of all %x% with %r% 1s, %s% 2s and %t% 3s. (There’s redundancy in the notation, since %r+s+t=n%, but the symmetry more than makes up for it.) Secondly, a _triangular grid_ is a subset %T% of the set %T_n=\\{(r,s,t):r,s,t\\geq 0, r+s+t=n\\}% of the form %\\{(a+r,b+s,c+t):r,s,t\\geq 0,r+s+t=m\\}% for some choice of %a,b,c,m% such that %a,b,c\\geq 0% and %a+b+c\\leq n-m%. Triangular grids are just translates of %T_m% that live inside %T_n%. (It would be possible also to consider dilated triangular grids where you look at points of the form %(a+rd,b+sd,c+td)% for some fixed %d%, but at the moment I have no need of these.) A _triangular grid of slices_ is then just a collection of slices indexed by a triangular grid. Again, I think it makes sense to stop this comment here and continue in a new comment."},{"username":"gowers","timestamp":"2009-02-03T19:03:00.000Z","contents":"Thread title: Varnavides. Now I make the following claim. Suppose we choose a random point in %\\null [3]^n% as follows. First we choose a random element of %T_{n-m}%, where the probability of choosing %(a,b,c)% is %3^{-(n-m)}\\binom{n-m}{r,s,t}%. (Equivalently, we choose a random element of %\\null [3]^{n-m}% and let %a,b,c% be the number of 1s, 2s and 3s.) Next, we choose a random element %(r,s,t)% of %T_m%, this time from the _uniform_ distribution. Finally, we choose a random element of the slice %\\Gamma_{a+r,b+s,c+t}%. The claim is that the resulting probability distribution on %\\null [3]^n% is very close to uniform. (By this I mean that any event in one distribution has a probability that’s within a very small additive constant of the probability according to the other distribution.) The reason for this is that when %r,s,t% are all smaller than %m% and %a,b,c% are all around %n/3% (as they almost always are), then the ratio of %\\binom {n-m}{a,b,c}% to %\\binom{n}{a+r,b+s,c+t}% is very close to 1. An averaging argument tells us that there must exist %(a,b,c)% close to %(n/3,n/3,n/3)% such that a positive proportion of points in the triangular grid of slices indexed by %(a,b,c)+T_m% belong to %A%. So instead of looking at density Hales-Jewett in all of %\\null[3]^m% we can try to prove that if %A% is a dense subset of a big triangular grid of slices, then %A% contains a combinatorial line. In my next comment I’ll try to say why I think this is a good thing to do."},{"username":"gowers","timestamp":"2009-02-03T19:25:00.000Z","contents":"Thread title: Varnavides. Note that if we restrict to a large triangular grid of slices in this way, then we have various convenient properties. For example, all the points in all the slices have about %n/3% each of 1s, 2s and 3s. And any combinatorial line has a variable set of size at most %m%. I’d like to claim now that a Varnavides-type theorem applies to large triangular grids: that is, if you have a dense subset of a large triangular grid of slices, then not only do you get a combinatorial line, but you even get within a constant of the total number of combinatorial lines in the entire grid. The idea here, by the way, is not (yet) to try to prove this theorem directly, but just to check that it is true by showing that it follows from density Hales-Jewett. Here’s how density Hales-Jewett can be applied. Let’s assume that %A% is a dense subset of the triangular grid of slices indexed by %(a,b,c)+T_m%, where %(a,b,c)% belongs to %T_{n-m}%. Suppose that %h% is a fixed constant chosen to be large enough that every subset of %\\null [3]^h% of size at least %(\\delta/2)3^h% contains a combinatorial line. Now for every %(r,s,t)\\in T_{m-h}%, which means almost every %(r,s,t)\\in T_m% since we’re assuming that %m% is much much larger than %h%, let us randomly choose %a+r% places to put a 1, %b+s% places to put a 2 and %c+t% places to put a 3\\. That leaves %h% places unfilled. The set of ways of filling those places is a copy of %\\null [3]^h%, so if it contains at least %(\\delta/2)3^h% points from %A%, then it contains a Hales-Jewett line. An averaging argument shows that that happens with … OK, I’m sorry to say that I do after all need to consider dilated triangular grids. Since that is the case, I’m going to abandon this attempt to be precise, for now at least, and simply say in the next comment what I think should happen. Then maybe others will tell me that they’re not convinced, or that they are convinced, and perhaps we can zero in on something."},{"username":"ryan-odonnell","timestamp":"2009-02-03T19:48:00.000Z","contents":"If there’s a “Sperner” thread I guess this would be it. Boris, thanks for the reference to the Keevash paper. While you’re handing out references :)… I guess it would be relevant to ask, Is there a Sperner’s Theorem known for different product distributions on %\\{0,1\\}^n%? In the case relevant to us: What is the densest antichain in %\\{0,1\\}^n% under the %(1/3)%-biased product distribution? This must be known…"},{"username":"gowers","timestamp":"2009-02-03T20:16:00.000Z","contents":"Thread title: Varnavides. All I was really trying to get at in 63 was this. Choose a random translate and dilate of the triangular grid %T_h% in such a way that it is a subset of %(a,b,c)+T_m%. If you make the dilates you look at all have width at most %\\eta m% for some smallish %\\eta%, then nearly all aligned equilateral triangles in %(a,b,c)+T_m% will be included in one of these copies of %T_h% about the same number of times. Now let %S_h% be a dilate and translate of %T_h%, where the constant of dilation is %d%. One can randomly choose a copy %X_h% of %\\null [3]^h% inside %\\null [3]^n% in such a way that the numbers of 1s, 2s and 3s is given by points in %X_h% is given by points in %S_h%. To do this, suppose that %S_h=(a,b,c)+dT_h%. Randomly fix %a% coordinates to be 1, %b% coordinates to be 2, %c% coordinates to be 3, and randomly partition the rest of %\\null[n]% into %h% sets %D_1,\\dots,D_h%, each of size %d%. Then our random copy %X_h% consists of all sequences that are constant on each %D_i% and take the given fixed values outside the %D_i%. My claim is that a simple averaging argument should show that if a positive proportion of these %X_h% contain a Hales-Jewett line (as I claim they will if %A% has density %\\delta%) then the number of Hales-Jewett lines in %A% is within a constant of the trivial maximum. I’m going to leave this for now as I feel that to do it properly I’d have to go away and write something down, and I’m not allowing myself to do that."},{"username":"gowers","timestamp":"2009-02-03T20:26:00.000Z","contents":"Possible thread title: triangle removal. Actually, I was trying to prove a Varnavides-type result not because we need it but because it was a sort of reality check: if we can’t get such a result then it is unlikely that triangle removal will work. But since I’m now fairly confident that we can get such a result, I want to see what happens if one just goes ahead and tries to apply some kind of triangle-removal lemma in the case where %A% is a dense subset of a large triangular grid of slices. So let’s fix notation again. I’m going to suppose that %A% is a dense subset of the union of all slices %\\Gamma_{a+r,b+s,c+t}%, where %(a,b,c)% is some fixed element of %T_{n-m}% and %(r,s,t)% ranges over %T_m%. Also, I’m assuming that %a,b,c% are all around %n/3% so that all slices have about the same size. Now I want to apply the trick from comments N and P. So what is the graph now? Instead of taking %X% to be the full power set of %\\null[n]% I want to take just those sets that could conceivably occur as the set of 1s in a sequence in %A%. And that’s all sets of size %a+r%, where %0\\leq r\\leq m%. Similarly, %Y% consists of all sets of size between %b% and %b+m%, and %Z% consists of all sets of size between %c% and %c+m%. The edges are as before: I join %U\\in X% to %V\\in Y% if the sequence that’s 1 on %U% and 2 on %V% and 3 everywhere else belongs to %A%. And similarly for the other two bipartite graphs. And clearly a non-degenerate triangle in this graph gives rise to a combinatorial line. I’ll think about the implications of this in my next comment."},{"username":"gowers","timestamp":"2009-02-03T20:44:00.000Z","contents":"Thread title: triangle removal. I didn’t mention it explicitly, but of course a necessary condition for joining %U\\in X% to %V\\in Y% is still that %U% and %V% should be disjoint. So the first reason that we are not yet done is that the graphs are still very sparse, since two random sets of size %n/3% have a tiny probability of being disjoint. So the question is, has anything been gained by restricting attention to a large triangular grid of slices? Something’s been gained from the Varnavides point of view, I think, but has anything been gained from the triangle-removal point of view? That’s what I’m asking. At the moment it feels, rather disappointingly, as though _nothing_ has been gained. It’s still the case that the graph where you join two sets if they are disjoint is importantly non-quasirandom, since the remark made in comment LL carries over to this context too. I would hazard a guess that there are two morals to draw from this. (i) It may still be quite convenient to restrict to a triangular grid of slices, in order to make certain distributions uniform. (ii) We are probably forced to consider obstructions to uniformity. In fact, there’s a different way of justifying (ii). At some point, if a triangle-removal argument is going to work, we will have to prove a statement to the effect that if you have three bipartite graphs that are all “regular” and “dense”, then they must give rise to lots of triangles. In the normal dense-graphs case, “regular” just means quasirandom, and it even suffices for just one of the three bipartite graphs to be regular, provided there are plenty of vertices in the third set with high degree in both of the first two sets. In our case, there is a one-to-one correspondence between edges of one of the bipartite graphs and points in %\\null [3]^n%. So this suggests that at some point we are going to have to understand what a “regular” or “quasirandom” subset of %\\null [3]^n% is like. To be continued in the next comment."},{"username":"gowers","timestamp":"2009-02-03T20:54:00.000Z","contents":"Thread title: obstructions to uniformity. What does it mean to “understand what a quasirandom subset of %\\null [3]^n% is like? For those who don’t find the answer obvious, here’s an attempt to explain. (i) We are looking for a _definition_ of quasirandomness. (ii) Experience with other problems suggests that this definition should have the following property: if %A% is a dense quasirandom subset of %\\null [3]^n% and %B% is an arbitrary dense subset of %\\null [3]^n%, then there is a combinatorial line %(x,y,z)% with %x\\in A%, %y\\in B% and %z\\in B%. Indeed, one should have not just _one_ such combinatorial line but a _dense set_ of such combinatorial lines. (Here is a place where having a Varnavides-type statement is actually a big help and not just a mere reassurance.) (iii) Experience with other problems also suggests that if %A% does _not_ have the property in (ii) then there will be some simple reason for it. An example of such a reason is that %x_1% might be 1 for every %x\\in A%. (This would fail to have the property in (ii) because we could take %B% to be the set of all sequences with %x_1=2%.) (iv) If one can think of all possible “simple reasons” of the kind discussed in (iii), then it is often possible to pass to substructures or partitions of the ground set and obtain density or energy increases. (If you don’t know what this means, it’s not too important at this stage. The main point is that if you can do (iii) then there are standard techniques for using what you’ve done. In many other contexts, once you’ve got (iii) the problem is basically solved.) In the next comment, I’m going to write down all the obstructions to uniformity that I can think of. (An _obstruction to uniformity_ is basically what I called a “simple reason” in (iii).)"},{"username":"gowers","timestamp":"2009-02-03T21:15:00.000Z","contents":"Thread title: obstructions to uniformity. Before I start this comment, I want to make a metacomment. If you are reading this and are not familiar with the kinds of techniques people are throwing around, and therefore feel you can’t really contribute, here is a place where you don’t necessarily need to know _anything_ to contribute: we have a simply stated question, which may have a simple answer. (The question will be of the form, “Here are all the Xs I can think of off the top of my head. Can anyone think of any more?”) For the purposes of this comment, I’ll define a set %A\\subset[3]^n% of density %\\alpha% to be _uniform_ if for every set %B\\subset[3]^n% of density %\\beta% the number of combinatorial lines %(x,y,z)\\in A\\times B^2% is approximately %\\alpha\\beta^2% times the number of combinatorial lines in %\\null[3]^n%. Similar definitions apply if we restrict to a triple of slices or a triangular grid of slices. Obstruction 1\\. In any combinatorial line, a coordinate must either take all three values or just one. Therefore, a set %A% is not uniform unless the number of sequences %x% in %A% such that %x_i=j% is roughly %|A|/3% for every %i\\in[n]% and %j\\in\\{1,2,3\\}%. To prove this seems to be not quite trivial. An obvious set %B% to consider is to take %j% such that the number of points with %x_i=j% is significantly less than %|A|/3% and let %B% be the set of all sequences %x% such that %x_i=j%. Then the only points in %A% that can make lines with %B% are ones with %x_i=j%. Therefore, if we get the right number of combinatorial lines in %A\\times B^2%, then we can drop to the subset %A'% that consists of those points only and we find that we get more combinatorial lines than we would expect (given the densities of the restrictions of %A% and %B% to %x_i=j%). I think one can mess around with this information and obtain a set %B% that proves that %A% is not uniform. Given that that was slightly tricky (it might be nice to do it properly though), I’ll restrict attention for now to what I might call _strong_ obstructions to uniformity. I’ll call a set %A% _strongly non-uniform_ if I can find a dense set %B% such that there are _no_ combinatorial lines %(x,y,z)% with %x\\in A%, %y\\in B% and %z\\in B%. So finding more strong obstructions really is a genuinely elementary problem (in the sense of not needing special knowledge). Having come to this decision, let me start again in a new comment."},{"username":"gowers","timestamp":"2009-02-03T21:49:00.000Z","contents":"Thread title: obstructions to uniformity. Strong obstruction 1\\. If there exist %i% and %j% such that %x_i=j% for every %x% in %A%, then %A% is strongly non-uniform. Proof: let %k\\ne j% and let %B% be the set of all sequences such that %x_i=k%. You can’t have a combinatorial line in which the %i%th coordinate is %j% once and %k% twice. Strong obstruction 2\\. If there is a positive integer %m% such that for every %x\\in A% the number of 1s, 2s and 3s in %A% is always the same mod %m%, then %A% is strongly non-uniform. Proof: If %y% and %z% belong to a combinatorial line and have the same number of 1s, 2s and 3s mod %m%, then the size of the variable set must be a multiple of %m%. Therefore, we can choose %B% to be a set where the sequences again have fixed numbers of 1s, 2s and 3s mod %m%, but different numbers from those of %A%. Strong obstruction 3\\. This generalizes the previous example. Suppose that for every %x\\in A% we know that the number of 1s belongs to a set %E_1% of residues mod %m%, the number of 2s belongs to a set %E_2%, and the number of 3s belongs to a set %E_3%. Now let %F_1,F_2,F_3% be sets of residues mod %m%, and let %B% be the set of all sequences with the number of 1s in %F_1% etc. If %y% and %z% are points of %B% that belong to a combinatorial line, then let’s suppose that to get from %y% to %z% you change some 2s to 3s. This tells us that the size of the variable set belongs to %F_3-F_2% mod %m%. This in turn tells us that the number of 1s in the third point of the line belongs to %F_1+F_2-F_3% mod %m%. So if %E_1% is disjoint from any set of the form %F_1+F_2-F_3%, then %A% is strongly non-uniform. And similarly for the other variables. Of course, if %E_1% doesn’t consist of all residues, then one might say that we could take %F_1,F_2,F_3% to be singletons. But that’s true only if %m% is small, since otherwise %B% is not dense. This example works even when %m% is large — it can tend to infinity with %n%. (An example might be when we insist that for all sequences the numbers of 1s, 2s and 3s is between %-cm% and %cm% mod %m% for some small constant %c%.) Strong obstruction 4\\. One can generalize further by passing to a subset. That is, instead of looking at the numbers of 1s, 2s and 3s in the whole sequence, one just looks at the numbers of 1s, 2s and 3s in some specified set %X% of coordinates. The proof is more or less the same. I haven’t tried hugely hard to find more obstructions. Does anyone know of any? Does Boris’s comment 44 have any implications here?"},{"username":"boris","timestamp":"2009-02-03T21:52:00.000Z","contents":"Regarding the definition of triangular grid in #62 at 1:47 pm, to make sure I understood it properly, should it say %a+b+c\\leq n-m% instead of %a,b,c\\leq n-m%? _[Thanks — I’ve changed it now.]_ Regarding the obstructions to the uniformity mentioned in #69 3:54 pm, doesn’t it follow from discussion about Varnavides that to talk about pseudorandom sets we want our sets to be dense subsets in a suitable union of %\\Gamma_{a,b,c}% that live near the middle of the lattice? Answer to Ryan’s question from #65 at 2:48 pm: I can’t think of a reference for the result you want, but it follows from LYM inequality that the level of largest weight is the largest antichain."},{"username":"boris","timestamp":"2009-02-03T22:03:00.000Z","contents":"One can generalize obstructions from #71 at 4:49 pm further: instead of counting number of 1’s 2’s and 3′, weight individual coordinates by small integer weights. Passing to a subset is a special case of that when some weights are zero. _[Not sure what was going wrong, but I’ve deleted the other two copies of this comment.]_"},{"username":"gowers","timestamp":"2009-02-03T23:11:00.000Z","contents":"Thread title: obstructions to uniformity. In answer to Boris’s question (72, second paragraph), I agree that it looks as if any definition of pseudorandomness will have to restrict attention to something like a triangular grid of slices near the middle. But I was hoping that we could do things in two stages. Stage 1 would be to find strong obstructions to uniformity. Then in stage 2 one would weaken the definition back to what I first had: instead of getting no combinatorial lines in %A\\times B^2% one just asks for a number that differs from what you would expect for a random %A%. At that point, one would have to ask, “What do we expect from a random %A%?” and then it would become important to restrict to a triangular grid or some such structure."},{"username":"gowers","timestamp":"2009-02-03T23:25:00.000Z","contents":"Thread title: obstructions to uniformity. Let me check that I understand Boris’s comment 73\\. The idea is to pick an integer %m% and a sequence of integers %r_1,\\dots,r_n%, and to let %A% be the set of all sequences %x% such that %\\sum r_ix_i% belongs to a specified set %E% of residues. Now choose %B% to be the set of all sequences %x% such that %\\sum_ir_ix_i\\in F% mod %m%. Then if %x%, %y% and %z% all lie in a combinatorial line, and %y% and %z% belong to %B%, then the characteristic function %D% of the set of variable coordinates (or wildcards, as some are calling them) has the property that $\\sum_{i\\in D}r_i$ belongs to %F-F% mod %m%. It follows that %\\sum_ir_ix_i\\in F-(F-F)=F+F-F%. So if we can find a dense set %F% of residues mod %m% such that %E\\cap(F+F-F)=\\emptyset% then %A% is strongly nonuniform. That neatly covers a lot of examples. Can anyone think of any more? Or what about heuristic arguments that these are the only examples? (At the moment, my guess is that there are further examples waiting to be discovered.)"},{"username":"gowers","timestamp":"2009-02-03T23:39:00.000Z","contents":"Thread title: obstructions to uniformity. I think I’ve got a further generalization of 75\\. Just replace the group of integers mod %m% by an arbitrary finite Abelian group %G%. So now %r_1,\\dots,r_n% are elements of %G%, and %E% and %F% are dense subsets of %G%. Then repeat exactly what I wrote in 75 except that “residues” or “residues mod %m%” should be replaced by “elements of %G%“."},{"username":"terence-tao","timestamp":"2009-02-03T23:42:00.000Z","contents":"Thread title: Is triangle removal still necessary? Greetings again from the Pacific Standard Time zone! I had one thought about the bigger picture. The initial reasoning in the main post goes like this: 1\\. The standard proof of the corners theorem uses the triangle removal lemma.  \n2\\. The DHJ theorem generalises the corners theorem.  \n3\\. Hence, it is natural to look for a proof of DHJ that uses (some generalisation of) the triangle removal lemma. But, now that we have _used_ the corners theorem in our arguments (to locate a triple %(a+r,b,c), (a,b+r,c), (a,b,r+c)% of rich tuples), the above logic loses quite a bit of its force. Indeed, if we look at the standard way in which the corners theorem is embedded in the DHJ theorem, by representing a set %A \\subset [n]^2% as the set %{\\mathcal F}(A) := \\bigcup_{(a,b) \\in A} \\Gamma_{a,b,3n-a-b} \\subset [3]^{3n}%, we can see that any new proof of DHJ we have will not give any new proof of the corners theorem for a given dense set A, since we will need to use the corners theorem for _precisely that set_ A in order to get the rich triples. So there is no _a priori_ reason for the rest of the proof to involve a triangle removal argument at all. (Of course, triangle removal is still implicitly used in the invocation of the corners theorem.) (Note that the sketch of Sperner via Kruskal-Katona in Ryan.57 has this flavour: one uses things like the pair removal theorem from Gowers.21 to locate a rich pair %B_k, B_{k+r}%, and then uses completely different arguments from then on.) There is however one caveat that seems to suggest that either some vestige of triangle-removal still remains in the rest of the argument, or else some preliminary regularisation is needed. The reason for this is that there are other ways to embed the corners problem into the DHJ problem For instance, one can embed a corner-free set %A \\subset [n]^3% into a combinatorial line-free set %{\\mathcal F}(A) \\times {\\mathcal F}(A) \\subset [3]^{6n}%. The richness profile %\\{ (a,b,c): {\\mathcal F}(A) \\times {\\mathcal F}(A) \\cap \\Gamma_{a,b,c} \\hbox{ large} \\}% of this set is essentially A+A, and thus much more likely to contain corners than the original set A. So if we were to somehow prove DHJ on a rich triple without any further triangle removal, this would imply a “nearly-triangle-removal-lemma-free” proof of the corners theorem of A, reducing it to the substantially simpler-looking problem of finding corners in A+A. But if we could find some way to “detect” this sort of “product structure” in the set %{\\mathcal F}(A) \\times {\\mathcal F}(A)%, and pass to an appropriate sub-index set of [n] in order to isolate a more “essential” component of this set, such as %{\\mathcal F}(A)%, then this objection may disappear. This of course ties in with the obstructions-to-uniformity thread, and also with the counterexamples mentioned in Gowers.20, Gowers.38, and the “hereditary richness” idea I proposed in Terry.40, and maybe also the stationarity in Terry.4, Austin.16. A serious difficulty, though, would be if there are many other ways to embed corners into DHJ that are not easily “detectable” by inspecting things like the richness profile of the embedded set with respect to various index sets. If there are an enormous number of such embeddings then it seems unlikely that we can eradicate triangle removal completely from the remainder of our arguments."},{"username":"terence-tao","timestamp":"2009-02-03T23:46:00.000Z","contents":"Thread title: %c_n% for small n @Sune.59: The lower bound %c_4 \\geq 49% you have compares pretty well with the upper bound %c_4 \\leq 3c_3 = 54%. It may be possible to shave the upper bound down a notch, by trying to figure out what the extremal subsets of %{}[3]^3% are that have 18 elements but no combinatorial lines. Any 54-element subset of %{}[3]^4% with no combinatorial lines must be extremal on every three-dimensional slice, which looks unlikely if there are few extremals."},{"username":"gowers","timestamp":"2009-02-03T23:49:00.000Z","contents":"Thread title: Sperner As will be obvious to many people, given some of the discussion, the question I asked in 28 is not yet a good one. Indeed, if we take both %\\mathcal{A}% and %\\mathcal{B}% to be the set of all sets of size between %n/2-C\\sqrt n% and %n/2+C\\sqrt n%, then there will be far fewer than %3^n% disjoint pairs, but it will be very hard to remove all disjoint pairs. However, in the light of the Varnavides discussion, there is a natural way of rescuing the question. No time to check whether the result is sensible. Let’s just restrict attention to sets of size between %n/2-m% and %n/2+m% for some fairly small %m%. If %\\mathcal{A}% and %\\mathcal{B}% are two subsets of this “middle part” of the power set of %\\null [n]% such that the number of disjoint pairs %(U,V)% with %U\\in\\mathcal{A}% and %V\\in\\mathcal{B}% is far smaller than the number of disjoint pairs of sets of sizes between %n/2-m% and %n/2+m%, does it follow that we can remove a small number of sets from %\\mathcal{A}\\cup\\mathcal{B}% and end up with _no_ disjoint pairs?"},{"username":"gowers","timestamp":"2009-02-03T23:53:00.000Z","contents":"Thread title: Sperner Actually, one might as well just restrict to single layers rather than a whole cluster of layers around %n/2%. Choose %m% slightly less than %n/2%, say, and make %\\mathcal{A}% and %\\mathcal{B}% consist of subsets of the set of sets of size %m%."},{"username":"boris","timestamp":"2009-02-04T00:07:00.000Z","contents":"I am not sure whether generalization of Gowers #76 at 6:39 pm is truly a generalization since if a set F is nonuniform (as it must be for %F-F+F\\neq G% to hold), then it correlates with some character. That should imply correlation with some other set for which G=cyclic."},{"username":"ryan-odonnell","timestamp":"2009-02-04T00:23:00.000Z","contents":"Re Sperner: I have to run to class in a second so I can’t say if this is actually be relevant to TimG.79,80, but… See this paper (and its referencers/referencees) for the theory on structure vs. pseudorandomness for independent sets in Kneser graphs: [http://www.ams.org/mathscinet-getitem?mr=2178966](http://www.ams.org/mathscinet-getitem?mr=2178966)"},{"username":"tyler-neylon","timestamp":"2009-02-04T00:45:00.000Z","contents":"Thread title: %c_n% for small n. In comment 40, the set %D_n := \\{x\\in \\mathbb{Z}_3^n | \\sum x_i \\not\\equiv 0\\pmod{3} \\}% was introduced. This set only includes lines %\\ell% with %\\#\\{\\ell_i = \\ast\\} \\equiv 0 \\pmod{3}% and %(\\sum_{\\ell_i\\ne\\ast}\\ell_i) \\not\\equiv 0 \\pmod{3}%. So when n=4, we can just exclude (still thinking in %\\mathbb{Z}_3%) points %\\vec 1% and %\\vec 2%. This shows that %52 \\le c_4 \\le 54%. For n=5, we can keep %\\vec 1, \\vec 2% excluded, and omit the extra 15 points given by any choice of %\\{a,b\\}\\subset \\mathbb{Z}_3% plugged into each of %(ababa), (babab), (abbaa), (aaaab), (baaaa)%. This is decidedly less elegant than the n=4 case, but we see that %145 \\le c_5 \\le 162%."},{"username":"terence-tao","timestamp":"2009-02-04T01:12:00.000Z","contents":"Thread title: %c_n% for small n. @Tyler.83: nice! It seems that just excluding those x for which %\\sum_i x_i = 0 \\hbox{ mod } 3% already knocks out a significant fraction of combinatorial lines, at least when n is small. It may be that some secondary set of a similar nature can then be removed to knock out quite a few more lines for medium n (e.g. n = 5-10), at which point the rest can be picked off by ad hoc methods. This may lead the way to suggest a different asymptotic lower bound than the current Behrend-based lower bound of %c_n \\geq 3^{n - O(\\sqrt{\\log n})}%. If so, this may provide an excellent set of examples with which to test the upper bound strategies (the Behrend example tends to be rather intractable to compute with in practice). So it might be worth trying to push the lower bounds for %c_6, c_7%, etc. as best we can, though it’s much less likely now that we will be able to compute these numbers exactly. (I don’t see the upper bounds budging much from the trivial %c_6 \\leq 2 \\times 3^5 = 486%, %c_7 \\leq 2 \\times 3^6 = 1458% – after all, density Hales-Jewett is a _hard_ theorem – though I would imagine that one should be able to maybe knock one or two from the upper bound for, say %c_7% (after all, every 2-dimensional slice of an extremiser here has to have exactly 6 elements, and there are only three such slices available).)"},{"username":"terence-tao","timestamp":"2009-02-04T01:32:00.000Z","contents":"Thread title: A weak form of DHJ As I mentioned in Terry.8, DHJ is the statement that %c_n = o(3^n)% as %n \\to \\infty%. This is a hard theorem. But one can at least give an easy proof of the weaker statement that the trivial bound %c_{n+1} \\leq 3 c_n% must be strict (i.e. %c_{n+1} < 3 c_n%) for infinitely many n, as follows. Suppose that %c_{n+1}=3c_n% for all but finitely many n, then there exists an m, and an n much larger than m, such that %c_n = 3^{n-m} c_m%. Thus there exists a combinatorial line-free subset A of %{}[3]^n% with exactly %3^{n-m} c_m% elements. Now think of %{}[3]^n% as a whole bunch of parallel copies of %{}[3]^m%, each parameterised by an element from %{}[3]^{n-m}%. As A has exactly %3^{n-m} c_m% elements and is combinatorial line-free, each m-dimensional slice of A must have exactly %c_m% elements. We can assign each element of %{}[3]^{n-m}% a colour based on exactly what the associated m-dimensional slice of A looks like; there are at most %\\binom{3^m}{c_m}% colours available. Now we use the _colouring_ Hales-Jewett theorem, and conclude that if n is large enough depending on m, there exists a monochromatic line in %{}[3]^{n-m}%. Unwinding all this, we get a combinatorial line in A (note that all the slices of A are non-empty). [Incidentally, an argument like this appears in Szemeredi’s original proof of Szemeredi’s theorem, though of course many many more things need to be done in that proof beyond this simple argument. A lot of Szemeredi’s argument, by the way, applies to Hales-Jewett, except for a crucial step where he applies what is now known as the Szemeredi regularity lemma. In the Hales-Jewett setting, the graph that one would wish to apply this lemma to is far too sparse for the strategy to work, it seems.]"},{"username":"terence-tao","timestamp":"2009-02-04T02:03:00.000Z","contents":"Szemeredi’s proof of Roth’s theorem While on the subject of Szemeredi’s arguments, Endre also has a simple combinatorial proof of Roth’s theorem (avoiding Fourier analysis or the regularity lemma), instead relying on the Szemeredi cube lemma. This can for instance be found in Graham-Rothschild-Spencer’s book, or Chapter 10.7 of my book with Van, or in Ernie’s writeup [http://www.math.gatech.edu/~ecroot/szemeredi.pdf](http://www.math.gatech.edu/~ecroot/szemeredi.pdf) the basic idea is to observe that if a set A in %\\null [N]% is dense, then it must contain a high-dimensional cube Q. If A has no arithmetic progressions, then 2 . Q – A must be essentially disjoint from A. Expressing the d-dimensional cube Q as the final element of a sequence %Q_0 \\subset Q_1 \\subset \\ldots \\subset Q_d%, where each %Q_i% is one dimension larger than the preceding %Q_{i-1}%, and using the pigeonhole principle, we can find i such that %2 \\cdot Q_i - A% has roughly the same size as %2 \\cdot Q_{i+1} - A%. We can write %Q_{i+1} = Q_i + \\{0,v_i\\}% for some step %v_i%, and we thus see that %2 \\cdot Q_i - A% is very stable under shifting by %v_i%, and thus both it and its complement can be partitioned into long arithmetic progressions of step %v_i%. Since A is disjoint from %2 \\cdot Q_i - A%, it is squeezed into the other progressions, and on one of these we have a density increment. Now apply the usual density increment argument and we are done. It’s not clear whether this argument can be adapted to give the corners theorem, let alone the k=3 case of DHJ, but perhaps it should be looked at."},{"username":"gowers","timestamp":"2009-02-04T03:55:00.000Z","contents":"Thread title: obstructions to uniformity. Reply to 82. Boris, I was not sure it was a true generalization either, and I’m still not sure. Let me attempt to come up with a set where it might have a chance of being one. Let us consider two functions defined on the set of all sequences in %\\null [3]^n%. First of all, choose a largish %m% and a subset %P% of positive density in %\\mathbb{Z}_m^2% with the following properties: (i) %P+P-P% is not much larger than %P%, and quite a bit smaller than %\\mathbb{Z}_m^2%; (ii) for every %x% there are about the same number of %y% such that %(x,y)\\in P%, and vice versa. Now let’s choose some fairly random sequences of weights %(r_1,\\dots,r_n)% and %(s_1,\\dots,s_n)% and let %A% be the set of all sequences such that %(\\sum r_ix_i,\\sum s_ix_i)\\in P% mod %m%. OK that almost certainly doesn’t work, for the reason you say: there will be some character that correlates in a big way with %P%, and that will give us %u% and %v% such that %\\sum(ur_i+vs_i)x_i% has a tendency to lie in an unusual subset of %\\mathbb{Z}_m%. I’m becoming convinced that it isn’t a generalization, but I can’t quite rule out that there might be some further things one could do."},{"username":"gowers","timestamp":"2009-02-04T04:18:00.000Z","contents":"Thread title: is triangle removal still necessary? This is a response to Terry’s comment 77\\. Obviously what you ask is an important question, but I’d like to understand more precisely what you mean by it. To begin with, let me distinguish carefully between two interpretations of the question. One would be “Is it possible to do without any analogue of the triangle removal lemma from this point on?” and another would be “Is it unnatural to try to use the triangle removal lemma from this point on?” The background to the question is that if a positive proportion of the slices (for a definition of this word, see 62) is rich, then by the corners theorem one can find a triple of rich slices of the form %\\Gamma_{a+r,b,c}%, %\\Gamma_{a,b+r,c}%, %\\Gamma_{a,b,c+r}%. So one might take the view that you use the triangle removal lemma (via the corners theorem) to get yourself to a triple of rich slices and then let other methods take over from there. Now we haven’t actually said what we mean by “rich”. It has to mean more than “intersects densely with %A%” since it is easy to find examples where %A% intersects three such slices densely but there is no combinatorial line. (See e.g. comment 20.) And it seems unlikely that there is a definition of “rich” such that (i) every dense set %A% intersects a dense set of slices richly and (ii) given any triangle of slices intersected richly by %A% you must get a combinatorial line from that triangle of slices. At this point it’s worth going back and reading Terry’s comment 40\\. The idea there is that if you don’t get a dense set of rich slices, then perhaps you can pass to substructures (e.g. by fixing a few coordinates) where something like density or energy improves and try again. This would be a sort of mixed argument: first use density/energy-increase arguments to get to a situation where lots of slices are rich (which I see as some kind of quasirandomness property, but perhaps I shouldn’t), then apply the corners theorem to get a triple of rich slices, and finally apply some rich-slices counting lemma to get a combinatorial line. (I’m not at all sure that that’s what Terry had in mind, and would be interested to know.) Anyhow, perhaps that suggests that triangle removal could be used just to get a triple of good slices, and other arguments could be used to do the rest. In my next comment I’ll discuss the other interpretation of Terry’s question."},{"username":"gowers","timestamp":"2009-02-04T04:53:00.000Z","contents":"Thread title: is triangle removal still necessary? Is it clearly wrong to try to prove the whole theorem via a triangle-removal lemma? I would present the reasoning in the main post slightly differently. Admittedlly it starts out in the way that Terry describes at the beginning of his comment 77\\. However, I think that the argument gains considerably in force when you add the following fact. 4\\. It is possible to define a tripartite graph in such a way that nondegenerate triangles correspond to combinatorial lines in %A%. Moreover, the construction naturally generalizes the tripartite graph that you construct in order to deduce the corners theorem from the triangle removal lemma. Here’s a way one might justify this. For a long time it was known that the triangle removal lemma implied Roth’s theorem, but until Jozsef came along, nobody had observed that it would also do corners. Before that one might have reasoned as follows. Let us label the lines of slope 1 in %\\null [n]^2% according to the value of %x-y%. If we have a corner, then the labels of the lines that it lies in must form an arithmetic progression of length 3\\. And indeed, since our dense subset of %\\null [n]^2% might even _be_ a union of lines of slope 1, we see that the corners theorem implies Roth’s theorem. So perhaps we should use Roth’s theorem to find a rich triple of diagonals and then use other arguments to obtain a corner. Is this a fair analogy or does it break down somewhere? Before giving up on triangle removal, I would want answers to the following questions. (i) Is it possible to formulate a triangle-removal statement that isn’t obviously false and that implies that the tripartite graph discussed in 67 contains lots of triangles (because it contains lots of edge-disjoint degenerate ones)? (ii) If so, is there the slightest chance of proving the statement by imitating the proof of the usual triangle-removal lemma? Or does the sparseness and nonquasirandomness of the bipartite graph where we join two sets if they are disjoint make a project of this kind hopeless? I could perhaps add a third question, of a slightly different kind. (iii) Is it the case that in order to prove that a regular triple contained many triangles, one would have to develop tools (connected with obstructions to uniformity) that could be used to give an easier proof that didn’t go via triangle removal, regularity lemmas etc.? I feel reasonably optimistic about (i) if one restricts to a triangular grid of slices. I’m much less sure about (ii) and (iii)."},{"username":"sune-kristian-jakobsen","timestamp":"2009-02-04T05:02:00.000Z","contents":"%c_n% for small %n% (Re to 78 83 and 84) %c_4=52%: In 2 dimensions there are only 4 such set with 6 elements: %A_i=\\{(x,y)\\in\\mathbb{Z}_3^2: x+y\\neq -i \\mod 3\\}% for i=1,2,3 and %\\{(x,y)\\in\\mathbb{Z}_3^2:x\\neq y\\}%. In 3 dimensions there is only one such set S with 18 elements. Proof: Let S be such a set. Now let %S_i=\\{(x,y)\\in\\mathbb{Z}_3^2:(x,y,i)\\in S\\}%, for i=1,2,3\\. Now each %S_i% has to have 6 elements and be of the above from, and the intersection of the %S_i%‘s has to be empty. Thus the %S_i%‘s must be a permutation of the %A_i%‘s. The only permutation, that does not give any combinatorial lines is %S_i=A_i% for i=1,2,3. Now asssume that %c_4\\geq 53% and let S be a set %|S|\\geq 53% in 4 dimensions and with no combinatorial lines. Again let %S_i=\\{(x,y,z)\\in\\mathbb{Z}_3^3:(x,y,z,i)\\in S\\}%, for i=1,2,3\\. Now two of the %S_i%‘s has to have 18 elements and thus be of the above form. By if two of them are identical, the third can at most have 27-18=9 elements, and S at most %2*18+9=45%. Contradiction."},{"username":"gowers","timestamp":"2009-02-04T05:19:00.000Z","contents":"Thread title: obstructions to uniformity. I don’t know whether this is genuinely different, but in the Hales-Jewett theorem one doesn’t actually care that the set %\\null[3]% is %\\{1,2,3\\}%. So perhaps we could take three other numbers, such as %\\{4,7,9\\}%, and take the set of all %x% such that %\\sum r_ix_i% belongs to %E% mod %m%. I don’t immediately see how to get this from Boris’s construction, at least as I presented it in 75."},{"username":"ryan-odonnell","timestamp":"2009-02-04T05:44:00.000Z","contents":"Thread title: obstructions to uniformity. Let me try to sketch a another obstruction to strong uniformity. Hopefully I have understood the notion of strong non-uniformity correctly! For the remainder of this comment, let %\\log% denote %\\log_3%. Partition the coordinates [n] into disjoint “blocks” of size roughly %\\log n - \\log \\log n%. For a uniform random string %x \\in [3]^n%, let $F$ denote the event that %x% contains a block which is all-2’s. The probability a particular block is all-2’s is %\\log n / n%; hence the probability it is not-all-2’s is %1 - \\log n / n%. Since there are also about %n/\\log n% blocks, the probability that all blocks are not-all-2’s is basically %1/e%. So the probability of $F$ is basically %1 - 1/e%. Call it %.63%. So let %A% be the set of all strings which have an all-2’s block, which has density about %.63%. On the other hand, suppose we have a combinatorial line %(x,y,z)% with %x \\in A%. Then that block of 2’s stays fixed in %y% and %z%. So take %B% to be the set of strings which do *not* have a block of all-2’s. This is also plenty dense, having density about %.37%. But then we couldn’t have %y, z \\in B%. You might object that this only worked because we were insisting that it was %x% (i.e., the wildcard=1 point) that was in %A%, but this can be circumvented by making %A% equal the set of strings with a block of all-2’s *and* a block of all-1’s. (These events are practically disjoint, so this %A% would still have density about %.63^2%.) This example does seem to me different from Boris and Tim’s examples. At first it looks similar because you are counting that the number of 2’s in a particular set of coordinates is equal to a particular number. But then things differ because you take an “or” across all blocks. You can make it even more different by taking the “block-of-all-2’s *and* block-of-all-1’s” example. Or you could instead let %A% be the set of strings with a “run” of 2’s of length about %\\log n% somewhere within them. Does this example work? If so, I think my intuition agrees with Tim’s (expressed at the end of #75) that there may be “too many” obstructions…"},{"username":"jozsef","timestamp":"2009-02-04T05:46:00.000Z","contents":"Thread title: an alternative problem. In 60\\. Tim suggested a problem where we consider certain pairs of sets. “A sensible question would be some density-type condition on A that would guarantee a triple of the given kind. ” One candidate might be the following: Given a subset S of the %2^n% subsets of an n-element set, then two elements of S form a pair if their symmetric diference is also in S. I think, but I can’t prove it yet, that if S is dense, %|S|\\geq c2^n% then the number of such pairs (triples) is at least %c'2^{2n}%. If so, then there is a direct application of the triangle removal lemma, however I have to think about the interpretation of the result."},{"username":"ryan-odonnell","timestamp":"2009-02-04T05:59:00.000Z","contents":"Obstructions to strong uniformity. Can’t you also do other goofy things like take any example %A% which is strongly-non-uniform and, say, tack 23 onto the end of each string (changing %n% to %n + 2%)? This hurts the density by only %1/9%. Now take the dense %B% which failed with the original %A% and tack, say, 12 on the end of all of its strings. This also only loses density %1/9%. But if %A% and %B% were a bad example before, they are still bad now. And %A% is no longer of the weighted-sums-modulo-m type, even if it was before."},{"username":"gowers","timestamp":"2009-02-04T06:03:00.000Z","contents":"Thread title: Sperner I want to make a start on the question I eventually arrived at in 80, which I’ll repeat here and make more precise. Let %m% be slightly smaller than %n/2%, write %k% for %n-2m% and let %\\mathcal{A}% and %\\mathcal{B}% be two subsets of %\\binom{[n]}{m}%. Is it the case that for every constant %a>0% there exists a constant %c>0% such that if the number of disjoint pairs %(U,V)% with %U\\in\\mathcal{A}% and %V\\in\\mathcal{B}% is at most %c\\binom{n}{m,m,k}% then it is possible to remove at most %a\\binom{n}{m}% sets from %\\mathcal{A}\\cup\\mathcal{B}% and end up with two set systems %\\mathcal{A}'% and %\\mathcal{B}'% such that every %U\\in\\mathcal{A}'% intersects every %V\\in\\mathcal{B}'%? Actually, now that I’ve formulated the question (though I’m still not certain that it’s a sensible one) I find that I have no idea where to start. All I have to say is that Boris’s example in 44 is worth bearing in mind. But in a curious way that makes me feel the problem is worth pursuing. It seems that to prove Sperner via a generalization of the trivial pair removal lemma, one has to prove this non-trivial pair removal lemma. Working out how to do that therefore introduces an extra ingredient. Once one has that extra ingredient, one can see whether it generalizes to provide the extra ingredient needed to get from the usual triangle removal lemma to a more difficult Spernerish triangle removal lemma."},{"username":"gowers","timestamp":"2009-02-04T06:13:00.000Z","contents":"Thread title: obstructions to strong uniformity. Ryan, I think we need to formulate more carefully what it means to have a new obstruction. I would want not to count the example you’ve given because it’s contained a bigger example where %A% and %B% consist of all strings (before you do the tacking-on process). And that bigger example is covered by what I called strong obstruction 1 in comment 71."},{"username":"gowers","timestamp":"2009-02-04T06:52:00.000Z","contents":"Thread title: Sperner Actually, I have a small idea where to start (see 95). If we replace sets in %\\mathcal{B}% by their complements, then we obtain a set system that has to have a lower shadow that is very non-uniform. That is, if you count how many times a set is contained in the complement of a set in %\\mathcal{B}% then the resulting function mustn’t be close to constant. So it looks as though we do indeed have to understand obstructions to uniformity in Kruskal Katona."},{"username":"ryan-odonnell","timestamp":"2009-02-04T08:10:00.000Z","contents":"Obstructions. Hi Tim, are you referring to the example from 92 or 94? I don’t see how the #92 one fits into any of the example obstructions mentioned previously."},{"username":"terence-tao","timestamp":"2009-02-04T08:35:00.000Z","contents":"Thread title: is triangle removal still necessary? Tim, I guess my question was intended in the weak sense as in 88 (“can we now do without triangle removal?”) as opposed to the strong sense as in 89 (“should we keep trying to mimic triangle removal?”). Basically, I think we should keep pursuing the triangle removal tack, but bear in mind the option to also utilise many other tools (e.g. Kruskal-Katona, or some ideas from Szemeredi’s proofs of Roth’s theorem, about which I will say more in my next comment), and to also entertain the possibility that triangle removal might in fact only play a minor role in the rest of the argument."},{"username":"terence-tao","timestamp":"2009-02-04T08:43:00.000Z","contents":"100! (do I get a prize?) Thread title: Density incrementation One tool that seems to be underexploited so far is that of density incrementation. In the standard proof of Roth’s theorem (as well as Szemeredi’s proof), we may assume without loss of generality that the set A does not have significantly increased density on any reasonably long subprogression, since otherwise we could pass to that subprogression and apply some sort of induction hypothesis. We can do the same here: to prove DHJ for some set %A \\subset [3]^n% with density %\\delta%, we may assume without loss of generality that every slice of A of any reasonable size (m-dimensional, where m grows slowly with n) has density at most %\\delta + o(1)% (where I will be vague about exactly what o(1) means). By Markov’s inequality, this also implies that almost all (i.e. 1-o(1)) of all such slices will have density at least %\\delta - o(1)% as well. It is not clear to me how one can exploit such a fact, but it is something worth keeping in mind. (Admittedly, this structure is not used in the graph-theoretic proofs of Roth or corners, but as I said in my previous post, we are not bound to slavishly follow the graph-theoretic model.) In a somewhat similar spirit, we can use the colouring Hales-Jewett theorem to stabilise various low-complexity quantities. Indeed, if we have some quantity X that depends on a medium number m of the coordinates of %{}[3]^n%, and takes a bounded number k of values, then (if m is sufficiently large depending on d and k), we can reduce the m coordinates to d, passing to a slice, and force X to now be constant on these d coordinates. (I already used this argument in 85.) Again, I don’t see an immediate way to utilise this freedom, but if we are encountering a problem that the “richness” of various slices varies too much with the slice, this is presumably the thing to do to fix that. And there is also the Carlson-Simpson theorem to hold in reserve to soup this up even further."},{"username":"terence-tao","timestamp":"2009-02-04T08:52:00.000Z","contents":"%c_n% for small n. @Sune.90: Excellent… we now have the first five elements of %c_n%, namely %1, 2, 6, 18, 52%. Consulting the OEIS, [http://www.research.att.com/~njas/sequences/?q=1%2C2%2C6%2C18%2C52&language=english&go=Search](http://www.research.att.com/~njas/sequences/?q=1%2C2%2C6%2C18%2C52&language=english&go=Search) there are a number of sequences here, but none that look likely to be a fit, especially given the bounds %145 \\leq c_5 \\leq 3c_4 = 156% we have (see Tyler.83). Given that %c_5% does not look too close to being computable, perhaps it is time to submit what we have to the OEIS?"},{"username":"ryan-odonnell","timestamp":"2009-02-04T08:58:00.000Z","contents":"Kruskal-Katona obstructions. Even in the case of Kruskal-Katona, I’m not 100% what the right question is. Let me work off Boris’s #44 formulation of the problem. Say the density of %A% is %\\mu_A% and the density of %B% is %\\mu_B%. When Boris asks, when is the number of pairs %(a,b) \\in A \\times B% with $a \\subset b$ close to expected, I assume by “expected” he means “what the number would be if %A% and %B% were random sets of the given densities”. (Correct me if I’ve mistaken you here.) Introduce a uniformly random pair %(x,y) \\in \\binom{[n]}{n/2} \\times \\binom{[n]}{n/2+1}% subject to %x \\subset y%. Let %L% be the event that both %x \\in A% and %y \\in B%. Note that the marginal on both %x% and %y% is uniform. Hence if %A% and %B% were both random sets of the given densities, we’d expect the probability of %L% to be %\\mu_A \\mu_B%. Now Boris gave two very different examples where %\\mu_A \\approx \\mu_B \\approx 1/2% and yet %\\Pr[L] = 0%, much less than %\\mu_A \\mu_B% of course. On the other hand, there are plenty of examples where %\\mu_A \\approx \\mu_B% is a constant and yet $\\latex \\Pr[L] \\approx \\mu_A$, much *greater* than %\\mu_A \\mu_B%. To achieve this, fix %B% to be the upper shadow of %A%. Now we just need to take %A% to be any constant-density set whose upper shadow has about the same size as %A%. And there is a world of such sets; too many, I think, to have a meaningful characterization by obstructions."},{"username":"terence-tao","timestamp":"2009-02-04T09:05:00.000Z","contents":"The HJ cube lemma. As I mentioned in 86, there is a proof of Roth’s theorem by Szemeredi which relies on the Szemeredi cube lemma, which basically asserts that any dense subset of %{}[n]% contains a large cube %Q = \\{ a_0 + \\omega_1 r_1 + \\ldots + \\omega_d r_d: \\omega_1,\\ldots,\\omega_d \\in \\{0,1\\} \\}% (in practice one can take d to be something like %\\log \\log n%). There is a similar lemma for k=2 Hales-Jewett: if n is sufficiently large depending on %\\delta% and d, then any subset A of %{}[2]^n% of density at least %\\delta% contains a d-dimensional cube, which basically is the same thing as a combinatorial line except that one now has d wildcards instead of 1. The proof proceeds by induction on d and using the k=2 DHJ (i.e. Sperner). The d=1 case is of course exactly k=2 DHJ. Now if d > 1 and the claim is already proven for d-1, then we can foliate %{}[3]^n% into m-dimensional slices for some intermediate m. Many of these slices will be rich and thus contain a d-1-dimensional cube, by induction hypothesis. The placement of this cube can vary by slice, but the number of possible cubes is bounded by some constant depending on m, so by pigeonhole there is a positive density (depending on m) of slices for which the cube position is the same. Now apply Sperner (k=2 DHJ) one more time to get the result. One can of course embed %{}[2]^n% into %{}[3]^N% in a variety of ways, and so this cube lemma is available for use in the k=3 problem. I don’t know yet whether Endre’s proof (see 86) can carry over in this setting, but it might be worth taking a look at. Note that the colouring version of this cube lemma is used in the standard proof of k=3 colouring Hales-Jewett."},{"username":"gowers","timestamp":"2009-02-04T14:38:00.000Z","contents":"Obstructions to uniformity. Ryan, re your comment 98, I’m sorry but for some reason I failed to notice your very interesting comment 92\\. In my comment 96 I was referring to what you said in 94. Your example in 92 strikes me as a definite advance in the understanding of the problem. Let me try to explain why. One of the difficulties of the density Hales-Jewett theorem is its “combinatorial”, as opposed to “group-theoretic’, nature. That is, we do not have a group structure on %\\null [3]^n% (or at least, not one that seems to be relevant), and the notion of a combinatorial line is not one that can be expressed in purely group-theoretic terms. On the other hand, the kinds of obstructions that Boris and I were coming up with did have a group-theoretic flavour to them, even if they weren’t exactly group theoretic. I couldn’t help thinking that it was too optimistic to hope that all obstructions were of this kind. And now you have demonstrated (assuming your example checks out, which it feels to me as though it does) that there are indeed some more purely combinatorial obstructions. A quick thought at this stage. For each obstruction it is well worth thinking about the following question: is there an obvious way of restricting %\\null[3]^n% so as to obtain a substructure where %A% is denser? In your case I was about to say that I couldn’t see one, but I think I can: we could do something like restricting to a random combinatorial subspace of the following kind. Take your partition of %\\null [n]% into blocks and for each block choose a random subset (according to some sensible probability distribution — not sure what that is) to be a set of coordinates that vary simultaneously. But insist that at least once you … the more I write the more this feels completely wrong. So instead let me ask: can one get a density increase on a combinatorial subspace for Ryan’s set %A% in 92?"},{"username":"jozsef","timestamp":"2009-02-04T14:51:00.000Z","contents":"Thread title: Sperner Re: Tim’s question in 95\\. I just want to clarify, that the conjecture (?) is that in the Kneser graph %K_{n,m}% every induced bipartite graph with at most %c\\binom{n}{m,m,k}% edges has a vertex cover of size at most %a\\binom{n}{m}.% Or, equivalently, the largest matching is at most %a\\binom{n}{m}.% It seems to me that the small number of edges in the induced graph implies that the number of vertices is small, but I don’t remember what is the spectral gap in %K_{n,m}%. Anyways, if %K_{n,m}% is quasirandom then I don’t understand the question, but it is almost 1 am, so I will review it tomorrow…"},{"username":"jozsef","timestamp":"2009-02-04T14:54:00.000Z","contents":"Good morning Tim! Then mine is 104\\. … _[Now changed]_"},{"username":"gowers","timestamp":"2009-02-04T15:00:00.000Z","contents":"Obstructions to uniformity. For some reason in 103 I was blind to the obvious example: take a subspace where all sequences are constant on Ryan’s blocks, and it’s actually contained in %A%."},{"username":"gowers","timestamp":"2009-02-04T16:00:00.000Z","contents":"Sperner. In response to Jozsef comment 104, I think your rephrasing of my question is indeed the same question (but I’m slightly shaky on the terminology you use). Just to check: the Kneser graph %K_{n,m}% is the graph consisting of all subsets of %\\null [n]% of size %m%, with two of them joined if and only if they are disjoint. An induced bipartite graph just means that you pick two sets %\\mathcal{A}% and %\\mathcal{B}% of vertices in this graph and put in all the edges that you have from the graph: so you have two systems of sets of size %m% and join a set in %\\mathcal{A}% to a set in %\\mathcal{B}% if and only if they are disjoint. Finally, a vertex cover means a collection of vertices such that every edge contains a vertex in that collection. So if we have a small vertex cover then we can remove it and end up with no edges. It’s not the case that a small number of edges implies that the vertex sets are small. For example, %\\mathcal{A}% and %\\mathcal{B}% could both be the set of all sets of size %m% that contain the element 1\\. Then there are no edges between %\\mathcal{A}% and %\\mathcal{B}%, but they both have density about 1/2\\. Of course, this example doesn’t have much bearing on the problem I asked."},{"username":"gowers","timestamp":"2009-02-04T16:17:00.000Z","contents":"_[This comment is deliberately not numbered.]_ If you’ll forgive a metacomment, at some point I think we should consider trying to summarize everything we’ve learned so far (interesting examples, questions we haven’t yet answered, data about small %n%, ideas for general approaches, etc.), write a new post, and start all over again, this time from camp 1 rather than base camp, so to speak. It would have the small advantage that people wouldn’t have to scroll through over a hundred comments (my computer now takes quite a long time to load all the mathematical gifs — if that’s the right word). Note that I’m not suggesting splitting up the threads: I’ve rather enjoyed having them all jumbled together, and I think this suggestion would keep the current atmosphere but keep things manageable. Any views? (I’m thinking of introducing a poll facility to this blog so that people can express opinions about this kind of question without having to make actual comments.) _[To my amazement, I seem to have managed to produce a poll. So if you want to register an opinion, you have the chance to do so. It appears as a new and strange-looking post on the blog.]_"},{"username":"gowers","timestamp":"2009-02-04T16:31:00.000Z","contents":"Thread title: big picture. Terry has already described some possible ways that an eventual proof might go (see e.g. #77 and #100). In the next comment, I’ll have a go at some proof sketches (not very detailed because I don’t know the details!). Each one would ideally be the beginning of a top-down process that ends with a fully detailed proof. But the question in each case is whether the strategy is remotely realistic. Strategy 1\\. Density incrementation. This strategy is to model a proof on Roth’s proof of Roth’s theorem. The idea is to keep thinking about obstructions to uniformity until we think we’ve found all of them. At that point we try to prove the following: either a set %A% correlates with one of our obstructions to uniformity, in which case one can show that there is a combinatorial subspace inside which %A% has increased density; or %A% does not correlate with one of our obstructions to uniformity, in which case we can prove that %A% contains the expected number of combinatorial lines. Two remarks about the strategy. First, as we have noted, the “expected” number of combinatorial lines is a troublesome concept. But it ought to be possible to deal with that by restricting to a triangular grid of slices near the centre. Secondly, a density-increment proof of density Hales-Jewett would presumably yield a density-increment proof of the corners theorem. Now such a proof has been obtained by Shkredov, but it is considerably more complicated than the density-increment proof of Roth’s theorem. So we shouldn’t expect this strategy to be easy. (Actually, this has given me an idea about obstructions to uniformity in DHJ. I’ll explore it in a moment.) Strategy 2\\. This is roughly the original strategy I suggested. We have a tripartite graph where we join two sets if and only if they are disjoint, and we try to prove a regularity lemma for subgraphs of this graph (or perhaps portions of this graph where we restrict the cardinalities of sets so that they become “typical”). This would be a relative regularity lemma. Two remarks about this strategy. First, I think it ought to be easy to make at least some progress, either negative or positive, by trying to produce such a regularity lemma. Either it will rapidly become clear that the task is hopeless, or we’ll manage it, or we’ll generate a bunch of interesting questions that we can’t yet answer. Secondly, in a subtle way the regularity approach to corners allows one not to understand too well what the obstructions to uniformity are for that problem: it just automatically seeks them out and deals with them. Might something like that happen for DHJ?"},{"username":"gowers","timestamp":"2009-02-04T16:56:00.000Z","contents":"Obstructions to uniformity. No idea whether this is going anywhere, but in Shkredov’s proof of the corners theorem, he has to deal with examples of sets of the form %X\\times Y%, where both %X% and %Y% are subsets of %\\null [n]%. These sets have the feature that if they contain the two points %(x,y+d)% and %(x+d,y)% then they automatically contain the point %(x,y).% It ought to be possible to exploit this to obtain an obstruction to uniformity in DHJ, but I don’t yet know whether it will be a new one. So let’s choose an arbitrary pair of (reasonably dense and codense near %n/3%) subsets %X% and %Y% of %\\null [n]%, take %B% to be the set of all sequences such that the number of 1s belongs to %X% and the number of 2s belongs to %Y%, and take %A% to be the complement of %B%. Now let %(x,y,z)% be a combinatorial line (written in 1,2,3 order). The point %x% has the same number of 2s as %z%, and %y% has the same number of 1s as %z%. So if %x% and %y% belong to %B%, then %z% automatically belongs to %B%, so it can’t belong to %A%. This is open to the same objection that Ryan talked about in 92, namely that we’re insisting that it is %z% that belongs to %A% rather than %x% or %y%, but the basic idea can be used to produce an example that gets round this objection. We just have to play the same game with 2s and 3s, and the same game again with 1s and 3s, and then intersect the three resulting sets. If you choose them reasonably randomly then you’ll still get a dense %A% and a dense %B%. (Just to clarify what I’ve just said, let’s call the sets I constructed %A_3% and %B_3%, since 3 was the preferred value. Now you repeat the construction I gave twice over to produce %A_1% and %B_1% and %A_2% and %B_2%. Finally, you set %A% to be %A_1\\cap A_2\\cap A_3% and you set %B% to be %B_1\\cap B_2\\cap B_3%. Provided the sets %X% and %Y% are chosen reasonably randomly in each case, the sets %A% and %B% will be reasonably dense.) This feels like a genuinely different obstruction because it is very nonlinear. In fact, I’m basically pretty sure it’s different. So going briefly back to the big picture, one might argue that the problem with strategy 1 is that it is already difficult to carry out for corners, and looks as though it could be very much more difficult still here. So if it were just a choice between strategies 1 and 2, there would be a good reason for continuing to pursue 2 (not that I feel like giving up on 1 just yet)."},{"username":"gowers","timestamp":"2009-02-04T18:20:00.000Z","contents":"Thread title: counting lemma. I want to make a tentative appeal to procedural rule 6, the one that says that if you think you could work something out with the help of some private calculations, then you first see whether others think that would be a good thing to do. So here’s what I think I could do reasonably easily privately and not so easily if I have to type everything in here. It’s clear that if any kind of regularity approach is going to work, then there needs to be a lemma that says that if you’ve got three suitably quasirandom subgraphs of the is-disjoint-from graph arranged in a triple, then you must have lots of triangles. But it’s not clear with “suitably quasirandom” means. A natural way to attempt to get a handle on this, if you’ve ever been involved with counting lemmas in sparse random settings, is to try to use the Cauchy-Schwarz inequality to prove that the number of triangles is close to what you expect unless something bad happens. And that something bad then turns into your definition of “is not quasirandom”. I would like permission to go away, attempt some Cauchy-Schwarz calculations, and come back and report on the results."},{"username":"gowers","timestamp":"2009-02-04T18:29:00.000Z","contents":"Thread title: regularity lemma. Here, by the way, is the outline of how one might try to prove a regularity lemma. Almost certainly there will be an easy argument that says that this naive strategy has no chance of working. Let %G_0% be the bipartite graph where you join two sets (fixed to have size around %n/3%) if and only if they are disjoint. Let %G% be a dense subgraph of this graph. We want to partition the vertices into sets where %G% looks like a random subgraph of %G_0%. So why not argue as follows? Given partitions of the vertex sets, if there are lots of induced bipartite subgraphs where %G% doesn’t look random in %G_0%, then you can refine the partitions in such a way that the ratio of the energy, or mean square density, of %G% to that of %G_0% goes up. The idea would be that although the mean square density of %G_0% could well be going up all the time (since %G_0% is not a quasirandom graph), the mean square density of %G% would be going up faster so the iteration would have to terminate. As I say, this is probably a hopeless strategy, but it gives an idea of the kind of thing that I was hoping one might be able to do, and there might be ways of modifying it that made it more promising."},{"username":"gil-kalai","timestamp":"2009-02-04T20:02:00.000Z","contents":"A little thought about the issue Tim raised in 31\\. (Sorry if it duplicates some later comment or link.) The question Tim ask was what conditions (Fourier or different) will ensure semirandomness for Sperner and for our combinatorial line question for k=3, namely, a condition for a set to contain approximately “the right number” of combinatorial lines. One possibility to consider is to ask for density on a set X of solutions for arbitrary equation of the form %a_1x_1 + a_2x_2 +\\dots +a_nx_n \\in T.% (T can be any set but we can just think about a single number.) If the intersection of A and X is equal to its expected value up to a small additive error, for every X, does it suffices to imply that A has the right number of pairs %x \\subset y% ? (for the Sperner case) or even the right number of combinatorial lines for k=3? Are you aware of counterexamples? Talking about general solutions of linear equations may help also for cases where Fourier analysis gives a result but we would like to have a better one. (upper bounds on cup set? Influences of sets…) A difficulty with such a suggestion is that you need to argue recursively about dense sets inside sets which are the solution of a system of equations of the above kinds and maybe even more complicated. Also I do not see an argument that will show that if the number of solutions of linear equations is right then the number of combinatorial lines is right (for k<=3). Another possibility (or perhaps something forced by the above possibility if you try to prove things) is to consider for X sets which are described by even more general arithmetic or other type of circuits. Such sets may also be useful for lower bound examples; I find it yet hard to believe that the examples we know (for the various questions) are in the right ball park as reality."},{"username":"ryan-odonnell","timestamp":"2009-02-04T20:46:00.000Z","contents":"Obstructions. Tim, in your #105, I don’t quite see what you’re saying… And does it hold for the set %A% of all strings which contain a “run” of at least %\\log n - \\log \\log n% 2’s somewhere within them? (This example has no fixed “blocks”.)"},{"username":"ryan-odonnell","timestamp":"2009-02-04T20:52:00.000Z","contents":"Obstructions: These examples from #92 were really just based on an example (“Tribes”) from the %{}[2]^n% scenario. Like Jozsef#104, I haven’t wrapped my head around the counting/normalization/distribution used in Tim’s #95; this is why in my #100 I was writing about Boris’s #44 setup instead. I feel that before thinking too much about obstructions to combinatorial-line-uniformity in %{}[3]^n%, we should try to clarify things in the simpler scenario of obstructions-to-Sperner in %{}[2]^n%. In #100, I tried to get across that I can’t even think what this should mean."},{"username":"ryan-odonnell","timestamp":"2009-02-04T20:53:00.000Z","contents":"Sperner/Kruskal-Katona. Actually, I’m not even sure “obstructions” is the right way to think about Sperner. I think that what Boris said in #58 — that Sperner’s own proof by “pushing shadows around” (I like this phrase, Boris!) — may already be the “alternate combinatorial proof of Sperner” that Terry proposed to look for way back in #4! Can we prove DHJ by “pushing shadows around”? <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span>"},{"username":"jason-dyer","timestamp":"2009-02-04T21:40:00.000Z","contents":"@Tyler, Sune, Terry (on bounds) Nice! I hope to have some code working by the end of the week; %c_5% ought to be within brute force range, but I’m not sure past that. I’ve thought about the flow optimization algorithm, and I’m not convinced that will produce an improvement because of the symmetry of the original arrangment. I do still think there’s some CS trick that will help knock this down to size and get exact values for %c_6% and beyond. For the moment I’m just going to Monte Carlo it so we can get some perspective."},{"username":"gowers","timestamp":"2009-02-04T22:21:00.000Z","contents":"Obstructions. Ryan, 112, I won’t bother trying to explain what I was saying because there’s an even simpler example to show that your set yields a density increase on a subspace: you just fix a run of 1s and let everything else vary arbitrarily. That does both the blocks case and the fixed-length-of-run case."},{"username":"gowers","timestamp":"2009-02-04T23:00:00.000Z","contents":"Big picture, density incrementation, obstructions I’ve just realized that the example in 108 shows that strategy 1 in 107 doesn’t work as stated, or at least not completely straightforwardly. For the benefit of people unfamiliar with Shkredov’s work on corners, let me explain this in some detail. Suppose we try to prove the corners result using a density-increment strategy. That is, we try to prove that if a subset of %\\null[n]^2% doesn’t have roughly the expected number of corners then there’s a substructure inside which %A% is noticeably denser. What is the obvious substructure? Well, we want something that will support corners, so the obvious thing to take is a Cartesian product of two arithmetic progressions with the same common difference. However, this doesn’t work. Suppose we take %A% to be a Cartesian product of two random sets %X% and %Y%. Then there will be too many corners in %A% (because you just need %(x,y+d)% and %(x,y+d)% and you get %(x,y)% for free). But if %P% and %Q% are arithmetic progressions, then %X\\cap P% and %Y\\cap Q% will look like random subsets of %P% and %Q% so we won’t get any density increase. (But see a qualification to this remark later on.) This has an analogue in the Hales-Jewett setting. Suppose we take the example in 108, and we use random sets %X% and %Y% to define it. Then there won’t be a combinatorial subspace contained in the resulting set %A% (or even intersecting it unusually densely) because the set of sizes of the 1-set of a vector in a combinatorial subspace will be a very special kind of set: the set of sums of some positive integers %n_1,\\dots,n_k%, and a random set will intersect such sets randomly. Both these conclusions are, however, false if we allow ourselves to pass to much smaller substructures. For instance, in the first case we could use Szemerédi’s theorem to get ourselves an arithmetic progression contained in %X\\cap (Y+r)% for some %r%, and in the second we could use a cube lemma to get %n_1,\\dots,n_k% such that all sums belong to %X%. (I’m using these statements so that I don’t actually use the fact that %X% and %Y% are random, which in general we are not going to be given.)"},{"username":"ryan-odonnell","timestamp":"2009-02-04T23:17:00.000Z","contents":"Obstructions. Tim, re #115 — thanks! I understand what you mean. I wonder if you could suggest the “quantitatives” to think about for density increment arguments. Let’s work with a different strongly nonuniform set. Let %A% be the set of strings where the plurality digit in the first %n/3% coordinates is %1%, the plurality digit in the second %n/3% coordinates is %2%, and the plurality digit in the third %n/3% coordinates is %3%. This has density %1/27% and is strongly nonuniform. (The “counterexample” %B% is the set of strings where the plurality digit in the first %n/3% coordinates is *not* %1%, the plurality digit in the second %n/3% coordinates is *not* %2%, and the plurality digit in the third %n/3% coordinates is *not* %3%. This has density %8/27% and forms no lines with %A%.) But now it’s hard to improved %A%‘s density by restricting to a smallish-codimension combinatorial subspace. Your best bet is (I assume) to put, say, lots of %1%‘s into the first %n/3% coordinates. But so long as you fix only %o(\\sqrt{n})% %1%‘s, you’ve only improved %A%‘s density by %o(1)%. So I’m wondering what “quantitative aspects” one should hope for. Perhaps %o(1)% increase is enough, or %\\Omega(\\sqrt{n})% codimension is “not too much”."},{"username":"terence-tao","timestamp":"2009-02-04T23:29:00.000Z","contents":"Szemeredi’s proof of Roth’s theorem Good morning again… this topic is perhaps orthogonal to the others, but I think I begin to understand Szemeredi’s proof of Roth’s theorem(see 86) better. For instance, I realised that it has the curious property of proving Roth in Z/NZ but not in %(Z/3Z)^n% (at least, not without some further modification). I also begin to see how the “sparsity” of the corners or DHJ problem is a serious difficulty extending Endre’s argument to this setting. I don’t think this approach is totally hopeless, though, and might be worth pursuing in parallel with the “mainstream” obstructions-to-uniformity/triangle removal approach. Basically, Endre’s proof is based on three observations: 1\\. If A is dense, then it contains a structured set Q (specifically, a medium-dimensional cube). 2\\. If A has no 3-APs, and contains a structured set Q, then the complement of A contains a _large_ structured set, namely 2.Q – A. 3\\. If the complement of A contains a large structured set (or equivalently, if A is “squeezed” into the complement of a large structured set), then there is some subprogression of A of increased density. Combining 1, 2, 3 and the density increment argument one obtains Roth’s theorem in Z/NZ. If we try to prove Roth in %(Z/3Z)^n% rather than %Z/NZ%, then 1 and 2 still work fine, but I can only show 3 using Fourier analysis, but of course Roth in %(Z/3Z)^n% is trivial with Fourier analysis anyway. (Specifically, the question is whether one can find dense subsets A, B of %{}[3]^d% such that A is in the complement of %B + [2]^d%, where d is a slowly growing function of n, and for which A, B have no density increments on a subspace. Easy with Fourier analysis, and I don’t see how to do it otherwise.) Somehow the problem is that %(Z/3Z)^n% is not as “connected” as %Z/NZ%. If we try to prove corners or DHJ, then 1 still works (see 102), but 2 now breaks down, basically because Euclid’s axiom “two points determine a line” begins to fail horribly. Instead of A being squeezed into the complement of a large structured set, it seems that A is only squeezed into a complement of a small structured set, which is saying practically nothing. (For instance, in the DHJ problem, colouring HJ already tells us that the complement of A must contain arbitrarily large combinatorial subspaces (for n large enough), but this seems to be a mostly useless piece of information, although it does at least imply the weak DHJ in 85.) It my be that by localising to a well-chosen piece of %{}[n]^2% or %{}[3]^n% one can mitigate this problem, but I don’t see how yet. And even after problem 2\\. is solved, one still has to grapple with 3., which as we saw was already problematic even in the model %(Z/3Z)^n% case."},{"username":"gowers","timestamp":"2009-02-04T23:33:00.000Z","contents":"Obstructions. Ryan, by “plurality digit” do you mean “digit for which there is a long run” or do you mean “digit that occurs most often”? From your second paragraph I think you must mean the second. In fact, I see that you definitely mean the second, the idea being that you’d expect that digit to occur something like %n/9+c\\sqrt n% times, on average anyway. But I’ll leave this comment here in case it helps anyone else."},{"username":"terence-tao","timestamp":"2009-02-04T23:38:00.000Z","contents":"Density incrementation Re: Ryan.117 (as opposed to Terry.117 <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> _[now corrected — Tim]_, I can tell you what type of density incrementation you need to get in order to get a contradiction that finishes the proof. From the trivial bound %c_{n+1} \\leq 3c_n%, we know that the sequence %c_n/3^n% decreases monotonically to a limit %\\alpha%. DHJ is precisely the claim that %\\alpha=0%, so suppose for contradiction that %\\alpha > 0%. Then we can write %c_n = (\\alpha + o_{n \\to \\infty}(1)) 3^n%, where %o_{n \\to \\infty}(1)% denotes a quantity that goes to zero as %n \\to \\infty%. Note that the decay rate %o_{n \\to \\infty}(1)% here is completely ineffective. Thus, for n large, we can find a set %A \\subset [3]^n% of density %\\alpha + o_{n \\to \\infty}(1)% with no combinatorial lines. In particular, on any m-dimensional subspace, the density of A cannot exceed %c_m/3^m = \\alpha + o_{m \\to \\infty}(1)%. So, in order to get a contradiction, one needs to find a medium-size dimensional subspace on which one has a density increment that is bounded below uniformly in the dimension m of that subspace. A useful fact, by the way, is that upper bounds on density of all subspaces imply lower bounds on density of _random_ subspaces. Indeed, consider a random subspace V of %\\null [3]^n% with two properties: (a) the dimension of V is always at least m, and (b) any two subspaces which are “parallel” in the sense that their wildcard set is exactly the same will be drawn with equal probability. Then I claim that with probability %1 - o_{m \\to \\infty}(1)%, the relative density of A in V will be %\\alpha + o_{m \\to \\infty}(1)%. Indeed, we can condition to fix the wildcard set of V, and observe that the remaining subspaces partition %{}[3]^n%. The mean density of the subspaces is thus the global density %\\alpha + o_{n \\to \\infty}(1)% of A, while from the preceding discussion we know that the maximum density is at most %\\alpha + o_{m \\to \\infty}(1)%, and the claim now follows from Markov’s inequality."},{"username":"terence-tao","timestamp":"2009-02-04T23:57:00.000Z","contents":"Szemeredi’s uniformisation trick In 120 I noted how an extremal set A tends to have density close to %\\alpha% on most m-dimensional subspaces. In Endre’s big paper (the proof of Szemeredi’s theorem), he uses a very nice double counting argument to say a much stronger statement: given any specified dense subset %\\Omega% of %{}[3]^m%, A also has density close to %\\alpha% on “most” copies of %\\Omega%. (He eventually applies this fact to sets %\\Omega% that are the cells of a Szemeredi partition of a certain graph he constructs on %{}[3]^m%.) This trick does not appear to be as well known as it should be (and is absolutely crucial to Endre’s proof), so I am reproducing it here. Here is the formal claim: let %1 \\ll k \\ll d \\ll m \\ll n% (where %x \\ll y% means that “y is sufficiently large depending on x”, and let %\\Omega_1,\\ldots,\\Omega_k \\subset [3]^m%. Then one can find a copy of %{}[3]^d \\times [3]^m% in A such that for every x in %{}[3]^d% and all %1 \\leq j \\leq k%, the relative density of A in %x \\times \\Omega_j% is %\\alpha + o_{d \\to \\infty}(1)%. Proof. We need some intermediate numbers %d \\ll r \\ll l \\ll m%. Pick a random copy of %{}[3]^l \\times [3]^m% in A. By the previous discussion, every x in %{}[3]^l% has a probability %1-o_{m \\to \\infty}(1)% of being such that A has density %\\alpha + o_{m \\to \\infty}(1)% in %x \\times [3]^m%. By the union bound (assuming m is large depending on l), we may thus select a copy of %{}[3]^l \\times [3]^m% such that A has density %\\alpha + o_{m \\to \\infty}(1)% in _every_ %x \\times [3]^m%. For each j, let f_j(x) be the density of A in %x \\times \\Omega_j%, rounded to the nearest multiple of 1/r. Applying coloring Hales-Jewett (because l is large compared to k, d, r), we can pass from %{}[3]^l% to a subspace %{}[3]^d% such that the %f_j% are constant on this subspace for all j: %f_j(x) = a_j%. But by double counting the density of A on %{}[3]^d \\times \\Omega_j% we see that %a_j \\leq \\alpha + o_{d \\to \\infty}(1)%; similarly by double counting the density of A on %{}[3]^d \\times ([3]^m \\backslash \\Omega_j)% we have %a_j \\geq \\alpha - o_{d \\to \\infty}(1)%. The claim follows."},{"username":"ryan-odonnell","timestamp":"2009-02-05T00:01:00.000Z","contents":"Terry, re your first #120, thanks! Er, would it be possible to explain it in the form, “The new dimension %m% needs to be at least %f(n)%, and the density needs to increase by at least %g(m)%.”?"},{"username":"ryan-odonnell","timestamp":"2009-02-05T00:14:00.000Z","contents":"I guess one thing that worries me is that it already seems hard to give explicit (nonrandom) sets %A% — strongly nonuniform or not! — which don’t have a modest density increment. The only examples I know are those of the “modular arithmetic” type discussed by Boris and Tim earlier."},{"username":"jozsef","timestamp":"2009-02-05T00:41:00.000Z","contents":"Sperner Going back to Tim’s removal lemma in Kneser graphs 106 ; It actually says that every large matching in %K_{n,m}% spans at least the expected number edges. This would be a Ruzsa-Szemeredi type theorem in a sparse graph. From a different viewpoint it reminds me to Bollobas’ theorem, which states that any induced matching in %K_{n,m}% has size at most %\\binom{2m}{m}%. There is an algebraic proof of Bollobas’ theorem by Lovasz, it might give us some information about the number of edges spanned by a large matching."},{"username":"gil","timestamp":"2009-02-05T00:48:00.000Z","contents":"((follow on 111.) well, since the numeric values of the variables are not important for combinatorial lines this means that the equations (at least for k=3) can be slightly more general.)"},{"username":"kevin-obryant","timestamp":"2009-02-05T01:19:00.000Z","contents":"Regarding Terry’s post (45), I’ve extended the Green/Wolf version of Elkin’s strengthened form of Behrend’s construction (of sets of integers without 3-term APs) to longer APs. This parallels Rankin’s extension of Behrend’s construction. [http://arxiv.org/abs/0811.3057](http://arxiv.org/abs/0811.3057)"},{"username":"terence-tao","timestamp":"2009-02-05T01:30:00.000Z","contents":"Density incrementation @Ryan.122: This is true… where %f(n)% is any function you choose that goes to infinity, and %g(m)% is a function that you _don’t_ get to choose, and goes to zero very very slowly (and ineffectively) as %m \\to \\infty%. (But you are allowed to make f depend on g, for what it’s worth.) For sake of discussion, suppose the rate of convergence of %c_n/3^n% to %\\alpha% was inverse tower-exponential… then the game is basically “get a density increment of at least %1 / \\log_* m% on a subspace of dimension %m \\geq \\log_* \\log_* n%“. Of course, we don’t know in advance that we have an inverse tower-exponential rate – so you have to prove something like this but with %\\log_*% replaced by some other unknown slowly growing function."},{"username":"terence-tao","timestamp":"2009-02-05T02:04:00.000Z","contents":"Density incrementation One reason, by the way, why it’s so difficult to find dense subsets of %{}[3]^n% that don’t have any significant density incrementation on a large subspace is that, thanks to the k=3 DHJ theorem, they don’t exist! (In particular, any dense subset has to have density 1 on a subspace of dimension %f(n)% for some f going to infinity, by the same argument used in Terry.102.) Of course, we can’t actually _use_ this fact in our proof, as it would be circular."},{"username":"gowers","timestamp":"2009-02-05T03:55:00.000Z","contents":"Sperner For people who aren’t comfortable with induced matchings, here’s a translation of what Jozsef says in 124 about Bollobás’s theorem. An induced matching in a graph means a collection of edges %x_iy_i% such that no %x_i% is joined to any %y_j% with %j\\ne i%. So the theorem is saying that in the graph where you join two sets of size %m% if and only if they are disjoint, the biggest collection of disjoint pairs %(A_i,B_i)% you can find with %A_i% always intersecting %B_j% if %i\\ne j% is the one where you take all subsets %A_i% of a %2m%-element subset of %\\null [n]% and let %B_i=A_i^c% in each case. Jozsef, there clearly needs to be a lower bound on %m% in terms of %n% here: can you remember what it is?"},{"username":"gowers","timestamp":"2009-02-05T04:01:00.000Z","contents":"Metacomment: It would be useful if contributors to this discussion could look at the discussion over on the Questions of Procedure post. I’ve done a poll and there seems to be a small majority in favour of separate new posts for each thread. That would require summaries of the various threads, so I think we need to agree what they are and find out who would be prepared to write the summaries. Jason’s given us some suggestions about what the threads might be, though I think things may have moved on since then. I’ll discuss this over at the other post."},{"username":"terence-tao","timestamp":"2009-02-05T09:07:00.000Z","contents":"The k=2.5 case of DHJ I was trying to adapt Szemeredi’s proof of Roth’s theorem to the k=3 DHJ case, somehow using the k=2 DHJ (Sperner) as an initial step, when I discovered a very strange phenomenon for DHJ, not present for corners or for arithmetic progressions: I cannot logically relate k=2 DHJ to k=3 DHJ! Instead, k=3 DHJ trivially implies the following statement: k=2.5 DHJ: If n is sufficiently large depending on %\\delta%, and %A \\subset [3]^n% has density at least %\\delta%, then there exists a combinatorial line whose first two points lie in A. But I cannot deduce k=2.5 DHJ from the k=2 DHJ, or conversely, basically because %\\null [2]^n% is an incredibly sparse subset of %\\null [3]^n%. Actually, despite the Sperner-type appearance to k=2.5 DHJ, I was not able to come up with a combinatorial proof of this statement at all! Presumably I must be missing something obvious. But in any case, this looks like an obvious test problem to work on, being strictly easier than k=3 DHJ."},{"username":"terence-tao","timestamp":"2009-02-05T09:12:00.000Z","contents":"Szemeredi’s proof of Roth’s theorem This is perhaps only for my own benefit, but I was at least able to extend Szemeredi’s proof of Roth’s theorem (see 86, 118) to the %(Z/3Z)^n% setting. As in 118, we know that the complement of A contains 2.Q – A for some medium-dimensional cube Q. We can represent Q as an increasing sequence %Q_1 \\subset Q_2 \\subset \\ldots \\subset Q_d = Q% as in 86, and use the pigeonhole principle to find a place where %2.Q_i - A% stabilises, i.e. it has almost the same density as %2.Q_{i+h}-A% for some medium-sized h. Thus the set %2.Q_i - A% is almost invariant under the h steps %v_i, \\ldots v_{i+h}%, and so is approximately equal to a union of cosets of the subspace V spanned by these h vectors. So A largely avoids a dense union of cosets of V, so by pigeonhole, it must have significantly increased density on another coset of V, and this is enough for the density increment argument to kick in. But I do not see how to use this sort of argument to prove either DHJ or corners, the problem being that the analogue of 2.Q-A is far too sparse in either setting."},{"username":"ryan-odonnell","timestamp":"2009-02-05T10:26:00.000Z","contents":"The k=2.5 case of DHJ. Terry: how interesting! I sort of had thought of this k=2.5 version at some point (it was in my mind when writing #65). My two thoughts on it were:  \na) it probably should follow from some Sperner-pushing-shadows-around argument;  \nb) trying to extrapolate from it to the k=3 result looked very difficult (due to the incredible sparsity you mention) but worth thinking about (this was sort of in my mind at the end of #114). So I’m surprised and interested to hear that its combinatorial proof is not obvious. Seems indeed like a good problem to think about then."},{"username":"boris","timestamp":"2009-02-05T10:46:00.000Z","contents":"To prove k=2.5 in post Tao.#130 note that we can assume that the number of 1’s 2’s and 3’s is %n/3+O(\\sqrt{n})% of each in all words of %A%. Then look at copies of [2]^n of the following type. For a word %w% in %\\{1,2,3,*\\}% of length n with say %m% stars, consider all the words obtained from %w% by replacing stars with 1’s and 2’s. Hence, for each %w% we get a copy of [2]^n. Density on each such copy is at most %1/\\sqrt(m)%. If %m=o(\\sqrt{n})%, double-counting argument gives DHJ k=2.5."},{"username":"ryan-odonnell","timestamp":"2009-02-05T10:59:00.000Z","contents":"I was just going to say something similar. My version: Suppose %A% has density greater than %\\mu := \\binom{n}{n/2}2^{-n}% in %{}[3]^n%. Then there has to be a particular way of fixing the positions of the %3%‘s such that %A%‘s density in the resulting copy of %{}[2]^n% is still greater than %\\mu%. Now apply Sperner."},{"username":"ryan-odonnell","timestamp":"2009-02-05T11:05:00.000Z","contents":"The “pushing shadows around” Sperner proof gives (I think) nice statements like, “If %A%‘s density is a lot more than %\\mu% then it must contain lots of “half-combinatorial lines”.” My super-duper-naive hope in #114 was that if we could find “enough” of these half-lines in %A% (since, after all, %A% has density %\\delta = \\Omega(1) \\gg 1/\\sqrt{n} = \\mu%), we might be able to find a third point in %A% to finish off one of the half-lines. But such a plan would have to be awfully sophisticated to have a shot at working, I’d guess."},{"username":"jozsef","timestamp":"2009-02-05T12:30:00.000Z","contents":"135. The k=2.5 case of DHJ There is more true, namely there are subcubes in any dense subset of %\\null [3]^d%. An m-dimensional subcube is a set of elements each has some fixed positions where all elements are the same (1,2, or 3) and m blocks where the elements in one block are all one or all two. There is an element for every possible 1-2 combination of the blocks so there are %2^n% elements in a subcube. I tried to use such subcubes for a Szemeredi type argument, but without success. It is also remarkable, that the shadow of such cube (elements where one block has 3-s) can’t be in our set or there would be a line. The shadow also contains an m-1 dimensional – empty – subspace, %\\null [3]^{m-1}%. So, we know that for our set is full with empty subspaces, however I don’t know if we have enough for pushing the density up in another subspace."},{"username":"jozsef","timestamp":"2009-02-05T12:45:00.000Z","contents":"Contd. There is a relevant paper of Gunderson, Rödl, and Sidorenko, where they give sharp bounds on subcubes I mentioned in the previous note. Title: “Extremal problems for sets forming Boolean algebras and complete partite hypergraphs, J. Combin. Th. Ser. A 88 (1999), 342-367\\. ”  \nYou can find it at [http://home.cc.umanitoba.ca/~gunderso/published_work/paper7.pdf](http://home.cc.umanitoba.ca/~gunderso/published_work/paper7.pdf) ."},{"username":"jozsef","timestamp":"2009-02-05T13:08:00.000Z","contents":"Density increment Let me try to make the previous observation clear. We show that if %S\\subset [3]^n% is c-dense and line-free, then for every m there is a %c'>0, c'=c'(c,m)% such that c’ – fraction of the m-dimensional subspaces (%\\null [3]^m%) have empty intersection with S."},{"username":"jozsef","timestamp":"2009-02-05T13:34:00.000Z","contents":"Sperner On Tim’s question; I don’t think that there is a lower bound on m.  \nAlso, I don’t see what would be the right question here. If we take a perfect matching, extremal for Bollobas’ theorem, with m=n/2-k (k is a small integer) then there are only a few edges, but you have to remove half of the sets to destroy all edges. Or, it is late night again, and I overlooked something."},{"username":"gowers","timestamp":"2009-02-05T15:56:00.000Z","contents":"Sperner Ah yes, I was thinking that if %n% was much much larger than %m% then you would do better by choosing lots of completely disjoint pairs %(A_i,B_i)%, but I was overlooking the fact that %A_i% is not allowed to be disjoint from %B_j%. Let me check the arithmetic in your last remark. If %k% is a small integer, then the number of sets in %\\binom{[2m]}{m}% is about %2^{-k}% times the number of sets in %\\binom{2m+k}{m}%, I think. Yes, that’s right, since the ratio of %\\binom{n+1}{m}% to $\\binom{n}{m}$ is %(n+1)/(n+1-m)%, which for us is about 2\\. So if %k% is a small integer, then it is indeed the case that choosing all subsets of %\\null[2m]% gives us a positive fraction of all sets. But in this case, the only edges are the ones that join a set to its complement in %\\null[2m]%. The annoying thing (from the point of view of trying to formulate a decent conjecture) is that this makes the graph sparse. After all, the average degree in the graph, even when %k=1%, tends to infinity with %n%. (It is %\\binom{m+k}{m}% which is around %(n/2)^k%.) However, I think that there is still hope for a less neat conjecture that concerns several layers around the middle. And I also think that there may be some counterpart of this false conjecture for %\\{0,1,2\\}%-sequences that is also false. I think once we identify this counterpart, we will see that we never expected or needed it to be true, and that will make us feel better about whatever %\\{0,1\\}%-version we come up with. I’m busy for a few hours now but will try to make all this more precise later."},{"username":"gowers","timestamp":"2009-02-05T16:32:00.000Z","contents":"Sperner Just one tiny extra remark before I go. For now it seems that one way of rescuing the conjecture is to insist that %k% tends to infinity with %n%, so that %2^{-k}% is no longer a constant fraction. This feels like a natural restriction because it corresponds to insisting that the variable set has size that tends to infinity — which in general it will have to."},{"username":"gowers","timestamp":"2009-02-05T22:10:00.000Z","contents":"Sperner I think what I said in my previous two comments does indeed make sense. To see why, imagine that we tried in the DHJ case to a single triple of slices %\\Gamma_{a+r,b,c}%, %\\Gamma_{a,b+r,c}% and %\\Gamma_{a,b,c+r}%. Then the triangle removal lemma would say that if there were very few combinatorial lines coming from some subsets of these three slices, then one could remove a small proportion of points from the subsets and end up with no combinatorial lines. If %r% is a fixed positive integer, then this is false, but we don’t expect to be able to produce combinatorial lines with a fixed size of variable set: the size will have to depend on the density of %A%. If a triangle removal lemma is true for triples of slices like this when %r% is large, I’m not sure that it implies DHJ, because as I’ve already pointed out you don’t get degenerate triples that involve three different slices — obviously enough. In fact, this is a general point to think about: if we found the most wonderful triangle removal lemma imaginable, would it actually lead to a proof of density Hales-Jewett? (I think there is an analogy with the situation with corners that suggests to me that the answer isn’t obviously no.) Going back to Sperner, my current pair-removal problem, not yet shown to be true and not yet shown to be badly formulated, is to show that if %\\mathcal{A}% and %\\mathcal{B}% are subsets of %\\binom{[n]}{m}%, where %n=2m+k% and %k% tends to infinity slowly with %n%, and if there are few disjoint pairs %(U,V)\\in\\mathcal{A}\\times\\mathcal{B}%, then it is possible to remove a small number of sets (compared with %\\binom{n}{m}%) from %\\mathcal{A}\\cup\\mathcal{B}% and end up with no disjoint pairs."},{"username":"gowers","timestamp":"2009-02-05T22:44:00.000Z","contents":"Sperner In relation to 140, I tried to work out what the regularity lemma says for 1-uniform hypergraphs, but the resulting statement is even more ridiculous than pair removal in the dense case and not worth repeating here. This makes me think that there’s a genuine chance that if we could prove an appropriate disjoint-pair removal lemma, that it might be the extra ingredient that’s needed to mix with triangle-removal to get DHJ. I realize that I’ve sort of said this before."},{"username":"gowers","timestamp":"2009-02-05T22:57:00.000Z","contents":"Sperner Part of the Kneser-graphs philosophy is that they behave rather like the graph on the n-sphere where you join two points if they are almost antipodal to each other. So another question that might be worth investigating is whether there is any chance of an almost-antipodal-pair-removal lemma. It would say something like this. Let %A% and %B% be two subsets of %S^n% such that the measure of all pairs %(x,y)% such that %x\\in A%, %y\\in B% and %\\langle x,y\\rangle\\leq -1+c% is small. Then it is possible to remove sets of small measure from %A% and %B% and end up with no such pairs. It may take some work to turn that into a decent question, if indeed it can be done. I think I know what it means for the set of disjoint pairs to have small measure. It should mean small compared with the measure of all such pairs in %S^n\\times S^n% (which is equal to 1 times the measure of the spherical cap %\\{y:\\langle x,y\\rangle \\leq -1+c\\}%). In this continuous context, it might be necessary for the conclusion to say that after the removal what you no longer have is pairs that have inner product less than %-1+c'% for some %c'. I’m slightly suspicious of this variant, because it feels to me as though if you’ve got two sets with _no_ almost antipodal pairs then at least one of them must be very small, unless %c% is something like %Cn^{-1/2}%. In fact, that is indeed the case as it’s saying that the distance from %A% to %-B% is large, so it comes from concentration of measure. I’m not yet sure what happens if you just assume that %A\\times B% contains _few_ almost antipodal pairs. If it is possible for both %A% and %B% to be large, then the result is false, and if it is impossible then it is in a certain sense trivial. But I think this could be a useful observation: it shows that if there is any chance of a statement holding in the Kneser graph, then it must use some property of that graph that distinguishes it from the sphere graph."},{"username":"terence-tao","timestamp":"2009-02-05T23:30:00.000Z","contents":"Sperner Tim, just a small correction to 139: the number k of wildcards can be taken to be bounded by a quantity depending only on the density delta, either for Sperner or DHJ. Indeed, one can foliate %{}[2]^n% or %{}[3]^n% into slices of %{}[2]^{n_0}% or %{}[3]^{n_0}%, where %n_0 = n_0(\\delta)% is the first integer for which Sperner or DHJ applies. By the pigeonhole principle, one of these slices has density at least %\\delta%, and thus contains a combinatorial line with at most %n_0% wildcards. Now, there should also be lines with wildcard length as high as %\\sqrt{n}% or so, I think. I can reason by analogy with the corners setting: suppose one wanted to find corners in a dense subset of the strip %\\{ (a,b) \\in [n]^2: |a-b| \\ll \\sqrt{n} \\}%. One can cover this strip by grids of spacing k and of length at least %n_0% (where %n_0% is similar to as in the previous paragraph), and it seems to me that one should get a corner of length equal to a bounded multiple of k for any specified k between 1 and a small multiple of %\\sqrt{n}%. I think the same sort of argument should work in the DHJ case. But it seems to me that it is somehow easier to fish for lines in the small k end of the pool rather than the large k end, even though the graph is denser in the latter case. Re: 132, 133, 135: thanks, it’s clear now that 2.5 and 2 are equivalent. For some reason I was reluctant to partition the big cube %{}[3]^n% into variable-sized copies of %{}[2]^m%, but I see now that that is the way one should think about it. Jozsef@135,136,137: this fits pretty well with what I said at 118\\. The key step in Endre’s proof of Roth is that a small cube Q is suddenly amplified to a huge structured set 2.Q-A in the complement which has really big density (at least %\\delta%) and still contains tons of cubes (and more to the point, _parallel_ cubes). In our situation we have a much sparser set of cubes in the complement, and they are not all parallel to each other, and it doesn’t look like this is enough to force a significant density increment anywhere. (Note also that one could also get a similar sparse collection of non-parallel cubes in the complement from the colouring HJ theorem.) I tried to combine all this with Endre’s uniformisation trick (121) but without luck (again, the issue is sparsity, the %\\Omega_i% in 121 need to be dense)."},{"username":"jozsef","timestamp":"2009-02-05T23:59:00.000Z","contents":"Sperner I would like to see a removal lemma type argument for Kneser graphs very much and I agree that if k goes to infinity (Tim 138), then there should be a sensible one. Let me mention a light version of the triangle removal lemma, which is still strong enough for applications; If the number of triangles is %o(n^2)% in an n-vertex graph, then one can remove %o(n^2)% edges to destroy a positive fraction of the triangles. So, I don’t think that in the Kneser graph problem we should go for removing all edges, it should be enough that with a few vertices we can destroy much more than expected edges."},{"username":"jozsef","timestamp":"2009-02-06T00:02:00.000Z","contents":"Contd. I meant %cn^2% triangles …"},{"username":"ryan-odonnell","timestamp":"2009-02-06T00:09:00.000Z","contents":"Sperner. Hi Tim, re your #142\\. It is known that for all %c \\leq 0%, among sets %A% and %B% on the sphere of fixed measures, the ones which minimize the measure of pairs %x \\in A, y \\in B% with %\\langle x, y \\rangle \\leq c% are the “concentric” caps. Note that in high dimensions, having inner product %\\leq c% is very much similar to having inner product %\\approx c%. The proof is by shifting. One such proof appears here: [http://www.wisdom.weizmann.ac.il/~feige/Approx/maxcut.ps](http://www.wisdom.weizmann.ac.il/~feige/Approx/maxcut.ps) Based on this it may be possible to calculate the answer to your question. I actually prefer to do these things not on the high-dimensional sphere but on the equally good surrogate, high-dimensional Gaussian space. It is known that for all %\\rho \\leq 0%, the subsets %A% and %B% of fixed Gaussian measure which minimize the probability of %x \\in A, y \\in B%, where %(x,y)% is a pair of correlated Gaussians with covariances %\\rho%, are two “parallel” halfspaces. This is a theorem of Borell:  \n[http://www.springerlink.com/content/n761353703r7p675/](http://www.springerlink.com/content/n761353703r7p675/) Just wanted to get those references out there; will try to think about the ensuing calculations…"},{"username":"jozsef","timestamp":"2009-02-06T00:21:00.000Z","contents":"Sperner Terry @118 142, Yes, We need more structure to apply Endre’s trick. It was easy to apply when (almost) every element is a center of symmetry in the following way; Choose an element of your set (center). Any other element has a unique mirror image and if both are in your set, then your set contains a 3-term arithmetic progression. With HJ, most pairs are not in line, so the “symmetry” is only local."},{"username":"gowers","timestamp":"2009-02-06T00:56:00.000Z","contents":"Sperner Terry — indeed. In fact, I came to a similar realization in the DHJ context at the end of the first paragraph of 140\\. But I suppose one could regard things in another way and say that we’re trying to get the density to tend to zero (very very slowly) and therefore we need %k% to tend to infinity. Ryan — at first I skimmed what you said and thought it was repeating what I had said, and then I realized that it wasn’t at all. Although the sphere question seems genuinely distinct from the Kneser-graphs question, it is definitely good to know that something along these lines is known. If %A% and %B% are concentric caps and are not of tiny measure, then they must reach almost all the way to the equator, at least, so my guess is that that will give them a lot of almost antipodal pairs. So at the moment my money is on few antipodal pairs implying that at least one of %A% and %B% has small measure. If so, then we would have the true-for-not-quite-the-kind-of-reason-we-hoped-for option rather than the false option."},{"username":"gowers","timestamp":"2009-02-06T01:24:00.000Z","contents":"Sperner As I was sitting through my daughter’s piano lesson, my mind wandered back to pair removal in %K_{n,m}%, and I found myself wondering whether we were already done. Almost certainly not, but in the spirit of keeping thoughts public, let me attempt to deduce it from the theorem of Bollobás mentioned by Jozsef in 124. First, here’s the thought that’s in the back of my mind. It’s that the number of edges you need to remove to make a graph triangle free is within a constant of the largest possible number of edge-disjoint triangles in that graph. (This was pointed out to me by Tom Sanders.) The proof is that if you’ve got %m% edge-disjoint triangles then you need to remove at least %m% edges. Conversely, if you’ve got a maximal set of edge-disjoint triangles and there are %m% of them, then if you remove all edges from all those triangles you will be left with no triangles (because any triangle left would be edge-disjoint from the removed ones, contradicting maximality). So the two numbers are equal to within a factor of 3. Now suppose we are trying to get rid of all disjoint pairs %(U,V)% from %\\mathcal{A}\\times\\mathcal{B}.% If we can find %m% such pairs, with no two sharing a %U% or a %V%, then we have to remove at least %m% sets from %\\mathcal{A}\\cup\\mathcal{B}.% Conversely, if we’ve got a maximal set of disjoint pairs and we remove all sets that are involved in those pairs, then we cannot have any disjoint pairs left. I now see that Jozsef understood this way back in 104\\. And unfortunately, it’s not covered by Bollobás’s theorem because that concerns maximal _induced_ matchings rather than maximal matchings. However (and this too Jozsef clearly realized ages ago), this little argument allows us to rephrase the question: if you’ve got a lot of disjoint pairs %(A_i,B_i)% (of sets of size %m% in %\\null [2m+k]%), where all the %A_i% are distinct and all the %B_i% are distinct, then are you forced to have a lot of disjoint pairs %(A_i,B_j)% with %i\\ne j%. Bollobás’s theorem states that you must have at least one such pair. I suppose an averaging argument would lift that up to something, but probably not to within a constant of the trivial maximum. But as Jozsef suggested, the _proof_ of Bollobás’s theorem might do something for us."},{"username":"terence-tao","timestamp":"2009-02-06T01:28:00.000Z","contents":"Obstructions to uniformity It took me a while to catch up with this thread – this is perhaps one thread which is particularly ripe for a fresh start with a post summarising previous progress and listing the known examples. Here are the examples of interesting “obstructing sets” to either DHJ, Sperner, or Kruskal-Katona that I can find in the above threads: Gowers.38: A set A of words x whose last entry %x_n% depends on the parity of the number of 1s in all other entries. Boris.44, Gowers.71, Boris.73, Gowers.76, Gowers.108, Ryan.117: A set A such that for all x in A, the number (a,b,c) of 1s, 2s, and 3s of x respectively in %{}[n]% (or some subset S of indices in %{}[n]%, or using some other weighting on %{}[n]%) lie in a structured set. Gowers.69, 70, 71: A set A such that the probability that %x_i=j% for some %i \\in [n]%, some %j \\in \\{1,2,3\\}%, and x drawn uniformly from A, deviates significantly from 1/3. Ryan.92: A consists of all the sets which contain a medium-sized block of 2s among some special collection of blocks. Ryan.94: Take any existing example of a non-uniform set A and tack on some fixed string of digits (e.g. 23) at the end, increasing n to n+2 (say) and decreasing the density by a factor of 9 (say). It seems that one can unify all of the above examples so far. Given a set A and weighting w on [n] by integers, define the _richness profile_ of A with respect to the weight w to be the function that assigns to each triple (a,b,c), the number of elements of A whose weighted count of 1s, 2s, and 3s are equal to a, b, c, respectively. (In most examples, w is just the indicator function of some set S of indices in [n], so the profile is counting how many 1s, 2s, and 3s there are in S.) A set A has an _standard obstruction to uniformity_ if there is a reasonably large subspace V on which A has equal or larger density, and a weight w such that the richness profile of %A \\cap V% with respect to w, deviates in a structured way from what one would expect in the random case. (This is for instance the case if A has significantly large density on V than it does globally.) By “structured” I mean that the deviation is not pseudorandom in the sense of locating corners %(a+r,b,c), (a,b+r,c), (a,b,c+r)% in the richness profile. This seems to cover all known examples, while still excluding the random example. (Incidentally: have we shown yet that the random set is quasirandom? This seems to be a necessary step if we are to have any hope at all here.)"},{"username":"gowers","timestamp":"2009-02-06T01:50:00.000Z","contents":"Obstructions to uniformity. A couple of things I didn’t quite understand there Terry. If you pull over the obstructions from %\\null [n]^2% (as suggested in 116), do you call that a pseudorandom deviation? Where does that example fit into your scheme above? And what are you referring to when you talk about “the random example” in your last paragraph? Do you mean that your definition of a standard obstruction does not apply to random sets? I think I don’t really understand the last sentence of your penultimate paragraph."},{"username":"ryan-odonnell","timestamp":"2009-02-06T02:44:00.000Z","contents":"Re the last sentence in Tim’s #142: I mentioned this in #82 but perhaps I should have pointed to a later-on paper in the Irit Dinur oeuvre:  \n[http://www.cs.huji.ac.il/~dinuri/mypapers/intersecting.pdf](http://www.cs.huji.ac.il/~dinuri/mypapers/intersecting.pdf) Here she and Friedgut give a mostly complete characterization of large independent sets in Kneser graphs for %m% a little bit less than %n/2%. To be a large intersecting family, you have to nearly be a subset of a large intersecting family which is a “junta”. (A set family %\\mathcal{F}% is a “junta” if a set’s absence or presence in %\\mathcal{F}% depends only on its inclusion/exclusion of a constant number of coordinates from %{}[n]%.) They also discuss the “cross-intersecting” question, which is basically when you have *two* set families — as we do. I would also guess that the improved characterization in their “Conjecture 1.3” may well be provable with “today’s technology”."},{"username":"gowers","timestamp":"2009-02-06T03:54:00.000Z","contents":"Sperner Ryan, I didn’t manage to find an obviously relevant part of the paper you linked to in 82, but the reference you’ve just given does indeed look promising. Do you know enough about their argument to know whether it is robust? That is, if instead of assuming that you have an intersecting (or cross-intersecting) family you just assume that your sets intersect almost all the time, can you still say that there’s some small set of coordinates that explains this phenomenon? I see that their methods of proof are pretty analytic, so maybe there’s some hope here."},{"username":"ryan-odonnell","timestamp":"2009-02-06T04:50:00.000Z","contents":"Hi Tim. Since the proof is analytic, it should almost surely be fine if the initial set(s) are almost always intersecting. Perhaps an even better reference is the proof of Thm. 1.1 in the last 4 pages of this intermediate paper:  \n[http://www.cs.huji.ac.il/~dinuri/mypapers/DFR.pdf](http://www.cs.huji.ac.il/~dinuri/mypapers/DFR.pdf)"},{"username":"gowers","timestamp":"2009-02-06T07:04:00.000Z","contents":"Sperner. I’m in the middle of writing a summary of the Sperner thread (as well as a few other related ones), to be used when this discussion splits up. At the moment the plan is to have three new discussions. While writing the summary, I had the following thought. When proving the corners result, we can use a weaker triangle-removal lemma where the hypothesis is not that there are few triangles, but the stronger hypothesis that each edge is contained in at most one triangle. The pair-removal equivalent of this is that for each %U% the only disjoint pair is %(U,U^c)%. This starts to sound more Bollobás-like, though the sets %U% no longer all have the same cardinality. So I have two questions. 1\\. Is there a variant of Bollobás’s theorem where the sets don’t all have to have the same size (though I don’t mind insisting that the all have size close to %n/2%)? 2\\. If so, can we generalize its proof to a triangle-removal lemma that uses as its hypothesis that the only triangles in the tripartite graph are degenerate? This is a strategy that could in principle do the whole thing, so I’m expecting difficulties to arise very quickly … Unfortunately, I must go to bed now, so finishing the summary will have to wait till tomorrow."},{"username":"terence-tao","timestamp":"2009-02-06T07:06:00.000Z","contents":"Metacomment: All discussion on upper and lower bounds for %c_n%, and proof strategies related to Szemeredi’s proofs of Roth’s theorem, should go to the new thread on my blog, [http://terrytao.wordpress.com/2009/02/05/upper-and-lower-bounds-for-the-density-hales-jewett-problem/](http://terrytao.wordpress.com/2009/02/05/upper-and-lower-bounds-for-the-density-hales-jewett-problem/)"},{"username":"terence-tao","timestamp":"2009-02-06T07:18:00.000Z","contents":"Obstructions to uniformity @Tim.149: I’m thinking of a subset of %{}[n]^2% as being pseudorandom for the corners problem if it does not correlate with any Cartesian products (or if it has the right number of rectangles, or is small in what I like to call the “Gowers %\\Box^2% norm”). The example in 116, in particular, is not pseudorandom in this sense, being the Cartesian product of two random sets. So I will label this example (pulled back to %{}[3]^n% as a standard obstruction to uniformity (at least for one of the three indices 1, 2, 3 of a combinatorial line; for the other two indices one has to modify the notion of a Cartesian product slightly). By the “random example”, I take A to be a random subset of %{}[3]^n% of a fixed density %\\delta%. This should have no noticeable density increment or other fluctuation in the richness profile for any subspace of dimension bigger than, say, %\\log^{10} n%. But I haven’t checked yet (presumably it is an easy entropy calculation) that A is quasirandom in the sense you want, i.e. the number of lines between A, B, and B is the expected number for any large B. Obviously, this is something that has to be true if the obstructions to uniformity approach has any chance of working."},{"username":"rmoth","timestamp":"2009-02-06T11:08:00.000Z","contents":"I have a suggestion – a blog is perhaps the wrong format for this type of experiment (or future experiments). Its sequential format obviously presents difficulties in following/retrieving information. What you want is a well structured webforum where users can create new threads on a whim, and all subposts to those threads can be easily followed – something with a directory tree structure where posts can be followed in a hierarchy. You may even be able to find a forum that allows you to highlight threads spawned from posts in other threads – i know one exists…i just can’t remember the name of it – though i might be thinking of news reader….but a news group is probably getting beyond the scope of this initial experiment. When you start getting into hundreds, the blog is too one-dimensional to be efficient in following a line of thought. You most likely don’t need a newsgroup (not a public one anyway) – just a private forum with enough goodies to make this thing more accessible, otherwise I think you’re going to get swamped (think a thousand+ posts). just a thought."},{"username":"jozsef","timestamp":"2009-02-06T11:48:00.000Z","contents":"Sperner There is a removal lemma for Kneser graphs! It holds in a certain range; “If a large subgraph has many but not too many edges, then one can remove a small fraction of the vertices destroying most of the edges.”  \nSome notations: %G_N% is an N-vertex subgraph of %K_n,m% the Kneser graph defined by the m-element subsets of the n-element base set. The graph has e edges and D denotes the largest degree and d=2e/N is the average degree.  \nIf the vertices having degree at least Cd (for some large constant C) are incident to most of the edges, then we are done; removing N/C vertices destroys most of the edges. So, we can suppose that D<Cd (after throwing away the large degree vertices) As we will see, D<Cd is not possible in a certain vertex (edge) range.  \nI will follow it in a new windows, I want to check what did I write.(latex)"},{"username":"link-starbureiy","timestamp":"2009-02-06T11:53:00.000Z","contents":"Tim, I’ll publicize this on my blog, as well. This seems like a decent-enough collaborative scholastic."},{"username":"jozsef","timestamp":"2009-02-06T12:12:00.000Z","contents":"Contd. Well, I would change a few typos if I could, but it is OK. We are going to use the “permutation trick” of Lubell. For any of the n! permutations we say that it separates the disjoint pair A,B if no element of B precedes any element of A. Any of the n! permutations can separate at most %D^2% pairs (edges) from %G_N%. Indeed, consider the set A which ends first, it would be separated from at least D+1 sets if there were %D^2+1% separated pairs. Or, the rightmost set B (starting last in the permutation) would be separated from at least D+1 sets, which would mean that D was not the largest degree in %G_N%. The number of permutations separating two elements is %n!/\\binom{2m}{m}%. Collecting the two sides of this double counting we have %en!{\\binom{2m}{m}}^{-1}\\leq D^2n!% Using the initial bound on D, our final inequality is %N^2\\leq C^2{\\binom{2m}{m}}e%. Conclusion: If the previous inequality fails, then there are less than N/C vertices in $G_N$ which are covering most of the edges."},{"username":"jozsef","timestamp":"2009-02-06T13:40:00.000Z","contents":"Sperner contd. I don’t think that the previous inequality is sharp, but I think it gives some nontrivial results. For example, if %m\\sim n-\\sqrt{n}% and we take a positive fraction of the m-element sets, %N=c\\binom{n}{m}%, and the average degree is %o(2^{\\sqrt{n}}% then o(N) vertices (sets) cover almost all edges."},{"username":"jozsef","timestamp":"2009-02-06T13:46:00.000Z","contents":"The previous short note is full with typos. I should go to sleep now…  \nI meant: the size of m is around %n/2-\\sqrt{n}% and that the aver. degree is %o(2^{2\\sqrt{n}})% but not constant."},{"username":"gowers","timestamp":"2009-02-06T14:08:00.000Z","contents":"Sperner Jozsef, I must try to digest what you say. It seems very promising."},{"username":"gowers","timestamp":"2009-02-06T15:06:00.000Z","contents":"OK, any thoughts about Sperner, triangle removal, regularity lemmas in tripartite Kneser-type graphs, etc., should now be given as comments on [a new post](https://gowers.wordpress.com/2009/02/06/dhj-the-triangle-removal-approach/) that is meant to take over this thread of the discussion. If you would like me to modify the summary, then by all means suggest changes. For now, the obstructions-to-uniformity thread stays here, but we should get a summary written for that as soon as we can. _[Added 8/2/09 4.05pm GMT: there is now a [post on obstructions to uniformity](https://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/), so all further discussion of this topic should continue there.]_"},{"username":"boris","timestamp":"2009-02-09T01:23:00.000Z","contents":"Metacomment: one really needs to have a preview function."},{"username":"lior","timestamp":"0006-02-20T05:00:00.000Z","contents":"Bounds for %c''_n% Regarding the capset problem, [here](http://www.math.rutgers.edu/~maclagan/papers/set.pdf) is a review of known values up to %c''_5 = 45%."},{"username":"tyler-neylon","timestamp":"0002-02-20T05:00:00.000Z","contents":"Lower bounds for %c_n% Following Jason Dyer’s suggestion, I empirically found a few more lower bounds for %c_n% using a greedy algorithm on a bipartite graph representing combinatorial lines in %{[3]}^n%. Starting with %n=3%, they are: 18, 50, 140, 420, 1155, 3346, 11340, 32676. The greedy algorithm fails to reach an optimal result beginning with %c_4%, which has known value 52 (as noted above). It was also interesting to observe that the resulting graph was in some sense very skewed – a few points are in many more lines than most of the points. More specifically, if %x\\in \\Gamma_{a,b,c}%, then %x% is in %2^a + 2^b + 2^c - 3% combinatorial lines. Intuitively, most points are in the central %\\Gamma% slices, which are the least connected, and these slices “mostly” don’t share lines, hinting at why we need very dense sets to hit every line. The source code – it’s in python – is available here:  \n[http://thetangentspace.com/wiki/Hales-Jewett_Theorem](http://thetangentspace.com/wiki/Hales-Jewett_Theorem)"},{"username":"tyler-neylon","timestamp":"0003-02-20T05:00:00.000Z","contents":"Lower bounds for %c_n% I made a mistake in my argument that %145 \\le c_5% in comment 83 on the original post. In that argument, if you remove the indicated points with %a=0, b=1% to block the %0,1-%lines in %D_n%, the remaining set will still include the line %01\\ast\\ast\\ast%, for example. Sorry about the mistake. We still know, at least, that %140 \\le c_5 \\le 156=3c_4%, using the last post and %c_4=52%."},{"username":"jason-dyer","timestamp":"0007-02-20T05:00:00.000Z","contents":"Greedy algorithm Tyler, my Python is rusty, but I am correct in assuming the code does not randomize the choice of a node to remove when there’s a tie? If that’s the case, that would be the next step. Also, this brings up something related to **Question I.B**: **Question I.B.1** In what exact circumstances does the greedy algorithm fail; is there some larger structural concern besides corners and triangles? **Question I.B.2** Would it be possible to automate the program to avoid those circumstances?"},{"username":"tyler-neylon","timestamp":"0004-02-20T05:00:00.000Z","contents":"Algorithms Jason: Yes, that code is not randomized. I guess you might get better numbers by randomizing a bit, or by trying to improve on a first guess answer through some kind of residual graph technique. Another thought is that we can find a blocking set of minimal size as a hypergraph transversal problem. I’m a little confused about how hard this problem is. In [Lund, Yannakakis], they apparently show the problem as being NP-hard to approximate:  \n[http://portal.acm.org/citation.cfm?doid=185675.306789](http://portal.acm.org/citation.cfm?doid=185675.306789) On the other hand, a quick scholar.google search brings up several papers that claim to have efficient algorithms. It probably helps that we have a 3-uniform hypergraph. If finding a minimal hypergraph transversal is feasible, that might help a lot toward question I.B. If the algorithm is simple enough, it might help generate more bounds for %c_n%. Since it explicitly finds minimal blocking sets, it is also finding the sets considered in question I.C."},{"username":"jason-dyer","timestamp":"0006-02-20T05:00:00.000Z","contents":"Algorithms The problem with any NP-complete results is they seem to be on general cases. Even when restricting to 3-uniform hypergraphs, our graph is a lot more symmetrical than an arbitrary one. Surely there’s something about that we can exploit? Mathematically describing those symmetries might be an interesting exercise (although perhaps someone already has in the original 150+ comment thread)."},{"username":"terence-tao","timestamp":"0007-02-20T05:00:00.000Z","contents":"Algorithms One possible way to exploit symmetries is to work in (a,b,c) space rather than the cube %{}[3]^n%. Indeed, note that the set of all %(a,b,c)% with %a+b+c=n% forms a triangular lattice of length %n+1%, and that each triple (a,b,c) comes with a “weight” %\\frac{n!}{a! b! c!}%, which is the number of strings in %{}[3]^n% with a 1s, b 2s, and c 3s. Now pick some subset B of this lattice which contains no equilateral triangles %(a+r,b,c), (a,b+r,c), (a,b,c+r)%. Then the set %\\Gamma_B := \\bigcup_{(a,b,c) \\in B} \\Gamma_{(a,b,c)}% has no combinatorial lines, and has cardinality equal to the total weight of all the elements of B. So one now has a two-dimensional problem: maximise the total weight of a subset of the triangular lattice with no equilateral triangles. The best bounds for %c_n% for %n \\leq 4% all take this form. For instance, the %c_4=52% example takes B to be the entire triangle except for %(0,0,4), (1,1,2), (0,3,1), (3,0,1), (0,4,0), (0,0,4)%. It looks feasible to search for optimal Bs for the next few ns. Note that the set %D_n% in the main post corresponds to excluding those %(a,b,c)% for which %a=b \\hbox{ mod } 3%; after excluding those, the only equilateral triangles remaining are those whose length is a multiple of 3 and so the hypergraph splits into disconnected components based on the residues of a,b,c mod 3, which should simplify things significantly."},{"username":"sune-kristian-jakobsen","timestamp":"2012-02-20T05:00:00.000Z","contents":"Lower bounds for %c_n% The algorithm in Tao.206 is very effective. I has found the following _without_ useing a computer: For %c_5% take %D_5% and remove %(0,4,1), (0,5,0), (4,0,1), (5,0,0)%. This gives a set without triangles and with 150 elements. Thus %c_5\\geq 150%, and this bound cannot be improved using the algoritm in 206\\. For %c_6% take %D_6% and remove the permutations of %(0,1,5)%. This gives %c_6\\geq 450%, and agian, this cannot be improved using the algorithm in 206."},{"username":"none","timestamp":"2012-02-20T05:00:00.000Z","contents":"proof theoretic analysis Could it be interesting to try to prove there is NO combinatorial proof of the theorem, sort of like Kirby and Paris proved there is no PA proof of Goodstein’s theorem? I’m guessing “combinatorial proof” means one codeable in primitive recursive arithmetic, whose proof theoretic ordinal is omega**omega. So we’d try to use the Hales-Jewett theorem to prove some kind of well-ordering on those point collections, which has order type larger than omega**omega, which most interesting orderings probably would have."},{"username":"sune-kristian-jakobsen","timestamp":"0004-02-20T05:00:00.000Z","contents":"Lower bound for %c_n% Correction to 207: When I said that the bound could not be improved using the algorithm from Tao.206 I assume that triples with %a=b \\hbox{ mod } 3% had to be left out of the set. I don’t know if it can be improved without this assumption."},{"username":"sune-kristian-jakobsen","timestamp":"0004-02-20T05:00:00.000Z","contents":"Algorithms The idea in Tao.206 can only prove lower bound for %c_n%. But perhaps it can be modified to prove the values of some %c_n%: Let %A\\subset \\Gamma_{a+r,b,c}, B\\subset \\Gamma_{a,b+r,c}, C\\subset \\Gamma_{a,b,c+r}%. Now we want some inequality in %|A|, |B|,|C|, a, b, c, r% that holds for every %A,B,C% without combinatorial lines. More on this later. Now, instead of giving each triple %(a,b,c)% with %a+b+c=n% the weight %\\frac{n!}{a! b! c!}%, we give it an integer weight between 0 and %\\frac{n!}{a! b! c!}%. Instead of looking for subsets of the triangular lattice with no equilateral triangles, we look for weightings where the weights of the points in each equilateral triangle obey the above inequality. One example of such an inequality could be: %|A|+|B|+|C|\\leq% the sum of the two greatest of %\\frac{n!}{(a+r)! b! c!}, \\frac{n!}{a! (b+r)! c!}, \\frac{n!}{a! b! (c+r)!}%. I don’t think this inequality is strong enough, because we want to “remove” more than 1/3 of the total weight, but perhaps we can find some stronger inequalities."},{"username":"jason-dyer","timestamp":"0007-02-20T05:00:00.000Z","contents":"Sequence for %c_n% Sune.207 leaves [this sequence](http://www.research.att.com/~njas/sequences/A052979) as the last one standing in the OEIS. At our current rate of progress I suspect we can at least get an exact value for %c_5%, so I’d say wait a bit longer on sending our sequence in."},{"username":"jason-dyer","timestamp":"0008-02-20T05:00:00.000Z","contents":"Algorithms Here’s a summary of the different possible algorithm tracks. 1\\. Randomize the greedy algorithm, then run multiple times to see if any better lower bounds come out. If this doesn’t get any result that improves over Tao.206, that suggests that Tao.206 is optimal. 2\\. Implement Tao.206 with the dihedral group. As Sune.207 discovered this can be done well even by hand, so it might also be worth it to generate some graphics of lattices with the weights put in to let people try to solve as a puzzle — if anything in this project could be done by a non-expert, this is it. 3\\. Implement Tao.206 without the dihedral group. If no improvement is found then it would be worthwhile to attempt a proof that the dihedral group can *always* be removed. 4\\. Write a brute force confirmation algorithm that will leave no question once a %c_n% is found that it’s exact."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"Upper and lower bounds I added a table to the main post to reflect the progress so far on %c_n, c'_n, c''_n% for n up to 7\\. (I may expand the table up to n=10 or so at some later point.)"},{"username":"jason-dyer","timestamp":"0009-02-20T05:00:00.000Z","contents":"(Meta) Quick fix: you put 1115 when it’s 1155 for %c_7%"},{"username":"sune-kristian-jakobsen","timestamp":"0009-02-20T05:00:00.000Z","contents":"Upper and lower bounds. Shouldn’t the upper bound for %c_5% be 156? And I think the next upper bounds should be higher too."},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Thanks for the corrections! I decided to go ahead and extend the table to n=10, though I am sure some of the bounds here could be optimised further."},{"username":"sune-kristian-jakobsen","timestamp":"2010-02-20T05:00:00.000Z","contents":"Algorithm / Idea for bounding the %|A|,|B|,|C|% in 210 This is almost a copy of 340: In 331 Gowers gives a proof that if %\\mathcal{A}% is a antichain, %\\sum_{A\\in\\mathcal{A}}\\frac{1}{\\binom n{|A|}}\\leq 1%. Using the same idea it is possible to show that if %\\mathcal{A}% is a family, that intersects every chain %\\emptyset=A_0\\subset A_1\\subset A_2\\subset\\dots\\subset A_n=[n]%, then %\\sum_{A\\in\\mathcal{A}}\\frac{1}{\\binom n{|A|}}\\geq 1%. Notice that when the elements of %\\mathcal{A}% is only allowed to have one of two given sizes, these two theorems are equivalent. The later theorem seems to be easier to generalize to the k=3 case, and it might be useful, at least in the 200-thread."},{"username":"sune-kristian-jakobsen","timestamp":"2010-02-20T05:00:00.000Z","contents":"Algorithm. This is almost the same as I suggested in 215, but simpler and weaker: The chance that a random line with a 1s, b 2s, b 3s, and r wildcard intersects %A\\subset \\Gamma_{a+r,b,c}% is %\\frac{|A|}{\\binom n{a+r,b,c}}%. Using this and the similar for b and c we get  \n%\\frac{|A|}{\\binom n{a+r,b,c}}+\\frac{|B|}{\\binom n{a,b+r,c}}+\\frac{|C|}{\\binom n{a,b,c+r}}\\leq 2%, if %A\\cup B\\cup C% doesn’t contain any lines. Still, I don’t think this inequality is strong enough. I think something stronger is true when A, B and C all are non-empty."},{"username":"michael-peake","timestamp":"0001-02-20T05:00:00.000Z","contents":"Lower bounds for c_n  \nFollowing Jakobsen.207 who used Tao.206  \nI got a lower bound for c_7 of 1302: Remove {016,106,052,502,151,511,160,610} from D_7.  \nand a lower bound for c_8 of 3780: Remove {017,107,026,206,125,215,071,701,161,611,080,800,260,620} from D_8"},{"username":"michael-peake","timestamp":"0009-02-20T05:00:00.000Z","contents":"Lower bounds for c_n It is curious that the current lower bounds for c_2, c_5 and c_8 are respectively one-third of the current lower bounds for c_3, c_6 and c_9. I have a lower bound for c_12: 287892, built from (abc) = (A02,705,423,165,462) and permutations, with A=10); and a lower bound for c_15 of 7376850, built from (abc) = (40B,13B,708,168,735,465,762) and permutations, with B=11."},{"username":"terence-tao","timestamp":"2010-02-20T05:00:00.000Z","contents":"Lower bounds for %c_n% Hi Michael! I propagated your lower bounds back in n using (2), and they seem to give quite good bounds. Certainly our accuracy for %c_n% is much superior to what we currently have for %c'_n% or %c''_n%. I also like the suggestion in Jakobsen.210 to try to understand the ways in which a line-free set can intersect %\\Gamma_{a+r,b,c}, \\Gamma_{a,b+r,c}, \\Gamma_{a,b,c+r}%. A test problem that might be susceptible to brute force attack is the following: consider the set %S := \\Gamma_{1,1,2} \\cup \\Gamma_{1,2,1} \\cup \\Gamma_{2,1,1} \\subset [3]^4%. This is the union of three groups of 12 vertices, and it contains 24 triangles, each of which meets each of the three groups in exactly one vertex, with each vertex belonging to two triangles. Now let %A \\cup B \\cup C% be a line-free subset of S, where %A \\subset \\Gamma_{1,1,2}%, %B \\subset \\Gamma_{1,2,1}%, %C \\subset \\Gamma_{2,1,1}%. The question is: what constraints are there on the triple %(|A|, |B|, |C|)%? A simple double counting argument shows that %|A| + |B| + |C| \\leq 24% (this is already alluded to in 210). And it is easy to see that %(|A|, |B|, |C|)% can attain the values %(12, 12, 0), (12, 0, 12), (0, 12, 12)%. But are these the only ways that %|A| + |B| + |C|% can be as large as 24? If so, this suggests that the most efficient way to build line-free sets is to zero out some of the %\\Gamma_{a,b,c}% and pile all the vertices into the rest – in short, to perform the type of strategy we are already doing from 206 onwards."},{"username":"michael-peake","timestamp":"2012-02-20T05:00:00.000Z","contents":"Lower bounds for c_n I wish to correct the triples for c_12 in my previous post (see below). I have found a way to automate construction of lower bounds,  \nwhen n is a multiple of 3. The current lower bounds for c_{3m} are built like this,  \nwritten in terms of Tao.206’s (a,b,c):  \nc_3 from (012) and permutations  \nc_6 from (123,024) and perms  \nc_9 from (234,135,045) and perms  \nc_12 from (345,246,156,02A,057) and perms (A=10)  \nc_15 from (456,357,267,13B,168,04B,078) and perms (B=11)  \nTo get the triples in each row, add 1 to the triples in the previous row; then include new triples that have a zero. This method gives a lower bound for c_99 that is bigger than 3^98."},{"username":"michael-peake","timestamp":"0002-02-20T05:00:00.000Z","contents":"Lower bounds for %c_n% Hi Terry. An answer to your test question is that  \nyou can keep 8 from each group of 12: Remove those, such as 0201,  \nwhose doubled digits are in positions 1 and 3, or  \npositions 2 and 4\\. This removes all lines because the  \nwild-card digit has to match the digit two positions away  \nonce in each line."},{"username":"sune-kristian-jakobsen","timestamp":"0003-02-20T05:00:00.000Z","contents":"Algorithm / Upper bounds. One way to show that the idea in 210 can _not_ work would be the following:  \nFind a weighting w, such that for every a,b,c and r, %a+b+c+r=n%, there exist sets %A\\subset \\Gamma_{a+r,b,c}, B\\subset \\Gamma_{a,b+r,c}, C\\subset \\Gamma_{a,b,c+r}% with %|A|=w((a+r,b,c)), |B|=w((a,b+r,c)), |C|=w((a,b,c+r))% and no lines in %A\\cup B\\cup C%, but with %\\sum_{a+b+c=n} w(a,b,c) > c_n%. I don’t know if such a weighting exist, but in the k=2 case (Sperners theorem) it does: Let %w(a,b)=\\frac{1}{3}\\binom n{a,b}% if %n/3 < a < n/2% and 0 otherwise. Now for a given a, b and r we can choose a set A, of elements x with %x_1=2% and a set B, of elements x with %x_1=1%. These set are big enough, and don’t contain any lines. And the sum of the weight function is almost %\\frac{2^n}{3} > \\binom n{n/2}%."},{"username":"sune-kristian-jakobsen","timestamp":"0004-02-20T05:00:00.000Z","contents":"Alogrithm/ Upper bounds The obvious generalization to the k=3 case works: Let %w(a,b,c)=\\frac{1}{4}\\binom n{a,b,c}% if %a,b,c \\geq n/4%. Now we can choose a set A with elements x, such that %x_1=3% and a set C with elements x, such that %x_1=1% and a set B. Now %A\\cup B\\cup C% don’t contain any lines. This weight gives a density of almost ¼. So the idea in 210 won’t work for large n. Still, I think it might be interesting to look at the questions Tao asked in 219 for general a, b, c and r."},{"username":"michael-peake","timestamp":"0005-02-20T05:00:00.000Z","contents":"Lower bounds of %c_n% There is a connection between %c_4%, %c_5% and %c_6%:  \nOf the 450 length-six sequences defined in Peake.219,  \n150 begin with a 1….., and 52 begin with 12….. The same thing applies with the current lower bounds of c_7, c_8 and c_9 It also gives lower bounds of 32864 for c_10, and 837850 for c_13"},{"username":"terence-tao","timestamp":"0007-02-20T05:00:00.000Z","contents":"Lower bounds for %c_n% @Peake.219: This is quite interesting. I wonder if it is possible to extract an asymptotic for the lower bound obtained by this method as %n \\to \\infty% (i.e. Question II.C). It is also interesting to see that %c_n/3^n% is really decaying very slowly as %n \\to \\infty%. It may be time to set up some sort of communal spreadsheet for all this data – I’ll have a look into this. @Jakobsen.220: Ah well, it was worth a shot. (Note, by the way, that the bound %|A|+|B|+|C| \\leq 24% in 219 implies the sharp bound %c_4 \\leq 52%, since a line-free set must omit at least one element from %\\Gamma_{0,0,4} \\cup \\Gamma_{1,0,3} \\cup \\Gamma_{0,1,3}% and similarly for cyclic permutations; so a better understanding of when equality occurs in %|A|+|B|+|C|=24% would lead to a classification of when one has the maximal cardinality of %c_4=52%, which should assist with the upper bound for %c_5% as with Jakobsen.90). [_Update_, Feb 8: I made an arithmetic error here, this inequality is not enough by itself to get to 52; one also needs to figure out how to relate %\\Gamma_{2,0,0}%, etc. into the mix. But the general point may still be valid.] In light of the discussion in Gowers’ threads, it seems that the Kruskal-Katona theorem, [http://en.wikipedia.org/wiki/Kruskal%E2%80%93Katona_theorem](http://en.wikipedia.org/wiki/Kruskal%E2%80%93Katona_theorem) is likely to be relevant here, at least for large n. @Peake.223: Hmm. Does this mean that all of our optimal counterexamples are in fact nested (e.g. is the %c_6% example a slice of the %c_7% example?) This may be a transient phenomenon, but still an interesting one."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"Spreadsheet OK, I _think_ I’ve set up a collaborative spreadsheet at [http://spreadsheets.google.com/ccc?key=p5T0SktZY9DsU-uZ1tK7VEg](http://spreadsheets.google.com/ccc?key=p5T0SktZY9DsU-uZ1tK7VEg) to track the current data we have for %c_n, c'_n, c''_n%. It should be editable to anyone with a google account. It should automatically propagate the bounds (1), (2) to any new bound entered. (I also have a backup copy in case something goes wrong.) One should presumably be able to program the computations in 219 into here too. Please feel free to contribute to it."},{"username":"jason-dyer","timestamp":"0001-02-20T05:00:00.000Z","contents":"%c''_n% Just a quick note: %c''_n% is *not* in the OEIS."},{"username":"michael-nielsen","timestamp":"0006-02-20T05:00:00.000Z","contents":"This has perhaps already been pointed out, but there is at most one entry in the Encyclopedia of Integer Sequences consistent with what is already known about %c_n%. That is <a>A052979</a>. It matches %c_n% exactly where we already know the value, and matches the (current) bounds for at least the first fifteen entries."},{"username":"michael-peake","timestamp":"0007-02-20T05:00:00.000Z","contents":"Spreadsheet Hi, I looked at the spreadsheet, but can’t edit it.  \nI logged into Google, but the Edit menu is grayed out.  \nDoes anyone else have this problem?"},{"username":"terence-tao22","timestamp":"0007-02-20T05:00:00.000Z","contents":"Edel I’ve added the lower bound %c''_6 \\geq 236% from this [paper of Edel](http://www.springerlink.com/content/m55136x765161240/). In [this other paper of Edel](http://www.mathi.uni-heidelberg.de/~yves/Papers/ABound.pdf), an inequality is obtained which in our notation reads %\\displaystyle c''_n \\leq \\frac{3c''_{n-1} + 1}{1 + c''_{n-1}/3^{n-1}}% for %n \\geq 3%, and which thus slightly improves on (2) for this problem. I’ve updated the spreadsheet and the table accordingly. Michael.227: oops, I didn’t set the permissions properly. I think it should be OK now…"},{"username":"ks-chua","timestamp":"0008-02-20T05:00:00.000Z","contents":": %c_n% via optimization For HJ2, there is a well known method for expressing %c_n% as the maximization of a quadratic polynomial over a box %D_n=[0,1]^{2^n}% so that one can apply a continuous optimization method. Let %G% be a graph with vertex set given by all subsets of %{}[n]% and let %(I,J) \\in E_2% be an edge if and only if it defines a line. Then %c_n% is the size of a maximal independent set in %G%. Let %x% be a variable vector of length %2^n%, %x_{i_1...i_n},i_j \\in \\{0,1\\}% which we think  \nof as %x_I, I \\subset [n]% where %I=\\{j: i_j=1\\}% indexing the subset of %{}[n]%. The known quadratic programming problem defining %c_n% is %c_n[2]=\\max_{x \\in [0,1]^{2^n}} \\left( \\sum_{J} x_J -\\sum_{ (I,J) \\in E_2} x_I x_J \\right).% It just occurs to me that a similar result appears to hold for %k=3% and in fact for all %k%. For HJ3, let %G% be the 3-hypergraph with vertex set as before and set %(I,J,K) \\in E_3% to be a 3-edge if and only if it is a line. %c_n% is then the size of a maximal independent set as before.  \nLet %f(x)= \\sum_I x_I-\\sum_{(I,J,K) \\in E_3} x_Ix_Jx_K.% The following then holds, %c_n=\\max_{x \\in \\{0,1\\}^{2^n}}f(x)=\\max_{x \\in D_n}f(x)% To check the first equality, if %V% is an independent set in %G% of size %k%, set %x_J=1% for %J \\in V%, and zero otherwise. Then %f(x)=k% because %V% is independent. So %c_n \\le \\max_{x \\in \\{0,1\\}^{2^n}} f(x)%. Conversely, if for some %x \\in \\{0,1\\}^{2^n}, f(x)=k,% and if there are triplets %x_I=x_J=x_K=1% with %(I,J,K) \\in E_3%, we change %x_I% to 0\\. This reduces %\\sum x_I% by 1 but also reduce the second term of %f(x)% by at least one. So repeating we can find an %x \\in \\{0,1\\}^{2^n}% with %f(x) \\ge k% and %x_Ix_Jx_K=0% if %(I,J,K) \\in E_3%, i.e. an independent set of size %\\ge k%. So %c_n \\ge \\max_{x \\in \\{0,1\\}^{2^n}} f(x)%. The 2nd equality follows from the maximum principle since %f% is a square free polynomial. So an algorithm which can always determine the maximum of a quadratic polynomial in a box will be able to determine %c_n% but the problem is %NP% hard. Maybe one can try to find %c_n% this way using quadratic programming for small %n%. The same should hold for any %k>1% with  \n%f(x)= \\sum_I x_I-\\sum_{(I_1,...,I_k) \\in E_k} x_{I_1}...x_{I_k}.%"},{"username":"kristal-cantwell","timestamp":"0008-02-20T05:00:00.000Z","contents":"A052979 is within the current bounds for c_n for the first 27 entries of  \nthe spreadsheet."},{"username":"kristal-cantwell","timestamp":"0009-02-20T05:00:00.000Z","contents":"The sequence c_n’ matches the sequence A027994 for the first  \nfive values and lies between the upper and lower bounds  \nof c_n’ as calculated in the spreadsheet for the next 23 values,  \nit matches the sequence A027068 for the first five values  \nand lies within the bounds of the next 22 values. These are the only two sequences in the OEIS which could  \npossibly be equal to c_n’."},{"username":"boris","timestamp":"0001-02-20T05:00:00.000Z","contents":"Can one modify Behrend construction to give lower bound on %c_n'%? In a geometric line, the number of 1’s ,2’s and 3’s is %(a+t_1,b,c+t_2)%, %(a,b+t_1+t_2,c)% and %(a+t_2,b,c+t_1)% respectively, where %t_1% and %t_2% denote the number of wildcards of the two types. In other words, if %(a,b,c)% and %(a',b',c')% are the first two signatures, then the third is %(a'+c-c',b,c'+a-a')%. This is similar to when Behrend works: if one point is determined by the other two in a linear manner. Here “linear manner” is somewhat crooked, and I do not see how to apply Behrend, or any of the standard heuristics. Hence, question: how large is the largest subset of %\\{0,\\dotsc,M\\}^3% not containing a configuration of the above form?"},{"username":"michael-peake","timestamp":"0001-02-20T05:00:00.000Z","contents":"Lower bounds of %c_n% I have added some more lower bounds to Terry.225’s spreadsheet.  \nThe sets of (abc) that I have used are the following for N a multiple of 3.  \nI think that they are triangle-free. For N=3M-1, restrict the first digit of a 3M sequence to be 1;  \nFor N=3M-2, restrict the first two digits of a 3M sequence to be 12. For N<21, ignore any triple with a negative entry. For N=6M: (2x, 2x+2, N-4x-2) and permutations (x=0..M-4)  \n(2x, 2x+5, N-4x-5) and perms (x=0..M-4)  \n(2x, 3M-x-4, 3M+x+4) and perms (x=0..M-4)  \n(2x, 3M-x-1, 3M+x+1) and perms (x=0..M-4) (2x+1, 2x+5, N-4x-6) and perms (x=0..M-5)  \n(2x+1, 2x+8, N-4x-9) and perms (x=0..M-5)  \n(2x+1, 3M-x-1, 3M-x) and perms (x=0..M-5)  \n(2x+1, 3M-x-4, 3M-x+3) and perms (x=0..M-5) (N/3-7, N/3-3, N/3+10) and perms  \n(N/3-7, N/3, N/3+7) and perms  \n(N/3-7, N/3+3, N/3+4) and perms  \n(N/3-6, N/3-4, N/3+10) and perms  \n(N/3-6, N/3-1, N/3+7) and perms  \n(N/3-6, N/3+2, N/3+4) and perms  \n(N/3-5, N/3-1, N/3+6) and perms  \n(N/3-5, N/3+2, N/3+3) and perms  \n(N/3-4, N/3-2, N/3+6) and perms  \n(N/3-4, N/3+1, N/3+3) and perms  \n(N/3-3, N/3+1, N/3+2) and perms  \n(N/3-2, N/3, N/3+2) and perms  \n(N/3-1, N/3, N/3+1) and perms For N=6M+3: (2x, 2x+4, N-4x-4) and perms, x=0..M-3  \n(2x, 2x+7, N-4x-7) and perms, x=0..M-3  \n(2x, 3M+1-x, 3M+2-x) and perms, x=0..M-3  \n(2x, 3M-2-x, 3M+5-x) and perms, x=0..M-3 (2x+1, 2x+3, N-4x-4) and perms, x=0..M-4  \n(2x+1, 2x+6, N-4x-7) and perms, x=0..M-4  \n(2x+1, 3M-x, 3M-x+2) and perms, x=0..M-4  \n(2x+1, 3M-x-3, 3M-x+5) and perms, x=0..M-4 (N/3-7, N/3-3, N/3+10) and perms  \n(N/3-7, N/3, N/3+7) and perms  \n(N/3-7, N/3+3, N/3+4) and perms  \n(N/3-6, N/3-4, N/3+10) and perms  \n(N/3-6, N/3-1, N/3+7) and perms  \n(N/3-6, N/3+2, N/3+4) and perms  \n(N/3-5, N/3-1, N/3+6) and perms  \n(N/3-5, N/3+2, N/3+3) and perms  \n(N/3-4, N/3-2, N/3+6) and perms  \n(N/3-4, N/3+1, N/3+3) and perms  \n(N/3-3, N/3+1, N/3+2) and perms  \n(N/3-2, N/3, N/3+2) and perms  \n(N/3-1, N/3, N/3+1) and perms"},{"username":"gil","timestamp":"0003-02-20T05:00:00.000Z","contents":"Even if we are tentatively happy about the bounds (like (6)) based on AP free sets for k=3\\. Regarding problem II.b for higher k’s it looks we should be able to do much better (than just replacing Behrend by the examples for larger k). At least this is suggested by the way our bound changes between the k=2 case and the k=3 case. E.g we can try something like this: map words of length n in an alphabet of 4 letters to words in 3 letters. Say with one letter a representing 1 or 2 letter b representing 2 or 3 and letter c representing 3 or 4. Then try to find a large set of letters in a b and c whose preimage will automatically exclude a combinatorial line of length 4."},{"username":"jason-dyer","timestamp":"0007-02-20T05:00:00.000Z","contents":"(Meta) Kirstal, Terry’s original post already specified [this sequence](http://www.research.att.com/~njas/sequences/A003142) is %c'_n%."},{"username":"jason-dyer","timestamp":"0008-02-20T05:00:00.000Z","contents":"%c_n% via optimization Regarding Chua.229, shouldn’t that be %3^n% rather than %2^n% in the k=3 version of the algorithm? (Otherwise I’m confused how the indexing works.) It looks like this could be the best algorithm to get exact values of %c_n%, although the algorithm is elaborate enough I would be worried about the integrity of the code. Is there at least some implementation for k=2 out there already?"},{"username":"boris","timestamp":"0008-02-20T05:00:00.000Z","contents":"I want to just add that if we restrict the triple %(a,b,c)% to lie in a Cartesian product %A\\times A\\times A%, then “crookedness” disappears and we end up with system of three linear equations in nine variables, whose solutions we want to avoid in %A%. For the next few hours I will not have the time to try adapting Behrend to this setting though."},{"username":"boris","timestamp":"0001-02-20T05:00:00.000Z","contents":"Restriction to Cartesian products is too rough of a proposal. It does not even work for corners. Indeed the points %(x,y), (x+r,y), (x,y+r)% have four distinct coordinates among them, %x_1=x_2, x_2=x+r, y_1=y, y_2=y+r%. They are linked by the equation %x_2-x_1=y_2-y_1%. The largest set avoiding solutions to this equation has size $\\sqrt{n}(1+o(1))$ (this equation usually goes by the name of Sidon equation, and is extremely well-studied). Similarly, straightforward implementation of my proposal in 235 gives the equation %y_2-x_2=y_3-x_1=y_1-x_3% where %x_1=a, x_2=a', y_1=c, y_2=c', x_3=a'+c-c', y_3=c'+a-a'%. Pigeonhole principle tells us that the largest solution-free set is of size %O(\\sqrt{n})%. The question remains: is there a Behrend-type construction?"},{"username":"terence-tao","timestamp":"0004-02-20T05:00:00.000Z","contents":"Boris, this was discussed a little bit in 43, 48 of the original thread. The basic problem is the %t_1=t_2% case (in the notation of your 232) when two of the points on the crooked line coincide. It basically means that one cannot have more than one point on any line %c-a = const%. Conversely, any configuration with this property will be free of crooked lines, which I guess is what leads to the known %3^n/O(\\sqrt{n})% lower bound. A subproblem may be to work out whether one can find denser sets on the set %\\bigcup_{c-a=const} \\Gamma_{a,b,c}% that are free of geometric lines than just a single %\\Gamma_{a,b,c}%, and have a better density than %1/\\sqrt{n}% asymptotically. If so, then we can glue together such sets as %const% ranges over a Behrend set and get a competitive example."},{"username":"michael-peake","timestamp":"0008-02-20T05:00:00.000Z","contents":"Lower bounds of %c_n% The pattern I described in My.232 has a slight error  \nwhen N=6M+3, as x ranges from 0 to M-4, not M-3. This pattern, for N a multiple of 3, leaves no room for any more points – any new point would complete a line. I don’t know the asymptotic for this pattern’s value of c_n/3^n, but  \n2.7 sqrt(log(n)/n) is a good fit when n is between 4000 and 10000"},{"username":"ks-chua","timestamp":"0009-02-20T05:00:00.000Z","contents":"%c_n% by optimization. Sorry I made a mistake in 229\\. The box should be %{}[0,1]^{3^n}%  \nfor %k=3% instead of %2^n% as Jason Dyer noted. Also for  \n%k>2% it is no longer a QP problem and one needs a general  \noptimization routine."},{"username":"tyler-neylon","timestamp":"0003-02-20T05:00:00.000Z","contents":"Algorithms I think finding a minimum-sized hypergraph transversal in NP-hard, even for 3-uniform hypergraphs. We can reduce 3-SAT to this problem. We are given a boolean expression %\\phi_1\\wedge\\phi_2\\wedge\\ldots\\wedge \\phi_k% where each %\\phi_i% is the OR of 3 terms, each term either a boolean value from %(x_i)_{i=1}^m%, or the negation thereof. Form a hypergraph with vertices %x_1, \\neg x_1, x_2, \\neg x_2, \\ldots%, and each %\\phi_i% as a hyperedge. Add a dummy node %x_j'%, and a hyperedge %(x_j, \\neg x_j, x_j')% for each boolean value. These last hyperedges ensure there is no smaller solution than the one corresponding to a satisfying boolean assignment, if one exists. It’s not hard to check that a minimum transversal is size %m% (the number of boolean values) iff there is a satisfying assignment; this completes the reduction. What does this mean for us? If we want a polynomial time algorithm (in the size of the graph), we have to use more of the structure than just the fact that every line has 3 points, assuming %NP\\not\\subset P%. Also recall that the size of our graph itself is already exponential in n."},{"username":"michael-peake","timestamp":"2010-02-20T05:00:00.000Z","contents":"Upper bound for %c_5 < 156% This proof is along the lines of Sune.90  \nSune shows there is just one way to place 18 points in a cube,  \nand that at most 52 points fit in %3^4% Suppose 156 points may be chosen in %3^5%.  \nThere must be 52 points in each slice of %3^4% Divide %3^5% into nine cubes. There are 17 or 18 points in each cube, and one cube in each row and column has 18 points. For example 17, 17, 18  \n18, 17, 17  \n17, 18, 17 Cut the cubes further, into squares. The 18-point cubes must cut into three six-point slices x y and z, but the 17-point squares have more variation pqr, stu, xyz  \nxyz, abc, def  \nghk, xyz, lmn Take the three cubes in the top row, and slice them along a different axis so they are psx, qty and ruz.  \nOne of these cubes has 18 points, and so is xyz; so r=x and u=y. Similar logic in second column gives u=x and c=y. So there is a contradiction. There are four other ways to place the 17-point and 18-point cubes, but they all lead to contradictions. So 156 points can’t be placed in %3^5%"},{"username":"terence-tao","timestamp":"2011-02-20T05:00:00.000Z","contents":"%c_5 < 156% Michael: Nice! I’ve updated the spreadsheet and the table accordingly. Computing %c_5% exactly may well be within reach now."},{"username":"michael-peake","timestamp":"0009-02-20T05:00:00.000Z","contents":"3^5\\. %c_5 < 155% The same proof works for 155 points, except for the case 17 17 18  \n17 17 17  \n18 17 17 and permutations of that. I can assume that, when sliced along  \nany axis, each cube has at least 17 points. Cut the cubes into slices as in Michael.240.  \nSome of the slices must be x,y or z, as shown there pqx rsy xyz  \ntuy abc yde  \nxyz yfg zhk When the pqx cube is cut into squares along any of its three axes,  \nit has an x slice in third position. That forces the placement of  \nthirteen points in the cube, and needs four points to be  \nplaced in the remaining 2x2x2 sub-cube. It turns out that pqx is either  \nyz’x or y’zx, where z’ is a z slice with one point removed. If pqx = yz’x when sliced along one axis, then it is yz’x when sliced  \nalong all three axes. Then the cube paz, from reslicing the diagonal,  \nis yaz along all three axes, which is impossible. So pqx = y’zx when sliced along each axis, and rsy=tuy=zx’y For the same reasons, zhk = zxy’ So cube qbh = zbx along all three axes, which is impossible. I think this contradiction completes the proof."},{"username":"michael-peake","timestamp":"2010-02-20T05:00:00.000Z","contents":"%c_5 < 155% In 243., I made an error in the paragraph that begins “If pqx = yz’x”.  \nI just need the fact that a can’t have five points if yaz has no lines.  \nAlso, at the end, b can’t have five points if zbx has no lines."},{"username":"michael-peake","timestamp":"0005-02-20T05:00:00.000Z","contents":"%c_5 < 155% Recall that there is just one pattern to fit 18 points in a cube;  \nthe three square slices of this pattern (along any axis) are x, y and z. To fit 17 points in a cube, the only way is to remove one point from  \neither xyz, yzx or zxy. This makes the proof in 243\\. easier because the slices are formed  \nby removing points from yzx zxy xyz  \nzxy xyz yzx  \nxyz yzx zxy Now the major diagonal of the cube is yyy, and six points must be removed from that. Four of the off-diagonal cubes must also lose points. That leaves 152 points, which contradicts the 155 points we started with."},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"%c_5 < 155% Great! I’ve updated the table again (I see that the spreadsheet was already updated, excellent). It would be interesting to see what the 150 lower bound example looks like in xyz notation, this may clarify what needs to be done to narrow the upper and lower bounds further."},{"username":"kristal-cantwell","timestamp":"2012-02-20T05:00:00.000Z","contents":"Let us look at the set of 150  \npoints which form the lower  \nbound for c_5  \nIt would consist of all points whose sum is not zero mod 3 with  \nall points with 5 coordinates equal to 1 or or all 5 points equal to 2 removed  \ncombined with removal of all points  \nwith one coordinate equal to 3 and the rest equal to  \n1 and the removal of all points with one coordinate  \nequal to 3 and the rest equal to 2 This gives the following  \n17 17 18  \n18 14 17  \n14 18 17 which gives let u equal x with one point removed  \nv = y with one point removed  \nand t =z with one point removed  \nuyz xvz xyz  \nxyz ??? xvz  \n??? xyz uyz  \nNote we have chosen three coordinates to be within  \nthe cube and two other to determine  \nthe position of the cube within the square  \ncoordinates go from  \none to three from left to right  \nand one to three upwards  \nxyz are squares determined  \nby one of the coordinates of the  \ncube. x (or any letter in the leftmost position)  \ncorresponds to the  \nthe square with coordinate equal  \nto one the letters in the next two positions correspond  \nto the squares with the coordinate equal to 2 and 3.  \nThe question marks are for the cubes with 14 points. Are there problems with  \nhaving all the cubes having a value that is close to 18?  \nIs there a set of points having no combinatorial lines  \nwith 5 coordinates varying from 1 to 3 such  \nthat all of the cubes have value 17 or 18?"},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"Conventions for xyz notation I think it will be helpful to fix the conventions for the xyz notation. If we use Cartesian coordinates 13 23 33  \n12 22 32  \n11 21 31 to parameterise %{}[3]^2%, then (if I understand Michael’s posts correctly), x is the six-element set * _ *  \n* * _  \n_ * * (where * denotes elements of x and _ denotes non-elements), y is the six-element set _ * *  \n* _ *  \n* * _ and z is the six-element set * * _  \n_ * *  \n* _ * and xyz is the unique 18-element line-free set in %{}[3]^3% (with the convention that abc is the set whose first slice is a, second slice is b, and third slice is c). Michael, does this match your conventions? Also, when you say “to fit 17 points on the cube, the only way is to remove one point from xyz, yzx, or zxy”, are there some restrictions on what point one can remove? Clearly one can remove any point one wishes from xyz, but there seems to be more constraints for yzx and zxy."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"A hyper-optimistic conjecture from 400-499 Over at the [400-499 thread](http://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity), [Gil Kalai.455](http://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2114) and [Tim Gowers.459](http://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2119) have proposed a “hyper-optimistic” conjecture, and asked whether it could be falsified using the examples from this thread. Let me rephrase it as follows. Given a set %A \\subset {}[3]^n%, define the _weighted size_ %\\mu(A)% of A by the formula %\\mu(A) := \\sum_{a+b+c=n} |A \\cap \\Gamma_{a,b,c}|/|\\Gamma_{a,b,c}|% thus each slice %\\Gamma_{a,b,c}% has weighted size 1 (and we have been referring to %\\mu% as “slices-equal measure” for this reason), and the whole cube %{}[3]^n% has weighted size equal to the %(n+1)^{th}% triangular number, %\\frac{(n+1)(n+2)}{2}%. Example: in %{}[3]^2%, the diagonal points 11, 22, 33 each have weighted size 1, whereas the other six off-diagonal points have weighted size 1/2\\. The total weighted size of %{}[3]^2% is 6. Let %c^\\mu_n% be the largest _weighted_ size of a line-free set. For instance, %c^\\mu_0 = 1%, %c^\\mu_1 = 2%, and %c^\\mu_2 = 4%. As in the unweighted case, every time we find a subset B of the grid %\\Delta_n := \\{ (a,b,c): a+b+c=n\\}% without equilateral triangles, it gives a line-free set %\\Gamma_B := \\bigcup_{(a,b,c) \\in B} \\Gamma_{a,b,c}%. The weighted size of this set is precisely the cardinality of B. Thus we have the lower bound %c^\\mu_n \\geq \\overline{c}^\\mu_n%, where %\\overline{c}^\\mu_n% is the largest size of equilateral triangles in %\\Delta_n%. **Hyper-optimistic conjecture:** We in fact have %c^\\mu_n = \\overline{c}^\\mu_n%. In other words, to get the optimal weighted size for a line-free set, one should take a set which is a union of slices %\\Gamma_{a,b,c}%. This conjecture, if true, will imply the DHJ theorem (see the 400-499 discussion for details). Note also that all our best lower bounds for the unweighted problem to date have been unions of slices. Also, the k=2 analogue of the conjecture is true, and is known as the [LYM inequality](http://en.wikipedia.org/wiki/Lubell-Yamamoto-Meshalkin_inequality) (in fact, for k=2 we have %c^\\mu_n = \\overline{c}^\\mu_n = 1% for all n). If the conjecture is false, then perhaps this will be visible by computing upper and lower bounds for %c^\\mu_n, \\overline{c}^\\mu_n% for small n. By hand I can check that %\\overline{c}^\\mu_n = c^\\mu_n% for n=0,1,2… but perhaps with all the above progress we can also get good bounds for n=3,4,5?"},{"username":"tyler-neylon","timestamp":"0004-02-20T05:00:00.000Z","contents":"A hyper-optimistic conjecture Could someone clarify the definition of %\\bar c_n^\\mu%? As far as generalizations of LYM go, let %d_{abc} = |A\\cap \\Gamma_{a,b,c}|%, and suppose %\\sum_{a+b+c=n} d_{abc}/|\\Gamma_{a,b,c}| \\le f(n)% for all line-free %A%. Then Michael’s examples seem to suggest %f(n)% grows quickly – as opposed to %f(n) = 1% in LYM – so, intuitively, how do we hope for something like LYM? I don’t think this is exactly what’s needed, but we do have %\\sum_{a+b+c=n} d_{abc}p_{abc} \\le 2% where %p_{abc} = (2^a+2^b+2^c-3)/(4^n-3^n)% is the percent of lines passing through any single element of %\\Gamma_{a,b,c}%."},{"username":"terence-tao","timestamp":"0004-02-20T05:00:00.000Z","contents":"A hyper-optimistic conjecture One can define %\\overline{c}^\\mu_n% in two equivalent ways. One of them is that it is the largest weighted size %\\mu(A)% of a line-free set %A \\subset [3]^n% which is the union of slices %\\Gamma_{a,b,c}%. (This is in contrast to %c^\\mu_n%, which is the largest weighted size of _any_ line-free set, not necessarily the union of slices.) Another equivalent definition is that %\\overline{c}^\\mu_n% is the largest subset of the triangular grid %\\Delta_n := \\{ (a,b,c) \\in {\\Bbb Z}_+^3: a+b+c=n \\}% which contain no equilateral triangles. For instance, by deleting the two points (0,0,2) and (1,1,0) from %\\Delta_3% one removes all triangles, whereas removing one point is not enough, and so %\\overline{c}^\\mu_2 = 4%. As for your second question, I don’t think we have yet a strategy for this, but one could imagine some sort of extremal combinatorics argument by showing that a line-free set which is not a union of slices can be “improved” to increase its weighted size, without losing the line-free property. Showing this rigorously, though, would be, well, hyper-optimistic. (Incidentally, what you call f(n) would be what I would call an upper bound for %c^\\mu_n%.)"},{"username":"jason-dyer","timestamp":"0007-02-20T05:00:00.000Z","contents":"Hyper-optimist conjecture Am I understanding this right? Minimal deletion for %\\Delta_3% removes (0,3,0) (0,2,1) (2,1,0) (1,0,2), leaving 6 points. So %\\overline{c}^\\mu_3=6%?"},{"username":"terence-tao","timestamp":"0007-02-20T05:00:00.000Z","contents":"Well, that certainly makes the set triangle-free, so %\\overline{c}^\\mu_3 \\geq 6%. If you know that you cannot make %\\Delta_3% triangle-free by removing only three points, then yes, %\\overline{c}^\\mu_3% would equal 6."},{"username":"jason-dyer","timestamp":"0008-02-20T05:00:00.000Z","contents":"Proof %\\Delta_3% cannot be triangle free removing only 3 points Yes, with only three removals each of these (non-overlapping) triangles must have one removal:  \nset A: (0,3,0) (0,2,1) (1,2,0)  \nset B: (0,1,2) (0,0,3) (1,0,2)  \nset C: (2,1,0) (2,0,1) (3,0,0) Consider choices from set A:  \n(0,3,0) leaves triangle (0,2,1) (1,2,0) (1,1,1)  \n(0,2,1) forces a second removal at (2,1,0) [otherwise there is triangle at (1,2,0) (1,1,1) (2,1,0)] but then none of the choices for third removal work  \n(1,2,0) is symmetrical with (0,2,1)"},{"username":"michael-peake","timestamp":"2012-02-20T05:00:00.000Z","contents":"255. Krystal.247: I think my 245 shows that, if all the cubes  \nhave 17 or 18 points, then you have to delete ten points from  \na full 9×18 pattern of xs, ys and zs. That leaves 152 points,  \nwhich is less than 9*17\\. So at least one cube has to have  \n16 or fewer points. Another pattern of 150 points is this: Take the 450 points  \nin %{}[3]^6% which are %(1,2,3), (0,2,4)% and permutations,  \nthen select the 150 whose final coordinate is 1\\. That gives  \nthis many points in each cube: 17 18 17  \n17 17 18  \n12 17 17 Terry.248: Yes, that matches what I meant by x,y and z;  \nyzx and zxy have a full line on the cube’s major diagonal  \nso you have to remove one of those three points to leave  \na valid seventeen points."},{"username":"jason-dyer","timestamp":"0007-02-20T05:00:00.000Z","contents":"Hyper-optimist conjecture The %\\overline{c}^\\mu_3% case was originally proposed as a puzzle by Kobon Fujimura (who is best known for <a href=”[Kobon Triangles](http://mathworld.wolfram.com/KobonTriangle.html)). I have been searching the recreational mathematics literature for any other mention of our problem but I haven’t found a reference."},{"username":"jason-dyer","timestamp":"0007-02-20T05:00:00.000Z","contents":"(Meta) Coin puzzles Fujimura’s version of %\\overline{c}^\\mu_3% is simply given a pyramid of 10 coins, what is the smallest number you need to remove so there are no equilateral triangles? The most common type of coin puzzle (which goes back to [Dudeney](http://en.wikipedia.org/wiki/Henry_Dudeney)) involves changing one configuration of coins into another with a certain number of moves. For example, invert a pyramid of 10 coins in the smallest number of moves. [This article](http://arxiv.org/PS_cache/cs/pdf/0204/0204002v1.pdf) from the (highly recommended book) [_Games of No Chance_](http://books.google.com/books?id=cYB-ra2T8i4C) analyzes this type of puzzle in detail."},{"username":"sune-kristian-jakobsen","timestamp":"2010-02-20T05:00:00.000Z","contents":"%\\overline{c}^\\mu_n% %\\overline{c}^\\mu_4=9%:  \nThe set of all (a,b,c) in %\\Delta_4% with exactly one of a,b,c =0, has 9 elements and doesn’t contain any equilateral triangles. Let %S\\subset \\Delta_4% be a set without equilateral triangles. If %(0,0,4)\\in S%, there can only be one of (0,x,4-x) and (x,0,4-x) in S for x=1,2,3,4\\. Thus there can only be 5 elements in S with a=0 or b=0\\. The set of element with a,b>0 is isomorph to %\\Delta_2%, so S can at most have 4 elements in this set. So %|S|\\leq 4+5=9%. Similar if S contain (0,4,0) or (4,0,0). So if %|S| >9% S doesn’t contain any of these. Also, S can’t contain all of (0,1,3), (0,3,1), (2,1,1). Similar for (3,0,1), (1,0,3),(1,2,1) and (1,3,0), (3,1,0), (1,1,2). So now we have found 6 elements not in S, but %|\\Delta_4|=15%, so %S\\leq 15-6=9%."},{"username":"sune-kristian-jakobsen","timestamp":"2011-02-20T05:00:00.000Z","contents":"%\\overline{c}^\\mu_n% %\\overline{c}^\\mu_5=12%:  \nThe set of all (a,b,c) in %\\Delta_5% with exactly one of a,b,c=0 has 12 elements and doesn’t contain any equilateral triangles. Let %S\\subset \\Delta_5% be a set without equilateral triangles. If %(0,0,5)\\in S%, there can only be one of (0,x,5-x) and (x,0,5-x) in S for x=1,2,3,4,5\\. Thus there can only be 6 elements in S with a=0 or b=0\\. The set of element with a,b>0 is isomorph to %\\Delta_3%, so S can at most have 6 elements in this set. So %|S|\\leq 6+6=12%. Similar if S contain (0,5,0) or (5,0,0). So if %|S| >12% S doesn’t contain any of these. S can only contain 2 point in each of the following equilateral triangles:  \n(3,1,1),(0,4,1),(0,1,4)  \n(4,1,0),(1,4,0),(1,1,3)  \n(4,0,1),(1,3,1),(1,0,4)  \n(1,2,2),(0,3,2),(0,2,3)  \n(3,2,0),(2,3,0),(2,2,1)  \n(3,0,2),(2,1,2),(2,0,3)  \nSo now we have found 9 elements not in S, but %|\\Delta_5|=21%, so %S\\leq 21-9=12%."},{"username":"christian-elsholtz","timestamp":"0001-02-20T05:00:00.000Z","contents":"About the constant c”(n): the value of c”(6)=112 has been proved by  \nAaron Potechin: Maximal caps in AG (6, 3)  \nJournal Designs, Codes and Cryptography, Volume 46, Number 3 / March, 2008  \n[http://www.springerlink.com/content/h003577g11112308/](http://www.springerlink.com/content/h003577g11112308/) This gives a new set of slightly improved upper bounds: for n=7 to 15 as follows: {7, 292}, {8, 773}, {9, 2075}, {10, 5632}, {11, 15425},  \n{12, 42569}, {13, 118237}, {14, 330222}, {15, 926687} A discussion of the problem on caps, with plenty of references, can also  \nbe found in:  \nYves Edel, Christian Elsholtz, Alfred Geroldinger, Silke Kubertin, Laurence Rackham: Zero-sum problems in finite abelian groups and affine caps.  \nQuarterly Journal of Mathematics 58 (2007), 159-186.  \n[http://qjmath.oxfordjournals.org/cgi/content/abstract/58/2/159](http://qjmath.oxfordjournals.org/cgi/content/abstract/58/2/159)"},{"username":"jason-dyer","timestamp":"0001-02-20T05:00:00.000Z","contents":"%\\overline{c}^\\mu_n% Sune, could you give your explicit list for removals from %\\overline{c}^\\mu_4%? I am unable to reproduce a triangle-free configuration from your description. For example, (4,0,0) (0,4,0) (0,0,4) (2,1,1) (1,1,2) (1,2,1) leaves the triangle (2,2,0) (0,2,2) (2,0,2)."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"Updates to spreadsheet I’ve updated the spreadsheet to take into account the above developments (in particular, adding entries for upper and lower bounds for %c^\\mu_n% and %\\overline{c}^\\mu_n%. I’m using the trivial bounds %\\overline{c}^\\mu_n \\leq \\overline{c}^\\mu_{n+1} \\leq \\overline{c}^\\mu_n + n+1% and %\\overline{c}^\\mu_n \\leq c^\\mu_n \\leq |\\Delta_n| = \\frac{(n+1)(n+2)}{2}% but clearly we should be able to do better than these bounds."},{"username":"jason-dyer","timestamp":"0001-02-20T05:00:00.000Z","contents":"Lower bound for %\\overline{c}^\\mu_n% One easy lower bound is to note that one can always go triangle free by removing (n,0,0), the triangle (n-2,1,1) (0,n-1,1) (0,1,n-1), and all points on the edges of and inside the same triangle. For example, with %\\overline{c}^\\mu_4% remove (0,4,0), (1,2,1), (2,1,1), (1,1,2), (3,0,1), (2,0,2), and (1,0,3). Therefore a lower bound would be %\\frac{(n+1)(n+2)}{2}-1-\\frac{(n-1)(n)}{2}%. (That is, the triangular number of n+1 with (n,0,0) removed and then the triangular number of n-1 removed, referring to the triangle (n-2,1,1) (0,n-1,1) (0,1,n-1).)"},{"username":"michael-peake","timestamp":"0004-02-20T05:00:00.000Z","contents":"Three solutions for %c_4% There are exactly three solutions for 52 points in %{}[3]^4%  \nI use recent notation from %c_5%, x, y and z defined in Terry.248 x’ is x with one point removed; that point in %{}[3]^4% is either 2222 or 3333\\. Likewise for y’ and z’ All three solutions are made from full $latex Gamma_{abc}  \nEach row is a different solution. xyz yz’x zxy’  \ny’zx zx’y xyz  \nz’xy xyz yzx’"},{"username":"jason-dyer","timestamp":"0006-02-20T05:00:00.000Z","contents":"Finishing the calculations I guess that’s what I get for typing and running — the bound at 261 is just 2n. Also, 254 the proof can be much, much simpler: just note (3,0,0) or something symmetrical has to be removed, leaving 3 triangles which do not intersect, so 3 more removals are required."},{"username":"michael-peake","timestamp":"0008-02-20T05:00:00.000Z","contents":"Lower bound for %\\overline{c}^\\mu_n% Another lower bound is 3(n-1), as you keep most of all three sides of the triangle. Remove the corners and the inner triangle."},{"username":"jason-dyer","timestamp":"0008-02-20T05:00:00.000Z","contents":"Lower bound for \\overline{c}^mu_n Michael, I have a counterexample for that at 260."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"Wiki We’ve started up a wiki for the polymath project at [http://michaelnielsen.org/polymath1/index.php?title=Main_Page](http://michaelnielsen.org/polymath1/index.php?title=Main_Page) I’ve put in a page for the proofs of the best bounds for c_n for small n at [http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds](http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds) The last few sections still need some work. Of course, everyone is invited to contribute. (For lengthy new computations, it may be best to put the details on the wiki and announce an abridged version here.) At some point I’ll try to do something similar for %\\overline{c}^\\mu_n%."},{"username":"michael-peake","timestamp":"0008-02-20T05:00:00.000Z","contents":"Lower bound for %\\overline{c}^\\mu_4% You can fit nine points in a 15-point triangle (n=4),  \n(013,022,031,103,202,301,130,220,310)  \nbut the best result for %c_4% only uses seven points.  \n(022,112,121,202,220,310,301)"},{"username":"michael-peake","timestamp":"0009-02-20T05:00:00.000Z","contents":"Lower bound for %\\overline{c}^\\mu_n% Jason, The triangle made up by (2,2,0),(2,0,2) and (0,2,2) is upside down.  \nIt has a constant r=-2, so doesn’t apply to HJ sequences, which  \nuse r as a number of columns. I think you were right the first time, in 254., to do the long-winded proof."},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Wiki Here is a page for %\\overline{c}^\\mu_n%, the computation of which I have dubbed “Fujimura’s problem”: [http://michaelnielsen.org/polymath1/index.php?title=Fujimura%27s_problem](http://michaelnielsen.org/polymath1/index.php?title=Fujimura%27s_problem) Again, some cleanup is required to convert the blog comments into wiki format and to integrate the text smoothly."},{"username":"michael-peake","timestamp":"2010-02-20T05:00:00.000Z","contents":"Upper limit for %\\overline{c}^\\mu_n% From looking at triangles in the lowest two rows, we have %c_n \\leq c_{n-2} + \\frac{3n+2}{2}%"},{"username":"michael-peake","timestamp":"2011-02-20T05:00:00.000Z","contents":"Hyper-optimistic My point in 266\\. was, does that contradict the hyper-optimistic conjecture?"},{"username":"michael-peake","timestamp":"0004-02-20T05:00:00.000Z","contents":"Upper limit for Fujimura ( %\\overline{c}^\\mu_n% ) There are %\\binom{n+2}{3}% triangles, and each point is in n of them, so you must remove at least (n+2)(n+1)/6 points to remove all triangles.  \nSo an upper limit for %\\overline{c}^\\mu_n% is (n+1)(n+2)/3."},{"username":"terence-tao","timestamp":"0007-02-20T05:00:00.000Z","contents":"Hyper-optimistic Dear Michael, it’s not clear yet whether this example violates the hyper-optimistic one, because the extremiser for the %c_4% problem is not going to be the extremiser for the %c^\\mu_4% problem. Indeed, the %c_4% example, involving only seven of the slices, will have a weighted size of 7, whereas the %\\overline{c}^\\mu_4% example has a weighted size of 9\\. To contradict the conjecture we would need to find a line-free set in %{}[3]^4% with a weighted size exceeding 9. Incidentally, I found the lower bound %\\overline{c}^\\mu_{n+1} \\geq \\overline{c}^\\mu_n + 2% for %n \\geq 2%, since one can always take an extremiser in %\\Delta_n%, add a new edge to the triangular grid and put two points on it so that the triangle these two points generate doesn’t have the third point in the extremiser. Given how this thread is filling up, I will probably start a fresh thread in a day or so. The wiki is very helpful in this regard, it reduces the extent to which I will have to summarise all the progress…"},{"username":"jason-dyer","timestamp":"0007-02-20T05:00:00.000Z","contents":"When r<0 re: Peake.267, yes, I see now. I don’t remember r ever being defined as positive so I never thought about the case. (2,0,2) (2,2,0) (0,2,2) would be 1133, 1122, and 2233 which form no combinatorial lines. That does bring the interesting question of: is there anything important about this problem relating to the cases of r<0? They might affect the “subcubes” from O’Donnell.422 which seem to be working with “partial” combinatorial lines. Calling the points a, b and c: a and b form a partial combinatorial line, b and c do, and a and c do, but a, b, and c do not form a line together. I believe this situation may be unique for the r<0 case. Also, it means there are two variants of Fujimura, and while r<0 doesn’t apply to our problem it might be worth looking into as a side project (or at least noting it as a separate case)."},{"username":"sune-kristian-jakobsen","timestamp":"0008-02-20T05:00:00.000Z","contents":"273. Michael.219: “This method gives a lower bound for c_99 that is bigger than 3^98.” and  \nTerry.467: “We also have an example of a subset of %{}[3]^{99}% of density %>1/3% that still has no combinatorial line.” Then why is c_96/3^96=0.006155058319549\\. Shouldn’t it be >1/3?"},{"username":"sune-kristian-jakobsen","timestamp":"0008-02-20T05:00:00.000Z","contents":"273b The number I mentioned is L98 in the spreadsheet: Lower bound for %c_96/3^96%"},{"username":"kristal-cantwell","timestamp":"2012-02-20T05:00:00.000Z","contents":"The two examples we have for a lower bound for c_5 being 150 have cubes with 14 and 12 points is there an example with all cubes 16 or higher  \nand total 150 or higher? Or failing that all cubes 15 or higher and total 150 or higher?"},{"username":"thomas-sauvaget","timestamp":"0004-02-20T05:00:00.000Z","contents":"%c_5=154%: an example Inspired by the previous discussion, I think I’ve found an example showing that %c_5=154%, I’d be grateful if someone could check it. I’m using slightly different notations: instead of cartesian coordinates I’m using left-to-right ordering of words recursively to make visualisation easy. Namely, when %n% is even increase upwards, when %n% is odd increase righwards. In this way we can deal with 2-dimensional patterns only. For example %{}[3]^3% looks like this:  \n131 132 133 231 232 233 331 332 333  \n121 122 123 221 222 223 321 322 323  \n111 112 113 211 212 213 311 312 313 Given those notations here is what I think is a 154-element line-free pattern in %{}[3]^5%: [![](https://i1.wp.com/thomas1111.wordpress.com/files/2009/02/hj5a.png)](http://thomas1111.wordpress.com/files/2009/02/hj5a.png) The bottom-left black element is 11111 (not in the set), and so on. The two lone red elements are removed (Michael’s idea, here these are 12222 and 13333), then I found that removing the 6 other red elements leads to a line-free set (note that these 6 red elements are all on a super-diagonal). Total is thus %(9\\times 6 -2)+(9\\times 6 -3)+(9\\times 6 -3)=52+51+51=154%. This would also fit with the OEIS sequence A052979 mentionned above. I’ve built the example recursively by matching lower conditions starting from the bottom left %3\\times 3% square, first killing straight lines then diagonals, then doing the same with the other %3\\times 3% squares from %{}[3]^4%. In fact there is still some freedom in the pattern: the two squares I’ve circled with blue could have any of the 3 elementary %3\\times 3% patterns, but here I’ve chosen the most symmetric ones. It seems that this construction is fairly topological and systematic, it may well be possible to get an explicit recursive formulation, and thus a formula for estimating %c_n%. I’ll try to come back to it if indeed the example is correct."},{"username":"terence-tao","timestamp":"0005-02-20T05:00:00.000Z","contents":"%c_5 = 154% Dear Thomas, It unfortunately seems to me that the bottom row of the middle %{}[3]^4% contains some horizontal lines (e.g. the three centres of the three %{}[3]^2%‘s in this bottom row). There are also some lines on the top row and on the left and right columns of this middle %{}[3]^4%. (If you write each of the %{}[3]^2%‘s in xyz notation, one sees the problem; there are a bunch of rows and columns which only have two of the three patterns x, y, z and so let some lines slip in. The %{}[3]^2%‘s with three red squares in them are the intersection of two of the x, y, z and can kill more lines, but unfortunately this doesn’t cover all the possible lines.) But it could be that some permutation of this sort of strategy could work… it may provide the first good example we have which is not based off of the %D_n% set."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"Thread moving As this thread is beginning to get quite long, I am moving it to a new thread, starting at 700: [https://terrytao.wordpress.com/2009/02/13/bounds-for-the-first-few-density-hales-jewett-numbers-and-related-quantities/](https://terrytao.wordpress.com/2009/02/13/bounds-for-the-first-few-density-hales-jewett-numbers-and-related-quantities/)"},{"username":"michael-peake","timestamp":"2012-02-20T05:00:00.000Z","contents":"lower bound of %c_n% I tried a pattern of points based on Terry.39’s suggestion. It seems to give asymptotic results similar to what he thought. First define a sequence, of all positive numbers which, in base 3, do not contain a 1\\. Add 1 to all multiples of 3 in this sequence. This sequence does not contain a length-3 arithmetic progression. It starts 1,2,7,8,19,20,25,26,55, … Second, list all the (abc) triples for which the larger two differ by a number  \nfrom the sequence, excluding the case when the smaller two differ by 1, but then including the case when (a,b,c) is a permutation of N/3+(-1,0,1) This had numerical asymptotics for %\\log(c_n/3^n)% close to  \n$latex 1.2-\\sqrt(\\log(n)) between n=1000 and n=10000 Sune.273: Sorry, I haven’t written all the numbers in yet for my pattern – I am only up to 70, so another formula is used to give lower bounds after that."},{"username":"seva","timestamp":"2011-04-20T04:00:00.000Z","contents":"Edel? Just to remark that the upper bound for %c_n''%, ascribed in 228\\. to Edel, is actually present in Meshulam’s paper, in the most explicit form."},{"username":"gowers","timestamp":"2009-02-06T16:16:00.000Z","contents":"One of the main reasons for this comment is to establish the numbering. Comments on Terry’s post will be numbered 200-299 (with a convention that from now on if a post receives 100 comments then it must be summarized and restarted), comments on this post will be 300-399, and comments on the obstructions-to-uniformity post, when it is written, will be 400-499. To get the ball rolling for this post, I’d like to write out Jozsef’s proof of pair removal in full detail, mainly for my own benefit so that I understand it properly, but also because if, as I hope, a major subproject for this thread will be to see if we can generalize Jozsef’s argument, it will be convenient to have that argument here for easy referral. The first thing to note is that the result Jozsef proves is not quite as strong as the pair-removal lemma we originally asked for, because what he shows is not that you can remove a small number of sets and end up with no disjoint pairs, but rather that you can remove a small number of sets and end up with removing _almost_ all the disjoint pairs you started with. This kind of weakening of a removal lemma is good enough for many applications. For instance, if we applied it to the corners problem we would find that we could remove a small number of edges and get rid of almost all the degenerate triangles, which is just as much of a contradiction as the usual one. (In fact, even if we could show that we could get rid of one percent of the degenerate triangles we would have a contradiction.) I’ll get on to the actual proof in the next comment."},{"username":"gowers","timestamp":"2009-02-06T16:29:00.000Z","contents":"Pair removal in Kneser graphs. The _Kneser graph_ %K_{n,m}% is the graph whose vertices are all subsets of %\\null [n]% of size %m%, with two sets %U% and %V% joined if and only if they are disjoint. We should think of %n% as being %2m+k% for some fairly small but nonconstant %k% but for now I will avoid trying to specify them. Let %\\mathcal{A}% and %\\mathcal{B}% be collections of sets of size %m% (i.e., subsets of the vertex set of %K_{n,m}%. We can form a bipartite graph by joining %U\\in\\mathcal{A}% to %V\\in\\mathcal{B}% if and only if %U\\cap V=\\emptyset%: this is not quite a bipartite subgraph of %K_{n,m}% (since %\\mathcal{A}% and %\\mathcal{B}% are not required to be disjoint), but it is an induced subgraph of what one might call the bipartite version of %K_{n,m}% (where each vertex set contains all %m%-element sets and you join two such sets if they are disjoint). From now on I will blur the distinction between this bipartite graph and %K_{n,m}% itself. The maximum number of edges that could possibly join %\\mathcal{A}% to %\\mathcal{B}% is the number of edges in %K_{n,m}% (bipartite version), which is %\\binom nm\\binom{m+k}m=\\binom n{m,m,k}%. So we shall assume that the total number of edges is at most %c\\binom n{m,m,k}% for some very small constant %c>0%. Our aim is to remove at most %a\\binom nm% vertices and get rid of at least half these edges (or any other fixed fraction you might care to go for), where %a% tends to zero as %c% tends to zero. I’ll discuss how Jozsef does this in the next comment."},{"username":"gowers","timestamp":"2009-02-06T16:45:00.000Z","contents":"Pair removal in Kneser graphs. What Jozsef ends up showing is that there is a very simple way of deciding which vertices to remove: you just get rid of all vertices of large degree. It turns out that if you do this then you either get rid of a significant fraction of all the edges, or you had almost no vertices to start with (in which case you can just remove all of them). Now all the time we are hoping to remove a number of edges that is large as a fraction of the total number of edges, so “large degree” means “significantly larger than the average degree”. Let us suppose, then, that the average degree in our graph is %d%. (Here I am averaging over all vertices in %K_{n,m}%, including those that are not in %\\mathcal{A}% or %\\mathcal{B}%.) It follows from Markov’s inequality that for any %C% the number of vertices of degree at least %Cd% is at most %2C^{-1}\\binom nm%. So we will need to choose %C% so that %2C^{-1}% tends to zero as %c% tends to zero. Suppose that removing these vertices doesn’t get rid of half the edges. That means that after the removal we have a bipartite induced subgraph of %K_{n,m}% such that the average degree is at least %d/2% and the maximal degree is at most %Cd%. In the next comment I’ll explain what Jozsef does in this case."},{"username":"gowers","timestamp":"2009-02-06T17:26:00.000Z","contents":"Pair removal in Kneser graphs. What he does is to use what he calls “the permutation trick of Lubell” to get an upper bound on the number of vertices that there can be in a graph with average degree at least %d/2% and maximal degree at most %Cd%. Take a random permutation %\\pi% of %\\null [n]%. Given two disjoint sets %U% and %V%, we shall say that %\\pi% _separates_ %U% and %V% if every element of %\\pi(U)% is less than every element of %\\pi(V)% or vice versa. What is the expected number of separated pairs with %U\\in\\mathcal{A}% or %V\\in\\mathcal{B}%? Let’s suppose that the total number of vertices in the bipartite graph is %N%. Then the total number of disjoint pairs is at least %(d/2)N%. Each disjoint pair %(U,V)% has a probability of %2\\binom{2m}m^{-1}% of being separated (since there are %\\binom{2m}m% ways of ordering %U\\cup V% of which precisely two separate them), so the expected number of separated pairs is at least %dN\\binom{2m}m^{-1}%. In particular, there exists a permutation %\\pi% that gives rise to at least %dN\\binom{2m}m^{-1}% separated pairs. But how many separated pairs can there be for a single permutation? Well, let’s suppose first that that permutation is the identity (without loss of generality) and that we have separated pairs %(U_1,V_1),\\dots,(U_r,V_r)%, ordered in such a way that every element of %U_i% is less than every element of %V_i%. The crucial observation here is that if %U_1% is a set with the smallest maximal element out of any of the %U_i % (again without loss of generality), then %(U_1,V_i)% is a separated pair for all %V_i%. Therefore, there can be at most %Cd% distinct sets %V_i% (since we know that %U_1% has degree at most %Cd%). Similarly, there can be at most %Cd% distinct %U_i%, so the total number of separated pairs is at most %C^2d^2%. It follows that %dN\\binom{2m}m^{-1}\\leq C^2d^2%, and hence that %N\\leq C^2d\\binom{2m}m=C^2d\\binom{n-k}m%. And having got to this point, I realize that I don’t understand why the right-hand side is small. Does anyone see it, or do we wait till Jozsef wakes up? (I also have trouble understanding the significance of the final inequality in his comment 155)."},{"username":"gowers","timestamp":"2009-02-06T17:36:00.000Z","contents":"Pair removal in Kneser graphs. Maybe the point is that the right-hand side doesn’t have to be small if all we know about %d% is that it’s a small multiple of %\\binom{n+m}m%, but if we assume that %d% is much smaller (it’s possible that for applications to Sperner we may even be able to take %d=1%) and %k% is reasonably large, then the right-hand side is much smaller than %\\binom nm%. So this would fit in with my [comment 153](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1889) , where I suggested that using a stronger hypothesis might be helpful to prove a pair-removal lemma."},{"username":"gowers","timestamp":"2009-02-06T17:56:00.000Z","contents":"Triangle removal. Here is a possible toy problem to think about, as an initial contribution to the project of generalizing Jozsef’s proof. This time we let %n=3m+k%. Our basic objects will be pairs %(U,V)% of disjoint sets. We are given three collections %\\mathcal{A}%, %\\mathcal{B}% and %\\mathcal{C}% of such pairs. A _triangle_ is a triple of the form %(U,V),(U,W),(V,W)% with %(U,V)\\in\\mathcal{A}%, %(U,W)\\in\\mathcal{B}% and %(V,W)\\in\\mathcal{C}%. We would like to prove that if for each pair %(U,V)\\in\\mathcal{A}% there is at most one %W% such that %(U,W)\\in\\mathcal{B}% and %(V,W)\\in\\mathcal{C},% and similarly for the other two ways round, then it is possible to remove a small number of elements from %\\mathcal{A}%, %\\mathcal{B}% and %\\mathcal{C}% and end up removing a significant fraction of all the triangles. There are no degenerate triangles here, so this wouldn’t do DHJ straight off, but I think it would be a good way of trying to understand whether Jozsef’s methods generalize. An alternative line of investigation would be to generalize Jozsef’s methods to a statement about several central layers of the cube rather than just one."},{"username":"gowers","timestamp":"2009-02-06T18:16:00.000Z","contents":"Triangle removal. An optimistic guess about how it might go: we just routinely try to generalize Jozsef’s argument, and at a certain point a difficulty arises, which we turn out to be able to solve using the usual triangle-removal lemma."},{"username":"gowers","timestamp":"2009-02-06T18:38:00.000Z","contents":"Triangle removal. At the very least, let’s try to do the routine generalization and see what happens. So without any serious thought about what is sensible and what is not, let us say that a permutation %\\pi% _separates_ the three sets %U,% %V% and %W% if every element of %U% comes before every element of %V%, which in turn comes before every element of %W%, or any one of the other five such statements you get by permuting the three sets. We’ll also say that %\\pi% separates a triangle if it separates the three sets used to build it. Given any two disjoint sets %U% and %V%, let us define the %\\mathcal{A}%–_degree_ of %(U,V)% to be the number of %W% such that %(U,V)\\in\\mathcal{A}%, %(U,W)\\in\\mathcal{B}% and %(V,W)\\in\\mathcal{C}%. (In this case, we say that %U,V,W% form a triangle.) We have similar definitions for the other two possible degrees. Now let’s suppose that the average degree (over all three bipartite graphs) is %d% and the maximal degree is %Cd%. What can we say? Let’s think about the expected number of separated triangles. The total number of triangles is %d\\binom n{m,m,n-2m}=d\\binom{3m+k}{m,m,m+k}% (up to some absolute constant depending on whether one orders the vertices etc.), and the probability that %\\pi% separates any given triangle is %6\\binom{3m}{m,m,m}^{-1}%. So the expected number is %6d\\binom{3m}{m,m,m}^{-1}\\binom {3m+k}{m,m,m+k}.% Therefore, there must be some permuation %\\pi% that separates at least this number of triangles. Now let’s think about how many triangles a permutation can separate. Our assumption is that no pair of sets forms a triangle with more than %Cd% other sets. It’s at this point that, if our wildest dreams come true, we find that normal dense triangle removal is exactly what we need. But no reason to suppose that just yet. I’m going to continue in a new comment."},{"username":"gowers","timestamp":"2009-02-06T18:59:00.000Z","contents":"Triangle removal. Let’s suppose that %(U_1,V_1,W_1),\\dots,(U_m,V_m,W_m)% are separated triangles. We would like an upper bound on %m%. If we choose %i% such that the maximal element of %V_i% is minimized, we see that the number of distinct sets %W_j% can be at most %D=Cd%. Similarly, the number of distinct %U_i% can be at most %D%. But each pair %(U_i,W_j)% is allowed at most %D% sets %V_h% in the middle, so we seem to have at most %D^3% triangles separated by %\\pi%. This is slightly worrying as it came out a bit too simply for my liking. If it’s correct, it shows that %6d\\binom{3m}{m,m,m}^{-1}\\binom{3m+k}{m,m,m+k}\\leq Cd^3%, and therefore that %d^2\\geq 6C^{-1}\\binom{3m}{m,m,m}^{-1}\\binom{3m+k}{m,m,m+k}%, which is exponential in %k%. So for very small %d%, such as we might expect to have in DHJ, we get a contradiction. I’ve got to go now, so all I’ll say at this point is that this argument feels too simple to be correct, but I can’t see what’s wrong with it. (If it’s right, then the explanation must simply be that it is not what we need.)"},{"username":"ryan-odonnell","timestamp":"2009-02-06T20:14:00.000Z","contents":"Pair removal in Kneser graphs. Can someone help me with this dumb question? Suppose %A = B% are the family of sets not including the last element %n%. Then %A% and %B% have density about %1/2% within %KN_{n, n/2-k/2}%. (We’re thinking %k(n) \\to \\infty%, %k(n)/n \\to 0% here, right?) It seems that the fraction of Kneser graph edges which are in %A \\times B% is about %k/n%, which is “negligible”. So we should be able to delete any small constant fraction of vertices from %A = B% and make it an intersecting family. But %A% is %100\\%% of the Kneser graph %KN_{n-1, n/2-k/2}%, and doesn’t the largest intersecting family inside such a Kneser graph have density only around %1/2%?"},{"username":"gowers","timestamp":"2009-02-06T21:49:00.000Z","contents":"Pair removal in Kneser graphs. Ryan, all I can say is that I can’t see a flaw in your reasoning. If it’s correct, then it shows something rather interesting. I couldn’t get Jozsef’s argument to work unless I assumed that degrees were very small rather than just small, and your example suggests that it’s actually false when you merely assume that the degrees are a negligible fraction of the maximum possible. In your example, the degree of a vertex in %A% is still large, even if it’s proportionately small, so there’s still some hope that Jozsef’s argument is correct and adaptable to DHJ. Now I must stare at the argument that I produced in 308 and try to work out what’s going on."},{"username":"ryan-odonnell","timestamp":"2009-02-06T22:10:00.000Z","contents":"I guess you can make the degree go down fairly rapidly compared to the density by requiring the last %t% coordinates to be %{}0%. (Density goes down like %2^{-t},% fraction of edges goes down like %(k/n)^t%.)"},{"username":"jason-dyer","timestamp":"2009-02-06T22:11:00.000Z","contents":"Counting lemma This is idle wondering, but given a random (not quasi-random) tripartite graph, do we know the probability P of n triangles appearing given a density D? This seems like this sort of thing that ought to be just a textbook example."},{"username":"gowers","timestamp":"2009-02-06T22:13:00.000Z","contents":"Triangle removal. I’m going to try to do two things at once. The first is express myself more clearly when it comes to arguments such as the one in 308, and the second is to try to develop an argument that involves more than one slice, in the hope that degenerate triangles will come into play somehow. Let me begin by trying to define the nicest possible [triangular grid of slices](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1744) . I’ll define %T_m% to be the union of all %\\Gamma_{a,b,c}% such that %(a,b,c)% is a triple of integers that belongs to the convex hull of %(n/3+m,n/3-m,n/3)%, … actually, cancel this. It seems to be more natural, if one is going for the nicest possible and most symmetrical set of slices, to go for a hexagon. So let’s take all %(a,b,c)% that sum to %n% and that satisfy %\\max\\{|a-n/3|,|b-n/3|,|c-n/3|\\}\\leq m%. Here I’m imagining that %n% is a multiple of 3, not that it’s all that important. (This is a hexagon because there are six linear inequalities, and the symmetry of the situation makes it a regular hexagon.) Let’s write %\\Gamma% for the union of all these %\\Gamma_{a,b,c}%. We would like to prove that any dense subset %A% of %\\Gamma% contains a combinatorial line. Now I’m going to do roughly the same thing as before. I’ll assume that there are no combinatorial lines in %A%. For each sequence %x% in %A% and each %j\\in\\{1,2,3\\}%, let’s define the %j%–_set_ of %x% to be %\\{i:x_i=j\\}%, and we’ll denote this by %U_j(x)%. As ever, we define %\\mathcal{A}% to be the set of all pairs %(U_1(x),U_2(x))% with %x\\in A%, %\\mathcal{B}% to be the set of all %(U_1(x),U_3(x))% and %\\mathcal{C}% to be the set of all %(U_2(x),U_3(x))% (again, always with %x\\in A%). The assumption that there are no combinatorial lines tells us that there are no triples %(U,V,W)% with %(U,V)\\in\\mathcal{A}%, %(U,W)\\in\\mathcal{B}% and %(V,W)\\in\\mathcal{C}%, except in the degenerate case where %U\\cup V\\cup W=[n]%. In the next comment I’ll try to hit this with Jozsef’s permutation argument."},{"username":"jason-dyer","timestamp":"2009-02-06T22:15:00.000Z","contents":"(Meta-comment) Ryan, like the brackets problem you should toss a {} before the 0 there if you want to render it in LaTeX in WordPress."},{"username":"gowers","timestamp":"2009-02-06T22:31:00.000Z","contents":"Triangle removal. So now let’s choose a random permutation and see what we can say about the number of separated triangles. To do this, my first task is to say how many _actual_ triangles there are. And the answer is %|A|%, one degenerate triangle for each element of %A% and no others. Next, I’d like some idea of the probability that a random permutation %\\pi% separates a given triangle. Let’s suppose that the point %x% of %A% that gives rise to the triangle belongs to %\\Gamma_{a,b,c}%. Then the probability that %\\pi% separates %(U_1(x),U_2(x),U_3(x))% is %6\\binom n{a,b,c}^{-1}%. So the expected number of separated triangles is %6|A|\\binom n{a,b,c}^{-1}%. Now suppose I just choose an arbitrary permutation — without loss of generality the identity. How many separated triangles can there be? I think this may be where the problems arise, but let’s see. Suppose that %(U_1,V_1,W_1),\\dots,(U_s,V_s,W_s)% are separated triangles. We know that for each %(U_i,V_i)% … I may come back to this, but I’ve now seen where I went wrong in 308, so I should think about that first, as it’s a simpler problem."},{"username":"gowers","timestamp":"2009-02-06T22:49:00.000Z","contents":"Triangle removal: my mistake in 308. I started 308 with the claim that if you choose %(U_i,V_i)% so as to minimize the maximum element of %V_i%, then you could see immediately that there were at most %D% distinct %W_j%. The thinking behind this was that each %W_j% would form a triangle. But that was wrong: there is no reason for %(U_i,W_j)% to belong to %\\mathcal{B}% or for %(V_i,W_j)% to belong to %\\mathcal{C}%. So it’s back to the drawing board here. (But this, I should stress, is _good_ news as I wanted there to be a complication to give triangle-removal a chance to creep in.) Here is a new proof strategy. First I’ll copy the first part of Jozsef’s argument to get to the stage where each _vertex_ is contained in roughly the same number of triangles. (If I can’t, I’ll remove a few vertices and get rid of a significant fraction of the triangles.) Next, I’ll argue as above that the expected number of separated triangles is reasonably large. Then I’ll try to bound the number of separated triangles if no vertex is in too many triangles and no edge is in more than one triangle. The first step of that will be to pick … actually, I still don’t see it. I was going to say that one should pick the %U_i% with smallest maximum, but I don’t see what that gives us. What we _can_ say is this. For each %U% there will be some system of sets %(V_i,W_i)% such that %(U,V_i,W_i)% forms a triangle. If in this set system we choose %i% such that the maximum of %V_i% is minimized, then we can deduce that there are not too many distinct %W_j%. Except that I don’t know why I even say _this_ so confidently: what makes me think that %(V_i,W_j)% should lie in %\\mathcal{C}%? OK I’m floundering around a bit here so I’d better stop. But I’ll leave these messy thoughts here in case anyone can get a clearer idea of what ought to be done. It feels as though it’s in promising territory somehow."},{"username":"gowers","timestamp":"2009-02-06T23:14:00.000Z","contents":"Triangle removal. This is the kind of reason I find it promising. The graph where you join two sets if and only if one is completely to the left of the other is, under normal circumstances, much denser than the graph where you just join two sets if they are disjoint. So what Jozsef’s permutation argument is cleverly achieving seems to be to get us from a sparse context to a dense context. (I don’t yet have a fully precise formulation of this remark.) Now let’s suppose we have our separated triangles %(U_i,V_i,W_i)% above. Suppose also that we have managed to show somehow that many of the pairs %(U_i,V_j)% belong to %\\mathcal{A}%, and so on. We know that there won’t be many triangles, so this will imply, by the usual triangle-removal lemma, that we can remove a small number of edges and end up with no triangles. Could there be an argument along these lines that allows us to prove density Hales-Jewett by hitting it permutation by permutation? We’re in that familiar state where there’s a small probability %p% of being close to a solution and a large probability %1-p% of having written a load of nonsense."},{"username":"jozsef","timestamp":"2009-02-06T23:33:00.000Z","contents":"Pair removal in Kneser graphs. Bad news – good news. Good morning! I hope that I didn’t derail the project with my last calculation yesterday – that the “removal lemma” works when %m\\sim n - \\sqrt{n}%. The main inequality is correct, and we see that it tells us something non-trivial if m is small compare to n. As I see now, the average degree, d, grows as %m=o(n - \\sqrt{n})%. I think that this is the range where we have a removal lemma so far. This is a removal lemma we get by simple double-counting, so I’m afraid that we can’t expect too much out of it, but it supports Tim’s conjecture."},{"username":"gowers","timestamp":"2009-02-07T00:15:00.000Z","contents":"Pair/triangle removal. Hi Jozsef. Two remarks. It seems to me (but I’d be reassured if I knew you agreed about this) that we have a removal lemma if the average degree is very small. And in interesting contexts it is. For example, if %\\mathcal{A}% is a counterexample to Sperner’s theorem and we let %\\mathcal{B}% consist of al complements of sets in %\\mathcal{A}%, then each set in %\\mathcal{A}% is joined just to its complement, so the average degree is 1\\. The second remark is that it’s not impossible that a simple double counting gives us something good. The reason is that when going from 2 to 3 in the dense case you go from trivial to interesting, so when going from 2 to 3 here, perhaps you go from simple to very interesting."},{"username":"ryan-odonnell","timestamp":"2009-02-07T00:25:00.000Z","contents":"Sperner. Hi Jozsef, is that a typo? %m% needs to be less than %n/2%; otherwise the %KN_{n,m}% Kneser graph is empty. I’m not sure the scenario we should consider now… we have counterexamples to the Tentative Conjecture for %k% both constant and superconstant."},{"username":"jozsef","timestamp":"2009-02-07T00:28:00.000Z","contents":"Pair removal lemma Now I feel a bit dummy; I wanted to check where did the calculation go wrong yesterday in my last post, but I can’t see. Tim, in 304 you asked why is the right hand side small. By checking %\\binom{n}{m}/\\binom{2m}{m}% what I see – again – is that it should be around %2^{2\\sqrt{n}}%. I probably badly overlook the same thing again …  \nI’ll go to have a morning coffee."},{"username":"gowers","timestamp":"2009-02-07T00:34:00.000Z","contents":"Sperner Ryan, you may be ahead of me here and already see why the suggestion I’ve been making is a bad one, but what about strengthening the hypothesis of the Tentative Conjecture from the number of disjoint pairs being %o(1)\\binom{n}{m,m,k}% to each set belonging to at most one disjoint pair? (With this second hypothesis one tries to prove that the sets must be small.) Actually, isn’t that just Bollobás’s theorem? I think it is because if I take %d=C=1% in 303 then I get the correct bound of %\\binom{2m}m%. Is that the standard proof of Bollobás’s theorem?"},{"username":"jason-dyer","timestamp":"2009-02-07T00:35:00.000Z","contents":"Pair removal in Kneser graphs. Ok, I am trying to understand Jozsef’s proof (I notice it uses the greedy algorithm, interesting) and I must be missing something elementary at 303: The crucial observation here is that if %U_1% is a set with the smallest maximal element out of any of the %U_i% (again without loss of generality), then %(U_1,V_i)% is a separated pair for all %V_i%. Could someone expand on this a little more? I’m not seeing why."},{"username":"gowers","timestamp":"2009-02-07T00:37:00.000Z","contents":"Pair removal in Kneser graphs. Jason, if %U_1% has the smallest maximal element and %(U_i,V_i)% is separated, then the largest element of %U_1% is at most the largest element of %U_i% is less than the smallest element of %V_i%. If this doesn’t explain it then I don’t understand what you’re not understanding."},{"username":"gowers","timestamp":"2009-02-07T01:08:00.000Z","contents":"Triangle removal A quick technical question: I think it should be reasonably straightforward to answer. Suppose we have a dense set %A% in %\\null[3]^n%. Suppose now that we take a random permutation %\\pi% and just take from %A% the sequences that start with 1s, continue with 2s and finish with 3s (after you’ve used %\\pi% to scramble the coordinates). And now suppose you form the tripartite graph from just those sequences. That is, you let %\\mathcal{A}% consist of all pairs %(U,V)% that are equal to %(U_1(x),U_2(x))% for some %x\\in A%, and so on. We can think of %\\mathcal{A}% as a bipartite graph, whose vertices are subsets of %\\null [n]%, where we join %U% to %V% if %U% finishes before %V% starts. What I’d like to know is whether this graph tends to be dense. I think the straightforward answer is no, so what I really mean is whether it can be made dense if you remove a smallish number of vertices and edges, or something like that. The motivation for this question is that I’d very much like to be able to define a dense tripartite graph with few triangles and see what happens when we apply the usual triangle-removal lemma."},{"username":"jozsef","timestamp":"2009-02-07T01:30:00.000Z","contents":"Pair removals Re: 319, yes there was a typo, I meant n/2 there, thanks. But I still don’t see if anything is bad with my calculation; Please help me, is it correct that if %m=n/2-\\sqrt{n}% then d should be at least %c2^{2\\sqrt{n}}% or we have a removal lemma? After this I will try to digest Tim’s triangle removal technique."},{"username":"ryan-odonnell","timestamp":"2009-02-07T02:29:00.000Z","contents":"Kneser. Hi Tim, in #321, if each set is involved in at most one Kneser edge, then I think the least number of sets you need to delete in order to eliminate all Kneser edges is with a factor of two of the number of sets (vertices), no?"},{"username":"gowers","timestamp":"2009-02-07T02:37:00.000Z","contents":"Kneser Yes, so if you can prove that you can remove a small number of sets then you’ve proved that there weren’t very many sets in the first place."},{"username":"gowers","timestamp":"2009-02-07T02:43:00.000Z","contents":"Pair removal Jozsef in 325, my calculation (which I checked and I’m pretty sure it works out to be the same as yours) suggests that you get something interesting as long as %C% is large and %C^2d\\binom{2m}m% is a lot smaller than %\\binom{2m+k}m%. Since %\\binom{2m+k}m% is about %2^k\\binom{2m}m%, it follows that we need %d% to be small compared with %2^k/C^2% in order to get a removal lemma. I think this agrees with you, since you are taking %m=n/2-\\sqrt n% and %k=2\\sqrt n%. So it seems correct to you and it seems correct to me, which means that there’s an %\\epsilon^{3/2}% chance that it’s incorrect."},{"username":"jozsef","timestamp":"2009-02-07T02:53:00.000Z","contents":"Thank you Tim. I think I should apologize that, after reading 303, I didn’t pay much attention to later posts from where I could have learned that things were OK with the calculation. In the next post I will try to understand and extend your toy version of a triangle removal lemma. Unfortunately now I have to go to the university and probably I won’t read the posts in the next 5-6 hours."},{"username":"gowers","timestamp":"2009-02-07T04:11:00.000Z","contents":"Triangle removal Jozsef, I should make it clear that I don’t yet have an interesting new triangle removal lemma proved, so if you prove anything at all it counts as an extension of what I’ve done. But I might think just a little bit further about the vague idea of 324\\. Suppose then that we have a dense set of sequences in %\\null[3]^n%, and for simplicity let’s assume that they’re all reasonably well balanced (meaning that they all have roughly equal numbers of 1s, 2s and 3s). How many times do we expect a given set %U% to occur as %U_1(x)% for some %x\\in A%? The very rough answer is %\\binom{2n/3}{n/3}%. Let’s be even more rough and call that %2^{2n/3}%. Now %A% has size around %\\binom{n}{n/3,n/3,n/3}=\\binom{n}{n/3}\\binom{2n/3}{n/3}%. I’m going to interrupt this comment because I’ve had a different idea that I want to pursue urgently."},{"username":"gowers","timestamp":"2009-02-07T04:52:00.000Z","contents":"Sperner and DHJ The proof I know of Sperner goes like this. You take a random sequence of sets of the form %\\emptyset=A_0\\subset A_1\\subset A_2\\subset\\dots\\subset A_n=[n]%, where %A_i% has cardinality %i%. If %\\mathcal{A}% is an antichain, then it intersects each such chain in at most one set. Now let’s imagine we choose a set at random. What is the probability that it lies in %\\mathcal{A}%? Let’s write %\\delta_i% for the density of %\\mathcal{A}% inside layer %i% (that is, the number of sets in %\\mathcal{A}% of size %i% divided by %\\binom ni%). Then the expected size of the intersection of %\\mathcal{A}% with our random chain is %\\delta_0+\\delta_1+\\dots+\\delta_n%. It follows that the %\\delta_i% add up to at most 1 (since at least one random chain will have at least the expected number of elements of %\\mathcal{A}% in it). Since the middle layer(s) is (are) biggest, it’s clear that we maximize the size of %\\mathcal{A}% by choosing a middle layer. The idea that’s just occurred to me is that we’ve just deduced Sperner from the one-dimensional corners result (which states that if you have a dense subset of %\\null [n]% then you must be able to find %x% and %x+d% in it with %d\\ne 0%). What happens if we try to deduce density Hales-Jewett from 2-dimensional corners? Well, first we need to decide what replaces a random chain. For reasons of symmetry I’d like to think of a random chain as follows: you pick a random permutation of %\\null[n]% and then once you’ve scrambled the set you take all partitions %(A,B)% of %\\null [n]% into two sets such that every element of %A% is less than every element of %B%. At this point I’m just going to guess that it might be reasonable to choose for DHJ purposes a random permutation of %\\null[n]% followed by all partitions %(A,B,C)% such that every element of %A% comes before every element of %B%, which comes before every element of %C%. This is at least a two-dimensional structure, since it’s determined by the end points of %A% and %B%. I will also think of this as the set %S% of all sequences in %\\null[3]^n% that start with some 1s, continue with some 2s, and end with some 3s. Next question: if %\\mathcal{A}% contains no combinatorial line, can we deduce that the intersection of %\\mathcal{A}% with %S% is small? What I’d like to do is prove that it has no corners, in some useful sense of the word “corners”. How would we get a combinatorial line in this set? We can of course encode each element of %S% as a triple %(a,b,c)% where %a+b+c=n%. And at this point we have a problem: %S% contains no combinatorial lines. Well, I suppose if that had worked it would have been discovered by now. But it suggests an amusing problem. Let %T_m% be the hexagonal portion of the triangular grid %x+y+z=0% (a 2-dimensional subset of %\\mathbb{Z}^2%) defined by %\\max\\{|x|,|y|,|z|\\}\\leq m%. Let us call a function %f:T_m\\rightarrow [3]^n% a _corner isomorphism_ if it is an injection such that every aligned equilateral triangle in %T_m% maps to a combinatorial line. (An aligned equilateral triangle is a set of the form %(a+r,b,c),(a,b+r,c),(a,b,c+r)%.) If we could find a corner isomorphism for some fairly large %m%, then we could probably move the image about and prove density Hales-Jewett using an averaging argument. So can anyone either find a big corner isomorphism or (more likely) prove that no such function exists? I don’t expect this problem to be very hard."},{"username":"gowers","timestamp":"2009-02-07T06:48:00.000Z","contents":"Sperner and DHJ My last contribution for the day: I’ve just checked and there isn’t even a corner isomorphism from %T_2% into %\\null [3]^n%. The proof goes like this: you take an arithmetic progression of three points in a line and map them to %12333%, %12233% and %12223%. (I think I may also have wanted to insist that lines go to sets where the appropriate %j%-set is fixed.) Those three points determine a %T_2% and their images determine what the images of the other three points under a corner isomorphism would have to be, but those three images don’t lie in a combinatorial line. (For what it’s worth, they are %12133%, %12113% and %12213%.)"},{"username":"jozsef","timestamp":"2009-02-07T11:49:00.000Z","contents":"Triangle removal To understand the notations and arguments below, one should read 305-307-308 first. Tim’s proof in 307-308 works nicely, if we change the initial conditions. Our base set %\\mathcal{S}% is now just a collection of m-element subsets (where n=3m+k) instead of %\\mathcal{A},\\mathcal{B},\\mathcal{C}% and we are considering now every triangle formed by any three pairwise disjoint elements of %\\mathcal{S}%. For this set of triangles, we do have the removal lemma as Tim has calculated it. Let me add two technical remarks which might be needed for the complete proof. A triangle is called “normal” if none of its edges is edge of more than Cd other triangles. Then, the argument gives bound on the number of normal triangles. If most of the triangles aren’t normal, then we have a removal lemma. The conclusion is that if we have a small number of triangles in %\\mathcal{S}% compare to the size of %\\mathcal{S}%, then a small fraction of the edges covers most of the triangles. My second small remark is that when we bound the normal triangles separated by a permutation, then we consider the sets right from the leftmost (having the smallest last point) middle element and and the sets left from the rightmost (having the largest first point) element. These are both bounded by Cd and they span at most %(Cd)^3% separated triples, as Tim said. I’m just emphasizing the selection of the left size here, only a technical detail.  \nThis theorem is more restricted than Tim’s original statement, but I think it might be useful. In DHJ the dense subset is given and we will work with the induced substructure."},{"username":"jozsef","timestamp":"2009-02-07T12:01:00.000Z","contents":"Sperner I was wondering what would be the proper Sperner-type theorem in %{}[3]^n% similarly useful as Sperner in %{}[2]^n%. Is there something before DHJ?"},{"username":"gowers","timestamp":"2009-02-07T15:22:00.000Z","contents":"Sperner Jozsef, I think I have the answer to your last question. Let’s define the _Kneser product_ of two set systems %\\mathcal{A}% and %\\mathcal{B}% to be the set of all pairs %(U,V)% such that %U\\in\\mathcal{A}%, %V\\in\\mathcal{B}% and %U\\cap V=\\emptyset%. Then the theorem would be that every Kneser product that is dense in the set of all disjoint pairs (of which there are %3^n%) contains a corner: that is, a configuration of the form %(U,V)%, %(U,V\\cup D)%, %(U\\cup D,V)% with all three of %U%, %V% and %D% disjoint. I’ve stated it like that to make it as similar as I can to Sperner. But a more symmetrical version is this. Suppose that %\\mathcal{S}%, %\\mathcal{T}% and %\\mathcal{U}% are set systems such that there are at least %c3^n% ways of partitioning %\\null [n]% as %S\\cup T\\cup U% with %S\\in\\mathcal{S}% etc. Then the corresponding collection of sequences contains a combinatorial line. To see that this is a natural extension of Sperner, consider the following silly way of stating (weak) Sperner. If %\\mathcal{A}% and %\\mathcal{B}% are two set systems such that there are at least %c2^n% ways of partitioning %\\null [n]% as %A\\cup B% with %A\\in\\mathcal{A}% and %B\\in\\mathcal{B},% then the corresponding collection of 01-sequences contains a combinatorial line. It looks to me as though there should be a two-dimensional family of density Hales-Jewett statements. That is, for each %1\\leq j there should be a DHJ(j,k), where Sperner is DHJ(1,2), density Hales-Jewett in %\\null [k]^n% is DHJ(k-1,k), and the theorem just discussed (for which it looks as though we may have a proof) is DHJ(1,3). The idea would be that for DHJ(j,k), whether or not a sequence belongs to the set depends on a j-uniform hypergraph with %2^{[n]}% as its vertex set. We should see if there is any relation between this and [Terry’s DHJ(2.5) problem](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1853) . I suspect there isn’t, in which case it’s amusing to have come up with a different way of “interpolating” between Sperner and DHJ."},{"username":"gowers","timestamp":"2009-02-07T19:32:00.000Z","contents":"Sperner and DHJ. A small observation. One could regard the previous comment as showing that at least one natural generalization of Sperner to %\\null [3]^n% turns out not to be the statement we want. That is unfortunate, though by no means catastrophic. In this comment I want to point out that there is a different way of generalizing Sperner that leads (I think) to a deeper theorem, though it still isn’t DHJ. Let’s go back to comment 331 and try to generalize the proof of Sperner and see what comes out, not worrying if it isn’t DHJ. To do this, we associate with any triple %a+b+c=n% the three sets %\\{1,2,\\dots,a\\}%, %\\{a+1,\\dots,a+b\\}% and %\\{a+b+1,\\dots,n\\}%. Let’s call this the triple %U_{a,b,c}%. Now let’s suppose we have a corner, by which in this context I mean a triple of triples of the form %(a+r,b,c)%, %(a,b+r,c)% and %(a,b,c+r)% (with %a+b+c+r=n%). What combinatorial configuration do we get from the corresponding triples %U_{a+r,b,c}%, %U_{a,b+r,c}% and %U_{a,b,c+r}%? The answer is that we partition %\\null [n]% into sets %A_1=\\{1,2,\\dots,a\\}%, %A_2=\\{a+1,\\dots,a+r\\}%, %A_3=\\{a+r+1,\\dots,a+b\\}%, %A_4=\\{a+b+1,\\dots,a+b+r\\}% and %A_5=\\{a+b+r+1,\\dots,n\\}%. Then the sequence corresponding to %U_{a+r,b,c}% is 1 on %A_1% and %A_2%, 2 on %A_3% and %A_4%, and 3 on %A_5%. The sequence corresponding to %U_{a,b+r,c}% is 1 on %A_1%, 2 on %A_2%, %A_3% and %A_4%, and 3 on %A_5%. Finally, the sequence corresponding to %U_{a,b,c+r}% is 1 on %A_1%, 2 on %A_2% and %A_3%, and 3 on %A_4% and %A_5%. Thus, we have three fixed sets, namely %A_1%, %A_3% and %A_5%, and two variable sets, namely %A_2% and %A_4%. If we identify each set to a point, we can think of the three sequences as %11223,% %12223% and %12233%. If we forget about the fixed sets, the restrictions to the variable sets are going %12,22,23%. Also, and this is important, the two variable sets have the same size. (If they didn’t, then the resulting theorem would be much easier.) Given a dense set of triples that start with 1s, continue with 2s and end with 3s, then the corners theorem will tell you that you must have a configuration of the above kind. I haven’t checked this, but I’m about 99% certain that if you imitate the proof of Sperner by averaging over a random permutation, then you will be able to deduce that a dense subset of %\\null [3]^n% will contain a “peculiar combinatorial line” of the above form. (Just to be clear, a _peculiar combinatorial line_ is a triple of sequences that are fixed outside two sets %V% and %W% that have the same size, and inside %V% and %W% they go %12,22,23%.) The reason I call this a deeper theorem is that it relies on the corners theorem. Indeed, it implies the corners theorem if you restrict to sets that are unions of slices. (This is because we insist that %V% and %W% have the same size.) Again, this is a slightly depressing observation because it shows that a reasonably natural generalization of the Sperner proof gives us the wrong theorem. However, again it doesn’t actually tell us that we can’t find a different way of generalizing Sperner that gives the right theorem."},{"username":"gowers","timestamp":"2009-02-07T21:05:00.000Z","contents":"Sperner and DHJ A tiny observation, following on from 332\\. The main difficulty seems to be as follows. Let’s define the _order profile_ of a sequence to be what you get when you collapse each block of equal coordinates to just one coordinate. For example, the order profile of %113323311113% is %132313%. In %\\null [2]^n% you can have a combinatorial line where every element has the same order profile: e.g. %11122% and %12222%. But in %\\null [3]^n% that is impossible. (Proof: if the last fixed coordinate before a variable string starts is %i% and the first fixed coordinate after it finishes is %j%, then the variable string can only take the values %i% and %j%.) I’ve vaguely wondered about having a two-dimensional ground set, so that it’s possible for three sets to meet at a point, but I haven’t managed to define a nice two-parameter family of sequences."},{"username":"gowers","timestamp":"2009-02-07T21:13:00.000Z","contents":"Sperner and DHJ Actually, I can’t resist just slightly expanding on the last paragraph of 337\\. Let’s take not %\\null[3]^n% but %{}[3]^{n^2}%, which we think of as the space of all functions from %\\null [n]^2% to %\\{1,2,3\\}%. I’d like to regard such a function as “simple” if you’ve basically got three big simply connected patches, of 1s, 2s and 3s, and if the boundaries between these patches are not too wiggly. A combinatorial line of functions like this would have the desirable property that it wouldn’t be obvious from just one of the functions what the variable set was: in some sense the “two-dimensional order profile” would be the same for all three points. One could now ask whether the simpler structure here would make DHJ easier to prove. If so, then one could average up in the Sperner fashion. But it doesn’t look easy to say exactly what this simpler structure is."},{"username":"gowers","timestamp":"2009-02-07T21:58:00.000Z","contents":"Sperner and DHJ I take that back. Here’s an actual conjecture. Let’s take as our basic object the kind of thing one talks about in Sperner’s proof of the Brouwer fixed point theorem (surely a coincidence though). Chop out a large hexagonal piece from a tiling of the plane by hexagons and colour two of its edges (next to each other) red, two green and two blue, and extend these colourings from the boundary into the inside of the hexagon until you’ve coloured it all, and do so in such a way that you have three simply connected regions that meet at some point. Colourings of this kind are the basic objects that we’ll consider. Of course, “red”, “green” and “blue” is another way of saying 1, 2 and 3\\. The question is, if you have a dense set of such colourings, must you have a combinatorial line? A combinatorial line in this context will be like what I described except that the three regions meet not at a point but round the boundary of a fourth region, which is the set of variable coordinates. At the moment I know pretty well nothing about this question. I don’t know whether there’s an easy counterexample, I don’t know how to think about the measure on all those colourings (which feels like a fairly nasty object of a kind that the late and much lamented Oded Schramm would have been able to tell us about), and I don’t know whether a positive answer follows from density Hales-Jewett. The one thing I feel fairly confident of is that this, if true would _imply_ DHJ. Oh, and one other thing is that I don’t have any feel for whether this is more or less equal to DHJ in difficulty, or whether there is a chance that it is easier. Actually, there’s another thing that I’m confident of — in fact, certain of — which is that it is a natural generalization of the trivial fact that underlies Sperner. (That is the fact that if you have a dense set of 2-colourings of a line of points, then two of those 2-colourings will form a combinatorial line.) This conjecture is probably not to be taken too seriously, but I quite like the fact that it doesn’t follow easily from DHJ."},{"username":"sune-kristian-jakobsen","timestamp":"2009-02-08T00:09:00.000Z","contents":"Variant of Sperner First I must admit, that I haven’t read much in this thread, so I don’t know if this has been said before. I got this idea while I tried to find some low values of %c_n%.  \nIn 331 Gowers gives a proof that if %\\mathcal{A}% is a antichain, $\\sum_{A\\in\\mathcal{A}}\\frac{1}{\\binom n{|A|}}\\leq 1$. Using the same idea it is possible to show that if %\\mathcal{A}% is a family, that intersects every chain %\\emptyset=A_0\\subset A_1\\subset A_2\\subset\\dots\\subset A_n=[n]%, then $\\sum_{A\\in\\mathcal{A}}\\frac{1}{\\binom n{|A|}}\\geq 1$. Notice that when the elements of %\\mathcal{A}% is only allowed to have one of two given sizes, these two theorems are equivalent. The later theorem seems to be easier to generalize to the k=3 case, and it might be useful, at least in the 200-thread."},{"username":"jozsef","timestamp":"2009-02-08T02:56:00.000Z","contents":"Sperner I’m reading 335, and I’m trying to understand the conjecture. First, there is a coloring theorem for subsets that says that one can find three disjoint sets in the same colour class that their unions are also in that colour class. It is a special case of the Finite Union Theorem (Folkman’s Theorem) which is equivalent to the finite sums theorem. It states that any finite colouring of the natural numbers contains arbitrary large monochromatic IP sets. The density statement is clearly false without extra conditions on the dense subset. It is obvious that the finite union theorem implies the finite sum theorem, however the other direction is difficult; one can use Van der Waerden for the reduction. Maybe we can also prove some density theorem for disjoint set unions using Szemeredi’s thm. It is still not Tim’s cross-product conjecture, but it seems to be quite promising. Let me also mention that the other proof of finite union theorem uses HJ! About the details: a possible reference is Graham, Rothschild, Spencer, “Ramsey Theory”. Now it’s family lunch-time, so I will be away for a while, but I’m quite excited about this possible “density finite union theorem” so I will come back a.s.a.p."},{"username":"gowers","timestamp":"2009-02-08T03:42:00.000Z","contents":"Sperner Jozsef, What I wrote in 335 was meant to be more or less the same as what you suggested in 333 — a very slight generalization perhaps, but you can make it the same if you look at the special case %\\mathcal{S}=\\mathcal{T}=\\mathcal{U}%. And I think that, as you say, it can be proved by the argument of 308\\. Or are we talking about different things?"},{"username":"boris","timestamp":"2009-02-08T04:37:00.000Z","contents":"In comment 335 what is meant in general by DHJ(j,k)?"},{"username":"gowers","timestamp":"2009-02-08T05:02:00.000Z","contents":"Sperner/DHJ Boris, I hadn’t actually worked it out, but I think I can. Basically, DHJ(j,k) is density Hales-Jewett in %\\null [k]^n% but with %j% placing a restriction on the complexity of the dense set %A.% Let me give the case %j=k-1% first. This is equivalent to the normal density Hales-Jewett theorem, but I’ll state it slightly differently. Actually, I think I may as well go for the whole thing right from the start. Suppose that for each subset %E% of %\\null [k]% of size %j% you have a set %\\mathcal{A}_E% of functions from %E% to the power set of %{}[n]% such that the images of all the points in %E% are disjoint sets. Let’s write %(U_j:j\\in E)% for a typical element of %\\mathcal{A}_E%. I now define a set %\\mathcal{A}% of (ordered) partitions of %\\null [n]% into %k% sets %(U_1,\\dots,U_k)% by taking all partitions %(U_1,\\dots,U_k)% such that for every %E% of size %j% we have that %(U_j:j\\in E)% belongs to %\\mathcal{A}_E%. Thus, in a certain sense the set of partitions (which are of course in one-to-one correspondence with sequences in %\\null [k]^n%) depends only on what collections of %j% sets there are in the partition. Then DHJ(j,k) is the claim that if your dense set %\\mathcal{A}% has this special form, then it must contain a combinatorial line. My suggestion, which I think I could justify completely if pushed to do so, is that if %j=1% then one can prove DHJ(j,k) by a fairly simple adaptation of the proof of Sperner as given by Jozsef. Sperner, by the way, is DHJ(1,2), and the problem we have been considering is DHJ(2,3). In general, I suspect that DHJ(j,k) is of roughly equal difficulty to DHJ(j,j+1). It might be interesting to see if we can deduce DHJ(j,k) from DHJ(j,j+1), though at the moment my feeling is that we can prove DHJ(1,k) not by deducing it from DHJ(1,2) but by imitating the proof of DHJ(1,2), which is not quite the same thing."},{"username":"jozsef","timestamp":"2009-02-08T05:03:00.000Z","contents":"Triangle Removal in Kneser Graphs I will go back to the “density finite union theorem” soon as I think it is relevant to our project, but first here is a general remark about the removal lemma. I just want to make it clear that in a certain range we have a real removal lemma. Given three families of sets A,B, and C. If the number of disjoint triples (one element from each) is smaller than X (something depending on the sizes |A|,|B|, and |C|) then a small fraction of the disjoint pairs covers EVERY triangle. To see that, we just have to iterate the counting of “normal” triangles. (post 333.)"},{"username":"boris","timestamp":"2009-02-08T05:17:00.000Z","contents":"I think I misunderstand comment 344\\. Consider DHJ(k-1,k). We have %k% sets %\\mathcal{A}_E% each of which gives a rise to a subset of %{}[k]^n% which I call %B_E%. Then the subset %B% of %{}[k]^n% that corresponds to %\\mathcal{A}% that is the intersection %\\bigcap_{E} B_E%. However, nothing stops us from make %B_E% large, and their intersection empty."},{"username":"gowers","timestamp":"2009-02-08T05:33:00.000Z","contents":"Boris, I wasn’t clear enough in what I meant. Of course, you are right in what you say, but the statement is that _if_ the intersection is dense then you get a combinatorial line, and not the weaker assumption that each individual %B_E% is dense. Of course, in the case %j=k-1% you can consider just a set system %B_{[k-1]}%, just as in Sperner you can consider just the set of points where your sequence is 0."},{"username":"jozsef","timestamp":"2009-02-08T06:22:00.000Z","contents":"Density Finite Union (DFU) Theorem My previous remark was about a possible DFU thm. Let us see the simplest version. What would be a necessary density condition to guarantee a pair of disjoint sets A and B that A,B, and %A\\cup B% are in this “dense” set? Considering the arithmetic analogue, if we have a set %S\\subset [n]% with cn elements a that 2a is also in S, then there is a nontrivial solution for x+y=z."},{"username":"jozsef","timestamp":"2009-02-08T06:37:00.000Z","contents":"DFU contd. The first density condition would be that the number of disjoint pairs is at least %{}c3^n%.The second condition should be something related to sets having 2/3n+-k elements. I’m still not sure what could be a meaningful notation for density here."},{"username":"jozsef","timestamp":"2009-02-08T07:03:00.000Z","contents":"DFU A possible statement could be the following; Given a family F of subsets of [n]. Let us suppose that the number of disjoint pairs in F is at least %{}c3^n.% We are going to define a graph on F. The verices are the sets in F and two are connected iff they are disjoint and their union is also in F. Then, either one can remove %{}o(|F|) % vertices to destroy all edges, or there are %{}c'3^n.% edges where c’ depends on c only. I’m not sure that the statement is true, but something like this should be true."},{"username":"jozsef","timestamp":"2009-02-08T07:14:00.000Z","contents":"DFU (Density Finite Union) theorem. Well, now I see that %{}c'3^n% is way too ambitious; let me change it to f(n)|F| where f(n) goes to infinity."},{"username":"gowers","timestamp":"2009-02-08T07:30:00.000Z","contents":"DFU I’ve got to go to bed, but as a last comment, let me ask whether 350 is intended as a set-theoretic analogue of Ben Green’s theorem that if you have a set with few solutions of %x+y=z% then you can remove a small number of elements and end up with no solutions of %x+y=z%. It would be very nice to be able to prove something like that (and a generalization to larger finite unions). If one were going to try to find a counterexample, one would have a large collection of sets of size around %n/3%, and one would partition them into disjoint pairs, and one would add to the collection the set of unions of these pairs, which would turn them into a matching in your graph. The question would then be whether one could do this without creating %c'3^n% further edges. Here’s an attempt to do so. First we choose a random collection %\\mathcal{F}% of sets of size %2n/3%, big enough that its %n/3% lower shadow is all of %\\binom{[n]}{n/3}%. Next, we choose all sets of size at most %n/3%. Then the number of disjoint pairs is certainly at least %c3^n%. Also, the number of edges is at most %2^{n/3}|\\mathcal{F}|%. Next, it seems to me that it will be very hard to remove all edges: the fact that %\\mathcal{F}% is random and that every vertex is contained in an edge (and we could make %\\mathcal{F}% slightly bigger so that each vertex is in several edges) ought to mean that we can choose a large matching. How big is %\\mathcal{F}%? Well, each set in %\\mathcal{F}% covers %2^{n/3}% small sets, so by my calculations the number of edges is %2^{2n/3}%, which is a lot smaller than %3^n%. Sorry if some of this turns out to be nonsense — I must must sleep now."},{"username":"gowers","timestamp":"2009-02-08T07:32:00.000Z","contents":"DFU Just to say that I wrote 352 before seeing your 351."},{"username":"jozsef","timestamp":"2009-02-08T13:36:00.000Z","contents":"DFU (Density Finite Union conjecture) Let me state here a conjecture. It might not be directly connected to our project, however I’ve been looking for the right formulation for a while and I think that this is the right one; First we have to find the proper notation of density since simple density wouldn’t guarantee the result we are looking for. The “weighted density” of a set %S\\subset [n] % is given by the sum of the (normalized) number of pairwise disjoint k-tuples in S for all 1<kc>0 there are m pairwise disjoint sets, %B_1,...,B_m % that %\\cup_{i\\in I} B_i \\in S% for any nonempty %I\\subset [m] %. This form is clearly a density version of Folkman’s theorem; If we colour the subsets of [n] by K colours, then at least one class has density > 1/K."},{"username":"jozsef","timestamp":"2009-02-08T13:45:00.000Z","contents":"Somehow I lost the middle of the text between 10 355\\. DFU (Density Finite Union conjecture) Let me state here a conjecture. It might not be directly connected to our project, however I’ve been looking for the right formulation for a while and I think that this is the right one; First we have to find the proper notation of density since simple density wouldn’t guarantee the result we are looking for. The “weighted density” of a set %S\\subset 2^{[n]}% is given by the sum of the (normalized) number of pairwise disjoint k-tuples in S. %D(S)={\\frac{1}{n-1}}\\sum_{k=2}^n |\\{(A_1,...,A_k):A_i\\in S, |A_i\\cap A_j|=0\\}|/(k+1)^n%. The maximum number of pairwise disjoint k-tuples is %(k+1)^n% so D(S) is a number between 0 and 1\\. The conjecture is that for any number m, if S contains no m pairwise disjoint elements with all %2^m-1% possible nonempty unions, then it is sparse. D(S) goes to 0 as n goes to infinity. With different words, if S is dense (and large enough) then there are m pairwise disjoint sets, %B_1,...,B_m % that %\\cup_{i\\in I}B_i\\in S % for any nonempty index set %I\\subset [m]%"},{"username":"gowers","timestamp":"2009-02-08T16:51:00.000Z","contents":"DFU Dear Jozsef, I’ve edited your last comment so that the formulae show up, and made a couple of other tiny changes (such as changing %S\\subset [n]% to %S\\subset 2^{[n]}%). Please check that I haven’t introduced any errors. As it stands, I don’t understand your conjecture. A typical disjoint %k%-tuple has all its sets of size around %n/(k+1)%, so it seems to me that the set of all sets of size around %n/(k+1)% is dense in your sense. And there is of course no hope of getting finite unions in this set. Am I misunderstanding something?"},{"username":"jozsef","timestamp":"2009-02-08T23:22:00.000Z","contents":"DFU Tim, there is a 1/(n-1) multiplier, so I thought that one layer (k) will contribute a small fraction only. In particular, I think that rich layers form arithmetic progressions and it might be useful in a possible proof."},{"username":"gowers","timestamp":"2009-02-08T23:37:00.000Z","contents":"DFU Ah — I hadn’t spotted that you were averaging over lots of layers. Now it starts to make more sense to me."},{"username":"boris","timestamp":"2009-02-09T00:25:00.000Z","contents":"Averaging over multiple layers does not seem to help in comment 355\\. Consider the family %S% of all sets of odd size. It contains no disjoint %B_1,B_2\\in S% such that %B_1\\cup B_2\\in S%."},{"username":"boris","timestamp":"2009-02-09T00:30:00.000Z","contents":"Oops… %d(S)=1/n% for the example I gave."},{"username":"boris","timestamp":"2009-02-09T01:05:00.000Z","contents":"I think Jozsef’s conjecture is true, but not for an interesting reasons. I claim that for every fixed positive integer %t% the condition %d(S)>0% implies that %|S\\cap \\binom{[n]}{t}|\\geq (1-o(1))\\binom{n}{t}%. Suppose not, let %T=\\binom{[n]}{t}\\setminus S%. Now let us generate the partition of %{}[n]% into %k% parts as follows. Let %P_1,\\dotsc,P_k% be the partition of %{}[n]% into %k% random parts. If %k=cn% with %c>0%, then %Pr[P_i\\in T]=p% is positive, and is independent of %i%. Had %P_1,\\dotsc,P_k% been independent, it would have implied that %Pr[\\exists i\\ P_i\\in T]=p^k% is exponentially small. Clearly, %P%‘s are not independent, but they are nearly independent. More precisely, let %R_i% be the event that %|P_i|\\leq n^{1/4}%. Then %Pr[ P_i\\cap P_j\\neq \\emptyset |R_i\\wedge R_j]=1-O(n^{-1/2})%. Hence, sampling %n^{1/6}% random sets by choosing each element with probability %1/(k+1)% is virtually indistinguishable from choosing a random %k%-tuple of disjoint sets, and then looking at first %n^{1/6}% of them. I believe I can write the appropriate string of inequalities to show that if asked. Since density of %S% is nearly %1% on each %\\binom{[n]}{t}% the existence of any given configuration follows by simple averaging."},{"username":"jozsef","timestamp":"2009-02-09T02:18:00.000Z","contents":"362. Thanks Boris! It seems that this is still not the right formulation for a DFU conjecture. Actually, I should have noticed that a random selection of sets with probability 1/2 already gives density zero. So, I’m still looking for the right density definition. Boris, do you have any suggestion?"},{"username":"terence-tao","timestamp":"2009-02-09T05:03:00.000Z","contents":"DHJ(j,k) I had to struggle a bit to understand what DHJ(j,k) meant – I had not followed this thread for a while – but when I finally got it, I can see how it fits nicely with the whole hypergraph regularity/hypergraph removal story, and so does look quite promising. Anyway, I thought I’d try to restate it here in case anyone else also wants to know about it… In %{}[3]^n%, we can define the notion of a “(basic) complexity 1 set” to be a set %A \\subset [3]^n% of strings whose 1-set lies in some fixed class %{\\mathcal A}_1 \\subset 2^{[n]}%, whose 2-set lies in some fixed class %{\\mathcal A}_2 \\subset 2^{[n]}%, and whose 3-set lies in some fixed class %{\\mathcal A}_3 \\subset 2^{[n]}%. A simple example here is %\\Gamma_{a,b,c}%; in this case %{\\mathcal A}_1% is the set of subsets of %{}[n]% of size a, and so forth. As I just recently understood over at the obstructions to uniformity thread, basic complexity 1-sets are obstructions to uniformity, and so we definitely need to figure out how to find lines in these sets. In analogy with hypergraph regularity (and also with ergodic theory), the next more complicated type of set might be “non-basic complexity 1 sets” – sets which are unions of a bounded number of basic complexity 1 sets. (In graph theory, basic complexity 1 corresponds to a complete bipartite weighted graph between two cells with some constant weight, and non-basic complexity 1 corresponds to a Szemeredi-type weighted graph between a bounded number of cells, with any pair of cells %V_i, V_j% having a constant weight %d_{ij}% on the edges connecting them.) But it seems fairly obvious that if dense basic complexity 1 sets have lines, then so do dense non-basic complexity 1 sets (though one may have to be careful with quantifiers, and use the usual Szemeredi double induction to organise the epsilons properly). Then we have the basic complexity 2 sets in %{}[3]^n%, which are those sets A in which the 12-profile of A (i.e. the pair consisting of the 1-set and 2-set of A; one can also identify this with an element of %3^{[n]}%) lies in some fixed class %{\\mathcal A}_{12} \\subset 3^{[n]}%, the 23-profile of A lies in some fixed class %{\\mathcal A}_{23} \\subset 3^{[n]}%, and the 31-profile of $A$ lies in some fixed class %{\\mathcal A}_{31} \\subset 3^{[n]}%. But of course when k=3, any one of these complexity 2 profiles determine the entire set A, so every set A is complexity 2, which is why DHJ(2,3) is the same as DHJ(3). [Incidentally, to answer a previous question, this hierarchy seems to be unrelated to my DHJ(2.5) – I was simplifying the lines rather than the sets.] If we knew that sets A which were uniformly distributed wrt complexity 1 sets were quasirandom (a “counting lemma”), then we would be well on our way to reducing DHJ(2,3) to DHJ(1,3) by the usual regularity type arguments. (I’m sure Tim and the others here already know this, this is just me trying to get up to speed.)"},{"username":"gowers","timestamp":"2009-02-09T05:24:00.000Z","contents":"Big picture There is now a definite convergence of this thread and the obstructions-to-uniformity thread, which perhaps illustrates the advantages of not splitting the discussion up. At any rate, this last comment of Terry’s is closely related to comment 411, and also to earlier comments on the 400-499 thread that attempt to establish some sort of counting lemma. I now get the feeling that the problem has been softened up, and that it is not ridiculous to hope that we might actually solve it. But it could be that certain difficulties that feel technical will turn out to be fundamental. Unfortunately, I’ve got two big teaching days coming up, so my input is going to go right down for a while. But perhaps that’s in the spirit of polymath — we get involved, then we go off and do other things, then we come back and catch up with what’s going on, but all the while polymath continues churning away at the problem."},{"username":"terence-tao","timestamp":"2009-02-09T07:36:00.000Z","contents":"Metacomment: Ideally, I suppose we should be working in an environment in which comments from different threads can somehow overlap, coalesce, or otherwise interlink to each other. But, failing that, I think this division into subthreads has been the least worst solution (e.g. the %c_n% for small n thread seems to have taken off once it was separated from the “mainstream” threads of the project). And revisiting the threads every 100 comments or so seems to be just about the right tempo, I think."},{"username":"gowers","timestamp":"2009-02-09T15:49:00.000Z","contents":"Technical clean-up idea This is another comment that could go either here or on the obstructions-to-uniformity thread, but I think it is slightly more appropriate here. Terry, that’s a good point about the 100-comments-per-post rule, and if the 300s and 400s threads really do seem to be converging, then they can merge into the 500s thread. My comment is that I have a variant of DHJ that ought to be equivalent to DHJ itself but has the potential to tidy up a lot of the annoying technicalities to do with slices being of different sizes. (I don’t know for certain that it will work, however.) It’s motivated by the proof of Sperner that I gave in 331\\. There one shows the stronger result that if the slice densities add up to more than 1 then there is a “combinatorial line”. What should the analogue be for DHJ? My contention is the following: define the slice density %\\delta_{a,b,c}% to be %|A\\cap\\Gamma_{a,b,c}|/|\\Gamma_{a,b,c}|%. Then I claim that if %\\sum_{a+b+c=n}\\delta_{a,b,c}\\geq cn^2% (for fixed positive %c% and sufficiently large %n%), then %A% contains a combinatorial line. This conjecture bears the same relation to corners that the strong form of Sperner bears to the trivial fact that if you have at least two points in %\\null[n]% then you get a configuration of the form %x,x+d% with %d\\ne 0%. Given that the averaging argument for Sperner naturally proves this stronger result, I think there is reason to hope that the numbers will all work out miraculously better if we go for this revised version of DHJ. I hope that way we can avoid boring details about being near the middle. I haven’t begun to check whether this miracle really does happen, but I think it’s definitely worth investigating."},{"username":"gowers","timestamp":"2009-02-09T16:07:00.000Z","contents":"Technical clean-up idea. Another way of looking at 365\\. All we’re doing is choosing a random point of %{}[3]^n% not with the uniform measure but by first choosing a slice and then choosing a random point in that slice. It seems to me that this should have huge advantages. For instance, and most notably, the distribution of a random point used to be radically different if you conditioned on it belonging to a typical combinatorial line. Now it’s different but not that different. (With the new measure the situation is on a par with the situation for corners, where the difference between the distributions of random point and random vertex of a random corner is not a serious problem.) But I think there may be all sorts of arguments that will now work out exactly. Perhaps even the quasirandomness calculations I was trying to do will become clean and nice. No time to check though!"},{"username":"nicholas-locascio","timestamp":"2011-11-04T04:35:00.000Z","contents":"Hi everyone. While the Hales-Jewett theorem may be a little beyond my current level of mathematics, I did put something together that I believe the mathematicians that solved this problem would have interest in. I created a fully playable 12 dimensional tic-tac-toe iPhone game.  \nIt is available for free download on the app store. [http://itunes.apple.com/us/app/twelvetacto/id457438285?mt=8](http://itunes.apple.com/us/app/twelvetacto/id457438285?mt=8) My calculation gives 121,804,592 possible solutions for an unaltered playing space via the self-discovered formula:  \ns(d) = (1/2) * ( (k +2)^d – k^d )  \nwhere d = # of dimensions  \nand k = length of side I’ve also defined it recursively as:  \ns(d) = (k+2)*s(d-1) + k^(d-1) In my game, d = 12 and k = 3. Using the basic framework of the game, I do believe a computer-assisted proof would be possible. I do know that computer proofs are generally considered less elegant in the world of higher mathematics, but if anyone is interested, I would be willing to share my source code and collaborate on such a proof."},{"username":"gowers","timestamp":"2009-02-08T23:08:00.000Z","contents":"Quasirandomness. In [comment number 109,](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1816) I asked for the go-ahead to do certain calculations away from the computer. Nobody responded to the request, which suggests to me that making requests that demand blog comments for their replies is not a good idea. In future, if I have such a request I will do it via a poll. (If anybody else wants me to fix up a poll for them, I think I now understand how to get such a poll to appear in a comment, so I’d be happy to do it for them.) However, given that I have _not_ been granted permission to do these calculations, I am going to do them in public instead. The result may be a mess, but I hope that if it is, then at least it will be an informative mess. What I want to do is prove a counting lemma for triangles in a special class of tripartite graphs, where the vertices are subsets of %\\null[n]%, and any edge has to join disjoint sets. (This class has already been much discussed.) Such a lemma would say that if all three parts of the graph are quasirandom, then the number of triangles is roughly what you expect it to be. At this stage, we do not know what “quasirandom” means, so we might seem to have a serious problem. However, this may well not be a problem, since the existing definitions of quasirandomness didn’t just spring from nowhere and turn out to be useful: one can derive them by attempting to prove counting lemmas and seeing what hypotheses you need in order to get your attempts to work. That is what I plan to try to do here. Let me give the definitions that I will need in order to state a counting lemma formally (with the exception of the definition of “quasirandom”, which I’m hoping will emerge during the proof). I shall follow standard practice in additive combinatorics, and attempt to prove a result about %{}[-1,1]%-valued functions rather than about sets. Let %\\mathcal{U}%, %\\mathcal{V}% and %\\mathcal{W}% be three copies of the set of all subsets of %\\null[n]% of size between %n/3-m% and %n/3+m.% Let us write %\\mathcal{U}\\times\\mathcal{V}% not for the usual Cartesian product, but rather for the set of all pairs %(U,V)% such that %U\\in\\mathcal{U}%, %V\\in\\mathcal{V}% and %U\\cap V=\\emptyset%, and similarly for the other two possible products, and also for the product of all three. (I called this the _Kneser product_ in another comment, and may do so again in this thread.) Here is the counting lemma. **Lemma.** Let %f%, %g% and %h% be %c%-quasirandom functions from %\\mathcal{U}\\times\\mathcal{V}%, %\\mathcal{U}\\times\\mathcal{W}% and %\\mathcal{V}\\times\\mathcal{W}% to %\\null[-1,1]%, respectively. Then the absolute value of the expectation of %f(U,V)g(U,W)h(V,W)% over all triples %(U,V,W)\\in\\mathcal{U}\\times\\mathcal{V}\\times\\mathcal{W}% is at most %c'%, where %c'% tends to zero as %c% tends to zero. No guarantee that this is a remotely sensible formulation, but it’s a first attempt. To be continued in my next comment."},{"username":"gowers","timestamp":"2009-02-08T23:31:00.000Z","contents":"Quasirandomness. To understand the lemma, it is helpful to bear in mind the example where each of %f%, %g% and %h% is a function that randomly takes the values 1 or -1\\. Now let’s attempt to prove it, using standard Cauchy-Schwarz methods. To avoid overburdening the notation, I will adopt the following convention concerning whether it is to be understood that sets are disjoint. If I write something like %\\mathbb{E}_{U,V}% then %U% and %V% are disjoint, whereas if I write %\\mathbb{E}_U\\mathbb{E}_V% then they are allowed to overlap. An exception is that if I use the same letter twice then overlap is allowed: so if I write %\\mathbb{E}_{W,W'}% then %W% and %W'% do not have to be disjoint. I will also write %D(U,V)% for the set of %W% that are disjoint from both %U% and %V%, and so on. I hope it will be clear. With the benefit of many similar arguments to base this one on, we try to find an upper bound for the fourth power of the quantity that ultimately interests us. Here goes: %(\\mathbb{E}_{U,V,W}f(U,V)g(U,W)h(V,W))^4% %= (\\mathbb{E}_{U,V}f(U,V)(\\mathbb{E}_{W\\in D(U,V)}g(U,W)h(V,W)))^4% Before I continue, I need to decide whether it is catastrophic to apply Cauchy-Schwarz at this point. The answer is no if and only if I have some reason to suppose that the function %(U,V)\\mapsto\\mathbb{E}_{W\\in D(U,V)}g(U,W)h(V,W)% is reasonably constant (by which I mean that it doesn’t do something like being zero except for a very sparse set of %U%). It seems OK, so let’s continue: %\\leq(\\mathbb{E}_{U,V}f(U,V)^2)^2(\\mathbb{E}_{U,V}(\\mathbb{E}_{W\\in D(U,V)}g(U,W)h(V,W))^2)^2% Now let’s use the fact that %f% is bounded above by 1 in modulus to get rid of it. (This looks ridiculous as we’re throwing away all that quasirandomness information about %f%. But experience with other problems suggests that we shall need just one of the three functions to be quasirandom.) %\\leq(\\mathbb{E}_{U,V}(\\mathbb{E}_{W\\in D(U,V)}g(U,W)h(V,W))^2)^2% Now we multiply out the inner bracket. %=(\\mathbb{E}_{U,V}\\mathbb{E}_{W,W'\\in D(U,V)}g(U,W)h(V,W)g(U,W')h(V,W'))^2% Now interchange the order of summation. %=(\\mathbb{E}_{W,W'}\\mathbb{E}_{U,V\\in D(W\\cup W')}g(U,W)h(V,W)g(U,W')h(V,W'))^2% Hang on. That was wrong, and the first point where something happens that is interestingly different from what happens in other proofs of this kind. The problem is that there is a big difference between interchanging expectations and interchanging summations round here. In fact, even the very first step, which looked innocuous enough, was wrong. I’ll pause here and reflect on this in another comment."},{"username":"terence-tao","timestamp":"2009-02-08T23:41:00.000Z","contents":"Standard obstructions to uniformity I’ll try here to clarify my earlier comment 148\\. Let me start by reviewing the theory of corners %(a,b), (a+r,b), (a,b+r)% in %{}[n]^2%, or more precisely %(\\mathbb{Z}/n\\mathbb{Z})^2% to avoid some (extremely minor) technicalities. Say that a dense set %A \\subset ({\\Bbb Z}/n{\\Bbb Z})^2% is _uniform for the first corner_ if for any other dense sets B, C, the number of corners %(a,b), (a+r,b), (a,b+r)% with %(a,b) \\in A, (a+r,b) \\in B, (a,b+r) \\in C% is close to the expected number. It is standard that A is uniform for the first corner if and only if it does not have an anomalous density on a dense Cartesian product %\\{ (a,b): a \\in X, b \\in Y \\}%. There are similar results for the other corners, e.g. B is uniform on the second corner iff there is no anomalous density on a dense sheared Cartesian product %\\{ (a,b): a+b \\in Z, b \\in Y \\}%, etc. One can of course replace sets by functions: a function f(a,b) is uniform for the first corner iff it does not have an anomalous average on dense Cartesian products, etc. Now return to DHJ. Given a set A in %{}[3]^n%, we can define the function %f(a,b)% on %{}[n]^2% to be the number of elements of A with a 1s and b 2s (and thus c=n-a-b 3s, which I will ignore here). Let me call this function f the _global richness profile_ for A. We know that one good way to construct combinatorial line-free sets A is to ensure that f(a,b) is supported on a set with no corners (indeed, all the best examples in my own thread on this project come this way). More generally, it is pretty clear that A is not going to be quasirandom for the third point of combinatorial lines if its richness profile f is not uniform for the first corner. (I managed to interchange “first” and “third” in my notational conventions, but please ignore this for the sake of discussion.) That’s the “global” obstruction to uniformity. One can also do something similar using “local” obstructions. Consider a reasonably large subset S of [n], and define the _local richness profile_ %f_S(a,b)% of A to be the number of elements of A which have a 1s in S and b 2s in S (and thus |S|-a-b 3s in S). Now, if the local richness profile is supported on a corner-free set, then it is not quite true that A has no combinatorial lines; but the lines in A now must have all their wildcards outside of S, so this should cut down substantially on the number of lines in A (compared with the random set A) if S is even moderately large. For similar reasons, I would expect A to fail to be quasirandom for the third point of a combinatorial line if %f_S% is not uniform for the first corner, and analogously for the other two points of the combinatorial line. Boris suggested replacing the set S with a more general integer-valued weight %w: [n] \\to {\\Bbb Z}%, which should accomplish a similar effect. One can localise these local obstructions even further, by working with slices of A instead of all of A, e.g. by fixing some moderate number of coordinates and then considering a local or global richness profile for the slice on the remaining coordinates (or some subset S thereof, or some weight w). If this profile fails to be quasirandom for most slices, or its density deviates from the mean for most slices, then this should also be an obstruction to uniformity. So I guess my formalisation of a “standard obstruction to uniformity” for the third point of a combinatorial line for a set %A \\subset [3]^n% of density %\\delta%, would consist of a small set T of fixed indices, then a disjoint large subset S of sampling indices, and dense sets X, Y of %{}[n]% (or more precisely %{}[|S|]%) such that for most of the %3^{|T|}% choices %\\alpha% of the fixed indices, the local richness profile %f_{S,\\alpha}(a,b)%, defined as the number of elements of A with coordinates %\\alpha% on T, and with a 1s in S and b 2s in S, has density significantly different from %\\delta% on %X \\times Y%. (Actually, as I write this it occurs to me that one may need X, Y to depend on %\\alpha%.) Anyway, that’s my proposal for the standard obstruction… hopefully it makes some sense."},{"username":"gowers","timestamp":"2009-02-08T23:54:00.000Z","contents":"Standard obstructions to uniformity. Thanks very much for that Terry. I’ve just read it rather quickly, and I get the impression that because it focuses strongly on the cardinalities of sets, it may not have picked up the full (and new since your comment 148 ) generality of Example 2 in my post above. I’m not over-worried about that, as I have a strong feeling that that example can be dealt with. But if I am right that it isn’t a standard obstruction to uniformity then it would be good to try to generalize the definition."},{"username":"gowers","timestamp":"2009-02-09T00:45:00.000Z","contents":"Quasirandomness. What, then was the problem with the first line of the calculations in 401? What I did was to replace %\\mathbb{E}_{U,V,W}% by %\\mathbb{E}_{U,V}\\mathbb{E}_{W\\in D(U,V)}.% Here, the first expectation was over disjoint triples %(U,V,W)% while the second took an expectation over disjoint pairs %(U,V)% of the expectation over all sets %W% that were disjoint from %U\\cup V%. And there is a huge difference between the two, since in the second we count all disjoint pairs %(U,V)% equally, whereas in the first we count a pair %(U,V)% far more if the size of %|U\\cup V|% is smaller, since then there are many more %W% that are disjoint from %U% and %V%. And since the sizes of the sets are not fixed, this is a serious concern. For now I’m going to restrict attention to the simpler set-up where %n=3m+k% for some smallish %k% and all our sets have size %m%. So now the sizes of the sets %D(U,V)% will always be %\\binom{m+k}{m}%. However, while this corrects the first line, it doesn’t get us out of the woods, because there is no trick like that that we can pull off to sort out the second interchanging of the order of summation. And that is because there is no getting away from the fact that %W\\cup W'% can vary in size. However, it now occurs to me that this may not be such a huge problem. The reason is that most of the time %W\\cup W'% is of maximal size %m+k%, since the smallness of %k% makes it rather unlikely that a point will be in the complement of both %W% and %W'%. (What’s more, the probability that the complement has size %t% goes down like %(k/m)^t% while the gain in the number of pairs %(U,V)% goes up like %C^t%, so the exceptions should be insignificant.) I’m not sure that this reasoning is correct, but for now let us pretend that the problem doesn’t exist and that that interchange of summation was after all legal, except that I should have written %\\approx \\Bigl(\\mathbb{E}_{|W\\cup W'|=m+k}\\mathbb{E}_{U,V\\in D(W\\cup W')}% %g(U,W)h(V,W)g(U,W')h(V,W')\\Bigr)^2.% So the next stage is to apply Cauchy-Schwarz, which gives us %\\leq \\mathbb{E}_{|W\\cup W'|=m+k}\\Bigl(\\mathbb{E}_{U,V\\in D(W\\cup W')}% %g(U,W)h(V,W)g(U,W')h(V,W')\\Bigr)^2.% Now I’m getting worried because fixing %U\\in D(W\\cup W')% determines %V%, and this feels wrong. So I’m going to start all over again, this time using sums rather than expectations."},{"username":"gil","timestamp":"2009-02-09T01:00:00.000Z","contents":"density-increasing-strategies & quasirandomness Few remarks (which may be wrong so please correct me or hint if they are) Let me split them and start with one). a) when we talk about quasi-randomness we often talk about “codimension-one-quasi-randomness” we start with the characteristic function f of the set, we show that if the number of desirable configuration is not what we expect then the function has some large correlation with some other function g; then we use it to somehow show that the set has higher density on a small dimensional “space” (subcube of some kind.) We can talk about conditions for quasirandomness that refer directly about  \nlow dimensional subspaces. i.e. look at all the various irregular examples that we have and ask: do we see there unusually high density for combinatorial subspace (or some more complicated gadget) of lower dimension – square root of n, log n etc. (It is true that we do not have any proof-techniqus that jumps directly to low dimensional subspaces or “varieties” without passing through a codimension 1 gadget.)"},{"username":"gil","timestamp":"2009-02-09T01:14:00.000Z","contents":"Dear Tim, maybe rule 6 should be slightly relaxed and people could make off-line few-hour calculations without general concensus; (actually after just reading the rules, I would consider also a no-rules version of this endeavor.) In any case, I expect a wide concensus on letting you do the sum version on-line if you so prefer and perhaps granting you a wider mandate for related calculations. (I vote yes on both.)"},{"username":"gowers","timestamp":"2009-02-09T01:34:00.000Z","contents":"Quasirandomness. This time I do everything with sums and try to interpret it later. The disjointness conventions are as before. I haven’t repeated the remarks between lines. %(\\sum_{U,V,W}f(U,V)g(U,W)h(V,W))^4% %= (\\sum_{U,V}f(U,V)(\\sum_{W\\in D(U,V)}g(U,W)h(V,W)))^4% %\\leq(\\sum_{U,V}f(U,V)^2)^2(\\sum_{U,V}(\\sum_{W\\in D(U,V)}g(U,W)h(V,W))^2)^2% %\\leq|\\mathcal{U}\\times\\mathcal{V}|^2(\\sum_{U,V}(\\sum_{W\\in D(U,V)}g(U,W)h(V,W))^2)^2% Now we divide by the factor %|\\mathcal{U}\\times\\mathcal{V}|^2%. Having done that we multiply out the inner bracket. %=(\\sum_{U,V}\\sum_{W,W'\\in D(U,V)}g(U,W)h(V,W)g(U,W')h(V,W'))^2% Now interchange the order of summation. %=(\\sum_{W,W'}\\sum_{U,V\\in D(W\\cup W')}g(U,W)h(V,W)g(U,W')h(V,W'))^2.% Now the difficulty we face is much clearer. Normally one should apply Cauchy-Schwarz only if a function is broadly speaking reasonably constant. But here the inner sum is far bigger if %|W\\cup W'|% is small, and in fact it is zero for most %W% and %W'%. So let’s try to think what a typical quadruple of sets %(U,V,W,W')% looks like if we know that all four sets have size %m%, and all six pairs of them are disjoint apart from the pair %(W,W')%. (I’m still assuming, for simplicity, that %n=3m+k% for some smallish %k% and that all sets have size %m%.) I now realize I made a mistake earlier. If %|W\\cup W'|=m+j%, then the number of disjoint pairs %(U,V)% in %D(W\\cup W')% is %\\binom{2m+j}{m,m,j}=\\binom{2m+k-j}m\\binom{m+k-j}m.% If you decrease %j% by 1, the first of these binomial coefficients goes up by a factor of approximately 2, but the second goes up by more like %m/(k-j)%. As for the number of pairs %(W,W')% such that %|W\\cup W'|=m+j%, it equals %\\binom{3m+k}{m}\\binom{m}{m-j}\\binom{2m+k}j%. The second of these goes down by a factor of about %m/j% and the third goes down by a factor of about %2m/j%. So it still seems as though the sum is dominated by pairs with %|W\\cup W'|=m+k%. Ah, I’m forgetting that I should be thinking about the _square_ of the inner sum, and now things look a lot healthier. So if you decrease %j% by 1, then the square of the inner sum goes up by about %(2m/(k-j))^2% and the number of %(W,W')% pairs goes down by around %2(m/j)^2%. So some kind of concentration should occur  \naround the %j% that makes these two equal, which is %k/(1+\\sqrt{2})%. (I admit it: I cheated and used a piece of paper there.) I need to stop this for a while. To be continued."},{"username":"gowers","timestamp":"2009-02-09T03:48:00.000Z","contents":"Quasirandomness. I think at this point it helps to bring in at least one expectation. Where I’ve written %\\sum_{U,V\\in D(W\\cup W')}% I will now write %d_2(W,W')\\mathbb{E}_{U,V\\in D(W\\cup W')}%, where %d_2(W,W')% is the number of disjoint pairs %(U,V)% in %D(W\\cup W')%, and then apply Cauchy-Schwarz. That gives me %(\\sum_{W,W'}\\sum_{U,V\\in D(W\\cup W')}g(U,W)h(V,W)g(U,W')h(V,W'))^2% %=\\Bigl(\\sum_{W,W'}d_2(W,W')\\mathbb{E}_{U,V\\in D(W\\cup W')}% %g(U,W)h(V,W)g(U,W')h(V,W')\\Bigr)^2% %\\leq (\\sum_{W,W'}d_2(W,W'))\\times% %\\sum_{W,W'}d_2(W,W')\\Bigl(\\mathbb{E}_{U,V\\in D(W\\cup W')}% %g(U,W)h(V,W)g(U,W')h(V,W')\\Bigr)^2.% Now the first bracket is just the number of quadruples %(U,V,W,W')% of sets of size %m% such that all sets with different letters are disjoint. So let’s drop it and try to estimate the second. In this model, we are assuming that %U% and %V% both have size %m%. So I’m allowed to replace %\\mathbb{E}_{U,V\\in D(W\\cup W')}% by %\\mathbb{E}_{U\\in D(W\\cup W')}\\mathbb{E}_{V\\in D(U,W\\cup W')}%. That gives %\\sum_{W,W'}d_2(W,W')\\Bigl(\\mathbb{E}_{U\\in D(W\\cup W')}\\mathbb{E}_{V\\in D(U,W\\cup W')}% %g(U,W)h(V,W)g(U,W')h(V,W')\\Bigr)^2,% which by Cauchy-Schwarz and the boundedness of %g% is at most %\\sum_{W,W'}d_2(W,W')\\mathbb{E}_{U\\in D(W\\cup W')}% %\\Bigl(\\mathbb{E}_{V\\in D(U,W\\cup W')} h(V,W)h(V,W')\\Bigr)^2.% Expanding the bracket gives us %=\\sum_{W,W'}d_2(W,W')\\mathbb{E}_{U\\in D(W\\cup W')}\\mathbb{E}_{V,V'\\in D(U,W\\cup W')}% %h(V,W)h(V,W')h(V',W)h(V',W').% This is a slight mess but it makes me hopeful, for reasons that I’ll explain in my next comment."},{"username":"terence-tao","timestamp":"2009-02-09T04:29:00.000Z","contents":"Standard obstructions to uniformity Tim, you’re right, Example 2 (where the 1-set, 2-set, and 3-set of A are independently constrained) is quite different from the class I had in mind. It may be that this is the more fundamental example, and I will try to think about whether the other examples we have can be reformulated to resemble this one. It may take some time before I will be able to look at it, though."},{"username":"gowers","timestamp":"2009-02-09T04:30:00.000Z","contents":"Quasirandomness. The above calculation ends up with something a little bit strange, but here’s what I hope might happen if it is tidied up. Given any %U% we can define a bipartite graph whose vertex sets are the set of all %V% that are disjoint from %U% and the set of all %W% that are disjoint from %U% and whose edges are pairs %(V,W)\\in\\mathcal{V}\\times\\mathcal{W}% (still the Kneser product). If we restrict %h% to this bipartite graph, we can “count 4-cycles”, or rather evaluate the sum of %h(V,W)h(V,W')h(V',W)h(V',W'),% restricted to this bipartite graph. Could one perhaps hope that if this sum is on average small, then the function %h% is sufficiently quasirandom to make the original sum (at the beginning of 406) or expectation (at the beginning of 401) small? The calculations above suggest that something like that is true. But one needs to be wary since the intersection of the neighbourhoods of %V% and %V'% will get rapidly smaller as %|V\\cup V'|% gets bigger, and this probably needs to be allowed for somehow. But instead of trying to sort that out, I want to speculate a bit, on the assumption that it can be made to work."},{"username":"gowers","timestamp":"2009-02-09T05:13:00.000Z","contents":"Obstructions to uniformity. As a result of thinking about what sort of global obstruction to uniformity might come out of the failure of quasirandomness of the type just discussed, it has just occurred to me to wonder whether in fact all the obstructions we have so far are special cases of Example 2\\. That is, if %A% is one of our basic examples of a set with the wrong number of combinatorial lines, can we always find collections of sets %\\mathcal{U}%, %\\mathcal{V}% and %\\mathcal{W}% such that %A% correlates unduly with the set of all sequences %x% with %U_1(x)\\in\\mathcal{U}%, %U_2(x)\\in\\mathcal{V}% and %U_3(x)\\in\\mathcal{W}%, and this set of sequences is dense. (Recall that %U_j(x)% is the %j%-set of %x% is the set of coordinates i where %x_i=j%.) Let’s try Example 1\\. To keep things simple, let’s go for the example of the sum of all the coordinates being a multiple of 7\\. What could we take as %\\mathcal{U}%, %\\mathcal{V}% and %\\mathcal{W}% in this case? Easy: we can take them all to consist of all sets for which the number of elements is a multiple of 7\\. (If %n% isn’t a multiple of 7, then we can find %a,b,c% such that %a+b+c\\equiv n% and %a+2b+3c\\equiv 0% mod 7 and ask for sets in %\\mathcal{U}% to have cardinality congruent to %a% etc.) It’s clear that if we had different weights but still worked mod 7 we could do something similar, and I’m pretty sure that generalizing to structured sets in Abelian groups doesn’t make things substantially harder. Of course, all this is to be expected because of Terry’s comments about standard obstructions. How about Ryan’s example? There we could simply take %\\mathcal{U}%, %\\mathcal{V}% and %\\mathcal{W}% to consist of all sets that contain a run of %m% consecutive elements. Based on that, here is a conjecture. **Conjecture:** Let %A% be a subset of (the union of a suitable triangular grid of slices near the centre of) %{}[3]^n% that does not have the expected number of combinatorial lines. Then there are set systems %\\mathcal{U}%, %\\mathcal{V}% and %\\mathcal{W}% such that for a dense set of sequences %x% we have %U_1(x)\\in\\mathcal{U}%, %U_2(x)\\in\\mathcal{V}% and %U_3(x)\\in\\mathcal{W}%, and such that, setting %B% to be the set of all such sequences, the density of %A\\cap B% in $B$ is significantly greater than the density of %A%. This conjecture is a direct analogue of a fact that is known to be true for corners. (This fact was basically mentioned by Terry in comment 402.) It’s a slight generalization of what Terry was suggesting, and I think there’s a definite chance that it could have dealt with question 3 in the main post. If anyone can think of an obstruction to uniformity that is _not_ of this form, then they will have demonstrated that DHJ is different in a very interesting way from the corners problem. Such an example would give a big new insight into the problem. But I’m an optimist and at the moment I am guessing that the conjecture is true. _[Note: if this comment seems to ignore Terry’s comment 409, it’s because his comment appeared while I was writing it.]_"},{"username":"boris","timestamp":"2009-02-09T13:13:00.000Z","contents":"The conjecture in 411 is, I am afraid, too optimistic. Observe that there are at most %O(\\sqrt{n})% wildcards in every combinatorial line that is contained in a union of slices near the center of %{}[3]^n%. That is because the number of %1%‘s does not vary by more than that. Thinking of an element of %{}[3]^n% as a word of length %n% in %\\{1,2,3\\}%, let %X_1, X_2% be the number of occurrences of subwords %123%, and %321% respectively (“subword” means a sequence of _consecutive_ letters). Let %X=X_1-X_2%. Note that typically %X% varies in the interval of length %\\Theta(\\sqrt{n})% around origin. Thus, if %m\\ll O(\\sqrt{n})%, it is reasonable to suppose that %X\\bmod m% is going to be distributed uniformly in %\\{0,\\dotsc,m-1\\}%. Assume it is. Choose %m=n^{1/3}% for the sake of definiteness (we need it to be greater than %n^{1/4}% for what follows). Define %\\mathcal{A}% by the condition %X\\bmod m\\in \\{0,1,\\dotsc,m/2\\}%. The density of %\\mathcal{A}% is %1/2%, but I claim that the number of combinatorial lines is close to %1/2%. Indeed in a typical combinatorial line, the letters adjacent to wildcards are not wildcards themselves. By symmetry, as change value of wildcards the expected change in %X_1% is equal to expected change in %X_2%. Since both %X_1% and and %X_2% are concentrated in an interval of length %\\sqrt{n^{1/2}}=n^{1/4}% (recall that the number of wildcards is %n^{1/2})%, we infer that typically %X% does not change by more than %n^{1/4}%. Since %m\\gg n^{1/4}%, if a combinatorial line contains a single element of %\\mathcal{A}%, then it is very likely to be wholly contained in %\\mathcal{A}%. It seems intuitively clear to me that there is no density increment on %\\mathcal{A}% of the kind desired in comment 411."},{"username":"terence-tao","timestamp":"2009-02-09T13:56:00.000Z","contents":"Correlation with “Cartesian semi-products” I found a weird variant of the conjecture in 411 that I don’t know how to interpret. Let %{\\mathcal A}% be a dense subset of %{}[3]^n% without combinatorial lines, or equivalently a collection of partitions %{}[n] = A \\cup B \\cup C% of [n] that does not contain a “corner” %(A \\cup D, B, C), (A, B \\cup D, C), (A, B, C \\cup D)%. Now pick a random subset C of [n] of size about n/3 (up to %\\sqrt{n}% errors) and look at the slice of %{\\mathcal A}% with this C as the third set in the partition. This is basically a set system in %{}[n] \\backslash C%, which we expect to be dense on the average. Now, following the proof of Sperner as in 331, let us pick a random maximal chain %\\emptyset = A_0 \\subset \\ldots \\subset A_{n-|C|} = [n] \\backslash C%. On the average, we expect this chain to be dense in the equator (the part of the chain with cardinality %n/3 + O(\\sqrt{n})%. Thus %{\\mathcal A}% contains roughly %\\sqrt{n}% triples of the form %(A_j, [n] \\backslash (A_j \\cup C), C)% where %j =n/3+O(\\sqrt{n})%. Let J be the set of all such j. Since %{\\mathcal A}% avoids all combinatorial lines, this means that %{\\mathcal A}% must avoid the “Cartesian semi-product” %\\{ (A_j, [n] \\backslash (A_k \\cup C), C \\cup (A_k \\backslash A_j): j < k; j,k \\in J \\}%, which implies that the set %\\{ (j,k): j,k = n/3 + O(\\sqrt{n}); j < k; (A_j, [n] \\backslash (A_k \\cup C), C \\cup (A_k \\backslash A_j)) \\in {\\mathcal A} \\}% avoids the Cartesian product %J \\times J%. In particular, it fails to be quasirandom with respect to such products, and so by two applications of Cauchy-Schwarz, it must have an anomalously large number of rectangles. Averaging over all choices of C and over all choices of maximal chain, this seems to imply that %{\\mathcal A}% contains an anomalously large number of “rectangles” %(A_1, B_1, [n] \\backslash (A_1 \\cup B_1)), (A_2, B_1, [n] \\backslash (A_2 \\cup B_1)), (A_1, B_2, [n] \\backslash (A_1 \\cup B_2)), (A_2, B_2, [n] \\backslash (A_2 \\cup B_2))% where %A_1 \\subset A_2% and %B_1 \\subset B_2% all have cardinality about %n/3 + O(\\sqrt{n})%. If it were not for the additional constraints %A_1 \\subset A_2%, %B_1 \\subset B_2%, this would start coming quite close to the conjecture in 411\\. As it is, I don’t know where to take this next."},{"username":"terence-tao","timestamp":"2009-02-09T14:11:00.000Z","contents":"I was going to add that all the examples I mentioned in 148 happened to be of the right form for the conjecture in 411, but Boris in 412 seems to have created a totally new species of non-quasirandom set that needs to be understood better. This example is of course consistent with what I just discussed at 413, by the way: in a typical “Cartesian semi-product”, the 132 and 123 statistics don’t vary by much more than %O( n^{1/4} )%, and so such semi-products are likely to either lie totally in %{\\mathcal A}% or totally outside of %{\\mathcal A}%. (Incidentally, these Cartesian semi-products are closely related to the sets S discussed in Tim’s 331.)"},{"username":"gowers","timestamp":"2009-02-09T15:24:00.000Z","contents":"Obstructions to uniformity. No time for serious mathematics at the moment, but I did want to give my gut reaction to the last few comments. First, my usual experience when making an overoptimistic conjecture is to go to bed, wake up, and see why it’s nonsense (perhaps not immediately, but at some point when my mind wanders back to it and I’m in a more sceptical mood). With this collaboration it’s different: I wake up and find a counterexample on the blog! Having said that, I did very much like that 411 conjecture, so when I get the time I plan to do two things, unless someone has already done them. First is to try to define some set systems that do, contrary to expectation, give rise to a correlation with Boris’s example. But at the moment I share his intuition that the roles of the coordinate values are somehow too intertwined to make this possible, so my main reason for doing this would be to try to get at a proof that it really is a new obstruction, or at least a sketch of a proof. (That may not be too difficult to achieve, so I’ll check from time to time to see if someone else has already done it.) Other things that one could do in response to the example are to try to formulate a yet more general notion of obstruction to uniformity that includes his example. I don’t have any idea yet what that might look like, but it would be great if we could come up with something simple. It would also be good to find a combinatorial subspace in which his set is unusually dense. (Maybe Terry has done that above — I haven’t yet followed his comments enough to be able to see.) And finally, I would like to think hard about what obstructions to uniformity would arise naturally from a set’s not being quasirandom according to the definition that half-emerged from the calculations leading up to 410\\. (In 411 I just jumped to conclusions, and it now looks as though I did so prematurely.) Here I would take my cue from the way I proved one version of the regularity lemma for hypergraphs. I was in precisely this situation: there were a lot of bits of local non-uniformity (in small neighbourhoods) and one somehow had to add them up to obtain a global non-uniformity. It turned out to follow because the neighbourhoods with the local non-uniformities were nicely spread about. I think there’s a good chance of pulling off a similar trick here. (An argument of the type I’m talking about appears in Section 8 of [my 3-uniform hypergraphs paper](http://www.dpmms.cam.ac.uk/~wtg10/belapaper.pdf) in the run-up to the proof of Lemma 8.4, but our situation might be slightly simpler as we’re talking about graphs rather than hypergraphs.) Terry, I’m looking forward to understanding Cartesian semi-direct products better, but I’m coming to see the kind of thing you’re doing. Actually, the more I think about it, the more I wonder if it is the kind of thing I was hoping to get to in the calculations at the beginning of this thread. If so, then the remarks towards the end of the previous paragraph of this comment could be helpful in taking things further."},{"username":"gowers","timestamp":"2009-02-09T15:26:00.000Z","contents":"Obstructions to uniformity. It also occurs to me that Boris’s thought processes could be extremely helpful here. Did that example just jump out of thin air? If not, then were there some preliminary “philosophical thoughts” that led to it? If the latter, then it could be useful to know what they were so that we have a better chance of making correct guesses about how a proof might go."},{"username":"gil","timestamp":"2009-02-09T16:45:00.000Z","contents":"quasirandomness. (405 (point a) continued) So a concrete question is this: suppose that a set of density c has density at most 2c on all combinatorial subspaces of dimension 100 log n (or log^2 n; or n^{1/1000}) does it necessarily have the right number of combinatorial lines? (Is any of our non quasi random example violate it?)  \n(Of course in the next procedural remark by “on-line” I meant “off-line”.)"},{"username":"boris","timestamp":"2009-02-09T22:08:00.000Z","contents":"Answer to 416: My thought process was simple: First the analogue of Tim’s conjecture for Sperner is false (if one thinks of Sperner as DHJ(2,1), then it is not true that obstructions come from DHJ(2,0) because there is no DHJ(2,0)), and hence the conjecture should be false. Second, one needs some example where coordinates are somehow linked together. Third, if we try to impose a condition on number of pairs 12, then the only condition I really know how to impose is a modular condition. It fails to give a counterexample since if one element of the combinatorial line has some number of these, it tells us next to nothing about the number of 12’s in other elements of the combinatorial line because the number of 12’s decreases by a lot when we change wildcard from 2 to 3\\. Well, then I thought about compensating for this huge change by forming some linear combination. My first construction was to count number of 12, 23, and 31 and add them weighted with third roots of unity. Finally, I simplified the construction. Comment regarding to 417: If we take any known obstruction to the uniformity of density %c% and take a union with a random set of density %c/2% wouldn’t we obtain an example Gil seeks?"},{"username":"terence-tao","timestamp":"2009-02-09T23:57:00.000Z","contents":"Just a quick comment, will respond at length later when I am less busy: Boris’s 412 example has the property that all coordinates have extremely low influence: modifying a 1 to a 2, say, is unlikely to move you from A to not-A, or vice versa. As a consequence, it is easy to find lots of subspaces with a density increment, just by selecting a moderate number of indices and jiggling them from 1 to 2 to 3 independently. Presumably one can more generally push the density increment argument to reduce to sets A in which all indices (and maybe sets of indices) have small influence, but I have not thought about this carefully."},{"username":"gowers","timestamp":"2009-02-10T00:02:00.000Z","contents":"Obstructions to uniformity. Boris, many thanks for that — as I hoped, it was very illuminating. In particular, your initial step is thought-provoking. My first reaction to it is to try to prove you wrong by formulating DHJ(2,0), but with no particular hope of success. If indeed your reasoning is right, then it’s good news in a way, because it suggests that we can get a better understanding of obstructions in DHJ(3,2) by thinking a bit harder about obstructions in DHJ(2,1). Here’s what DHJ(2,0) might say. (In order to do this I’m keeping open [comment 344](https://gowers.wordpress.com/2009/02/06/dhj-the-triangle-removal-approach/#comment-1964) in a different tab and trying to extend the definition of DHJ(j,k) downwards to DHJ(0,k).) For every empty subset %E\\subset[k]% you have a set %\\mathcal{A}_E% of functions from %E% to the power set of %\\null[n]% such that the images of all points in %E% are disjoint. So far so good: there is precisely one such function, so %\\mathcal{A}_E% is a singleton that contains just that trivial function. Now we define %\\mathcal{A}% to be the set of all ordered partitions of %{}[n]% into %k% sets %U_1,\\dots,U_k% such that for every empty subset %E% of %{}[k]% the sequence %(U_j:j\\in E)% belongs to %\\mathcal{A}_E%. But that happens for every ordered partition of %{}[n]% into %k% sets, so %\\mathcal{A}% corresponds to the set of all sequences. So I claim that DHJ(0,k) is the assertion that %{}[k]^n% contains a combinatorial line! This doesn’t really affect the substance of what Boris is saying, but it allows us to reexpress it. The conjecture DHJ(0,k) (which, come to think of it, I think I see how to prove), suggests a class of 0-level obstructions: you may well not get the expected number of combinatorial lines if you have the wrong correlation with the constant function 1 — that is, if you have the wrong density. So the real point that Boris is making is that having the wrong density is not the only obstruction to Sperner (whereas it is to the trivial one-dimensional analogue of corners, since the number of one-dimensional corners is precisely determined by the density of the set). Thus, Boris’s thought processes do indeed contain a valuable new insight: that we should think hard about obstructions to Sperner, because these will feed into DHJ as an _additional_ source of obstructions to DHJ(2,3), over and above the ones that come from DHJ(1,3). In this way, we see that there is something about DHJ(2,3) that genuinely goes beyond what happens for corners. Unfortunately, it is still completely mysterious to me what that extra something is, but equally it is clear that what we must try to do is generalize Boris’s obstruction as much as we possibly can and hope that we can reach a new and more sophisticated conjecture. A good start will be to see what we can do with obstructions to Sperner. (And perhaps there is some hope here, as people already seem to have thought about it. If anyone feels like giving a detailed summary of what is known, they will be doing me at least a big favour.)"},{"username":"gil","timestamp":"2009-02-10T00:55:00.000Z","contents":"Boris, i do not think your example is a counter example. If you take the obstructions for pseudorandomness (at least the 2 I checked) you gets large combinatorial subspaces in the set. E.g. if the number of 1,2,3 coordinates is divisible by 7 or if there is a long runs. OK i see, what you probably suggest is this: start with a obstruction with high density say 7/8 take a random subset of it which has density c and then even in the large combinatorial spaces the density will be below 8/7\\. (You do not need to add anything random) Right. Anyway, we can still ask: If the number combinatorial lines is more than 10 times or less than 1/10 times the expected number, must we have a combinatorial subspace of dimension 100logn or log^2n or n^1/100 that the density on it is more than 1.01c."},{"username":"ryan-odonnell","timestamp":"2009-02-10T03:00:00.000Z","contents":"This is response to Terry’s #413\\. Let me just try to rephrase it in my own language. Let %\\mathcal{A}% be a subset of %{}[3]^n% of density at least %\\delta%. Form a random “subcube” as follows: Pick a random string %r \\in \\{\\ast, 3\\}^n% by choosing each coordinate to be %\\ast% with probability %2/3% and %3% with probability %1/3%. Let %Q_r% be the subcube %\\{1,2\\}^m%, given by taking all ways of filling in %\\ast% with %1% or %2%. (Here %m% is the number of %\\ast%‘s in %r%.) For a given string %x \\in Q_r%, let’s abuse notation slightly by writing %(x,r)% for the associated string in %{}[3]^n%. It is easy to see that with probability at least %\\delta/2% over the choice of %r%, the density of %\\mathcal{A}% on %Q_r% is at least %\\delta/2%, a positive constant. So we can apply some kind of souped-up Sperner. In particular, I believe it should be relatively easy to show that %\\mathcal{A}% contains many “half-lines” on %Q_r%. I.e., if we look at pairs %(x,y) \\in Q_r% such that %x \\prec y% and %(x,r), (y,r) \\in \\mathcal{A}%, it should be that the set of %x%‘s involved in such half-lines has positive density in %Q_r%, and the same for the %y%‘s. But since %r% was random from the right distribution, we conclude the following: Look at all “half-lines” in %\\mathcal{A}%; i.e., pairs %u,v% with the potential to be the %1%– and %2%-points of a combinatorial line. Then the %u%‘s participating in these half-lines have positive density in %{}[3]^n%, and same for the %v%‘s. Now it almost seems like we’re there, because we have a pretty dense set of half-lines. We just need to find one point in %\\mathcal{A}% to cap off a half-line. I guess the trouble is that, while the set of points capping off a half-line has positive density in %{}[3]^n%, it still might completely miss %\\mathcal{A}%. So perhaps the goal should be to show that either these capping points are *very* dense, or else we can see some structure in them out of Sperner."},{"username":"ryan-odonnell","timestamp":"2009-02-10T03:31:00.000Z","contents":"Re Boris’s example #418: One thing I do not quite understand: What is the definition of “typical combinatorial line”?"},{"username":"ryan-odonnell","timestamp":"2009-02-10T03:55:00.000Z","contents":"Re Terry’s #419. I’m still a bit confused by this density-increment search. One finds counterexamples with the “wrong” number of combinatorial lines, but then one says, “Aha, but if I fix these coordinates this way, I get a large density increment.” Now Boris gives a counterexample in which each coordinate has low “influence”. This is the only way to beat the above density-increment parry — if your example has coordinates with large influence, then you can of course fix these coordinates to get density increment. But now in #419 Terry says, “Aha, if the coordinates all have *low* influence, we should be able to find a density increment.” It seems like we can always find a density increment! This is related to my comment #123."},{"username":"boris","timestamp":"2009-02-10T04:00:00.000Z","contents":"Answer to Ryan’s question in #423: it is not important, as long as the definition is invariant under permutation of %{}[n]%. The only thing the construction uses is that no combinatorial line uses more than %O(\\sqrt{n})% wildcards. Then if we take an orbit of any combinatorial under the action of symmetric group on %{}[n]% most elements of the orbits will have virtually no adjacent wildcards."},{"username":"boris","timestamp":"2009-02-10T04:03:00.000Z","contents":"I do not follow Ryan’s argument in #424\\. If there are variables of large influence, it does not mean we can fix them to get density increment. Think of XOR function on the Boolean cube. All coordinates have large influence, but on every coordinate subcube the density is still %1/2%."},{"username":"gowers","timestamp":"2009-02-10T04:04:00.000Z","contents":"Re 424,419 Now I’m confused by Ryan’s comment. If %A% is a random set, then doesn’t every coordinate have large influence? And you can’t get a density increase. Apologies if this shows that I don’t properly understand what “influence” means."},{"username":"gowers","timestamp":"2009-02-10T04:08:00.000Z","contents":"Obstructions and possible technical simplification. I would like to draw attention to [my comments 365 and 366](https://gowers.wordpress.com/2009/02/06/dhj-the-triangle-removal-approach/#comment-2047) because they could be relevant here. Boris, if we use this alternative measure on %\\null [3]^n%, then does your example disappear?"},{"username":"boris","timestamp":"2009-02-10T04:27:00.000Z","contents":"Answer to #428: I do not think it disappears. It has density 1/2 on every %\\Gamma_{a,b,c}%. This example seems to be rather hard to kill since as mentioned in #425 the only thing it really uses is invariance under permutations of %{}[n]%. Actually, now thinking about it, there is a way to simplify/modify the example from #412 somewhat. I considered %X=X_1-X_2%, but I could consider %X_1% or simply the number of subwords %12%. Yes, these quantities do vary by a lot, but this variation is predictable: it has some mean %M% that can be computed explicitly, and is concentrated in the interval around %M% of length %n^{1/4}%. Choose %m% so that %M\\bmod m% is distance at least %n^{1/3}% from zero (large sieve should tell us that this can be done for most %M%, and maybe there is some elementary argument to do it for all %M% that I do not see). Then define %\\mathcal{A}% by condition that %(X_1\\bmod m)\\bmod(M \\bmod m)\\in \\{0,...,(M\\bmod m)/2\\}%."},{"username":"terence-tao","timestamp":"2009-02-10T04:31:00.000Z","contents":"Influence vs. Bias Ah, where is Gil when you need him <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":-)\">:-)</span>. There is a distinction between influence and bias for, say, a boolean function %f(x_1,\\ldots,x_n)% of n boolean inputs. We say that the input %x_i% has large influence if, whenever we flip %x_i%, we have a high probability of flipping %f(x_1,\\ldots,x_n)%. Having low influence means that f is close to being invariant in the %e_i% direction, where %e_1,\\ldots,e_n% is the standard basis. We say that there is large bias in the %e_i% direction (or more precisely, the basis vector %e^i% in the dual space, but never mind this), if f has a density increment on the hyperplane %\\{ x_i = 0 \\}% or on the hyperplane %\\{x_i = 1 \\}%. There is only a partial relationship between the two properties: low influence implies low bias, but not conversely; for instance, as pointed out above, a random boolean function has low bias but high influence for each variable. High bias leads directly to a density increment and so we “win” in this case. We also “win” if there are many directions of low influence, because f is close to constant on subspaces generated by these directions and is therefore going to be unusually dense on some of them. But there is the great middle ground of low bias, high influence sets (such as the random set) in which we have to do something else."},{"username":"gowers","timestamp":"2009-02-10T04:47:00.000Z","contents":"Obstructions. Boris, in 425 you said you used the fact that the set of wildcards has size %O(\\sqrt n)%. With the measure where all slices are the same size, I think the size of the set of wildcards of a typical line becomes %cn% instead. So I allowed myself to be hopeful, because you needed %m=O(\\sqrt n)% and you also needed %m% to be bigger than the square root of the size of the set of wildcards. But I admit that I still don’t feel as though I fully understand your example, so there may well be an obvious answer to this question."},{"username":"terence-tao","timestamp":"2009-02-10T04:52:00.000Z","contents":"Locality in the edit metric. Something just occurred to me that, in retrospect, was extremely obvious. We know already that we may as well restrict our dense sets to have %n/3 + O(\\sqrt{n})% 1s, 2s, and 3s, which means we may as well restrict our combinatorial lines to have at most %O(\\sqrt{n})% wildcards. This implies that the points on our combinatorial lines differ by at most %O(\\sqrt{n})% in the edit (Hamming) metric. In contrast, the diameter of the cube %{}[3]^n% is n. Because of this, our obstructions to uniformity need to be at the scale of %\\sqrt{n}% rather than n in the edit metric. Boris’s example is a good example of this; it’s basically a union of Hamming balls of radius m for some %m \\gg \\sqrt{n}%. So maybe conjecture 411 can be recovered by somehow localising it to Hamming balls of size %O(\\sqrt{n})%? There is a major technical issue coming from the fact that the metric is hugely non-doubling at this scale, but perhaps if one is careful one can still localise properly."},{"username":"boris","timestamp":"2009-02-10T04:53:00.000Z","contents":"Aha…. I missed the point about typical line having length %cn% with your measure. It does seem that the example disappears."},{"username":"terence-tao","timestamp":"2009-02-10T04:56:00.000Z","contents":"Locality in the edit metric cont. (The numbering on my previous comment should be 432.) To illustrate what I mean by localising the obstruction, consider the problem of obstructions to length three progressions in %{\\Bbb Z}/N{\\Bbb Z}%. Fourier analysis tells us that the obstructions are given by plane waves %e(x \\xi/N)% on the whole interval [N]. But now suppose one is only interested in counting progressions of some much smaller length O(m) (e.g. %m = \\sqrt{N}%). Then the right kind of obstructions are given by _local_ plane waves – waves which look like a plane wave %e(x \\xi_j/N)% on each interval %I_j% of length m, but whose frequency %\\xi_j% can vary arbitrarily with j. Of course, one just zoom in on an interval of length m where the original set A is quite dense and get rid of this technicality; similarly here, we may have to zoom into a Hamming ball and work locally on that ball."},{"username":"boris","timestamp":"2009-02-10T05:15:00.000Z","contents":"We can assume that the density is uniform across all smallish Hamming ball. That is because if we get a density increment on a smallish Hamming ball, we get a density increment on a tiny combinatorial cube by double-counting analogous to comment #132 from the original thread. Thus, we can increment density on Hamming balls until we are stuck."},{"username":"ryan-odonnell","timestamp":"2009-02-10T05:29:00.000Z","contents":"Re 426, 427, 430, distinction between influence and bias: Right, my mistake. Most of the examples I’ve been thinking about have been “monotone”, in which case the notions coincide."},{"username":"terence-tao","timestamp":"2009-02-10T08:31:00.000Z","contents":"Quasirandomness To answer Gil’s question: if we manage to settle DHJ(1,3) (or more generally to find lots of combinatorial lines in our “obstructions to uniformity”), we should presumably be able to iterate and also find lots of combinatorial subspaces in all of the obstructions to uniformity. If we double count properly, this should (hopefully) tell us that any set which has anomalous density on an obstruction to uniformity has an anomalous density on at least one combinatorial subspace, and then we “win”."},{"username":"terence-tao","timestamp":"2009-02-10T08:37:00.000Z","contents":"Soft obstructions to uniformity One possible route to keep in mind, by the way, is to back away from the “hard” goal of explicitly classifying all “obstructions to uniformity” in a very concrete manner, but instead settle for a “soft” classification which expresses these obstructions as something like a “dual function” from my primes in AP paper with Ben Green. In some cases, one can tools such as an enormous number of applications of the Cauchy-Schwarz inequality to deal with these sorts of dual functions. Another example of a “soft” obstruction to uniformity, this time for length three progressions in [N], is that of an almost periodic function: a function %f: [N] \\to {\\Bbb C}% whose shifts %T^h f% are “precompact in %L^2%” in some quantitative sense. One can show that these guys contain all the obstructions to uniformity by a soft argument based primarily on Cauchy-Schwarz (and does not use the Fourier transform). [Using the Fourier transform, of course, one can approximate almost periodic functions by quasiperiodic functions, e.g. bounded linear combinations of characters, thus moving from soft obstructions to hard obstructions]. It is also not hard to prove (again by “soft” means avoiding Fourier analysis) that almost periodic functions have lots of 3-term APs (and also the important strengthening that functions close to an almost periodic function also has lots of 3-term APs; there are some subtle issues with the hierarchy of epsilons here that I don’t want to discuss here). Anyway, it is another option to consider if the “hard” classification problem turns out to be too complicated."},{"username":"gil","timestamp":"2009-02-10T12:36:00.000Z","contents":"density-increase-strategies, quasi randomness. Can somebody remind me what is DHJ(1,3) DHJ(2,3) and DHJ(1,2)? thanks! The point I was making is that our strategy for a proof for a density theorem is  \n1) Find obstruction to quasirandomsess in terms of inner product with some function in a class of “obstraction” (This is what I referred to as a codim 1 condition)  \n2) deduce that this implies an increased density on lower dimensional subspace, and iterate.  \nIn may be possible (although I am not aware how) to move directly from having too many or too few combinatorial lines to a statement about density on small dimensional spaces. Also we can look at the example and see if the density increase on small spaces does not sugget “too good to be true” result. For example, if in all these obstructions we get a subspace of linear size with density-increase of (1+t) then this will imply a bound of the form 3^n/n^alpha which is toogood. BTW, there is a large density of people interested in the density HJ around here. The approach we are taking is a fixed k approach. Saharon Shelah gave the best bound for HJ using inductive argument which juggles different ks and diffferent dimensions for the combinatorial subspaces. He was quite interested to see if approaches of similar nature can apply for DHJ. Regarding Ryan’s long runs/tribe-like examples; It looks that this may extend to sets defined by a large class of bounded depth circuits."},{"username":"gowers","timestamp":"2009-02-10T14:11:00.000Z","contents":"Local obstructions. I’ve just woken up with a better grasp of what Terry is talking about in 431 and 433, so I’d like to rephrase it here (not very much actually, but perhaps just enough to be useful). Terry points out that because, as we know, our example could live in a central area of slices of width around %\\sqrt n% (meaning that the numbers of 1s, 2s and 3s are always within %C\\sqrt n% of %n/3%), for the simple reason that almost all sequences in %\\null[3]^n% have this property, the set of wildcards in any combinatorial line can be assumed to have size at most %C\\sqrt n%. This means that what happens at one sequence %x% should have nothing much to do with what happens at another sequence %y% if their Hamming distance (the number of coordinates where they differ) is substantially greater than %\\sqrt n%. This immediately suggests a generalization of the obstructions considered in Conjecture 411\\. The idea would be this: you would move a Hamming ball (that is, a ball in the Hamming metric) of radius %m% around in %\\null[3]^n%, and in each translate you would have a different obstruction of the type in Conjecture 411\\. The analogy Terry draws is with the case of %\\mathbb{Z}_n%, where you could partition it into small intervals and have a different Fourier mode in each interval, and that would exhibit nonquasirandom behaviour for local phenomena such as arithmetic progressions of length 3 with small common difference. Now if one is going to carry out such a programme, there is an issue that at first appears to be merely technical and then comes to seem more fundamental. It’s that you can’t do something different on each translate because the translates overlap. This is where the importance lies of Terry’s remark that “the metric is hugely non-doubling on this scale”, which I’ll explain in a moment. But first, note that in %\\mathbb{Z}_n% we don’t have a huge problem: of course the intervals overlap, but you can just partition %\\mathbb{Z}_n% into disjoint intervals and you’re basically OK. Back to Terry’s remark. What does he mean? He means that if you double the radius of a Hamming ball, then its volume (or cardinality) hugely increases. This has the effect of making it extremely hard to pack Hamming balls efficiently. Now it isn’t essential to use Hamming balls, but if we are going to partition %\\null[3]^n% into sets %H_i% and use a different 411-style obstruction in each %H_i%, then we will need one crucial property: that if you modify an element of %H_i% by changing around %C\\sqrt n% coordinates, then with non-negligible probability you remain inside %H_i%. (Otherwise, the supposed obstruction in %H_i% will not affect more than one point per typical combinatorial line.) The isoperimetric inequality in the cube tells us that the sets where you have the best chance of staying inside when you modify an element by a given amount are Hamming balls, and something similar is true in %{}[3]^n%. And if you do the calculations for Hamming balls, you find that if you modify by %C\\sqrt n% for some large constant %C%, then you hugely expand the set, unless the set is already quite large in which case you get everything. In other words, you cannot do some partitioning argument like this unless you partition into huge Hamming balls, or sets of similar type, which would turn the “local” obstructions into global ones. I think I’ve actually said a bit more than Terry now — I am claiming that Terry’s “major technical issue” is in fact a fundamental issue that makes it impossible to devise local obstructions of the type he is talking about. Where does that leave Boris’s example? I don’t have a full explanation for this, but I think it is that he somehow doesn’t do completely unrelated things in each %H_i%. In other words, it is not a purely local obstruction and can’t be massively generalized in the way suggested above. If anyone else can give a more precise explanation, I’d be very interested."},{"username":"terence-tao","timestamp":"2009-02-10T14:17:00.000Z","contents":"A potential strategy I think I have a strategy for proving that dense sets %{\\mathcal A} \\subset [3]^n% “with no exploitable structure” contain combinatorial lines, basically by fooling around with the Cartesian semi-products from 413. We have a density %\\delta% and an extremely large n. I will also need an intermediate integer %m = m(\\delta)% depending only on %\\delta%. I am going to try to find a combinatorial line with at most m wildcards inside %{\\mathcal A}%, and I think my strategy works unless %{\\mathcal A}% has some explicit structure obstructing me, which looks exploitable by other methods. Pick a random point x in %{}[3]^n%. We are going to look for a line inside the Hamming ball of radius m centred at x. (The discussion above has begin to persuade me that we should be working inside small Hamming balls rather than roaming across all of %{}[3]^n%.) It is convenient to embed %{}[3]^n = \\{0,1,2\\}^n% in %{\\Bbb Z}^n% and give the latter the standard basis %e_1,\\ldots,e_n%. Thus for instance %x = \\sum_{b \\in B} e_b + 2\\sum_{c \\in C} e_c% where A, B, C are the 0-sets, 1-sets, and 2-sets of x (thus A, B, C partition [n]). Generically, of course, A, B, C have size about n/3. Now we pick m random elements %a_1, \\ldots, a_m% of A, which will generically be distinct. I’m going to run the Sperner argument used in 413\\. For %i=1,\\ldots,m%, let’s look at the points %x_i :=x + e_{a_1} + \\ldots + e_{a_i} + 2 e_{a_{i+1}} + \\ldots + 2 e_{a_m}%, thus %x_i% is the same string as x but with the coefficients at %a_1,\\ldots,a_i% increased from 0 to 1, and the coefficients at %a_{i+1},\\ldots,a_m% increased from 0 to 2\\. Each %x_i% has a probability about %\\delta% of lying in %{\\mathcal A}%, so on average we expect to have about %\\delta m% indices i with %x_i \\in {\\mathcal A}%. In fact there should be some sort of concentration of measure, unless %{\\mathcal A}% has some blatant structure to it which should be exploitable, so I’m going to just assume that we do have %|I| \\sim \\delta m%, where %I \\subset [m]% is the collection of indices i such that %x_i \\in {\\mathcal A}%. Since %{\\mathcal A}% avoids all combinatorial lines, we know that the points %x_{i,j} :=x + e_{a_1} + \\ldots + e_{a_i} + 2 e_{a_{j+1}} + \\ldots + 2 e_{a_m}% (*) lie outside of %{\\mathcal A}% whenever %1 \\leq i < j \\leq m% is such that %i, j \\in I%. On the other hand, assuming concentration of measure I expect that about %\\delta m^2/2% of the %x_{i,j}% with %1 \\leq i < j \\leq m% lie in %{\\mathcal A}%. Applying Cauchy-Schwartz twice, I thus expect to get an anomalously large number of rectangles %x_{i_1,j_1}, x_{i_1,j_2}, x_{i_2,j_1}, x_{i_2,j_2}% in %{\\mathcal A}%, where %1 \\leq i_1 < i_2 < j_1 < j_2 \\leq m%. (I’m too lazy to calculate the expected number of rectangles right now, it’s something like %\\delta^4 m^4/4!%.) So far, I’ve mostly been rephrasing 413 in a different language. But now for the new idea: observe from (*) that each %x_{i,j}% depends on a different subset of the random variables %a_1,\\ldots,a_m%. Because of this, it is likely that the events %x_{i,j} \\in {\\mathcal A}% exhibit a high degree of independence from each other (even after conditioning x to be fixed). Also, assuming concentration of measure again, these events should have probability about %\\delta%. Assuming they behave like jointly independent variables, this should mean that the expected number of rectangles %x_{i_1,j_1}, x_{i_1,j_2}, x_{i_2,j_1}, x_{i_2,j_2}% in %{\\mathcal A}% should be close to what is expected by random chance, contradicting the previous paragraph. (In fact, one does not need joint independence for this; independence of any four %x_{i,j}% arranged a rectangle will suffice, thanks to linearity of expectation.) But what if joint independence fails? Then I think what happens is that there is some irregularity in one of the events %x_{i,j} \\in {\\mathcal A}%, in the sense that the %i + (m-j)%-uniform hypergraph %\\{ (a_1,\\ldots,a_i,a_{j+1},\\ldots,a_m): x_{i,j} \\in {\\mathcal A} \\}% is non-uniform in the sense of hypergraph regularity (i.e. it has a large octahedral norm, in the notation of Tim’s hypergraph regularity paper). [This is presumably the contrapositive of some nasty counting lemma that one would have to work out.] But I feel that this type of non-uniformity is a very usable type of structure on %{\\mathcal A}% (it seems to be some sort of hypergraph generalisation of the concept of “low influence”). At any rate, now that we see some uniform hypergraphs enter the picture, it is tempting to break out the hypergraph counting, regularity, and removal machinery and see what one gets."},{"username":"mathias","timestamp":"2009-02-10T17:02:00.000Z","contents":"Re: A potential strategy from Terry I’d like to point out, that the size of the hyperedges of the hypergraph at the end of 439 could be of order %\\Theta(m)%. I believe in order to use a removal lemma machinery here, one may have to chose %m=m(\\delta)% to be an extremely slow growing function (like inverse Ackerman in %1/\\delta% or worse). But this looks bad, as  \nthere are subsets of density %3^{-m}%, which contain no combinatorial line with at most %m% wildcards."},{"username":"gowers","timestamp":"2009-02-10T19:08:00.000Z","contents":"General strategy This is an idea that occurs to me after reading Terry’s 439 and sort of half mixing it with my [comment 331.](https://gowers.wordpress.com/2009/02/06/dhj-the-triangle-removal-approach/#comment-1938) In that comment I tried, and miserably failed, to deduce DHJ from corners in exactly the way that Sperner follows from the trivial one-dimensional equivalent of corners. The problem, as I soon discovered, was that you can’t embed grids into %{}[3]^n% in a nice way, analogous to the way you embed lines into %{}[2]^n% to prove Sperner. (The image of a line is a chain of sets such as empty set, 1, 12, 123, 1234, ….) However, even if you can’t get combinatorial lines to map to combinatorial lines, it does seem as though you can do at least something. Let’s embed a point in the triangular grid %\\{(a,b,c):a,b,c\\geq 0, a+b+c=n\\}% into %{}[3]^n% by sending %(a,b,c)% to the point with %a% 1s followed by %b% 2s followed by %c% 3s. In fact, I’ll use some slightly more general notation: a1,b2,c3 means what I’ve just said, but e.g. a1,b2,c3,d1,e2 would mean a 1s, b 2s, c 3s, d 1s, e 2s. Then we can never have all three of (a1,b2,c3), ((a+b)1,c3) and (a1,(b+c)3). More generally, we can never have (a1,b2,c3,d2), ((a+b)1,c3,d2) and (a1,(b+c)3,d2). Now in 311 I was trying to use this kind of idea to find a grid with no corners, and thereby to deduce that %A% was sparse in that grid, and hence (by a suitable averaging) that it was sparse everywhere. But what if instead of going for gold straight away, we merely try to find some kind of local atypical structure all over the place, which we then try to argue must have resulted from a global atypical structure? In the last paragraph but one, I can associate the sequences I’ve written down with the points (a,b,c), (a+b,0,c), (a,0,b+c). Now it’s not very difficult to find dense sets of triples adding up to n that contain no configurations of that kind. But I think it is less easy to make them quasirandom. So it seems (and this is one of those situations where another contributor — in this case Terry — has understood what I am writing for some while and I am catching up) that we do have some kind of ubiquitous local non-quasirandomness. (The presence of the 0 is disturbing, but that’s why it could be useful to add a d2 on to the end so that the sequences look more typical.) I’ve got more to say about this, but no time for several hours so I’ll post the comment as it stands for now."},{"username":"gowers","timestamp":"2009-02-11T01:11:00.000Z","contents":"General strategy. Let me amplify on one small comment that I made in the penultimate paragraph of the previous comment. I said, “I think it is less easy to make them quasirandom”. Here’s what I mean. The possible pairs (a+b,0,c), (a,0,b+c) form a 1-parameter family, since a+b+c is required to equal n. If r<s and we have the pairs (r,0,n-r) and (s,0,n-s), then we are forbidden to have the point (r,s-r,n-s). Thus, if R is the set of r such that the line (r1,(n-r)3) belongs to A, we find that A is not allowed to contain any point of the form (r,s-r,n-s) with r and s in R. Since the outer two coordinates determine the middle one in a simple way, we obtain a forbidden set that is basically a  \nCartesian product of R with itself, or rather in a natural one-to-one correspondence with RxR. It seems to follow that either R is very small or we have a density increment on a different Cartesian product (of R with its complement or of the complement with itself). If R is often very small, then by considering the extra d2 above and averaging, one might hope to get some kind of global non-quasirandomness, whereas if R is often large we might hope to find global non-quasirandomness for the reason outlined in the above paragraph. A key point in all this is how to argue that one can patch together all the little bits of non-quasirandomness in order to obtain a big bit. I’ll think about that in another comment."},{"username":"terence-tao","timestamp":"2009-02-11T01:24:00.000Z","contents":"443. Dear Mathias, Yes, we have to take m to be a rapidly growing function of %1/\\delta% to avoid the problems you describe. But I don’t think this is necessarily a problem for two reasons. Firstly, the hypergraph is going to be on about n/3 vertices, with n extremely large compared to both m and %1/\\delta%, so hypergraph regularity is still going to say something nontrivial. Secondly, I am not planning to rely solely on hypergraph regularity to finish the job; I envision something along the lines of “the hypergraph counting lemma (combined with the Sperner argument) will produce a combinatorial line unless one of the hypergraphs here is not %\\varepsilon(m,\\delta)%-regular”, and then the hope is to use this irregularity (which is some sort of assertion of low influence, I think) to locate a density increment or some other structural control on the original set in %{}[3]^n%. The point is that Sperner+Cauchy-Schwarz seems to eliminate the role of %\\{1,2\\}^m% from the task of detecting combinatorial lines in %\\{0,1,2\\}^m%, leaving only the need to understand the distribution of %{\\mathcal A}% in positions containing at least one zero. This is the type of situation in which hypergraph regularity/counting/removal theory kicks in – we want to understand solutions to a system of equations, each of which only involves a proper subset of the variables. Tim: I think we are trying to do almost exactly the same thing, though instead of working with a random embedding of the big triangular grid %\\{ (a,b,c): a+b+c=n\\}%, I am working with a random embedding of the much smaller grid %\\{ (a,b,c): a+b+c=m\\}%, which I like to think of as pairs %(i,j) = (a, a+b)% with %1 \\leq i \\leq j \\leq m%. To embed this grid into %{}[3]^n%, one has to pick a random base point x, which has 0s at some set A of size about n/3, and then pick m indices %a_1,\\ldots,a_m%. The pair %(i,j)% then embeds into the point x with the coordinates at %a_1,\\ldots,a_i% increased from 0 to 1, and the coordinates from %a_{j+1},\\ldots,a_m% increased from 0 to 2, creating the point that I called %x_{i,j}% previously. You observed that in the big triangular grid, that (r,0,n-r), (s,0,n-s), (r,s-r,n-s) cannot all embed into A. In my notation, if one lets %E_{i,j}% be the event that %x_{i,j} \\in A%, this is the assertion that for any %1 \\leq i < j \\leq m%, the events %E_{i,i}, E_{j,j}, E_{i,j}% cannot all be true. On the other hand, we expect each of the events %E_{i,j}% to occur with probability %\\delta%, which means that with high probability, the %E_{i,j}% must avoid a dense Cartesian product, and thus the joint event %E_{i_1,j_1} \\wedge E_{i_2,j_1} \\wedge E_{i_1,j_2} \\wedge E_{i_2,j_2}% for %1 \\leq i_1 < i_2 < j_1 < j_2 \\leq m% must occur with probability noticeably higher than %\\delta^4%. But the key is that each of the %E_{i,j}% only depend on a proper subset of the random variables %a_1,\\ldots,a_m%, and this is the place where the hypergraph counting lemma should step in and say something useful."},{"username":"gowers","timestamp":"2009-02-11T02:19:00.000Z","contents":"Sperner and nonquasirandomness. I want to do a little exercise. It goes like this. Suppose I take random chains as in the proof of Sperner, and I find that the average number of elements of %\\mathcal{A}% in each chain is %\\delta n%, but the average of the square of this number is not about %\\delta^2n^2%. What does that tell us about %\\mathcal{A}%? One obvious thing it tells us is that the number of pairs %A\\subset B% in %\\mathcal{A}% is larger than expected—at least if one interprets “number of pairs” is interpreted appropriately. Let’s just quickly see what we actually get. If I pick a random pair %A\\subset B% (not necessarily in %A%) by choosing a random maximal chain and then a random pair of elements of it, then what is the probability that I pick the particular pair %A\\subset B%? Let’s assume that %|A|=r% and %|B|=s>r%. Then the probability that the chain passes through %A% is %\\binom nr^{-1}%, and the probability that it passes through %B% given that it passes through %A% is %\\binom{n-r}{s-r}^{-1}%. Finally, the probability that I pick the pair %(A,B)% given that the chain passes through both %A% and %B% is %\\binom n2^{-1}%. So the weighting of pairs is according to the following model: first pick a random pair of slices (according to the uniform distribution) and then pick a random pair %A\\subset B% from that pair of slices. Thus, the hypothesis that %\\mathcal{A}% has too many pairs is the hypothesis that if you pick a random pair %A\\subset B% as in the above paragraph, then there is a more than %\\delta^2% chance that both %A% and %B% belong to %\\mathcal{A}^2%. Can we use this to get a global conclusion? In a non-quasirandom bipartite graph the trick is to choose a random edge and take the neighbourhoods of its vertices. Since there are too many 4-cycles, the resulting induced bipartite graph is on average too dense. What would be the analogue here? The obvious one would be to choose a random point of %A% and take the set of all pairs %B% such that %B\\subset A% or %A\\subset B%. This gives a structure (one might visualize it as a sort of “double cone”) that typically contains more points of %A% than it should. But this doesn’t count as a global obstruction because it is absolutely tiny compared with %{}[2]^n%. So the next question is whether it is possible to have a set of density %c% (according to the slices-equal weighting of %{}[2]^n%) such that (i) it intersects many of these “double cones” in density significantly different from %c% and (ii) one cannot obtain a density increase on a subspace in any simple way. I have to bath my 15-month-old now, so will pursue this later if nobody else has."},{"username":"jozsef","timestamp":"2009-02-11T05:22:00.000Z","contents":"Tim, I’m just reading your ecercise, It might be more relevant to us to consider sets of size %n/2\\pm \\sqrt{n}%. In general, I don’t see why do you have %\\delta n {}% and not %\\delta \\sqrt{n} {}% elements in a random chain."},{"username":"terence-tao","timestamp":"2009-02-11T05:22:00.000Z","contents":"Sperner and nonquasirandomness Tim, I think in order to get a meaningful answer one has to localise the chains somewhat. Currently, one can get a ridiculously large number of pairs %A \\subset B% from a very small %\\mathcal{A}% by using the extreme ends of the chain, for instance by letting %{\\mathcal A}% be those A for which %|A|>2n/3% or %|A| To move things closer to my proposals in 439, 443, I would instead do this: pick a random A in [n], then pick m random points %b_1,\\ldots,b_m% in the complement of A (where m is bounded in n, e.g. %m = \\lfloor \\delta^{-100} \\rfloor%), and then look at the random variable X, defined as the number of %1 \\leq i \\leq m% such that %A \\cup \\{ b_1,\\ldots,b_i\\}% lies in %{\\mathcal A}%. This variable X has expectation about %\\delta m%; what can we say if it has significant variance?"},{"username":"gowers","timestamp":"2009-02-11T05:30:00.000Z","contents":"Obstructions to Sperner. I feel that we haven’t thought about this hard enough. Boris’s remark in 418 that obstructions to Sperner (=DHJ(1,2)) don’t follow from lower-level obstructions is something we still need to come to terms with. It seems that one source of obstructions is “usually continuous” sets, by which I mean sets where if you pick a random sequence and change a random coordinate then you will usually not change whether that sequence belongs to the set. Except that as I write that I see that there is absolutely no reason to believe that it is true. As you go up a random chain, even if you only rarely switch from belonging to the set to not belonging, or the other way, you may well get the expected density by the end. (Note that all these discussions are very different if we take the uniform measure on %{}[2]^n%, but I am trying to push the slices-equal measure.) If we let $\\mathcal{A}$ be the collection of all sets that contain %1%, then what happens? The density is %1/2%, but this is not quite as trivial a fact as usual: it follows from the fact that the bijection from a set to its complement is measure preserving. If we pick a random pair %A\\subset B% then there’s obviously a correlation between %A% containing 1 and %B% containing 1 — indeed, it’s about as extreme a correlation as one could hope for. What’s more, we can think about it like this: if we pick a random chain, then we will add in the element 1 at a random moment between 1 and n, so the intersection of %\\mathcal{A}% with the chain has a density that is uniformly distributed between 0 and 1, which means that the mean square density of %\\mathcal{A}% in a random chain is %1/3%, rather than the %1/4% that it is supposed to be. I now see that what I’m groping for is something like this: we want more or less any monotone set system that isn’t too much like the collection of all sets of size at least %n/2%. What I mean by that is the following. If %\\mathcal{A}% is a monotone set, then given any chain, it will start outside %\\mathcal{A}% and then end up inside. So the density of the intersection is determined by the point at which your set first joins %\\mathcal{A}%. So one method of building an obstruction would be to take a reasonably dense subset %\\mathcal{B}% of %\\binom{[n]}{n/3}%, say, and take all sets that contain a set in %\\mathcal{B}%. But this isn’t going to give a set of density %1/2% unless the sets in %\\mathcal{B}% are very bunched together, or else their upper shadows are very soon going to be pretty well everything. I know this has already been discussed, but it feels as though it will be hard to come up with examples that aren’t based on heavy bias (or influence if we stick to these monotone examples, or similar). Boris had something for two layers, but what happens if we are forced to look at several layers, as we are with the slices-equal measure? It’s tempting to look at Fourier expansions, as Gil suggested in 29."},{"username":"gowers","timestamp":"2009-02-11T05:44:00.000Z","contents":"Obstructions to Sperner. My response to both Jozsef and Terry is the same. I’m very deliberately not localizing, because I’m hoping to use a different measure in DHJ, where you choose a point by first choosing a random slice according to the uniform distribution and then choosing a random point in the slice. There are several motivations for that. First of all, it is exactly what one does when proving Sperner via random chains: you get the stronger result that you just need the sum of the densities in all the layers to be greater than 1\\. In a sense, it is rather artificial to go on and say that the middle layer is the largest. Secondly, if you do this then there is a real chance that the distribution of a random point, given that it belongs to a randomly chosen combinatorial line, is not horribly different from the distribution of a random point full stop. All this I said in my comment near the end of the 300s thread. (To be precise, it’s [comments 365 and 366.](https://gowers.wordpress.com/2009/02/06/dhj-the-triangle-removal-approach/#comment-2047)) Thirdly, proving DHJ for this measure is equivalent to proving it for the uniform measure, by the usual averaging trick. (I confess that I haven’t actually checked this, but it is surely true.) So when Terry says that the sets of size at least 2n/3 or at most n/3 form a ridiculously small set system, I say no they don’t: that set system has density 2/3 in the slices-equal measure. Similarly, when Jozsef says that a set of density %\\delta% might well intersect a random chain in only %\\delta \\sqrt{n}%, he is talking about the uniform measure. But in the equal-slices measure the density of a set is _defined_ to be the expected density of the intersection with a random chain! Having said all that, there may well be other objections to my proposals. _[Off to bed now. I look forward to plenty of reading material when I wake up …]_"},{"username":"terence-tao","timestamp":"2009-02-11T09:25:00.000Z","contents":"Obstructions to Sperner Dear Tim, I’m leery of working globally because one is going to encounter things like Boris’s counterexample 412 again. Also, while I suppose one could work with all %n% layers %\\Gamma_{a,n-a}% of %{}[2]^n% as you are proposing, and even give them all equal weight, this seems very strange to me, since we know that we may without loss of generality restrict to the middle %O(\\sqrt{n})% (or more precisely %O( \\sqrt{n \\log \\frac{1}{\\delta}})%) layers, since all the other layers add up at most to a set of density %\\delta/2% and thus can be discarded without loss of generality at the very first step (at the trifling cost of replacing the density %\\delta% with %\\delta/2%). Nevertheless, the idea of trying to work out what the consequences are of knowing that %{\\mathcal A}% has “low influence” in the sense that flipping one digit does not change membership in %{\\mathcal A}% as often as it should, looks like a promising one. I think my own arguments in 443 say that we are done unless the influence (or something like the influence) is somewhat less than the influence of the random set, though the influence is not so low that we automatically get huge subspaces. More thoughts on this later…"},{"username":"terence-tao","timestamp":"2009-02-11T10:08:00.000Z","contents":"DHJ(j,k) To answer Gil’s question in 438, here is what I understand DHJ(j,k) to mean: DHJ(j,k): Every dense subset of %{}[k]^n% which has “complexity at most j” contains a combinatorial line (of length k). Now, what does “complexity at most j” mean? We’re still negotiating a precise definition, I think, but here was one proposal. We can think of an element x of %{}[k]^n% as a partition of [n] into k sets %A_1, \\ldots, A_k%. Now suppose we only considered j of these sets, say the first j %A_1,\\ldots,A_j% – call this a _j-shadow_ of x, so that there are %\\binom{k}{j}% different j-shadows of x. When j=k-1, each j-shadow of x can be used to recover x completely, but for smaller values of j, each j-shadow only gives partial information as to what x is. Now, let us call a set %{\\mathcal A} \\subset [k]^n% _sub-basic of complexity at most j_ if membership of a point %x \\in [k]^n% in %{\\mathcal A}% can be defined purely in terms of a single j-shadow of x (e.g. the first j sets %A_1,\\ldots,A_j%.) Example: the set %{\\mathcal A}% of all strings with exactly a 3s is a sub-basic set of complexity 1\\. the set of all strings with exactly b occurrences of the substring 31 is a sub-basic set of complexity 2\\. Observe that every subset of %{}[k]^n% is a sub-basic set of complexity k-1. Define a _basic set of complexity at most j_ to be the intersection of sub-basic sets of complexity j. Example: the set %\\Gamma_{a,b,c}% of strings in %{}[3]^n% with a 1s, b 2s, and c 3s is a basic set of complexity 1\\. (My notation here is coming from point set topology, in particular the notion of a sub-base and base of a topology.) Finally (and continuing the toplogical analogy) one could define a set of complexity j to be the union of a bounded number of basic sets of complexity j. With these conventions, DHJ(k-1,k) is the same as DHJ(k). DHJ(1,3) is DHJ(3) specialised to those sets %{\\mathcal A}% of the form %{\\mathcal A} = {\\mathcal A}_1 \\cap {\\mathcal A}_2 \\cap {\\mathcal A}_3%, where membership of a string %x \\in [3]^n% in %{\\mathcal A}_i% is determined entirely by the i-set of x."},{"username":"terence-tao","timestamp":"2009-02-11T10:31:00.000Z","contents":"01-insensitivity One key insight in Shelah’s proof of Hales-Jewett is that if you have a colouring of 012 strings which is “01”-insensitive in the sense that flipping a 0 to a 1 or vice versa does not affect the colour of the string, then the k=3 HJ theorem collapses to the k=2 HJ theorem. In a similar spirit, if a dense subset %{\\mathcal A}% of %{}[3]^n% is mostly 01-insensitive in the sense that flipping a random 0 entry of a random element of %{}[3]^n% to a 1 (or flipping a random 1 entry to 0) is almost certain not to change the membership status of that string in %{\\mathcal A}%, then (by looking at a randomly chosen medium-dimensional slice of %{\\mathcal A}%) we can reduce this instance of DHJ(3) to DHJ(2). This notion of “mostly 01-insensitive” is close to Tim’s notion of “usually continuous” from 447 or the notion of “low (average) influence” we have been discussing previously, but note that it is only about 0 and 1; it doesn’t say anything about what happens if a 0 is flipped to a 2, for instance. After talking with Tim Austin a bit about how the Furstenberg-Katznelson proof proceeds, I am beginning to think that instead of just incrementing density, we may also have to increment “01-insensitivity”. I think I can push my arguments in 413, 439 to say something along the lines of “either we have enough quasirandomness to find a combinatorial line, or else there is a little bit of 01-insensitivity present”. By a “little bit” of insensitivity, I mean that if we take a random string x, and then flip one of the 0s of x randomly to a 1 to create a random string y, then there is a small amount of correlation (something like %\\delta^{O(1)}%) between the events %x \\in {\\mathcal A}% and %y \\in {\\mathcal A}%. [I am oversimplifying here; I need a more generalised notion of insensitivity, but this is the flavour of what I think is possible.] At present I don’t see how to iterate this observation (though I can imagine bringing in existing Ramsey theorems, and perhaps also multidimensional Szemeredi, to assist here), to try to increase the 01-insensitivity through repeated passage to sub-objects until we get to the point where Shelah’s observation lets us drop from k=3 to k=2\\. Tim Austin tells me that this is broadly how the ergodic theory proof of DHJ proceeds; at some point I should go read that paper more thoroughly and see this for myself."},{"username":"boris","timestamp":"2009-02-11T10:53:00.000Z","contents":"In #444 Tim asks what can we conclude if the average number of elements in the chain is %\\delta n%, but the average of the square is much different from %\\delta^2 n^2%. My hunch is that it should imply that there is a pair %(i,j)\\in \\binom{[n]}{2}% such that swapping the order of %i% and %j%. The intuition is that if it is not the case, then some kind of Azuma-type or isoperimetric inequality should establish concentration. This is probably very similar to what Terry suggests in #449. My experience working on forbidden subposet generalization of Sperner suggests Tim is probably right in wanting to weight everything. The reason for this is that even if we working with a few levels near the middle level, then though the vertices (=the sets) are are weighted approximately the same in all of them, the edges (=the pairs %A\\subset B%) are weighted very differently according to the distance between the levels that contain them. The weight scales like %n^{|A|-|B|}%. The reason why weighting edges is unavoidable is that if suppose we know by density considerations that there are pairs $\\latex A\\subset B$ in our set family, but we want to give a bound on the number of pairs %A\\subset B%. Then the optimal bound will depend strongly on how far the levels on which the family lives are from one another. So, even if we localize to a stack of layers of size %\\sqrt{n}%, we probably will have to weight edges anyways. Unfortunately, I cannot see anything resembling chains for %{}[3]^n%."},{"username":"terence-tao","timestamp":"2009-02-11T11:11:00.000Z","contents":"451. Boris, I think the random model I have in 439 (in which one is looking at random embeddings of %{}[3]^m% into %{}[3]^n%, and then analyses correlations between elements of the resulting random subset of %{}[3]^m%, rather than deal directly with the original set in %{}[3]^n%) seems to do the edge weighting you want automatically. (It also fits well with the ergodic theory approach to these sorts of problems.) The analogue of the chains for %{}[3]^n% (or for the embedded copy of %{}[3]^m%) seem to be the strings consisting of a connected string of 1s, 0s, and 2s. For instance we get triangles such as this one in %{}[3]^4%: 0000 0002 0022 0222 2222  \n1000 1002 1022 1222  \n1100 1102 1122  \n1110 1112  \n1111 note that every right-angled triangle with the hypotenuse on the diagonal of the above grid is a combinatorial line. My strategy in 439 is to look at all the random embeddings of this type of picture into %{}[3]^n%, and then use Cauchy-Schwarz to eliminate the role of the diagonal and only focus on the interior (in which all strings have at least one 0 in them, which sets one up for hypergraph counting lemmas). Tim Austin tells me that this is related to the van der Corput type lemmas used in the ergodic proof of DHJ, which is an encouraging sign for me."},{"username":"gowers","timestamp":"2009-02-11T16:30:00.000Z","contents":"Slices-equal weights. I’m not wedded to my suggestion of putting a different measure on %\\null[3]^n% (or %{}[2]^n% for that matter) but I think the idea has enough going for it that I don’t want to drop it unless I see a truly convincing argument for doing so. And at the moment I don’t. First of all, no argument that is given _from the perspective of the uniform measure_ can be an argument against using a different measure. One has to step outside both. So this informal argument of Terry doesn’t persuade me: _Also, while I suppose one could work with all layers of as you are proposing, and even give them all equal weight, this seems very strange to me, since we know that we may without loss of generality restrict to the middle %O(\\sqrt n)% (or more precisely %O(\\sqrt{\\log(\\delta^{-1})n}%) layers, since all the other layers add up at most to a set of density %\\delta/2% and thus can be discarded without loss of generality at the very first step (at the trifling cost of replacing the density %\\delta% with %\\delta/2%)._ Of course if you are working with the uniform measure you can discard everything that isn’t near the middle, but if you are working with the slices-equal measure then you can’t. So the equal-slices measure is different, but that doesn’t make it “very strange”, any more than it’s very strange to use the uniform measure because WLOG you can restrict to a set (if you’re a slices-equal person) that keeps away from the middle layer. To summarize: no argument that basically boils down to saying that the slices-equal measure is different from the uniform measure carries any weight: it’s supposed to be different! A more serious point raised by Terry is the general one that global arguments are vulnerable to Boris-type counterexamples. But, as discussed in 428, 429, 431, 432, it is not completely obvious that that is the case with the slices-equal measure. The reason is that then the wildcard sets become (I’m fairly sure, but must check) of linear size. What’s more, I think the standard deviation of the size of the wildcard set is linear too. So it seems to me that the phenomena that Boris was exploiting don’t arise with the slices-equal measure. I was therefore allowing myself to hope that conjecture 411 might actually be true for the slices-equal measure. However, I am still disturbed by Boris’s point that the analogue of this one level down seems to be false (since the number of %A\\subset B% pairs in Sperner doesn’t just depend on the density). That is why I wanted to think hard about what the obstructions to uniformity are for Sperner with the slices-equal measure. I just want to add, once again, that the slices-equal measure has one very strong argument going for it, from the point of view of what is natural, which is that all (I think) the proofs of Sperner prove something about the slices-equal measure and only at the very end convert that into a statement about the uniform measure. One reason for resisting the slices-equal measure is that it is less symmetric than the uniform measure. However, the formulations of Sperner/DHJ are not symmetric either, or rather the symmetries they do have are also symmetries of the slices-equal measure. Another argument for its naturalness is that it seems rather strange to look only for lines with very small wildcard sets, when almost all lines have linear-sized wildcard sets. With the uniform measure, this is forced, but that is because of the unfortunate fact that the measure of a typical point on a typical line is radically different from the measure of a typical point. Note that this is an argument for the unnaturalness of the uniform measure that is taken from an _external_ perspective: we would like a measure that doesn’t get completely distorted when you condition on belonging to a random combinatorial line, we then look at the two measures, and we find that the slices-equal measure does better in that respect. I’m not at all dismissing Terry’s very interesting suggestions for dealing with things locally. It’s more that I would love to do something global if it turned out to be possible, so I’d like to invite people to attempt to find reasons for its not being possible. For example, if there is an obstruction to uniformity that’s not of type 411 then I may well be forced to abandon my hopes of a global argument. If no such reason is forthcoming, then I think we should keep global arguments in play. If it turns out that the only obstructions to Sperner (in the slices-equal set-up) are ones that depend on a tiny number of variables, then we might be lucky and find that Conjecture 411 is true. At first that seems strange, when it’s false one level down. But the point is that the assumption of disjointness that we have all over the place concerns _pairs_ of sets, at whatever level one is, so it could be that Sperner is exceptional because it occurs at a level below the level of the disjointness restriction. In other words, examples that depend on just a few variables affect you at all levels, but when you get to DHJ they can be absorbed into Conjecture 411, whereas for Sperner they can’t. This is, as usual, an optimistic suggestion, and as usual I welcome a cold blast of realism if anyone can supply it."},{"username":"gil-kalai","timestamp":"2009-02-11T19:42:00.000Z","contents":"(Clarification to 452) In particular, do you conjecture that a dense set in the slices-equal measure must contains a combinatorial line? This is stronger than DHJ, right? (Can you prove it as a consequence of DHJ?) (Just to make sure I understood correctly: slice equal measure is this: you give equal weight to all solutions of a+b+c =n and then splt this weight uniforly for all sequences of a ‘0’s b 1’1s and c ‘2’s. Indeed this natural measure appears in various places.)"},{"username":"ryan-odonnell","timestamp":"2009-02-11T20:04:00.000Z","contents":"Re Sperner. It was suggested in #446 to pick a random %x_0 \\in \\{0,1\\}^n%, form %x_1, \\dots, x_m% by taking a random “upwards” walk of length %m = 1/\\delta^{100}%, and then look at the random variable %X = \\#\\{i : x_i \\in \\mathcal{A}\\}%, where %\\mathcal{A} \\subseteq \\{0,1\\}^n% has density %\\delta%. Although the %x_i%‘s are not uniformly distributed, they are very close to being so, since %m% is small. Hence the expectation of %X% should be very nearly %\\delta m%. Consider the case that %\\mathcal{A}% is a monotone subset (“up-set”). Then %X% will be very nonconcentrated. Indeed, conditioned on %x_0 \\in \\mathcal{A}% (probability %\\delta%), so are all the %x_i%‘s. I.e., in this case, %X% is %m% with probability %\\delta% and therefore nearly %{}0% with probability %1 - \\delta%. On the other hand, it’s not clear that this is an immediate problem for the plan; in #439, one may well be even happier to have %|I| = m% with %\\delta% probability."},{"username":"gil-kalai","timestamp":"2009-02-11T20:32:00.000Z","contents":"If we want a ridicously-hyper-optimistic statement extending LYM to 3-letters alphabet we can take this: Consider all blocks %B% of slices %\\Gamma_{a,b,c}% not containing a combinatorial line. Let %S% be a subset of %\\{0,1,2\\}^n=\\Gamma% without a combinatorial line, then the sum of densities of %S% in some weighted collections of blocks which form a fractional covering of %\\Gamma% is at most 1."},{"username":"gil","timestamp":"2009-02-11T21:03:00.000Z","contents":"There is another finer notion of slices which may be a righter extensions from the k=2 case. Here you grade the elements of %\\Gamma% according to words in ‘0’ and ‘1’ that you obtain by arasing the ‘2’s. It would be nice to see (estimate) what is the maximum size of unions of such fine slices without a combinatorial line."},{"username":"terence-tao","timestamp":"2009-02-11T21:18:00.000Z","contents":"Slices-equal measure It occurs to me that there may in fact be essentially no difference of opinion between Tim’s “global, slices-equal measure” viewpoint and my “local, uniform-measure” viewpoint, because in order to get from a slices-equal measure DHJ to a uniform measure DHJ, one has to average on local subspaces anyway. (Note that one can easily construct sets of large uniform measure with small slices-equal measure, e.g. the strings %x \\in [2]^n% with %n/2+O(\\sqrt{n})% 1s, and conversely, so localisation is necessary here.)"},{"username":"ryan-odonnell","timestamp":"2009-02-11T22:20:00.000Z","contents":"Sorry for the double-post above. _[Now sorted out — Tim.]_ I guess it was already clear from the first para of Terry’s #451 that if %\\mathcal{A}% is, say, “01-monotone”, meaning that changing a 1 to a 0 cannot take you out of %\\mathcal{A}%, then DHJ holds for %\\mathcal{A}%, and even with the lower density $\\latex \\approx 1/\\sqrt{n}$ of Sperner. This already covers a number of my previous obstruction examples; e.g., having runs of 0’s or having plurality digit 0."},{"username":"gowers","timestamp":"2009-02-11T22:40:00.000Z","contents":"Slices-equal measure Terry, I was going to say something similar. It seems to me that localization and slices-equal measure are different ways of forcing typical lines to contain typical points. But the difference is more of a technical one than a fundamental one. I am hoping that slices-equal measure will turn out to be technically rather convenient, but I haven’t done enough calculations with it to be certain that it will be. Gil, I’m pretty sure that DHJ for the slices-equal measure is an easy consequence of DHJ, and vice versa. Here’s a sketch of how to prove that DHJ for any reasonably continuous measure (meaning that the weight of a point is usually very similar to the weight of a neighbouring point) is equivalent to the usual DHJ. You just pick some %m% such that %1< and pick a random subspace by randomly fixing %n-m% coordinates and letting the remaining %m% vary. Suppose %A% has density %\\delta% in measure 1\\. Then the average density in one of these subspaces is roughly %\\delta%. Moreover, by hypothesis the measure in almost all of the subspaces is almost uniform, so we can apply the normal DHJ to get it for the funny measure. And in the reverse direction, if we regard each tiny subspace as a copy of %{}[3]^m% and apply the funny measure to it, then the average of all these funny measures will be roughly uniform, so once again we can find a subspace where %A% is dense with respect to the funny measure. (Note that I needed continuity of the funny measure only in one direction of this equivalence.) Gil, I’m not sure I understand your hyper-optimistic conjecture, but let me give one of my own. Apologies if it’s the same. Let %\\alpha(n)% be the minimal density that guarantees a non-trivial aligned equilateral triangle in the set of non-negative (a,b,c) such that a+b+c=n. Then if the slices-equal measure of A is greater than %\\alpha%, A must contain a combinatorial line. This conjecture, if true, is trivially best possible since a corner-free union of slices contains no combinatorial line. It is also a direct generalization of Sperner. I wonder if there’s some simple way of seeing that it is false."},{"username":"terence-tao","timestamp":"2009-02-11T23:16:00.000Z","contents":"The ergodic perspective I think I now see more clearly the way to put my approach and Tim’s approach on a common footing, and that is using the lessons drawn from the ergodic theory approach to density Ramsey problems that proceeds via the Furstenberg correspondence principle. Now before everyone runs away screaming, let me try to explain the Furstenberg philosophy without actually using the words “ergodic”, “limit”, “infinite”, or “measure space” (though I will need “random” as a substitute for “measure space”). In a nutshell, the philosophy is this: * Study a global dense object by looking at a random local suboject. This trades a globally dense deterministic object %{\\mathcal A}% for a _pointwise_ dense random object %{\\mathcal A}'%. Furthermore, this random object obeys various symmetry (or more precisely, _stationarity_) properties. So, one gets much more structure on the object %{\\mathcal A}'% being studied, provided that one lets go of the idea of thinking of this object deterministically, and instead embracing its random nature. Let me give some concrete examples of this philosophy. Firstly, Roth’s theorem in %{\\Bbb Z}/N{\\Bbb Z}%, with N assumed prime for simplicity. To locate length three progressions in a set %A \\subset {\\Bbb Z}/N{\\Bbb Z}%, what one can do is pick a medium size number m, and look at a random arithmetic progression %a, a+r, \\ldots, a+(m-1)r% of length m in %{\\Bbb Z}/N{\\Bbb Z}%. One can then pull back the deterministic set A by this progression to create a random subset %A' := \\{ j \\in [m]: a+jr \\in A \\}% of %{}[m]%. If we can find a progression of length 3 in A’ with positive probability, then we get a progression of length 3 in A (in fact, we get lots of such progressions; this is essentially the proof of Varnavides theorem). The original deterministic set A had global density %\\delta%. Because of this, the random set A’ has a better property: it has a _pointwise_ density %\\delta%, in the sense that every j in [m] belongs to A’ with probability %\\delta%. From linearity of expectation this implies that the expected density of A’ with respect to uniform measure of [m] is %\\delta%, but it is far stronger – indeed, it implies that the expected density of A’ with respect to _any_ probability measure on [m] is %\\delta%. Similarly, the deterministic set A has no symmetry properties: A+1, for instance, may well be a completely different set from A. However, the random set A’ has an important symmetry property: the random set A’+1, while being different from A’, _has the same probability distribution_ as A’ (ignoring some trivial issues at the boundary of [m]; in practice one eliminates this by taking the limit %m \\to \\infty%). In the language of probability theory, we say that the probability distribution of A’ is _stationary_; in the language of ergodic theory, we say that the transformation %T: A' \\to A'+1% is a _measure-preserving transformation_ with respect to the probability distribution %\\mu% of A’. Now, we know that the events %0 \\in A'%, %1 \\in A'%, %2 \\in A'% each occur with probability %\\delta%. If they were independent, then we would have %0,1,2 \\in A'% with probability %\\delta^3% and we would get Roth’s theorem. But of course they could be correlated – but one can try to quantify this correlation by a variety of tools, e.g. using the spectral theory of the measure-preserving transformation T (this is the analogue of using Fourier analysis in the original combinatorial setting). This is the starting point for the ergodic theory method for solving density Ramsey theory problems. Now we return to Hales-Jewett. Similarly to above, if we wish to study a subset %{\\mathcal A}% of %{}[3]^n% which has density %\\delta% wrt uniform measure, we can take a random m-dimensional subspace of %{}[3]^n% (where I will be a little vague for now as to exactly what “random” means; there are a variety of options here) and pull back the deterministic %{\\mathcal A}% to get a random subset %{\\mathcal A}'% of %{}[3]^m%. Again, finding lines in %{\\mathcal A}'% will generate lines in %{\\mathcal A}% (and there will be an appropriate Varnavides statement).  \nAlso as before, the global density of %{\\mathcal A}% ensures that the random subset %{\\mathcal A}'% has _pointwise_ density very close to %\\delta% (there is a slight distortion here, as pointed out in Ryan.454, but it is minor). Thus, the expected density of %{\\mathcal A}'% with respect to _any_ particular measure on %{}[3]^m% – be it uniform, slices-equal, or anything else – is basically %\\delta%. It is at this point that the three approaches to the problem (ergodic approach, Tim’s approach, and my approach) begin to diverge. The ergodic theory approach would then proceed by studying correlations between events such as “%0000 \\in {\\mathcal A}'%, %1111 \\in {\\mathcal A}'%, %2222 \\in {\\mathcal A}'%. If these events are independent, we are done; otherwise, we hope to discern some usable structural information on %{\\mathcal A}'% out of this. The original set %{\\mathcal A}% is not used at all in the ergodic theory approach; it’s only legacies are the pointwise density and stationary properties it bestows on %{\\mathcal A}'%. Tim’s approach is proceeding by using the first moment method to locate a specific _deterministic_ instance of %{\\mathcal A}'% which is dense wrt slices-equal measure, and then proceeding from there. As with the ergodic theory method, the original set %{\\mathcal A}% (which was dense in the uniform measure, rather than slices-equal) is now discarded, and one is now working entirely with the slices-equal-dense %{\\mathcal A}'%. As I understand it, Tim is then trying to show that lack of combinatorial lines in %{\\mathcal A}'% will force some new structure on %{\\mathcal A}'%. My approach starts off similarly to the ergodic theory approach – studying correlations between events such as %ijkl \\in {\\mathcal A}'% – but the difference is that once a correlation is detected, I try to see what this tells me about the original set %{\\mathcal A}%, in particular to show that it has some “low-influence”, “insensitivity”, “mostly continuous”, or “monotone” like properties. I have a vague idea of trying to iterate this procedure until we have enough structure on %{\\mathcal A}% to either pass to a denser subspace or find combinatorial lines directly, but I have not worked this part out yet."},{"username":"terence-tao","timestamp":"2009-02-11T23:28:00.000Z","contents":"Ergodic perspective cont. Perhaps one more remark to illustrate the connection between my approach and Tim’s approach. For sake of illustration I will take m=4, since I can then use the triangular grid %\\Delta_4% from 451. The construction in 460 gives a random subset %{\\mathcal A}'% of %{}[3]^4%, with pointwise density essentially %\\delta%. In particular, the equal-slices density of %{\\mathcal A}'% in %{}[3]^4% has expectation about %\\delta%, and similarly the uniform density of %{\\mathcal A}'% on %\\Delta_4% has expectation about %\\delta%. But because a permutation of a cube is still a cube, we see that applying any of the 4! permutations of the indices to %{\\mathcal A}'% will give a new random set with the same probability distribution as %{\\mathcal A}'%; thus %{\\mathcal A}'% is stationary with respect to the action of %S_4% on %{}[3]^4%. But observe that equal-slices measure is nothing more than the uniform measure on %\\Delta_4% averaged by the action of %S_4% (%\\Delta_4% intersects each slice in exactly one point). So the expected density of %{\\mathcal A}'% wrt equal slices measure is necessarily equal to the expected density of %{\\mathcal A}'% wrt uniform measure %\\Delta_4%. But the fact that both of these densities are %\\delta% is weaker than the statement that the pointwise density of %{\\mathcal A}'% is %\\delta%, which is what I think we should be using."},{"username":"jason-dyer","timestamp":"2009-02-11T23:44:00.000Z","contents":"Taxonomy of slices This is more a general-organization comment, but at some point could we get together a “taxonomy of slices”? I’m starting to lose track of all the different methods people are proposing. I believe it also might be helpful to apply the slices side-by-side to a single example to get an impression of how the sets are different."},{"username":"gowers","timestamp":"2009-02-11T23:52:00.000Z","contents":"Ergodic perspective. Terry, thanks for that useful summary. I think it’s helped me to understand the ergodic side of things better. Let me test that, chiefly for my own benefit, by trying to put part of it in my own words. The idea would be that you take a random copy of %{}[3]^m% in %{}[3]^n% and intersect it with %\\mathcal{A}%. If you do that, then instead of a function from %{}[3]^m% to %\\{0,1\\}% you’ve got a Bernoulli random variable at each point of %{}[3]^m%, which is 1 with probability %\\delta% (the density of %\\mathcal{A}%) and 0 with probability %1-\\delta%. At first it seems as though one has gained precisely nothing by doing this. For example, here’s a way of obtaining some random variables: you take a dense set %\\mathcal{B}\\subset [3]^m%, and for each permutation %\\pi% of %{}[m]% you let %\\mathcal{B}_\\pi% be the set of sequences you get by taking a sequence in %\\mathcal{B}% and using %\\pi% to permute its coordinates. Then the probability that the random set %\\mathcal{B}_\\pi% contains a combinatorial line is trivially the same as the probability that %\\mathcal{B}% contains a combinatorial line (and moreover the expectation of the characteristic function of %\\mathcal{B}_\\pi% is %\\delta%). However, the point is that if you know that the random set %\\mathcal{B}\\subset[3]^m% comes from random restrictions of a much larger set %\\mathcal{A}\\subset[3]^n%, then you will be very surprised if with probability 1 it is always obtained from some fixed set by a simple permutation of the coordinates. A phenomenon like that would have the potential to show that %\\mathcal{A}% has some very strange and exploitable structure. More generally, if the elements of the random set exhibit significant dependences, this should come from structure in %\\mathcal{A}%, and if they are independent then as you point out you get a combinatorial line. If that sketch is not too wide of the mark, then I see that extreme localization of the kind you propose does potentially give you quite a lot of extra resources. And given that the ergodic-theory proof is known to work it is always encouraging if one’s argument has formal similarities with it. My feeling about slices-equal is that it’s a bit of a long shot, but it might be possible to get away without that extra power and obtain a stronger result as a consequence. But if that doesn’t work out, then we know that we’ve got localization to fall back on. Or we could have two parallel threads, one exploring a global approach and one a local approach. This one is already getting to the point where it takes a boring amount of time to load on to my computer, and the 300s thread seems to have come to a standstill for now. Incidentally, it occurs to me that we as a collective are doing what I as an individual mathematician do all the time: have an idea that leads to an interesting avenue to explore, get diverted by some temporarily more exciting idea, and forget about the first one. I think we should probably go through the various threads and collect together all the unsolved questions we can find (even if they are vague ones like, “Can an approach of the following kind work?”) and write them up in a single post. If this were a more massive collaboration, then we could work on the various questions in parallel, and update the post if they got answered, or reformulated, or if new questions arose."},{"username":"randall","timestamp":"2009-02-12T00:01:00.000Z","contents":"Speaking of ergodic theory: If anyone’s interested in reading the ergodic  \nproof for k=3, we’ve been discussing it in seminar  \nin Memphis; the proof can be found on my webpage.  \n(You need the 1989 paper of Furstenberg and Katznelson,  \nin which the proof is set up but not given, too; that’s  \non Katznelson’s webpage.) It’s far less intimidating  \nthan the general proof as given in their 1991 paper. Is there a proof (of the kind you guys are looking for)  \nof the IP Szemeredi theorem for k=3? That would seem  \nto be logically prior."},{"username":"gowers","timestamp":"2009-02-12T00:23:00.000Z","contents":"Varnavides. Something I’ve been meaning to do for quite a while is prove, or at least sketch a proof, that a Varnavides-type statement holds for the slices-equal measure. I don’t think it’s going to be hard, but I want to check it to be sure. (It is supposed to be another argument in favour of slices-equal, though I can see that the local approach gives a local Varnavides statement, so it’s not an argument that slices-equal is more natural than localization, but it is an argument that slices-equal is more natural than a global uniform approach.) Let %\\delta>0%, and let %m% be large enough that every subset of %{}[3]^m% of density at least %\\eta=\\eta(\\delta)% contains a combinatorial line. (Here, %\\eta% is a constant to be chosen later.) Now choose a small constant %c>0% (depending on %m%) and embed %{}[3]^m% randomly into %{}[3]^n% as follows. First pick %m% integers %r_1,\\dots,r_m% independently, randomly and uniformly from %\\{1,2,\\dots,cn\\}%. Next, randomly pick disjoint sets %V_1,\\dots,V_m%, with %|V_i|=r_i%. Finally, randomly assign 1s, 2s and 3s to the complement of the union of the %V_i%. Sorry, I don’t mean that. I mean that you randomly choose %a+b+c=n-\\sum r_i% and then you randomly assign %a% 1s, %b% 2s and %c% 3s to the coordinates in the complement of the union of the %V_i%. And now define a %m%-dimensional combinatorial subspace by fixing the coordinates outside the %V_i% according to the random assignment of 1s, 2s and 3s, and taking all possible %3^m% assignments to the rest of the coordinates that are constant on the %V_i%. (Since %m% is constant, I don’t think it matters if one does this uniformly, but I would prefer to do even this in a slices-equal way. That is, I choose %d+e+f=m% randomly and then I randomly choose %d% of the sets %V_i% to get a 1, etc.) There are a few statements that I don’t immediately see how to argue for without going away and doing calculations. I claim that there is some constant %\\eta=\\eta(\\delta)>0% such that the proportion of copies that intersect %\\mathcal{A}% in a density of at least %\\eta% is at least %\\eta%. I also claim that no combinatorial line is significantly more likely than average to belong to a copy. And from that I claim that a double-count gives that a random combinatorial line has a probability depending on %\\delta% only of belonging to %\\mathcal{A}%. I might try to think about justifying those statements a bit better."},{"username":"gowers","timestamp":"2009-02-12T00:56:00.000Z","contents":"Hyper-optimistic conjecture Terry, re last paragraph of 459, it occurs to me that with your upper-and-lower-bounds thread you may already know that the hyper-optimistic conjecture is false. Do you know of any example that beats the best example that’s constant on slices?"},{"username":"terence-tao","timestamp":"2009-02-12T02:13:00.000Z","contents":"Various Randall, thanks for the pointer to your notes (which, incidentally, are at [http://www.msci.memphis.edu/~randall/preprints/HJk3.pdf](http://www.msci.memphis.edu/~randall/preprints/HJk3.pdf) ). I will try to digest them. It looks like it is indeed soon time to close this thread and open some new ones. I can host a thread on ergodic-flavoured combinatorial approaches, focusing on my localisation approach and also on trying to explicate the Furstenberg-Katznelson proof of DHJ (I am imagining some internet version of a “reading seminar”). Then I guess Tim could try to summarise the other main active thread (based on obstructions to Sperner, etc.; my approach, incidentally, doesn’t seem to directly need to understand Sperner obstructions, though it does need to understand the closely related concept of low influence). I will also try to translate the hyper-optimistic conjectures 455, 459 to something that the 200-299 thread can chew on. Gil, can you explain your conjecture in 455 in more detail? I’m afraid I didn’t really get it (e.g. I don’t know what a “block” or a “fractional covering” is). As for all the “background” of discarded questions (e.g. the IP-Szemeredi thing Randall mentioned was already brought up all the way back in Jozsef’s comment 2)… here I think this is a job that requires more manpower than we have right now, since the “foreground” threads are already keeping all of us very busy. It also feels like this stuff would be better placed in something like a wiki than in a blog thread. I would say for now that we just continue the “active” threads for now and leave the trawling through the remainder for a later point in time if our current active threads get stuck."},{"username":"terence-tao","timestamp":"2009-02-12T02:18:00.000Z","contents":"Various cont. Actually, what I might do is set up a Google Document to hold random threads, questions, etc. that can be collaboratively edited. This has worked pretty well for the 200-299 thread so far. Here’s one little tidbit from that thread which might be useful here. Currently, the best %\\delta% for which we can combinatorially prove DHJ(3) is 0.633\\. Thus, for instance, we don’t know without ergodic theory that if one takes 60% of %{}[3]^n% for large n, that one gets a combinatorial line. So, if one wants to play with a quantitative value of delta: take 0.6. We also have an example of a subset of %{}[3]^{99}% of density >1/3 that still has no combinatorial line. So even with moderately enormous dimension, it is not possible to handle delta=1/3."},{"username":"terence-tao","timestamp":"2009-02-12T03:04:00.000Z","contents":"Ergodic perspective Re Tim’s 462: yes, this is one way of thinking about the ergodic perspective, as providing a inter-related (and presumably correlated) network of Bernoulli random variables (aka “events”), and the game is to get enough control on how these events overlap each other that one can eventually find a line of three events which are simultaneously satisfied with positive probability. By the way, another distinction between the three approaches lies in the range of m to pick, a point which has already been alluded to by Tim (when referring to my approach as “extreme localization”). In my approach I am picking %m = m(\\delta)% to just depend on %\\delta% (in fact I think I may be able to take a very concrete m, e.g. %m = \\lfloor \\delta^{-100} \\rfloor%). In the ergodic approach, %m = m(n)% is a very slowly growing function of n (so in the limit, m is infinite). For Tim’s approach, one can take m close to %\\sqrt{n}% (or anything smaller, so long as it is still sufficiently large depending on %\\delta%)."},{"username":"randall","timestamp":"2009-02-12T03:30:00.000Z","contents":"IP Roth: Just to be clear on the formulation I had in mind (with apologies for the unprocessed code) _[now processed — Tim]_: for every %\\delta>0% there is an %n% such that any %E\\subset [n]^{[n]}\\times [n]^{[n]}% having relative density at least %\\delta% contains a corner of the form %\\{a, a+(\\sum_{i\\in \\alpha} e_i ,0),a+(0, \\sum_{i\\in \\alpha} e_i)\\}%. Here %(e_i)% is the coordinate basis for %{}[n]^{[n]}%, i.e. %e_i(j)=\\delta_{ij}%. Presumably, this should be (perhaps much) simpler than DHJ, k=3."},{"username":"terence-tao","timestamp":"2009-02-12T03:45:00.000Z","contents":"Hyper-optimistic conjecture Ah, I see now that Gil’s conjecture is a weakening of Tim’s conjecture in which one takes a different measure on the space of slices than the uniform measure. As Tim’s is more concrete, that’s the one I’ve posted on the 200-299 thread to be tested for small n."},{"username":"terence-tao","timestamp":"2009-02-12T04:27:00.000Z","contents":"Google document OK, I’ve set up a repository for unsolved questions for Polymath1 at [http://docs.google.com/Doc?id=dhs78pth_15dfw8jhz4](http://docs.google.com/Doc?id=dhs78pth_15dfw8jhz4) Unlike the spreadsheet over at 200-299, I can’t make this editable by the general public (I suppose there are security risks in documents that are not present in spreadsheets), but I’ll add Tim as a “collaborator”, and anyone else who asks can have permission to edit it also."},{"username":"ryan-odonnell","timestamp":"2009-02-12T04:54:00.000Z","contents":"Re Terry’s strategy (#439). Hi Terry, could you help me understand the strategy a bit in the case that %\\mathcal{A}% is, say, the set of strings in %\\{0,1,2\\}^n% whose 0-count is congruent to 0 mod %1/\\delta%? (Assume %1/\\delta% is an integer.) I’m worried about the first concentration of measure statement. Or does this set already have some exploitable blatant structure — perhaps being a 1-sub-basic complexity set? Thanks!"},{"username":"terence-tao","timestamp":"2009-02-12T05:23:00.000Z","contents":"Terry’s strategy 439 It’s true, the first concentration of measure statement breaks down; either all the %x_i% lie in %{\\mathcal A}% (when the number of 0s in x is equal to m mod %1/\\delta%) or none of them do. But it turns out we don’t really need that particular concentration, it’s enough to have a lot of %x_i% in %{\\mathcal A}% with positive probability, which is true from the first moment method. A more necessary concentration of measure is the assertion that the density of %x_{i,j}% that lie in %{\\mathcal A}% is close to %\\delta%. In this particular case, this is true: the set of pairs %(i,j)% with %x_{i,j} \\in {\\mathcal A}% is given by %\\{ (i,j): i+j=c \\hbox{ mod } 1/\\delta\\}%, where c is some constant depending on m and x. The second moment method tells us that we can get some concentration of measure (enough for our purposes, in fact) as soon as we have small pairwise correlations between the events %E_{ij}%, which are the events that %x_{ij} \\in {\\mathcal A}%. Now, in your example, there are extreme pair anti-correlations: for instance, %E_{00}% and %E_{10}% are never simultaneously true. To put it another way, if we take an element in %{\\mathcal A}% and flip a 0 to 1, we _always_ leave %{\\mathcal A}% as a consequence. This is certainly some sort of structure. But can we exploit it? Well, one thing I was thinking of is that in the setup in 439, the m different generators %e_{a_1},\\ldots,e_{a_m}% of the embedded m-dimensional cube are just single basis vectors; to put it another way, I’m playing with a string with m distinct wildcards, with each wildcard being used exactly once. But one could instead have each of the m wildcards appear r times (or a Poisson number of times, or whatever), e.g. picking %e_{a_{1,1}},\\ldots,e_{a_{m,r}}% at random and defining %x_{i,j} = x + \\sum_{k=1}^i \\sum_{j=1}^r e_{a_{i,j}} + 2 \\sum_{k=j+1}^m \\sum_{j=1}^r e_{a_{i,j}}%. Then the correlation between, say, %E_{0,0}% and %E_{0,1}% relates to what happens when one flips r 0s to 1s, rather than just a single 0 to 1\\. This continues to be negatively correlated until r hits %1/\\delta% (or a multiple thereof), at which point the correlation shoots up to be massively positive and in fact we find an abundance of combinatorial lines. So we probably have to average over the “dilation” scale r rather than just set it equal to 1 as is done currently. (Much like with the corners problem: it’s not enough to look for corners of size 1, or any other fixed size, but we must average over the length r of the corner. Actually, one should probably let each of the m wildcard symbols appear a different number of times, e.g. assign a Poisson process to each.) In the current example, averaging r over any range significantly larger than %1/\\delta% will eliminate pair correlations. In fact, if one allows each wildcard symbol to have an independent number of occurrences, triple correlations such as between %E_{0,0}, E_{0,1}% and %E_{1,1}% also seem to vanish, which incidentally yields combinatorial lines already (since %x_{0,0}, x_{0,1}, x_{1,1}% form a line)."},{"username":"gowers","timestamp":"2009-02-12T05:47:00.000Z","contents":"Metacomment. Just briefly on the subject of new threads, I’m not completely convinced that the multiple-threads idea has worked. Or rather I think it has partially worked. It seems that the upper-and-lower bounds thread was sufficiently separate that it has thrived on its own. But the experience of the other two threads was that once the 400 thread was opened the 300 thread died pretty rapidly. (Actually, at some point I wouldn’t mind revisiting it, but that’s not my main point.) So in effect we’ve just had one linear thread, but at any given moment we’ve been reasonably focused and not had too many different discussions going on at once. As a result, I’m not sure that splitting up into a global thread and a local thread is a good idea. I think that many of the ideas for one approach are likely to be relevant to the other, and I also think it is possible for the two discussions to coexist. I think the one thing I might suggest (but I’d be interested in other people’s opinion on this and everything else) is that we should restrict the number of comments further. An advantage of the multiple threads was that we had multiple summaries: I think it would be quite good to be forced to summarize more often. Also, a number of people have commented that it is difficult to catch up with the discussion if you come to it late. We should try to start each thread in a way that makes it as self-contained as possible. (I think it is legitimate to expect people to read the posts, but we should try not to force them to wade through hundreds of comments.)"},{"username":"gowers","timestamp":"2009-02-12T06:08:00.000Z","contents":"Obstructions. Something I want to think about, but won’t be able to think about tonight as I’m going to have to stop this soon, is whether it is possible to put together a lot of local obstructions to get a global one. What I mean is this. Let us regard a point in %{}[3]^n% as a pair of disjoint sets (A,B) (which are the 1-set and 2-set of a sequence). Suppose that an argument of the Cartesian semi-products variety (see Terry 413) can be used to prove that for a large number of sets H (in the slices-equal measure) you get unexpectedly many 4-cycles, by which I mean quadruples (A,B), (A,B’), (A’,B’), (A’,B) of points with all four sets contained in H. (Here H would in fact arise as the complement of some set C.) Just to make my question a bit more concrete, let me go for an extreme statement and suppose that what actually happens is that for many H you have two systems %\\mathcal{A}_H% and %\\mathcal{B}_H% of subsets of %H% such that there are many disjoint pairs %(A,B)% and they are all points in the original dense set. (In previous terminology, I want to suppose that many sets %H% contain a large Kneser product.) Can we somehow piece together these set systems %\\mathcal{A}_H% and %\\mathcal{B}_H% to obtain set systems %\\mathcal{A}% and %\\mathcal{B}% such that the set of disjoint pairs %(A,B)\\in\\mathcal{A}\\times\\mathcal{B}% correlates with the original dense subset of %{}[3]^n%? If the answer is no, I feel it ought to be obviously no: you would just more or less randomly choose your %\\mathcal{A}_H% and %\\mathcal{B}_H%. But if for some reason that turns out to be difficult (because there is too much overlap and in order to get compatibility on the overlap you are forced to relate the various %\\mathcal{A}_H% to each other and similarly the %\\mathcal{B}_H%) then perhaps you can get from local to global in the way I would like. I’m not sure how easy it would be to prove a lemma of the desired kind, but I’m hoping it will be easyish to decide whether anything like that has a chance of being true. And now it’s bedtime."},{"username":"terence-tao","timestamp":"2009-02-12T06:21:00.000Z","contents":"Metacomment I guess the number of active threads is, in practice, bounded by the number of distinct groups of active participants. The participants in the 200 thread are largely disjoint from those in the 300 and 400 threads, but the 300 and 400 participants have been basically the same, so the 400 thread has basically served as an update to the 300 thread. Perhaps we can still have two threads: one thread (the 500 thread?) would be the local and global approaches to the problem, and another thread, which I could host, would be a slower-paced “reading course” on the ergodic proof of k=3 DHJ, presumably using Randall’s notes as a guide. This would probably proceed by different rules than the existing threads, since the aim is now to understand a paper rather than to solve a problem (I will have to think about what the ground rules of such a reading course would be). But I would imagine that the readership here would be a bit disjoint from the one in the “mainstream” threads – in particular, I expect more ergodic theorists to participate – and so should be able to operate independently."},{"username":"gowers","timestamp":"2009-02-12T06:25:00.000Z","contents":"Re Terry’s metacomment — that all sounds pretty sensible to me, so let’s do it unless someone comes up with an objection to the plan in the near future. Re 473, I just wanted to add (in case there was any misunderstanding) that when I counted quadruples above, it was with slices-equal measure, so you couldn’t say things like that almost all the sets A and B had roughly the same size, etc. etc."},{"username":"gowers","timestamp":"2009-02-12T06:41:00.000Z","contents":"Obstructions. Thinking about it a bit further, I now think 473 is true and not that hard. More details tomorrow if I wake up in a similar frame of mind."},{"username":"ryan-odonnell","timestamp":"2009-02-12T06:51:00.000Z","contents":"A Fourier approach to Sperner. (This would probably make more sense in the 300’s thread, but I concur with Tim’s #472.5.) I was thinking about Terry’s approach and his #472\\. (Thanks for that, btw; I agree that the second concentration of measure is the important one and that it holds in my simple example.) More specifically, I was trying to think about the “structure” of the pairs %x_i, x_j \\in \\mathcal{A}% (half-lines, as I was calling them in #134). This led me to think again about a Fourier approach to Sperner. Not sure if it has any relevance at all to DHJ(3), but I thought I’d throw it out there. One difficulty with such an approach (discussed already in comments #29–#37) is that it’s not immediately clear how to come up with a good distribution on pairs %(x,y)% with %x \\prec y%. But I think Terry’s idea of localising quite severely — to “constant” distances like %m = 1/\\delta^{100}% — is very promising. Since things are always better in Fourier-land when one uses a product distribution, I suggest the following: Let %\\epsilon% denote %m/n%. Let %f : \\{-1,1\\}^n \\to \\{0,1\\}% be the indicator of a subset %\\mathcal{A} \\subseteq [2]^n% of density %\\delta%. Jointly choose a pair of strings %x, y \\in \\{-1,1\\}^n% as follows: For each coordinate %i%, independently:  \n. with probability %1 - \\epsilon%, set %x_i = y_i = % a random %\\pm% bit.  \n. with probability %\\epsilon%, set %x_i = -1%, %y_i = 1%.  \nFinally, reverse the roles of %x% and %y% with probability %1/2%. Under this distribution, certainly %x \\leq y% or vice versa always, with the distance between them being distributed roughly as Poisson(m). The strings %x% and %y% are not uniformly distributed (which is annoying) but they are nearly so assuming %m% small. To be continued in the next post. (PS: sorry, I guess I’m violating the “Don’t calculate on scratch paper” rule.)"},{"username":"ryan-odonnell","timestamp":"2009-02-12T06:52:00.000Z","contents":"A Fourier approach to Sperner. (cont’d) If one does the usual Fourier analysis thing, one gets that the probability of %x, y% both in %\\mathcal A% equals %\\sum_{S,T \\subseteq [n]} (1-2\\epsilon)^{|S \\cap T|}(\\epsilon)^{|S \\Delta T|} c(S,T)%, where %c(S,T)% is 0 if %|S \\Delta T|% is odd, and is %\\pm 1% otherwise, depending on the parity of %|S \\setminus T|%. I believe (and feel free to press me on this) that with some linear/matrix algebra, one can show that this expression is approximately %(1/2)\\mathbb{S}_{1-2\\epsilon}^{(\\epsilon)}(f) + (1/2)\\mathbb{S}_{1-2\\epsilon}^{(-\\epsilon)}(f)%, where %\\mathbb{S}_{\\rho}^{(\\xi)}(f)% is the “noise stability of %f% at %\\rho% under the %\\xi%-biased distribution”. This post is getting long, so let’s say this is roughly something like %\\mathbb{E}[f(u)f(v)]%, where %u% is a random string and %v% is formed by flipping each coordinate of %u% with probability %\\epsilon%. Rather like the original distribution, but note that %u% and %v% are likely incomparable. Even if you buy my calculations, you may ask what this has bought. I guess it’s that this noise stability is a fairly well understood quantity, so we pretty much know exactly what kind of %f% have this quantity near %\\delta%, what kind of %f% have this quantity near %\\delta^2%, etc."},{"username":"terence-tao","timestamp":"2009-02-12T08:56:00.000Z","contents":"Reading seminar I’ve begin a “reading seminar” on the ergodic proof of DHJ at [http://terrytao.wordpress.com/2009/02/11/a-reading-seminar-on-density-hales-jewett/](http://terrytao.wordpress.com/2009/02/11/a-reading-seminar-on-density-hales-jewett/) No idea how lively it will be, but I will try to “read aloud” my way through paper #1 at least, and see if anyone jumps in to help out. I expect the pace to be slower than on these threads."},{"username":"ryan-odonnell","timestamp":"2009-02-12T10:26:00.000Z","contents":"Followup to #472. Hmm, what if the set %\\mathcal{A}% is just the strings whose plurality digit is 0? This will very likely have either all or none of the %x_{ij}%‘s in %\\mathcal{A}%, if I’m not mistaken. Of course, being “monotone” this set has plenty of structure; I’m just wondering what we’re seeking. Indeed, there are Boolean functions (see “The Influence of Large Coalitions” by Ajtai and Linial) where every subset of %n / \\log^3 n% coordinates has negligible “influence” — meaning, almost surely the function is determined as soon as you fix the other %n - n/\\log^3 n% coordinates randomly. These should be easy to adapt to the %{}[3]^n% setting."},{"username":"terence-tao","timestamp":"2009-02-12T11:28:00.000Z","contents":"Reply to #479 Yeah, that’s a pretty blatant violation of concentration of measure. But the plurality set %{\\mathcal A}% you give has very low influence, and as such it is very easy to create combinatorial lines (or even moderately large combinatorial subspaces) – just pick a random element in %{\\mathcal A}% and jiggle one or more of the coordinates, and with high probability you’ve created a combinatorial line in %{\\mathcal A}%. Low influence, as we’ve said before, leads to an “instant win” for us. Almost as instant a win is near-total “01-insensitivity”, which is like low influence but restricted to swapping 0 and 1, rather than the full range of swapping 0, 1, and 2\\. I discussed this in 451\\. In terms of the %x_{i,j}%, this type of insensitivity is basically the same as saying that the event %x_{i,j} \\in {\\mathcal A}% is almost maximally positively correlated with %x_{i+1,j} \\in {\\mathcal A}%. What I don’t understand well yet, though, is what happens when there is a smaller amount of positive correlation; this gives a little bit of insensitivity, but not so much that the argument in 451 immediately applies. A typical example here might be a random subset of, say, the set in 479\\. Maybe some sort of “regularity lemma” describes all moderately-correlated sets as unions of random subsets of very highly correlated (and thus very structured and low-influence or insensitive) sets?"},{"username":"terence-tao","timestamp":"2009-02-12T11:31:00.000Z","contents":"Metacomment. Michael Nielsen has kindly transplanted the google document above to a Wiki format, where it really can be edited by everyone, and which allows for LaTeX support, creation of subpages, etc: [http://michaelnielsen.org/polymath1/index.php?title=Main_Page](http://michaelnielsen.org/polymath1/index.php?title=Main_Page) So I guess this can serve as the “background” repository for the project, to complement the “foreground” threads here and on my blog. (We’re also running out of numbers on this thread… time to start the 500 thread soon!)"},{"username":"randall","timestamp":"2009-02-12T12:03:00.000Z","contents":"Z_k Roth: I’d like to propose a further simplification of the problem, motivated by Gowers’ observations in post 31\\. Here it is: For all $\\delta>0$, there is an $n$ such that any $E\\subset {\\bf Z}_3^n$ having relative density at least $\\delta$ contains a configuration $\\{a,a+\\sum_{i\\in \\alpha} e_i,a+2\\sum_{i\\in \\alpha} e_i\\}$, where $e_i=(0,0,\\ldots ,0,1,0,\\ldots ,0)$, the “1” occurring in the $i$th place. (Addition is modulo 3.) I will assume no decent quantitative version of this is known. (If I’m mistaken, the following may be ignored.) This is stronger than just getting $\\{a,a+b,a+2b\\}$ for some $b\\in {\\bf Z}_3^n$, which, if I understand things properly, one could obtain via Roth’s density increment method. And, indeed, the set $A$ mentioned in post 31, the one which has sequences where the number of 0s, 1s and 2s all multiples of 7, will contain too many of these configurations. On the other hand, the ergodic story suggests (heuristically, at least) that restrictions may come into play under “quadratic” Fourier analysis (perhaps modulo some normalization of some kind). The basis for this heuristic is the existence of an ergodic proof of the result (never written down) in which a compact extension of the Kronecker factor is shown to be characteristic. (This is dramatically different from DHJ or even IP Roth, where I should think no distal factor would be characteristic, suggesting that perhaps no degree of $\\alpha$-uniformity would yield constraints on the number of configurations.) This is only a heuristic, but the implication seems to be that perhaps the above mentioned result is at the approximate level of “depth” of existence of progressions of length 4 in sets of positive density in ${\\bf N}$. And, if all this is right, that it’s the sort of problem that might be attacked by familiar methods."},{"username":"gil","timestamp":"2009-02-12T14:49:00.000Z","contents":"Various types of slices and ultra-optimism My UO conjecture meant to be an extension of the LYM inequality which is stronger than just saying that maximum untichain is the maximum slice. Here we have various slices and we can conjecture that the maximum size of a line-free set is attained at the union of slices (which do not contain a line). We can also conjecture (as Tim’s) that the maximum measure (when every slice has the same overall measure) for line-free set is attained with union of slides. (This reduces to LYM for the sperner case.) I think that my conjecture (or at least what I meant) is a bit stronger. Unlike the Sperner case there are many ways to cover or fractionally cover all slices by line-free union of slices. Now there are several meanings for “slices”. The coarser is $\\latex \\Gamma_{a,b,c}$ but there are finer ones. One finer one is to associate to each word %w% in ‘0’ and ‘1’ the slice %\\Gamma_w% of all words of length n in 0 1 2 that if you arase the 2s you get %w%.  \nWe can ask if the ultra optimistic conjecture for line-free sets is true for union of slices $\\latex \\Gamma_w$. (This seems a feasible problem.) We can also talk together about such slices in all 3 directions. There is an even finer division to slices. Consider all triples of words %w_1,w_2% in 0-2 and 1-2 respectively. Than let %\\Gamma_{w_1,w_2}% be all words of length n in 012 s.t. when you delete 2 you get %w_1% and when you delete 1 you get %w_2%. We can ask if blocks in finer slices gives us better examples than blocks in courser slices.  \n(If yes, we can update our ultra optimistic conj.) (Again you can take slices in all 3 possible “directions”)."},{"username":"terence-tao","timestamp":"2009-02-12T15:03:00.000Z","contents":"Randall, your question sounds a bit like that of looking for progressions of length 3 whose difference is one less than a prime, which was tackled by Bergelson-Host-Kra using the nilsequences machinery and the Gowers norms. My guess is that a similar approach would work for your problem, though I didn’t check it. Gil, I am still having trouble understanding the precise formulation of your conjecture. For instance, what does “fractionally cover” mean? And what is a “block”? Also, what does “erase” or “delete” mean – are you compressing the deleted string, so that for instance deleting 1 from 012 gives you 02? It all sounds quite interesting, but I am afraid I may have to see a formal statement of the conjecture in order to understand it properly."},{"username":"gil","timestamp":"2009-02-12T15:03:00.000Z","contents":"Regarding influences which were mentioned in several comments. It looks that indeed examples with unusual number of lines has unusual (compared to random examples) influence, but it does not look clear how such a property can be exploited to lead to higher density subspaces. So I am not sure if influence would be a useful notion in attacking the problem at hand. But there may be some useful analogies between problems about influences and about finding lines of various types. When we talk about influence we look at a projections: we look at a subspace describe by a subset of the coordinates (but maybe we can also talk about different notions of subspace for some appropriate notion of “projection” ) and consider the projection of our set to this subspace. For our problem we are more interested in the intersection of our set with various subspaces."},{"username":"gil","timestamp":"2009-02-12T15:11:00.000Z","contents":"clarification for slices, also seconding Ryan Dear Terry, yes deleting 1 from 012 gives you 02 and from 011220212012  \ngives you 0220202, a block of slices is the union of different slices (in our original notion as in the refined ones when you talk only about one direction the slices are disjoint) (We are only interested in line-free blocks) We say that a colloection of block of slices covers %\\Gamma% if every point in %\\Gamma% is contained in one (or more) of them. A fractional collection of blocks is assigning a nonnegative number to every block. A fractional cover is a fractional collection where for every %x \\in \\Gamma% the sum of weights for blocks containing it is at least 1. Also, I second Ryan that giving analogs to Ajtai-Linial examples may be useful for our problem and perhaps also for the cap set problem. (I will try to explain why sometime later.)"},{"username":"gowers","timestamp":"2009-02-12T18:23:00.000Z","contents":"Uniformity norms This is an attempt to push forward in a small way a strategy for a global density-increment strategy. I would like to find a norm on functions from %{}[3]^n% to %\\mathbb{R}% with the property that if %\\|f\\|% is small then the expectation of %f(x)f(y)f(z)% over all combinatorial lines is small, and if %\\|f\\|% is large then one can find a structured set on which the average of %f% is positive and bounded away from 0\\. If we’ve got these two properties, then it should be pretty easy to finish the proof. Here is my imprecise proposal for such a norm. In fact, it’s imprecise to the extent that it may well not even be a norm: the idea would be to massage the details until it is. I’ll just draw it out of a hat, but of course it hasn’t sprung out of nowhere. It comes from a mixture of various sources: analogies with corners, and also the kinds of things that were coming out of my calculations at the beginning of the 400s, and also Terry’s Cartesian semi-product thoughts. In all the density-increment proofs we know where the obstructions are of a simplicity that we expect for DHJ(3), a key concept is something like a rectangle: for Roth’s theorem it’s a quadruple %(x,x+a,x+b,x+a+b)%, for Shkredov it’s %((x,y),(x+a,y),(x,y+b),(x+a,y+b)%, and for bipartite graphs it’s a 4-cycle %(xy,x'y,x'y',xy')%. What would be the obvious quadruples to look at here? Well, we know that we can think of a subset %\\mathcal{A}\\subset{}[3]^n% as a bipartite graph: its elements are pairs of sets %(A,B)% such that there exists %x\\in\\mathcal{A}% such that %A% is the 1-set of %x% and %B% is the 2-set of %x%. So a natural suggestion (if you don’t think too hard about it) for a norm to put on a function %f% is %\\|f\\|=\\mathbb{E}f(A,B)f(A,B')f(A',B')f(A,B'),% where the expectation is over all quadruples %(A,B,A',B')% such that the above four pairs are disjoint pairs. (We don’t need complex %f% but if we did then we’d put bars over the second and fourth terms in the product.) What are some immediate objections to this? Well, given such a quadruple, each element has seven possibilities: it can belong to up to two of the four sets %A,B,A',B'% but cannot belong to both an %A% and a %B%. Therefore, a random such quadruple is obtained by randomly partitioning %{}[n]% into seven parts and choosing the first one to be disjoint from all four sets, the second one to be in just %A%, and so on. In particular, the set that’s disjoint from all of them has cardinality very close to %n/7% with very high probability, so it’s got nothing much to do with combinatorial lines. But not so fast. This is uniform-measure speak rather than slices-equal speak,a nd I’m a slices-equal person. If slices are equal, then the fact that %A\\cup A'% is disjoint from %B\\cup B'% has a much smaller bearing on their cardinalities. (It does have some effect, but it’s just not a big deal.) So we have no a priori reason to reject this norm. In the next comment I’ll try to argue (as much to myself as to anyone else) that it’s actually good for something."},{"username":"gowers","timestamp":"2009-02-12T18:51:00.000Z","contents":"Uniformity norms. Now for a small technical comment. Suppose we take the subset of %\\mathbb{Z}^2% consisting of all points %(a,b)% such that %a,b\\geq 0% and %a+b\\leq n%. This is an isosceles right-angled-triangular chunk of the integer lattice. And now suppose we try to investigate functions on this set using a box norm (that is, averaging expressions like %f(x,y)f(x+a,y)f(x,y+b)f(x+a,y+b)).% We will have a little bit of trouble with this because some points are contained in lots of boxes and some in hardly any. So what we might decide to do instead is a modest localization: choose some square patch of the triangle where the set we are interested in is dense. We have essentially this problem with a uniformity norm for DHJ if we use the slices-equal measure. To combat it, I want to apologize to the number 3 and lose some symmetry. Recall (or prove as a very easy exercise) that DHJ(3) is equivalent to the following problem: if you have a dense set %\\mathcal{A}% of pairs %(A,B)% of disjoint subsets of %{}[n]% then you can find disjoint sets %A,B,D% such that %(A,B)%, %(A\\cup D,B)% and %(A,B\\cup D)% all belong to %\\mathcal{A}%. I’m going to focus on this version of the statement, but with the added stipulation that the cardinalities of %A% and %B% are both at most %n/2%, so that I have a square grid of slices. All the time, I need hardly add, we use slices-equal measure. So a random point in the space is obtained by choosing two random numbers %r% and %s% that are at most %n/2% and then a random pair of disjoint sets %A% and %B% of sizes %r% and %s%. Unfortunately I have to go while I’ve still got a lot to say, and won’t have a chance to add to this for about nine hours, if not more. Anyhow, the next step is to try to do a sort of Sperner-averaging style lift to get from the usual proof that the box norm controls the number of corners to a proof that the above attempt at a uniformity norm controls the number of set-theoretic corners. And if that works, I’d like an inverse theorem for the uniformity norm to tell us that we’ve got a 411-style obstruction."},{"username":"gowers","timestamp":"2009-02-12T22:31:00.000Z","contents":"Uniformity norm. Another technical point. In order to define a norm, we need that expression to be non-negative (and zero only if %f% is zero). To get that we can do the following. For convenience define %f(A,B)% to be zero if %A% and %B% intersect. Then define the norm as follows. First pick a random permutation. Then take the average of %f(A,B)f(A,B')f(A',B')f(A',B)% over all quadruples of _intervals_ in the permuted %{}[n]%. This is then easily seen to have the positive definiteness that one wants. Does it matter that we are now averaging over lots of intersecting pairs of sets? Not really, because the intersecting pairs are given a lot less weight (because intervals, on average, intersect far less frequently than random sets of the same size). So this is my revised candidate for a uniformity norm (perhaps with some restriction on the sizes of the sets involved — I’m not sure that matters much any more though)."},{"username":"ryan-odonnell","timestamp":"2009-02-13T05:24:00.000Z","contents":"Terry’s strategy #439. I’m still slowly plugging through that post… <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> Perhaps it’s not necessary to show that the second concentration of measure (for %x_{ij} \\in \\mathcal{A}%) holds *because* of small pairwise correlations between the events. For example, it holds anyway in the case of the “mod %1/\\delta%” set (#472) and the “random subset of plurality” set (end of #480). Except, wait: I suppose it doesn’t hold in a variant of #480’s example. Take the set of strings which have at least %n/3 + c \\sqrt{\\log(1/\\delta)}\\sqrt{n}% 0’s in them, where that deviation is cooked up so that the overall density is %2\\delta%. Then take a random subset of half of these strings. In this case, it seems like you could get too *many* of these %x_{ij} \\in \\mathcal{A}% events happening. Is this the problem? I must confess, I haven’t gotten myself up to speed yet on the particulars of the second half of the plan in #439."},{"username":"gowers","timestamp":"2009-02-13T05:55:00.000Z","contents":"Uniformity norms. Now I would very much like to be able to prove that if %f% has a small uniformity norm in the sense of 486/7, then the expectation of %f(A,B)f(A\\cup D,B)f(A,B\\cup D)% is small. But let me go for the slightly easier task of showing that if %f% is a 01-valued function and the expectation of %f(A,B)f(A\\cup D,B)f(A,B\\cup D)% is almost zero, then the expectation of %f(A,B)f(A,B')f(A',B')f(A',B)% is larger than it should be. Incidentally, my convention now has to be that if the average of %f% is %\\delta%, then %f(A,B)=\\delta% whenever %A% and %B% intersect. Actually, I want to revise the definition of the uniformity norm. I take a random permutation as in 486, but I insist that %A% and %A'% are initial segments (rather than just any old intervals) and that %B% and %B'% are final segments. But they’re still allowed to overlap, so that the expression for the norm splits up as a sum of squares. Now let’s go back to Terry’s Cartesian semi-products, as I discussed them in 442\\. First of all, if we take a random permutation of %{}[n]% and then take a random interval %J% in the permuted set, there will on average be several pairs of subintervals %(A,B)% that partition %J%. For fixed %J% each such pair is of course determined by %A% only. And if we have two such pairs %(A,B)% and %(A',B')% with %A\\subset A'%, then the pair %(A,B')=(A,J\\setminus A')% cannot lie in %\\mathcal{A}%. This means that %\\mathcal{A}% has to avoid the Cartesian product of the set of all the %A% such that %(A,J\\setminus A)\\in\\mathcal{A}% with the set of all the %B% such that %(J\\setminus B,B)\\in\\mathcal{A}%. But that gives us a nonuniformity in the function %f%. Averaging over all permutations %\\pi% and all intervals %J% gives us (I think) that the norm in 486/7 is large. What I _think_ this sketch shows is that the first stage of the two-step programme is OK: if a set contains no combinatorial lines then its slices-equal uniformity norm is too big. I’ve got to go to bed now, but I’ll write a very brief comment about the second stage."},{"username":"gowers","timestamp":"2009-02-13T06:03:00.000Z","contents":"Inverse theorem. The second stage is to prove an inverse theorem for the uniformity norm: that is, to prove that if %f% has a large uniformity norm then it has positive correlation with an exploitable structure (meaning a structure that can be almost uniformly covered by subspaces). The approach one would take to this in a dense bipartite graph would be to take a random edge and take the two neighbourhoods of its end vertices. The analogue here would be to take a random disjoint pair of sets %(A',B')% and then to take all disjoint pairs %(A,B)% such that %(A,B')% and %(A',B)% belong to %\\mathcal{A}%. Note that this is a Kneser product of two sets, just as we want, and that on average we expect %\\mathcal{A}% to be too dense in it. I was about to say that the problem is that there are far too few such pairs, but it’s just beginning to occur to me that perhaps that doesn’t matter. I’ve got to go to bed now, but I’m not completely certain that the inverse theorem isn’t almost as trivial here as it is for graphs. I’ll go to bed on that note of hyperoptimism — it feels as though the whole problem could be basically solved, but with me that feeling is extremely unreliable, and all the more so as there’s a giant claim in this paragraph that I haven’t checked at all."},{"username":"gowers","timestamp":"2009-02-13T06:35:00.000Z","contents":"Inverse theorem. Oops. Forget last paragraph of 490\\. It didn’t survive the going-to-bed process. I’ll think about a possible inverse theorem tomorrow."},{"username":"gil","timestamp":"2009-02-13T16:15:00.000Z","contents":"DHJ is equivalent for different measures. Dear Tim,  \n“I’m pretty sure that DHJ for the slices-equal measure is an easy consequence of DHJ, and vice versa. Here’s a sketch of how to prove that DHJ for any reasonably continuous measure (meaning that the weight of a point is usually very similar to the weight of a neighbouring point) is equivalent to the usual DHJ.” “…the measure in almost all of the subspaces is almost uniform, so we can apply the normal DHJ…” I do not understand the argument. Is it ok?"},{"username":"gowers","timestamp":"2009-02-13T19:16:00.000Z","contents":"DHJ equivalence. Dear Gil, I see what you mean. I had in my mind the false idea that the function %\\binom nk% is roughly continuous in %k%, when of course that is true only near the centre. I now think you are right and that DHJ for slices-equal measure is a stronger result than DHJ (and therefore, I imagine, not a known result at all). The reason I think that is that Sperner for slicees-equal seems to be stronger than Sperner for the uniform measure. Of course, the other direction is the one we actually need, so this doesn’t mean that proving DHJ for slices-equal measure is a waste of time."},{"username":"gowers","timestamp":"2009-02-13T20:28:00.000Z","contents":"DHJ equivalence. Actually, perhaps there is a way of deducing slices-equal DHJ from uniform DHJ. Let %A% be a dense subset of %{}[3]^n% in the slices-equal measure. Find a small triangular grid of slices %\\Gamma_{a,b,c}% where %A% is on average dense, where all %(a,b,c)% in the grid are close to %(a_0,b_0,c_0)% that are bounded away from 0 and %n%. Suppose, for the sake of example, that your slices are all very close to %(n/4,n/4,n/2)%. Now choose a random %n/4% coordinates and fix them to be 3\\. On average, your set will still be dense, and now you have a new %n% (it is the old %3n/4%) and all slices are near the middle so that they all have roughly the same weight."},{"username":"gil-kalai","timestamp":"2009-02-14T03:56:00.000Z","contents":"DHJ equivalence. Actually, on second thought, I do not think the DHJ for sliced-equal measure easily implies DHJ or is easily implied by DHJ. Since the two measures in questions are mutually singular it seems a priori that an easy reduction will be a little miracle. For Sperner, the result that in the silced-equal measure the maximum measure of an antichain is at most 1/n indeed easily implies that the maximum measure for an antichain in the uniform measure is at most %c/\\sqrt n%. Now, the heuristic argument from 494 is appealing but suspicious. Will all the slices be sufficiently near the middle, or rather look near but actually be in the shallow boundary of the middle where “roughly the same” still be too “roughly”?"},{"username":"gowers","timestamp":"2009-02-14T05:58:00.000Z","contents":"DHJ equivalence. There’s no problem getting from an arbitrary measure to the uniform measure. The proof is as follows. Suppose I know that every set that is dense in the measure %\\mu% contains a combinatorial line. Now suppose I want to find a combinatorial line in a set %A% that has density %\\delta% in the uniform measure on %{}[3]^n% if %n% is sufficiently large. Pick %m% such that every subset of %{}[3]^m% of %\\mu%-density %\\delta% contains a combinatorial line. Now randomly embed %{}[3]^m% into %{}[3]^n% by choosing %m% variable coordinates and fixing the rest. We may suppose that every point in %A% has %n/3+O(\\sqrt n)% of each coordinate value and that %m<. Therefore, changing %m% coordinates hardly changes the density of a slice. It follows that each point of %A% is in approximately the same number of these random subspaces. Therefore, by averaging, there is a random subspace inside which %A% has %\\mu%-density at least %\\delta/2%. (We could think of it Terry’s way: as we move the random subspace around, what we effectively have is a bunch of random variables, each with mean approximately %\\delta%, so by linearity of expectation we’ll get %\\mu%-density at least %\\delta/2% at some point, whatever the measure %\\mu% is.) The other way round I am less sure of. I think I’ll have to go away and actually try to write out the calculation rigorously."},{"username":"gil","timestamp":"2009-02-14T13:21:00.000Z","contents":"Another general question (I think we discussed it but I forgot what was the conclusion.) How far can we bound from above the number of wild coordinates in the combinatorial lines so that DHJ (for k=3) is still correct? For Sperner it is enough that the number of wild coordinates tends to infinity as slow as we want, is that so also for DHJ?"},{"username":"terence-tao","timestamp":"2009-02-14T13:41:00.000Z","contents":"Dear Gil, Yes, we can in fact make the number of wildcards bounded by %m = m(\\delta)% depending only on m. Indeed, one basically takes m to be the first index for which DHJ is true at this density. The pigeonhole principle then tells us that there is an m-dimensional slice of the big set which also has density %\\delta%, and the claim follows."},{"username":"gowers","timestamp":"2009-02-14T14:15:00.000Z","contents":"Dear Gil, In the end I didn’t have to do a calculation to see that my argument for the reverse implication was wrong. If you condition on %n/4% particular coordinates equalling 3, then that hugely affects what slice you are likely to have chosen, so you no longer have slices-equal measure. This is one area where the uniform measure scores over the slices-equal measure."},{"username":"gil","timestamp":"2009-02-15T01:17:00.000Z","contents":"DHJ for various measures Thanks Tim and Terry; The implication from general measures to uniform measures is neat and simple. (it is even simpler when the problem is group-invariant like for cap sets.) I suppose that there is no simple implication in the other direction and this has the benefit of making the DHJ problem for the slice-equal measure an open problem. I find the idea that changing the measure will help (e.g. difficulties described by some counter examples go away) quite appealing. (We talked in the threads about biased Bernoulli measures and about the slice-equal measure which is simply the simple average of all Bernoulli measures.) I wonder if there is a nice basis of orthgonal functions (like the Fourier transform) suitable for the slice-equal measure."},{"username":"terence-tao","timestamp":"2009-02-15T09:41:00.000Z","contents":"DHJ for various measures (Getting squeezed for space in this thread!) I think I can derive equal-slices DHJ from regular DHJ. Suppose that %A \\subset [3]^n% has density %\\delta% in the equal-slices sense. By the first moment method, this means that A has density %\\gg \\delta% on %\\gg \\delta% on the slices. Let m be a medium integer (much bigger than %1/\\delta%, much less than n). Pick (a, b, c) at random that add up to n-m. By the first moment method, we see that with probability %\\gg \\delta%, A will have density %\\gg \\delta% on the %\\gg \\delta% of the slices %\\Gamma_{(a',b',c')}% with %a' = a + m/3 + O(\\sqrt{m})%, %b' = b + m/3 + O(\\sqrt{m})%, %c' = c + m/3 + O(\\sqrt{m})%. This implies that A has expected density %\\gg \\delta% on a random m-dimensional subspace generated by a 1s, b 2s, c 3s, and m independent wildcards. Applying regular DHJ to that random subspace we obtain the claim."},{"username":"gil","timestamp":"2009-02-15T14:28:00.000Z","contents":"Influence strategy of proof There were several comments regarding the relation with influences, so let me mention a few connections and start with a “strategy” based on influence proofs. (probably related to Ryan’s suggested approach to Sperner.) a) 0-1 We have a subset A of %\\{0,1\\}^n% [or %\\{0,1,2\\}^n%].  \nLet %\\ell% be a combinatorial line let %R% be the set of fixed coordinates and %S% be the set of wild coordinetes. Let’s define the projection of A in the direction orthogonal to %\\ell% as the vectors %z% in %\\{0,1\\}^R% such that completing %z% by either the all 0 vector on the %S% coordinates or the all 1 vector [or the all 2 vector] gives you a vector in A. We can define the influence as the measure of the projection minus the measure of A. if %S% is a single coordinate this is the ususal influence. If A has density c and A contains no lines then its influence in every direction is c. (We would like to show that the influence must be substantially smaller in some directions.) So we would like to develop some strategy for showing that there is always a direction (maybe restrict our attention to directions for which R is small) so that the influence in this direction is smaller than c. (Or even smaller than %c-c^2/100%.) (To immitate these proofs we do not need to adopt the influence/projection terminology and can just talk about combinatorial lines.) If we try to immitate the influence proofs we need to do the following: a) To give a Fourier expression for the influence (or equivalently the number of combinatorial lines) in direction with %S% as wild coordinates. b) To note that (perhaps just when S is small) such an expression must obey some restrictions because it is the Fourier expansion of a set or some similar restraint function. c) to reach a contradiction, somehow. (Usually in the influence proofs b is applying some hypercontractive inequality; a major difference is that in these proofs we want to reach a contradiction from having too small influence and here from having too large influence.) b) 0-1-2 We have a subset A of %\\{0,1,2\\}^n%.  \nLet %\\ell% be a combinatorial line let %R% be the set of fixed coordinates and %S% be the set of wild coordinetes. We need to give a Fourier description for the numbers of vectors on %\\{0,1,2\\}^R% which are the projection of at least 1, and at least 2 vectors from A. Having no combinatorial line amounts to untypical relations between these two quantities and we want to show that they cannot all hold for every S (or every small S). Again this may be easier for DHJ(2.5) and alike. In this vague plan, what seems clearly doable (but require calculations) is giving the Fourier expansion for the different projections and influences."},{"username":"gil","timestamp":"2009-02-15T14:57:00.000Z","contents":"DHJ for different measurse  \nhmm. Right! _[We already have a comment 500 over on the other thread, so this looks like the time to declare this thread officially closed! (However, I have something to say about DHJ for different measures, so I’ll do so over there.)]_"},{"username":"gowers","timestamp":"2009-02-13T16:30:00.000Z","contents":"Inverse theorem. This is continuing where I left off in 490\\. Let me briefly say what the aim is. I am using the disjoint-pairs formulation of DHJ: given a dense set %\\mathcal{A}% of pairs %(A,B)% of disjoint sets, one can find a triple of the form %(A,B)%, %(A\\cup D,B)%, %(A,B\\cup D)% in %\\mathcal{A}% with all of %A,% %B% and %D% disjoint. Here “dense” is to be interpreted in the equal-slices measure: that is, if you randomly choose two non-negative integers %a,b% with %a+b\\leq n%, and then randomly choose disjoint sets %A% and %B% with cardinalities %a% and %b%, then the probability that %(A,B)\\in\\mathcal{A}% is at least %\\delta%. Now I define a norm on functions %f:[2]^n\\times[2]^n\\rightarrow\\mathbb{R}% as follows. (Incidentally, this is the genuine Cartesian product here, but pairs of sets will not make too big a contribution if they intersect.) First choose a random permutation %\\pi% of %{}[n]%. Then take the average of %f(A,B)f(A,B')f(A',B)f(A',B')% over all pairs %A,A'% of initial segments of %\\pi[n]% and all pairs %B,B'% of final segments of %\\pi[n]%. Finally, take the average over all %\\pi%. A few things to note about this definition. First, if you choose the initial and final segments randomly, then the probability that %A\\cup A'% is disjoint from %B\\cup B'% is at least %1/16% (and in fact slightly higher than this). Second, the marginal distributions of %(A,B),(A',B),(A,B')% and %(A',B')% are just the slices-equal measure. Third, for any fixed %\\pi% the expectation we take can also be thought of as %\\mathbb{E}_{A,A'}(\\mathbb{E}_Bf(A,B)f(A',B))^2%, so we get positivity. If we take the fourth root of this quantity and average over all %\\pi% then we get a norm, by standard arguments. (I’m less sure what happens if we first average and then take the fourth root. I’m also not sure whether that’s an important question or not.) Given a set %\\mathcal{A}% of density %\\delta% (as a subset of the set of all disjoint pairs, with slices-equal measure), define its _balanced function_ %f:[2]^n\\times[2]^n\\rightarrow\\mathbb{R}% as follows. If %A% and %B% are disjoint, then %f(A,B)=1-\\delta% if %(A,B)\\in\\mathcal{A}% and %-\\delta% otherwise. If %A% and %B% intersect, then %f(A,B)=0%. Then %\\mathbb{E}f=0% if we think about it suitably. And the way to think about it is this. To calculate %\\mathbb{E}f% we first take a random permutation of %{}[n]%. Then we choose a random initial segment %A% and a random final segment %B%. Then we evaluate %f(A,B)%. This is how I am _defining_ the expectation of %f%. Though I haven’t written out a formal proof (since I am not doing that — though I think a general rule is going to have to be that if a sketch starts to become rather detailed, and if nobody can see any reason for it not to be essentially correct, then the hard work of turning it into a formal argument would be done away from the computer), I am fairly sure that if a dense set %\\mathcal{A}% contains no combinatorial lines and %f% is the balanced function of %\\mathcal{A},% then %\\|f\\|% is bounded away from 0. That was just a mini-summary of where this uniformity-norms idea may possibly have got to. Because it feels OK I want to think about the part that feels potentially a lot less OK, which is obtaining some kind of usable structure where %\\|f\\|% has a large average, on the assumption that %\\|f\\|% is large."},{"username":"gowers","timestamp":"2009-02-13T16:48:00.000Z","contents":"Inverse theorem. What I would very much like to be able to do is rescue a version of the conjecture put forward in [comment 411](https://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2019). The conjecture as stated seems to be false, but it may still be true if we use the slices-equal measure. In terms of functions, what I’d like to be able to prove is that if %\\|f\\|% is at least %\\delta%, then there exist set systems %\\mathcal{U}% and %\\mathcal{V}% such that the density of disjoint pairs %(A,B)% with %A\\in\\mathcal{A}% and %B\\in\\mathcal{V}% is at least %c(\\delta)>0% and the average of %f(A,B)% over all such pairs is at least %c'(\\delta)>0%. (The first condition could be relaxed to positive density in some subspace if that helped.) I’m fairly sure that easy arguments give us the following much weaker statement. If you choose a random pair %\\sigma=(A_0,B_0)% according to equal-slices measure, then you can find set systems %\\mathcal{U}_\\sigma% and %\\mathcal{V}_\\sigma% with the following properties. (i) Every set in %\\mathcal{U}_\\sigma% is disjoint from %B_0% and every set in %\\mathcal{V}_\\sigma% is disjoint from %A_0%. (ii) The set of disjoint pairs %(A,B)% such that %A\\in\\mathcal{U}_\\sigma% and %B\\in \\mathcal{V}_\\sigma% is on average dense in the set of disjoint pairs %(A,B)% such that %A\\cap B_0=B\\cap A_0=\\emptyset%. (iii) For each %\\sigma% the expectation of %f(A,B)% over all disjoint pairs %(A,B)\\in\\mathcal{U}_\\sigma\\times\\mathcal{V}_\\sigma% is at least %c'(\\delta)%. If this is correct, then the question at hand is whether one can piece together the pairs %(\\mathcal{U}_\\sigma,\\mathcal{V}_\\sigma)% to form two set systems %\\mathcal{U}% and %\\mathcal{V}% such that the density of disjoint pairs %(A,B)% with %A\\in\\mathcal{U}% and %B\\in\\mathcal{V}% is at least %c(\\delta)% and the average of %f(A,B)% over all such pairs is at least %c''(\\delta)%."},{"username":"gowers","timestamp":"2009-02-13T17:36:00.000Z","contents":"Inverse theorem. In this comment I want to explain slightly better what I mean by “piecing together” local lack of uniformity to obtain global obstructions. To do so I’ll look at a few examples. **Example 1** Terry has already mentioned this one. Let %f% be a function defined on %\\mathbb{Z}_N%. Suppose we know that for every interval %I% of length %m% we can find a trigonometric function %\\tau_I% such that %\\mathbb{E}_{x\\in I}f(x)\\tau_I(x)\\geq\\delta%. Can we find a trigonometric function %\\tau% defined on all of %\\mathbb{Z}_N% such that %\\mathbb{E}_xf(x)\\tau(x)\\geq c(\\delta)%? The answer is very definitely no. Basically, you can just partition %\\mathbb{Z}_N% into intervals of length a bit bigger than %m% and define %f% to be a randomly chosen trigonometric function on each of these intervals. This will satisfy the hypotheses but be globally quasirandom. **Example 2** Now let’s modify Example 1\\. This time we assume that %f% has trigonometric bias not just on intervals but on arithmetic progressions of length %m% (in the mod-%N% sense of an arithmetic progression). I don’t know for sure what the answer is here, but I think we do now get a global correlation. Suppose for instance that %m% is around %\\sqrt{N}%. Then if we choose random trigonometric functions on each interval of length %m%, they will give rise to random functions on arithmetic progressions of common difference %m%. So the frequencies are forced to relate to each other in a way that they weren’t before. **Example 3.** Suppose that %G% is a graph of density %\\delta% with vertex set %X% of size %n%. Suppose that %m% is some integer that’s much smaller than %n% and suppose that for a proportion of at least %c% of the choices %Y% of %m% vertices you can find a subset %Z\\subset Y% of size at least %cm% such that the density of the subgraph induced by %Z% (inside %Z%) is at least %\\delta+c%. Can we find a subset %W% of %X% of size at least %c'n% such that the density of the subgraph induced by %W% is at least %\\delta+c'%? The answer is yes, and it is an easy consequence of standard facts about quasirandomness. The hypothesis plus an averaging argument implies that %G% contains too many 4-cycles, which means that %G% is not quasirandom, which implies that there is a global density change. (Sorry, in this case I misstated it — you don’t actually get an increase, as the complete bipartite graph on two sets of size %n/2% illustrates, but you can get the density to differ substantially from %\\delta%. If you want a density increase, then you need a similar statement about bipartite graphs.) **Example 4.** Suppose that %G% is a random graph on %n% vertices with edge density %n^{-1/100}%. Let %H% be a subgraph of %G% of relative density %\\delta%. Then one would expect %H% to contain about %(\\delta n^{-1/100})^3n^3% labelled triangles. Suppose that it contains more triangles than this. Can we find two large subsets %U% and %V% of vertices such that the number of edges joining them is substantially more than %\\delta n^{-1/100}|U||V|%? Our hypothesis tells us that if we pick a random vertex in the graph, then its neighbourhood will on average have greater edge density than %\\delta n^{-1/100}%. So we have a large collection of sets where %H% is too dense. However, these sets are all rather small. So can we put them together? The answer is yes but the proof is non-trivial and depends crucially on the fact that the neighbourhoods of the vertices are forced (by the randomness of %G%) to spread themselves around. **Question.** Going back to our Hales-Jewett situation, as described at the end of the previous comment, is it more like Example 1 (which would be bad news for this approach, but illuminating nevertheless), or is it more like the other examples where the non-uniformities are forced to relate to one another and combine to form a global obstruction?"},{"username":"gowers","timestamp":"2009-02-13T22:23:00.000Z","contents":"Combining obstructions. I want to think as abstractly as possible about when obstructions in small subsets are forced to combine to form a single global obstruction. For simplicity of discussion I’ll use a uniform measure. So let %X% be a set, let %E_1,\\dots,E_N% be subsets of %X%, and let us assume that every %x\\in X% is contained in exactly %m% of the %E_i%. Let us assume also that the %E_i% all have the same size (an assumption I’d eventually hope to relax) and that a typical intersection %E_i\\cap E_j% is not _too_ small, though I won’t be precise about this for now. For each %E_i% let %g_i:E_i\\rightarrow[-1,1]% be some function. Then one simple remark one can make (which doesn’t require anything like all the assumptions I’ve just made) is that if we can find %f% that correlates with many of the %g_i%, then there must be lots of correlation between the %g_i% themselves. To see this, recall first that %m^{-1}\\sum_i\\chi_{E_i}% is the constant function 1\\. Therefore, %g=m^{-1}\\sum_ig_i% is a function that takes values in %{}[-1,1]%. Furthermore, %\\|g\\|_2^2=m^{-2}\\sum_{i,j}\\langle g_i,g_j\\rangle%. Since %m^{-2}\\sum_{i,j}\\langle\\chi_{E_i},\\chi_{E_j}\\rangle=1%, and we can rewrite it as %m^{-2}\\sum_{i,j}|E_i\\cap E_j|%, we find that on average %\\langle g_i,g_j\\rangle% must be a substantial fraction of its trivial maximum %|E_i\\cap E_j|%. If %m% is large, so that the diagonal terms are negligible, then this says that there are lots of correlations between the %g_i%. In my next comment I will discuss a strategy for getting from this to a globally defined function, built out of the %g_i%, that correlates with %f% and is likely to have good properties under certain circumstances."},{"username":"jozsef","timestamp":"2009-02-14T00:13:00.000Z","contents":"Combining obstructions. Tim – Before you define a general local-global uniformity, please let me ask a question. For extremal set problems it was very useful – and we used it before – to take a random permutation of the base set [n] and considering only sets which are “nice” in this ordering. For example, let us consider sets where the elements are consecutive. If the original set system was dense, then in this window, that we had by a random permutation, is a dense set of intervals. Here we can define/check uniformity easily. Disjoint sets are disjoint, and one can also analyze intersections. I didn’t do any formal calculations, however this window might give us some information we want and it is easy to work with. (We won’t find combinatorial lines there, so I don’t see direct a proof for DHJ, but it’s not a surprise …)"},{"username":"randall","timestamp":"2009-02-14T02:06:00.000Z","contents":"Uniformity norms: The ergodic proof of DHJ reduces it to an IP recurrence theorem. Now, for an IP system setup, we can easily define analogs of the (ergodic) uniformity norms. So, let %(T_a)% be a measure preserving IP system; %a% ranges over finite subsets of the naturals. Put %||f||_1=IP-\\lim (\\int f T_a f)^{1\\over 2}%. Now let %P% be the projection onto the factor that is asymptotically invariant under %(T_a)%; we can write %||f||_1=||Pf||_{L^2}% if we like. Now put %||f||_2= IP-\\lim_b IP-\\lim_a (\\int f T_a f T_b f T_{a\\cup b} f)^{1\\over 4}%. (This should look awfully familiar.) Presumably, if %||f||_2% is small, where %f% is the balanced version of the characteristic function of a set, then the set should have, asymptotically, the right number of arithmetic progressions of length 3 whose difference comes from some relevant IP set (or something like that). On the other hand, if %||f||_2% is big, %f% must correlate with….well, let’s just say if %||f||_1% is big then %f% correlates with a rigid function, that much is easy. At any rate I think the above may be on the right track. I may try to translate from ergodic theory to a more recognizable form, but right now a baby is waking up behind me…. However, I will say this much…there is no conventional averaging in any proof of any ergodic IP recurrence theorem (IP limits serve that purpose in this context). Also, there are extremely strong analogies between the uniformity norms on %\\mathbb{Z}_N% and the uniformity norms in ergodic theory (actually…to me it’s not really an analogy; these are exactly the same norms). Although no norms are defined in ergodic IP theory, there could be…what does a norm stand in for? To an ergodic theorist, the dth uniformity norm is a stand-in for d iterations of van der Corput. In the IP case, we do in fact use d iterations of a version of van der Corput. If the correlation holds to form, the above norms may well be the ones to be looking at."},{"username":"jozsef","timestamp":"2009-02-14T02:08:00.000Z","contents":"Combining obstructions. In my previous post I said that we won’t see combinatorial lines in a random permutation. Now I’m not that sure about it. So, here is a simple statement which would imply DHJ (so it’s false most likely, but I can’t see a simple counterexample) If you take a dense subset of pairwise disjoint intervals in [n] then there are four numbers a, b, c, d in increasing order that [a,b],[c,d] and [a,c],[c,d] and [a,b],[b,d] are in your subset.  \nThe rough numbers – counting the probability that there is no such configuration in a random permutation – didn’t give me any hint. I have to work with a more precise form of Stirling’s formula. I need time and I will work away from the computer. (I have to leave shortly anyways)"},{"username":"jozsef","timestamp":"2009-02-14T02:18:00.000Z","contents":"506. The “simple statement” above is clearly false, just take pairs where  \none interval is between 1 and n/2 and the second is between n/2 and n. But I think this is not a general problem for our application where we might be able to avoid such “bipartite” cases."},{"username":"bengreen","timestamp":"2009-02-14T06:07:00.000Z","contents":"507. Tim, Regarding your Example 2 in 502 above, here is a cheat way to get a global obstruction. If you correlate with a linear phase on many progressions of length sqrt{N} then you also have large U^2 norm on each such progression. Summing over all progressions, this means that the sum of f over all parallelograms (x,x+h,x+k,x+h+k) with |k/h| \\leq \\sqrt{N}, or roughly that, is large. But even if you only control such parallelograms with k = 2h this is the same as controlling the average of f over 4-term progressions, and one knows that is controlled by the *global* U^3 norm and hence by a global quadratic object (nilsequence). That will allow you to “tie together” the frequencies of the trig polys that you correlated with at the beginning, though I haven’t bothered to think exactly how. One just needs to analyse which 2-step nilsequences correlate with a linear phase on many progressions of length sqrt{N}, and it might just be ones which are essentially 1-step, and that would imply that f correlates globally with a linear phase. BY the way much the same argument shows that if you have large U^k norm on many progressions of length sqrt{N} (say) then you have large global U^{2^k -1} norm. Ben"},{"username":"randall","timestamp":"2009-02-14T06:13:00.000Z","contents":"Ergodic-mimicking general proof strategy: Okay, I will give my general idea for a proof. I’m pretty sure it’s sound, though it may not be feasible in practice. On the other hand I may be badly mistaken about something. I will throw it out there for someone else to attempt, or say why it’s nonsense, or perhaps ignore. I won’t formulate it as a strategy to prove DHJ, but of what I’ve called IP Roth. If successful, one could possibly adapt it to the DHJ, k=3 situation, but there would be complications that would obscure what was going on. We work in $X=[n]^{[n]}\\times [n]^{[n]}$. For a real valued function $f$ defined on $X$, define the first coordinate 2-norm by $||f||^1_2=(\\iplim_b\\iplim_a {1\\over |X|}\\sum_{(x,y)\\in X} f((x,y))f((x+a,y))f((x+b,y))f((x+a+b,y)))^{1\\over 4}$. The second coordinate 2-norm is defined similarly (on the second coordinate, obviously). Now, let me explain what this means. $a$ and $b$ are subsets of $[n]$, and we identify $a$ with the characteristic function of $a$, which is a member of $[n]^{[n]}$. (That is how we can add $a$ to $x$ inside, etc. Since $[n]$ is a finite set, you can’t really take limits, but if $n$ is large, we can do something almost as good, namely ensure that whenever $\\max\\alpha<\\min\\beta$, the expression we are taking the limit of is close to something (Milliken Taylor ensures this, I think). Of course, you have to restrict $a$ and $b$ to a subspace. What is a subspace? You take a sequence $a_i$ of subsets of $[n]$ with $\\max a_i<\\min a_{i+1}$ and then restrict to unions of the $a_i$. Now here is the idea. Take a subset $E$ of $X$ and let $f$ be its balanced indicator function. You first want to show that if *either* of the coordinate 2-norms of $f$ is small, then $E$ contains about the right number of corners $\\{ (x,y), (x+a,y), (x,y+a)\\}$. Restricted to the subspace of course. What does that mean? Well, you treat each of the $a_i$ as a single coordinate, moving them together. The other coordinates I’m not sure about. Maybe you can just fix them in the right way and have the norm that was small summing over all of $X$ still come out small. At any rate, the real trick is to show that if *both* coordinate 2-norms are big, you get a density increment on a subspace. Here a subspace surely means that you find some $a_i$s, treat them as single coordinates, and fix the values on the other coordinates."},{"username":"gowers","timestamp":"2009-02-14T06:19:00.000Z","contents":"Quasirandomness. Jozsef, re 504, what you describe is what I thought I was doing in the second paragraph of 500\\. The nice thing about taking a random permutation and then only looking at intervals is that you end up with dense graphs. The annoying thing is that these dense graphs don’t instantly give you combinatorial lines. However, they do give correlation with a Cartesian product, so my hope was to put together all these small dense Cartesian products to get a big dense “Kneser product”. At the moment I feel quite hopeful about this. I’ve got to go to bed very soon, but roughly my planned programme for doing it is this. Suppose you have proved that there are a lot of pairwise correlations between the %g_i%. Then I want to think of these %g_i% as unit vectors in a Hilbert space and find some selection of them such that most pairs of them correlate. Here’s a general dependent-random-selection procedure for vectors in a finite-dimensional Hilbert space. Suppose I have unit vectors %x_1,\\dots,x_n\\in\\mathbb{R}^N%. Now I choose random unit vectors %y_1,\\dots,y_k% and pick all those %x_i% such that %\\langle x_i,y_j\\rangle>0% for every %j\\leq k%. The probability of choosing any given %x_i% is obviously %2^{-k}%. Now the probability that %\\langle x,y_i\\rangle% and %\\langle x',y_i\\rangle% are both positive is equal to the proportion of the sphere in the intersection of the positive hemispheres defined by %x% and %x'%, which has measure %1-\\theta/\\pi%, where %\\theta% is the angle between %x% and %x'%. So the probability of picking both %x% and %x'% is %(1-\\theta/\\pi)^k%. So if %k>>1/\\delta% and we have lots of correlations of at least %\\delta%, then we should end up with almost all pairs that we choose having good correlations. It is some choice like this that I am currently hoping will give rise to a global obstruction to uniformity. The more I think about this, the more I am beginning to feel that it’s got to the point where it’s going to be hard to make progress without going away and doing calculations. I don’t really see any other way of testing the plausibility of some of the ideas I am suggesting."},{"username":"terence-tao","timestamp":"2009-02-14T13:37:00.000Z","contents":"Stationarity The reading seminar is already paying off for me, in that I now see that the random Bernoulli variables associated to a Cartesian half-product automatically enjoy the stationarity property, which will presumably be useful in showing that those half-products contain lines with positive probability. (I also plan to write up this strategy on the wiki at some point.) Let’s recall the setup. We have a set A of density %\\delta% in %{}[3]^n% that we wish to find lines in. Pick a random x in %{}[3]^n%, then pick random %a_1,\\ldots,a_m% in the 0-set of x, where %m = m(\\delta)% does not depend on n. This gives a random embedding of %{}[3]^m% into %{}[3]^n%, and in particular creates the strings %x_{i,j}% for %0 \\leq i \\leq j \\leq m%, formed from x by flipping the %a_1,\\ldots,a_i% digits from 0 to 1, and the %a_{j+1},\\ldots,a_m% digits from 0 to 2\\. I like to think of the %x_{i,j}% as a “Cartesian half-product”, which in the m=4 case is indexed as follows: 0000 0002 0022 0222 2222  \n1000 1002 1022 1222  \n1100 1102 1122  \n1110 1112  \n1111 Observe that %x_{i,i}, x_{i,j}, x_{j,j}% form a line whenever %0 \\leq i < j \\leq m%. We have the Bernoulli events %E_{i,j} := \\{x_{i,j}\\in A\\}%. We already observed that each of the %E_{i,j}% have probability about %\\delta%, thus the one-point correlations of %E_{i,j}% are basically independent of i and j. More generally, it looks like the k-point correlations of the %E_{i,j}% are translation-invariant in i and j (so long as one stays inside the Cartesian half-product). For instance, when %m=4%, the events %E_{0,4} \\wedge E_{0,3}% and %E_{1,4} \\wedge E_{1,3}% (which are the assertions that the embedded copies of 0000, 1000 and 0002, 1002 respectively both lie in A) have essentially the same probability. This is obvious if you think about it, and comes from the fact that if you take a uniformly random string in %{}[3]^n% and flip a random 0-bit of that string to a 2, one essentially gets another uniformly random string in %{}[3]^n% (the error in the probability distribution is negligible in the total variation norm). Indeed, all the probabilities of %E_{i,j+1} \\wedge E_{i,j}% are essentially equal for all i,j for which this makes sense. And similarly for other k-point correlations. I don’t yet know how to _use_ stationarity, but I expect to learn as the reading seminar continues."},{"username":"gowers","timestamp":"2009-02-14T14:42:00.000Z","contents":"Local-to-global Re 502 Example 2 and 507, I think I now have a simple way of getting a global obstruction in this case, but it would have to be checked. Suppose that %f% is a function that correlates with a trigonometric function on a positive proportion of the APs in %\\mathbb{Z}_N% of length %m%. Here %m% could be very small — depending on the size of the correlation only I think. Now suppose I pick a random AP of length 3\\. Then it will be contained in several of these APs of length %m%, and if %N% is prime each AP of length 3 will be in the same number of APs of length %m%. At this stage there’s a gap in the argument because lack of uniformity doesn’t imply that you have the wrong sum over APs of length 3, but it sort of morally does, so I think it might well be possible to deal with this and get that the expectation of %f(x)f(x+d)f(x+2d)% is not small — perhaps by modifying %f% in some way or playing around with different %m%. And then we’d have a global obstruction by the %U^2% inverse theorem. Alternatively, instead of using APs of length 3, it might be possible to use additive quadruples %(x,x+a,x+b,x+a+b)% but under the extra condition that there is a small linear relation between %a% and %b%. That would give a useful positivity. I haven’t checked whether lack of uniformity on such additive quadruples implies that the %U^2% norm is large, but the fact that 3-APs give you big %U^2% is a promising sign."},{"username":"ryan-odonnell","timestamp":"2009-02-14T22:15:00.000Z","contents":"Stationarity. Just a small note on Terry.#510: Is there any particular reason for choosing the indices %a_i% from the 0-set of the initial random string %x%? If not, the probabilistic analysis might be very slightly cleaner if we simply say, “Choose %m% random coordinates %a_i% and fill in the remaining %n - m% coordinates at random. Now consider the events %E_{ij}%…”"},{"username":"terence-tao","timestamp":"2009-02-15T02:57:00.000Z","contents":"Stationarity Yeah, you’re right; I was rather clumsily thinking of the base point x as being the lower left corner of the embedded space, when in fact one only needs to think of what x is doing outside of the variable indices. (I’ve updated the wiki to reflect this improvement.)"},{"username":"terence-tao","timestamp":"2009-02-15T13:06:00.000Z","contents":"DHJ(2.6) Back in 130 I asked a weaker statement than DHJ(3), dubbed DHJ(2.5): if A is a dense subset of %{}[3]^n%, does there exist a combinatorial line %w^0,w^1,w^2% whose first two positions only %w^0, w^1% are required to lie inside A? It was quickly pointed out to me that this follows easily from DHJ(2). From the reading seminar, I now have a new problem intermediate between DHJ(3) and DHJ(2.5), which I am dubbing DHJ(2.6): if A is a dense subset of %{}[3]^n%, does there exist an r such that for each ij=01,12,20, there exists a combinatorial line %w^0,w^1,w^2% with exactly r wildcards with %w^i, w^j \\in A%? So it’s three copies of DHJ(2.5) with the extra constraint that the lines all have to have the same wildcard length. Clearly this would follow from DHJ(3) but is a weaker problem. At present I do not see a combinatorial proof of this weaker statement (which Furstenberg and Katznelson, by the way, spend an entire paper just to prove)."},{"username":"gowers","timestamp":"2009-02-15T20:53:00.000Z","contents":"Different measures. Terry, I’m not sure I understand your derivation of slices-equal DHJ from uniform DHJ. For convenience let me copy your argument and then comment on specific lines of it. _Suppose that %A \\subset [3]^n% has density %\\delta% in the equal-slices sense. By the first moment method, this means that A has density %\\gg \\delta% on %\\gg \\delta% on the slices._ _Let m be a medium integer (much bigger than %1/\\delta%, much less than n)._ _Pick (a, b, c) at random that add up to n-m. By the first moment method, we see that with probability %\\gg \\delta%, A will have density %\\gg \\delta% on the %\\gg \\delta% of the slices %\\Gamma_{(a',b',c')}% with %a' = a + m/3 + O(\\sqrt{m})%, %b' = b + m/3 + O(\\sqrt{m})%, %c' = c + m/3 + O(\\sqrt{m})%._ _This implies that A has expected density %\\gg \\delta% on a random m-dimensional subspace generated by a 1s, b 2s, c 3s, and m independent wildcards._ _Applying regular DHJ to that random subspace we obtain the claim._ One of my arguments for deducing slices-equal from uniform collapsed because sequences belonging to non-central slices are very much more heavily weighted than sequences that belong to central ones. I don’t understand why that’s not happening in your argument (though I haven’t carefully done calculations). Consider for example what might happen if %(a,b,c)=(n/6,n/3,n/2)-(m/3,m/3,m/3)%. Then the weights of individual sequences in the slices of the form %(a,b,c)+(r,s,t)% go down exponentially as r increases and as t decreases, even if %(r,s,t)% is close to %(m/3,m/3,m/3)%. Or am I missing something? In general, there seems to be a difficult phenomenon associated with the slices-equal measure, which is that unless you’re lucky enough for %A% to contain bits near the middle, it will be very far from uniform on all combinatorial subspaces where it might have a chance to be dense. Worse still, it doesn’t restrict to slices-equal measure on subspaces."},{"username":"gowers","timestamp":"2009-02-15T22:28:00.000Z","contents":"Different measures. On second thoughts, I do now follow your argument!"},{"username":"terence-tao","timestamp":"2009-02-15T23:29:00.000Z","contents":"Different measures I’ve put a summary of the different measures discussion on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure) (More generally, I will probably be more silent on this thread than previously, but will be working “behind the scenes” by distilling some of the older discussion onto the wiki.)"},{"username":"randall","timestamp":"2009-02-16T02:43:00.000Z","contents":"DHJ (2.6): Here’s what looks like a combinatorial proof, modulo reduction to sets and Graham-Rothschild being kosher (at least no infinitary theorems are used). Put everything in the sets formulation, where you have sets $B_w$ of measure at least $\\delta$ in a prob. space. (No need for stationarity.) Now color combinatorial lines $\\{ w(0),w(1),w(2)\\}$ according to whether $B_{w(0)}\\cap B_{w(1)}$, $B_{w(0)}\\cap B_{w(2)}$ and $B_{w(1)}\\cap B_{w(2)}$ are empty or not (8 colors). By Graham-Rothschild, there is an $n$-dimensional subspace all of whose lines have the same color for this coloring. Just take $n$ large enough for the sets version of DHJ (2.5)."},{"username":"terence-tao","timestamp":"2009-02-16T03:59:00.000Z","contents":"519. Dear Randall, I like this proof, but I am a little worried that A is still dense on the subspace one passes to by Graham-Rothschild. If A is deterministic, it may well be that the space one passes to is completely empty; if instead A is random, then the Graham-Rothschild subspace will depend on A and it is no longer clear that all the events in that space have probability %\\delta%."},{"username":"randall","timestamp":"2009-02-16T05:40:00.000Z","contents":"Terry…the subspace doesn’t depend on the random set A. What I am proving is the (b) formulation, not the (a) formulation. There seems to be some disconnect between the language of ergodic theory and the language of “random sets”, so let me try to translate: one reduces to a subspace where, for each ij=01,12,02, one of the following two things happens: 1\\. for all variable words w over the subspace, the probability that both w(i) and w(j) are in A is zero.  \n2\\. for all variable words w over the subspace, the probability that both w(i) and w(j) are in A is positive. If you still think there is an issue about a density decrement, just read my proof as a proof of the (b) formulation…there, you can’t have a density decrement upon passing to a subspace because all the sets B_w have density at least \\delta."},{"username":"terence-tao","timestamp":"2009-02-16T06:50:00.000Z","contents":"Oh, I see now. That’s a nice argument – and one that demonstrates an advantage in working in the ergodic/probabilistic setting. (It would be interesting to see a combinatorial translation of the argument, though.) I’ll write it up on the wiki (on the [DHJ page](http://michaelnielsen.org/polymath1/index.php?title=Density_Hales-Jewett)) now."},{"username":"ryan-odonnell","timestamp":"2009-02-16T10:35:00.000Z","contents":"DHJ(2.6). Hmm, I see how this is a bit trickier than DHJ(2.5). For example, for any of the three %ij% pairs you can probably show that if %r ~ \\text{Poisson}(m)% for some moderate %m% then there is at least an %\\epsilon = \\epsilon(\\delta) > 0% chance of having an “%ij% pair” at distance %r%. But the trouble is, these could potentially be different measure %\\epsilon% sets, and since they’re small it’s not so obvious how to show they’re not disjoint. As an example of the trickiness, let %q = 1/\\sqrt{\\delta}%, assumed an integer. Perhaps $\\latex A$ is the set of strings whose 0-count is divisible by %q% and whose 1-count is divisible by %q + 1%. Now 02-pairs have to be at distance which is a multiple of %q%, and 12-pairs have to be at a distance which is a multiple of %q+1%. So you have to be a bit clever to extract a common valid distance, a multiple of %q(q+1)%."},{"username":"gil","timestamp":"2009-02-16T13:01:00.000Z","contents":"Ryan, Can you explain more your approach in comment 477\\. ?"},{"username":"terence-tao","timestamp":"2009-02-16T13:38:00.000Z","contents":"DHJ(2.6) A small observation: one can tone the Ramsey theory in Randall’s proof of DHJ(2.6) a notch, by replacing the Graham-Rothschild theorem by the simpler Folkman’s theorem (this is basically because of the permutation symmetry of the random subcube ensemble). Indeed, given a dense set A in %{}[3]^n% we can colour [n] in eight colours, colouring a shift r depending on whether there exists a combinatorial line with r wildcards whose first two (or last two, or first and last) vertices lie in A. By Folkman’s theorem, we can find a monochromatic m-dimensional pinned cube Q in [n] (actually it is convenient to place this cube in a much smaller set, e.g. %{}[n^{0.1}]%). If the monochromatic colour is such that all three pairs of a combinatorial line with r wildcards can occur in A, we are done, so suppose that there is no line with r wildcards with r in Q with (say) the first two elements lying in A. Now consider a random copy of %{}[3]^m% in %{}[3]^n%, using the m generators of Q to determine how many times each of the m wildcards that define this copy will get. The expected density of A in this cube is about %\\delta%, so by DHJ(2.5) at least one of the copies is going to get a line whose first two elements lie in A, which gives the contradiction."},{"username":"terence-tao","timestamp":"2009-02-16T13:59:00.000Z","contents":"p.s. I have placed the statement of relevant Ramsey theorems (Graham-Rothschild, Folkman, Carlson-Simpson, etc.) on the wiki for reference."},{"username":"gil-kalai","timestamp":"2009-02-16T19:22:00.000Z","contents":"Strategy based on influence proofs Let me say a little more (vague) things about possible strategy based on mimmicking influence proofs. We want to derive facts on the Fourier expansion of our set based on the fact that there are no lines and let me just consider k=2 (Sperner). Suppose that your set A has density c and you know that there is no lines with one wild cards. This gives that  \n%\\sum f^2(S) |S| = c n/2%. (In a random set we would expect %(c-c^2)n/2%; anyway this is of course possible.) Next you want to add the fact that there are no lines with 2 wild cars.  \nThe formula (in fact there are few and we have to choose a simple one) for the number of lines with two wild cards i and j and the conclusion for the Fourier coefficients are less clear to me. Maybe we can derive conclusions on the higher moments %\\sum f^2(S)|S|^2% or some other indormations. Here, we do not want to reach a subspace with higher density but rather a contradiction. In the ususal influence proofs the tool for that were certain inequalities asserting that for sets with small support the Fourier coefficients are high. This particular conclusion will not help us but the inequalities can help in some other way. If indeed we are getting from the no line assumptions that the Fourier coefficients are high and concentrated this may be in conflict with facts about codes. Namely that our A will be a better than possible code in terms of its distance distribution. Anyway the first thing to calculate would be a good Fourier expression for the number of lines with 2 wildcards."},{"username":"ryan-odonnell","timestamp":"2009-02-16T20:18:00.000Z","contents":"DHJ(2.6). Thanks for the observation about using Folkman’s Theorem, Terry, it looks quite nice. I have some other ideas about trying to give a Ramsey-less proof based on Sperner’s Theorem; I’ll try to write them soon."},{"username":"ryan-odonnell","timestamp":"2009-02-16T20:52:00.000Z","contents":"Fourier approach to Sperner. Gil, here area few more details re #477\\. First, I see I dropped the Fourier coefficients at the beginning of the post, whoops! What I meant to say is that if you do the usual Fourier thing, you get the probability of %x,y \\in \\mathcal{A}% is %\\mathcal{B}(f) := \\sum_{|S \\Delta T| \\text{ even}} (1-2\\epsilon)^{|S \\cap T|}\\epsilon^{|S \\Delta T|}(-1)^{|S \\setminus T|} \\hat{f}(S) \\hat{f}(T)%. The intuition here is that since %\\epsilon% is tiny, hopefully all the terms involving a positive power of %\\epsilon% are “negligible”. Obviously this is unlikely to precisely happen, but if it were true then we would get %\\mathcal{B}(f) \\approx \\sum_{S} (1-2\\epsilon)^{|S|} \\hat{f}(S)^2%, a noise stability quantity. Now there are something like %2^{2n}% terms we want to drop, so it would be hard to do something simple like Cauchy-Schwarz. My idea is one from a recent paper with Wu. Think of %B(f)% as a quadratic form over the Fourier coefficients of %f%. I.e., think of %\\hat{f}% as a vector of length %2^n% and think of %\\mathcal{B}(f) = \\hat{f}^\\top B \\hat{f}%, where %B% is a matrix whose %(S,T)% entry is the obvious expression from above with %\\epsilon%‘s and %|S \\Delta T|%‘s and so on. Our goal is to find two similar-looking matrices %A, C% such that %A \\preceq B \\preceq C%, meaning %C - B, B - A% are psd. Then we get %\\mathcal{A}(f) \\leq \\mathcal{B}(f) \\leq \\mathcal{C}(f)%. (I see I’ve used %\\mathcal{A}% for two different things here; whoops.) To be continued…"},{"username":"ryan-odonnell","timestamp":"2009-02-16T20:53:00.000Z","contents":"Fourier approach to Sperner. Let me explain what I hope %C% will be; the matrix %A% will be similar. Let %C_1% be the 2×2 matrix with entries %1, \\epsilon, \\epsilon, 1 - \\epsilon%. This is like a little “%\\epsilon%-biased %\\epsilon%-noise stability” matrix. Let %C_n = C_1^{\\otimes n}%. Similarly, let %{C'}_1% be the 2×2 matrix with entries %1, -\\epsilon, -\\epsilon, 1 - \\epsilon% and define %{C'}_n%. Finally, let %C = (C_n + {C'}_n)/2%. The quadratic form associated to %C% should be that average of two noise stabilities I wrote in post #477, except with noise rate %1 - \\epsilon%, not %1 - 2\\epsilon%. My unsubstantiated claim/hope is that indeed %B \\preceq C%. Why do I believe this? Uh… believe it or not, because I checked empirically for %n% up to 5 with Maple. <span class=\"wp-smiley wp-emoji wp-emoji-wink\" title=\";)\">;)</span> Sorry, I guess this isn’t how a real mathematician would do it. But I hope it can be proven. To define the matrix %A%, do the same thing but set %A_1% to be the 2×2 matrix with entries %1, \\epsilon, \\epsilon, 1 - 3\\epsilon%, etc. So eventually I hope to sandwich %\\mathcal{B}(f)% between the average of the %\\pm\\epsilon%-biased %\\epsilon%-noise stabilities of %f% and the average of the %\\pm\\epsilon%-biased %3\\epsilon%-noise stabilities of %f%. PS: Although it may work for Sperner, I’m less optimistic ideas like this will work for DHJ. The trouble is, in a distribution on comparable pairs %(x,y)% for Sperner, there is *imperfect* correlation between %x% and %y%. Roughly, if you know %x%, you are still unsure what %y% is. On the other hand, in any distribution on combinatorial lines %(x,y,z)% for DHJ, there is *perfect* correlation between %(x,y)% and %z% (and the other two pairs). If you know %x% and %y%, then you know %z% with certainty. This seems to break a lot of hopes to use “Invariance”-type methods like those in Elchanan’s follow-on to MOO. This is related to the fact that there is no hope to prove a generalization of DHJ of the form, “If %\\mathcal{A}, \\mathcal{B}% are two subsets of %{}[3]^n% of density %\\delta%, then there is a combinatorial line %(x,y,z)% with %x,y \\in \\mathcal{A}% and %z \\in \\mathcal{B}%.”"},{"username":"ryan-odonnell","timestamp":"2009-02-16T22:10:00.000Z","contents":"Ramsey-less DHJ(2.6) plan. Here is a potential angle for a Ramsey-free proof of DHJ(2.6). The idea would be to soup up the Sperner-based proof of DHJ(2.5) and show that the set %D% of (Hamming) distances where we can find a 01-half-line in %\\mathcal{A}% is “large” and “structured”. So again, let %x% be a random string in %{}[3]^n% and condition on the location of %x%‘s 2’s. There must be some set of locations such that %\\mathcal{A}% has density at least %\\delta% on the induced 01 hypercube (which will likely have dimension around %(2/3)n%). So fix 2’s into these locations and pass to the 01 hypercube. Our goal is now to show that if %\\mathcal{A}% is a subset of %\\{0,1\\}^n% of density %\\delta% then the set %D% of Hamming distances where %\\mathcal{A}% has Sperner pairs is large/structured. (I’ve written %n% here although it ought to be %n' \\approx (2/3)n%.) Almost all the action in %\\{0,1\\}^n% is around the %\\Theta(\\sqrt{n})% middle slices. Let’s simplify slightly (although this is not much of a cheat) by pretending that %\\mathcal{A}% has density %\\delta% on the union of the %\\sqrt{n}% middle slices *and* that these slices have “equal weight”. Index the slices by %i \\in [\\sqrt{n}]%, with the %i%th slice actually being the %n/2 - \\sqrt{n}/2 + i/2% slice. Let %\\mathcal{A}_i% denote the intersection of %\\mathcal{A}% with the $i$th slice, and let %\\mu_i% denote the relative density of %\\mathcal{A}_i% within its slice. To be continued…"},{"username":"ryan-odonnell","timestamp":"2009-02-16T22:10:00.000Z","contents":"Ramsey-less DHJ(2.6) plan continued. Here is a slightly wasteful but simplifying step: Since the average of the %\\mu_i%‘s is %\\delta%, a simple Markov-type argument shows that %\\mu_i \\geq \\delta/2% for at least a %\\delta/2% fraction of the %i%‘s. Call such an %i% “marked”. Now whenever we have, say, %3/\\delta% marked %i%‘s, it means we have a portion of %\\mathcal{A}% with a number of points at least %3/2% times the number of points in a single middle slice. Hence Sperner’s Theorem tells us we get two comparable points %x \\leq y% from among these slices. Thus we have reduced to the following setup: There is a graph %G = (V,E)%, where %V \\subseteq [\\sqrt{n}]% (the “marked %i%‘s”) has density at least %\\delta/2% and the edge set %E% has the following density property: For every collection %V' \\subset V% with %|V'| \\geq 3/\\delta%, there is at least one edge among the vertices in %V'%. Let %D% be the set of “distances” in this graph, where an edge %(i,j)% has “distance” %|i - j|%. Show that this set of distances must be large and/or have some useful arithmetic structure."},{"username":"gil","timestamp":"2009-02-17T02:57:00.000Z","contents":"I wonder if the following “ideology” can be promoted or justified: A set without a line with at most r wildcards “behaves like” an error correcting code with minimal distance r’ where r’ grows to infinity (possibly very slowly) when r does. for k=2 of course we do not get en error correcting codes but when we have to split the set between many slices points from different slices are far apart so it is a little like a bahavior of a code. for k=3 this looks much more dubious but maybe has some truth in it."},{"username":"randall","timestamp":"2009-02-17T03:40:00.000Z","contents":"Need for Carlson’s theorem? Terry’s reduction to a use of Folkman instead of Graham-Rothschild, as well as his ideas for increasing the symmetry available, got me thinking there might be a way to reduce to a situation where you are using Hindman’s theorem instead of from Carlson’s theorem in the entire proof. Here is my first attempt…perhaps others can check if this makes sense. Fix a big $n$ and let $A$ be your subset of $[k]^n$ that you want to find a combinatorial line in. Let $Y$ be the space $2^{[k]^n}$, that is, all sets of words of length $n$. $U^i_j$ is the map on $Y$ that takes a set $E$ of words and interchanges 0s and js in the ith place to get a new set of words, namely the set $U^i_j E$. (Some of the words in $E$ may be unaffected; in particular, if no word in $E$ has a 0 or j in the ith place, $E$ is a fixed point.) Now let $X$ be the orbit of $A$ under $G$, the group gen. by these maps. It seems that to find a combinatorial line in any member of $X$ will give you a line in $A$. Let $B=\\{ E:00…00\\in E\\}\\subset X$. In general, for $U\\subset X$, define the measure $\\mu(U)$ to be the fraction of $g$ in $G$ such that $gA\\in U$. This should give measure to $B$ equal to that of the relative density of $A$, and I am thinking the measure is preserved by the $U^i_j$ maps. If all of this is right, then things are simpler than FK imagined, because we have that $U^i_j$ commutes with $U^k_l$ for $i\\neq k$, which should allow for the use of Hindman in all places were before Carlson was used. In particular, it allows for the use of Folkman in any finitary reduction (rather than Graham-Rothschild). And, in the case of Sperner’s Lemma, well, that becomes a simple consequence of the pigeonhole principle. (Because it’s just recurrence for a single IP system.) To be a bit more precise, in the $k=3$ case, I want to prove that there is some $\\alpha\\subset [n]$ such that $\\mu(B\\cap \\prod_{i\\in \\alpha} U^i_1 B\\cap \\prod_{i\\in \\alpha} U^i_2 B)>0$ and conclude DHJ (3). The point is you get to IP systems much more easily with the extra commutativity assumption. Sorry for the ergodic wording. Really at this state I am just hoping someone will check to see if everything I am saying is wrong for some simple reason I have overlooked. If not, we can write up a trivial proof of Sperner and a simplified version of $k=3$ and go from there."},{"username":"randall","timestamp":"2009-02-17T04:10:00.000Z","contents":"Well it seems that was wrong…the maps don’t take lines to lines. Can anyone else think of a way to preserve some of the symmetry that Terry has gotten in the early stages?"},{"username":"jozsef","timestamp":"2009-02-17T05:08:00.000Z","contents":"Folkman’s theorem Back in 341 and in some following comments I tried to popularize Folkman’s theorem as a statement relevant to our project. I didn’t find a meaningful density version yet, however Tim pointed out that statements like Ben Green’s removal lemma for linear equations might work. It would be something like this; If the number of IP_d-sets is much less than what isexpected from the density, then removing a few elements one can destroy all IP_d sets. In the next post I will say a few more words about the set variant of Folkman’s theorem, however I’m not sure that this 500 blog series is the best forum for that."},{"username":"jozsef","timestamp":"2009-02-17T05:41:00.000Z","contents":"Folkman’s theorem on second thought I decided not to talk about Folkman’s theorem here, since this blog is about possible proof strategies for DHJ. I will try to find another forum to discuss some questions on Folkman’s theorem."},{"username":"terence-tao","timestamp":"2009-02-17T23:27:00.000Z","contents":"536. Jozsef: I guess this is the best forum we have right now (we tried splitting up into more threads earlier, but found that all but one of them would die out quickly). I can always archive the discussion at the wiki to be revived at some later point. I wanted to point out two thoughts here. Firstly, I think I have a Fourier-analytic proof of DHJ(2.6) avoiding Ramsey theory. Firstly, observe that the usual proof of Sperner shows that the set of wildcard-lengths r of the combinatorial lines in a dense subset of %{}[2]^n% contains a difference set of the form A-A where A is a dense subset of %{}[\\sqrt{n}]%. Using the deduction of DHJ(2.5) from Sperner, we see that the set of wildcard lengths of combinatorial lines whose first two points lie in a dense subset of %{}[3]^n% also contains a similar difference set. Similarly for permutations. So it comes down to the claim that for three dense sets A, B, C of %{}[\\sqrt{n}]%, that A-A, B-B, C-C have a non-trivial intersection outside of zero. This can be proven either by the triangle removal lemma (try it!) or by the arithmetic regularity lemma of Green, finding a Bohr set on which A, B, C are dense and pseudorandom. There may also be a more direct proof of this. Secondly, I think we may have a shot at a combinatorial proof of Moser(3) – that any dense subset A of %{}[3]^n% contains a _geometric_ line rather than a combinatorial line. This is intermediate between DHJ(3) and Roth, but currently has no combinatorial proof. Here is a sketch of an idea: we use extreme localisation and look at a random subcube %{}[3]^m%. Let B be the portion of A on the corners %\\{1,3\\}^m% of the subcube, and let C be the portion of A which is distance one away (in Hamming metric) from the centre %2^m% of this subcube. The point is that there are a lot of potential lines connecting one point of C with two nearly opposing points of B. I haven’t done it yet, but it looks like the number of such lines has a nice representation in terms of the low Fourier coefficients of A, and should therefore tell us that in order to be free of geometric lines, A has to have large influence. I am not sure where to take this next, but possibly by boosting this fact with the Ramsey theory tricks we can get some sort of contradiction."},{"username":"gil","timestamp":"2009-02-18T01:04:00.000Z","contents":"Here is a nice example I heard from Muli Safra. (It’s origin is in the problem of testing monotonicity of Boolean functions.) Consider a Boolean function %f(x_1,x_2,\\dots,x_n)=g(x_1,x_2,\\dots,x_m)% where %m =o(\\sqrt n )% and %g% is a random Boolean function. When you consider strings x and y that correspond to two random sets S and T so that S is a subset of T with probability close to one f(x) = f(y) because almost surely the variables where S and T differs miss the first m variables. We can have a similar example for an alphabet of three letters."},{"username":"ryan-odonnell","timestamp":"2009-02-18T01:06:00.000Z","contents":"DHJ(2.6) Hi Terry, re your #536: great! this is just what I was going for in #529 & #530… But one aspect of the deduction I didn’t quite get: in my #530, you don’t have the differences from all pairs in %V%, just the ones where you have an “edge”. So do you produce your set %A% by filtering %V% somehow? Or perhaps I’m missing some simple deduction from the proof of Sperner."},{"username":"gilgil-kalai","timestamp":"2009-02-18T01:20:00.000Z","contents":"Fourier of line avoiding sets. Thanks a lot for the details, Ryan, it is very interesting. I think that using Maple **is** the way a real mathematician will go about it! As for my vague (related) suggestions: There is a nice Fourier expression of subsets of %\\{0,1\\}^n% without a combinatorial line with one wild cards. But already when you assume there is no lines with one and two wildcadrs its get messy. (In particular these scare expressions %\\hat f(S) \\hat f(R)%.) Maybe there would be a nice expression for sets in %\\{0,1,2\\}^n% without a combinatorial line with one wildcard. This can be nice. Another little (perhaps unwise) question regarding avoiding special configurations of “small distance” vectors. Suppose you look at subsets of %\\{0,1\\}^n% without two distinct sets S and T so that S\\T has precisely twice as many elements as T\\S. Does this implies that the size of the family is %o(2^n)%? (Again you can take a slice.) The problem about Sperner theorem which is mentioned in the the last paragraph was completely reolved by Imre Leader and Eoin Long, their paper Tilted Sperner families [http://front.math.ucdavis.edu/1101.4151](http://front.math.ucdavis.edu/1101.4151) contains also related results and conjectures."},{"username":"gil-kalai","timestamp":"2011-04-10T12:24:00.000Z","contents":"The problem about Sperner theorem which is mentioned in the the last paragraph was completely reolved by Imre Leader and Eoin Long, their paper Tilted Sperner families [http://front.math.ucdavis.edu/1101.4151](http://front.math.ucdavis.edu/1101.4151) contains also related results and conjectures."},{"username":"ryan-odonnell","timestamp":"2009-02-18T01:24:00.000Z","contents":"Moser(3). For this problem I think it might be helpful to go all the way back to Tim.#70 and think about “strong obstructions to uniformity” of the following form: dense sets %\\mathcal{A}% for which %\\{y : \\exists x,z \\in \\mathcal{A} \\text{s.t. } (x,y,z) \\text{ is a geom. line}\\}% is not almost everything."},{"username":"terence-tao","timestamp":"2009-02-18T02:06:00.000Z","contents":"540. Re: Sperner: if A is a dense subset of %[2]^n%, then a random chain in this set is going to hit A in a dense subset of its equator (which is the middle %O(\\sqrt{n})% of the chain, which has length n). If A hits this chain in the %i^{th}% and %j^{th}% positions, then we get a combinatorial line with j-i wildcards. This is why the set of r arising from lines in A contain a difference set of a dense subset of %[\\sqrt{n}]%. Incidentally, I withdraw my claim that the joint intersection of A-A, B-B, C-C can be established from triangle removal – but the arithmetic regularity lemma argument should still work. Finally, for Moser, I agree with Ryan’s 538 – except that I think one should restrict y to be very close to the centre %2^n% of the cube %\\{1,2,3\\}^n%, otherwise y is only going to interact with a small fraction of the points of the cube and I don’t think this will be easily detectable by global obstructions. I still haven’t worked out what goes on when y is just a single digit away from %2^n% but I do believe it will have a nice Fourier-analytic interpretation."},{"username":"gowers","timestamp":"2009-02-18T02:07:00.000Z","contents":"Metacomment. I’d just like to chip in here and say that I’ve been devoting most of my polymath1 energies to developing the associated wiki. If anyone else has any thoughts that are in a sufficiently organized state to make wiki articles (they don’t have to be perfect of course) then it would be great. It will make it much easier to keep track of what we know, what we’ve asked, what we think we might be able to prove and roughly how, etc. So far, I think Terry and I are the only two contributors. The other thing I wanted to say is that I get the sense that other people are doing a certain amount of thinking away from the blog. I’ve got to the point where I want to do this too (and report back regularly if I come up with anything) but I don’t want to do it if others are in fact not doing so. Anybody care to tell me? It somehow feels as though we’ve got to the stage where a certain amount of private thought is needed to push things further, but because I formulated rule 6 I have felt obliged to stick to it. (Gil, I know you are relaxed about all this.)"},{"username":"randall","timestamp":"2009-02-18T02:20:00.000Z","contents":"Cave man methods: Here’s a curious low-tech trick to get difference sets to intersect: suppose you have sets A, B, C of density d. Let m>1/d and use ramsey’s theorem to choose n so that for any 8-coloring of the 2-member sets of an n element set, you get an m element set whose 2-member sets are of one color. Now color {x,y}, x1/d, the x_i+A cannot all be disjoint, hence x_j-x_i is in A-A for some, hence all, i,j and similarly for B-B, C-C."},{"username":"randall","timestamp":"2009-02-18T02:24:00.000Z","contents":"Cave man again: I see stuff in what are interpretted as comment brackets gets left out! Let’s try this again. Here’s a curious low-tech trick to get difference sets to intersect: suppose you have sets A, B, C of density d. Let m>1/d and use ramsey’s theorem to choose n so that for any 8-coloring of the 2-member sets of an n element set, you get an m element set whose 2-member sets are of one color. Now color {x,y}, x less than y, according to which of A-A, B-B, C-C y-x is in. Pass to an m member set {x_1,…,x_m} whose 2-member subsets are of one color for this coloring. Since m is greater than 1/d, the x_i+A cannot all be disjoint, hence x_j-x_i is in A-A for some, hence all, i,j and similarly for B-B, C-C."},{"username":"ryan-odonnell","timestamp":"2009-02-18T02:28:00.000Z","contents":"Metacomment. I too feel like trying out longer calculations off the blog from time to time. Tim, why don’t you go ahead and try the ones you’re thinking of? Perhaps we can just report here how the calculations go…"},{"username":"terence-tao","timestamp":"2009-02-18T11:11:00.000Z","contents":"Metacomment. I think there may be some tension here between the objective of solving the problem by any means necessary, and the objective of trying to see whether a maximally collaborative approach to problem-solving can work. I would favour a relaxed approach at this point; it seems that we are already getting enough benefit from the collaborative forum here that we don’t need to be utterly purist about keeping it that way. And I think things are moving to the point where, traditionally, one of us would simply sit down for an hour or two and work out a lot of details at once."},{"username":"terence-tao","timestamp":"2009-02-18T11:19:00.000Z","contents":"Moser(3) Hmm, there do seem to be some obstructions to uniformity here that are annoying. For instance, if m is an odd number comparable to 0.1 n, and A is the set of points in {1,3}^n which have an even number of 3’s in the first m positions, and B is the set of points formed by taking 2^n and changing one of the last n-m positions to a 1 or a 3, then there are no geometric lines connecting A, B, and A despite A having density 0.5 and B having density 0.9 in {1,3}^n and the radius 1 Hamming ball with centre 2^n respectively. On the other hand, A has a lot of coordinates with low influence wrt swapping 1 and 3; if 1 and 3 were completely interchangeable then Moser(3) would collapse to DHJ(2), so perhaps there is something to exploit here. Randall: I’m beginning to realise that if one wants to hold on to permutation symmetry on the cube, then one can’t use Ramsey theorems such as Graham-Rothschild or Carlson, as the symmetry is broken when one passes to a combinatorial subspace (the wildcard lengths are unequal). So it may actually not be a good tradeoff."},{"username":"gil","timestamp":"2009-02-18T13:15:00.000Z","contents":"Metacomment. I also agree that we need some off-line calculations, and we can keep the spirit of point 6 (asking not to go away for weeks to study some avenue) by participants trying to document and describe what they try to do and what they do (even if unsuccessful) in short time intervals."},{"username":"gowers","timestamp":"2009-02-18T15:00:00.000Z","contents":"Metacomment. I think we all basically agree about off-line calculations. At some point fairly soon I will try to do some, but I will be careful to follow Gil’s suggestion. I will write a comment in advance to say what I am going to do, and I will report back frequently and in detail on what results from the calculations. (I may give less detail about the things that don’t work, but even there I will try to give enough detail to save anyone else from needing to go off and repeat my failures.) Briefly, the kinds of things I’d like to do are to write out a rigorous proof that a quasirandom set (in a sense that I have defined on the wiki) must contain many combinatorial lines, and to try to deduce something from a set’s not being quasirandom. I’d also like to play around with Fourier coefficients on %{}[2]^n% with equal-slices measure and try to prove that the only obstruction to Sperner in that measure is lots of low-influence variables. I won’t get down to either of these today, if anyone has any remarks that will potentially stop me or change how I go about it."},{"username":"gowers","timestamp":"2009-02-18T18:47:00.000Z","contents":"Corners(1,3) Over at the wiki I have written about a special case of the corners problem that is like DHJ(1,3). It’s corners when your set A is of the form %\\{(x,y):x\\in U,y\\in V,x+y\\in W\\}.% Can anyone see an elementary proof that a dense set of this kind contains a corner? I can prove it fairly easily if I’m allowed to use Szemerédi’s theorem, so at least the transition from one to two dimensions is elementary, but I’d rather a fully elementary proof. My account of the problem (and of DHJ(1,3)) can be found [here](http://michaelnielsen.org/polymath1/index.php?title=DHJ%281%2C3%29)."},{"username":"terence-tao","timestamp":"2009-02-18T21:49:00.000Z","contents":"Metacomment. I should point out that over at the 700 thread, things have already advanced to the point where we have already had several people run programs to get the latest upper and lower bounds (and writing programs is not something that one can really do collaboratively, at least not with the structures in place right now)."},{"username":"gowers","timestamp":"2009-02-19T04:12:00.000Z","contents":"Fourier analysis on equal-slices %{}[2]^n% This isn’t the calculation I was talking about, as this one was possible to do in my head. I don’t yet know where we could go with this, but it felt like a good idea to think about how Walsh functions behave with respect to equal-slices measure. (I used to call this slices-equal measure, which I feel is slightly more correct, but in the end I have had to concede that “equal-slices” trips off the tongue much better.) In particular, I wondered about orthogonality. The calculation turns out to be rather pretty. Let’s write %w_A% for the Walsh function associated with the set %A%. That is, %w_A(x)% is 1 if x has an even number of 1s in A, and -1 if it has an odd number of 1s in A. Then %w_A(x)w_B(x)=w_{A\\Delta B}(x)%. Therefore, the inner product %\\mathbb{E}_xw_A(x)w_B(x)% is equal to %\\mathbb{E}_xw _{A\\Delta B}(x)%, so what we really care about is the expectation of individual Walsh functions. In the case of uniform measure, these expectations are all zero except when A is the empty set, and this implies the orthogonality of the Walsh functions. It turns out to be quite easy to think about the value of %\\mathbb{E}_xw_A(x)% when expectations are with respect to equal-slices measure. This is because we can rewrite the expectation over %{}[2]^n% as the expectation over all permutations %\\pi% of %{}[n]% and all integers %m% between %{}0% and %n% (in both cases chosen uniformly) of the sequence %x% such that %x_{\\pi(i)}% is 1 if and only if %i\\leq m%. (In set terms, we randomly permute %{}[n]% and then pick a random initial segment.) Suppose that %A% has cardinality %k%. Then the calculation we need to do is this: what is the probability that if you choose a random subset %B\\subset[n]% of size %k% and take a random %m\\in\\{0,1,\\dots,n\\}%, then %B% will have an even number of elements less than or equal to %m%? (If the probability is %p%, then %\\mathbb{E}_xw_A(x)% will be %2p-1% for every set %A% of size %k%.) Now if %k% is odd and %n% is even, then we can replace %B% by %n+1-B% and we can replace %m% by %n-m% and the number of elements of %n+1-B% that are at most %n-m% is the number of elements of %B% that are greater than %m%. Also, %B% cannot equal %n+1-B%. Therefore, there is a bijection between pairs %(B,m)% that give an odd intersection and pairs %(B,m)% that give an even intersection. So the probability is %1/2% in this case, and we find, as we want, that %\\mathbb{E}_xw_A(x)=0%. From this we find that %w_A% is orthogonal to %w_B% whenever %A% and %B% have different parities (assuming that %n% is even, which we may as well). If %A% has even cardinality, things do not work so well. Consider, for example, the case where %A% consists of two elements. Here we choose a random %m% and a random set %\\{x,y\\}% and ask for the probability that an even number of x and y are at most %m%. For fixed %m=cn% the probability is approximately %c^2+(1-c)^2%, which integrates to %2/3%. So we get %\\mathbb{E}_xw_A(x)=4/3-1=1/3.% Another way to think of this is that x and y divide up %{}[n]% into three subintervals, each of expected size %n/3%, so the probability that %m% lands in the middle subinterval is on average %1/3%. This second way is quite useful, as it shows that for general even %k=2s% we will get approximately %(s+1)/2s% for the probability of an even intersection and therefore %1/s% for %\\mathbb{E}_xw_A(x)%. So we end up not with an orthonormal basis but with something fairly nice: a basis that splits into two sets %B_0% and %B_1% of vectors, one for even parity sets and one for odd parity sets, such that every vector in %B_0% is orthogonal to every vector in %B_1%, and vectors %w_A% and %w_B% in the same set are nearly orthogonal except if the symmetric difference of %A% and %B% is small."},{"username":"gowers","timestamp":"2009-02-19T04:59:00.000Z","contents":"Fourier analysis and Sperner obstructions. Now I want to try to do a similar calculation to see if we can say anything about obstructions to Sperner in the equal-slices measure. The model will be this. Let %f% be a bounded function defined on %{}[2]^n%. Now we pick a random pair of sets %(U,V)% by first picking a random permutation %\\pi% of %{}[n]% and then picking two random initial segments of the permuted set. Then we take the expectation of %f(U)f(V)%. I am interested in what we can say if this expectation is not small. The dream would be to show that almost all the influence on %f% comes from just a small set. Clearly the calculation we want to do is this: we pick two sets %A% and %B% and work out the expectation of %w_A(U)w_B(V)% over all pairs %(U,V)% chosen according to the distribution above. I think it is fairly clear that this expectation will be small if either of %A% or %B% is large, by the arguments of the previous post. This is promising, but I don’t know that I can see, without going off and calculating, whether it will be small enough to give us something useful (particularly as I’m not quite sure what substitute we will have, if any, for Parseval’s identity). If %A% and %B% are both small, then there is quite a bit of dependence between %w_A(U)% and %w_B(V)%, or so it seems. For example, if %A% has size 2, %B% has size %4%, %A\\subset B%, and for a particular permutation the probability that an initial segment contains an even number of elements of %A% is close to 1, then that permutation bunches %A% up to one of the two ends of the interval, so %B% behaves like a set of size 2 and the probability of getting an even intersection with %B% is roughly 2/3. This is all suggesting to me that lack of uniformity in Sperner implies that a few variables have almost all the influence. If any influence experts think they can see where these thoughts would be likely to lead, then I’d be interested to hear about it. Otherwise, this is one of the private calculations I’d like to do. If it worked, then we would have some modest evidence for what I very much hope will turn out to be the case: that with equal-slices measure the only obstructions to uniformity for DHJ(3) come from sets of complexity 1 (which are defined in [the discussion of DHJ(1,3) on this wiki page](http://michaelnielsen.org/polymath1/index.php?title=Density_Hales-Jewett_theorem). This would be because of the fluke that Sperner-type obstructions happened to correlate with sets of complexity 1."},{"username":"ryan-odonnell","timestamp":"2009-02-19T11:25:00.000Z","contents":"Equal slices measure / Sperner. Hi Tim. I’m still not 100% on board for equal-slices measure <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> but… 1\\. If I’m not mistaken, it’s the same as first choosing %p \\in [0,1]% uniformly, and then choosing from the %p%-biased product distribution on %\\{0,1\\}^n%. Thinking of it this way might help with Fourier methods, since they tend to mate nicely with product distributions. 2\\. When you say “obstructions to …”, do you mean obstructions to having probability close to %\\delta% for the event %U,V \\in \\mathcal{A}% or for probability close to %\\delta^2%? The former may be difficult, since any monotone %\\mathcal{A}% will lead to probability close to %\\delta%, and there are an awful lot of monotone functions. If you are interested in the standard product measure (as opposed to equal-slices) then the Fourier calculations from #477 and company seem like they could lead to a pretty good understanding of density-Sperner for a small Poisson number of wildcards. But I’m not currently sure if understanding density-Sperner really helps at all for understanding HJ or even Moser. (I may say more on why I feel this way later; I’m feeling a bit gloomy about Fourier approaches right now…)"},{"username":"gil","timestamp":"2009-02-19T13:36:00.000Z","contents":"Yes, integrating over %\\mu_p% is a nice way to think about equal-slices probability. It would be interesting to come up with a good orthonormal basis of functions w.r.t. the equal slices probability. (Indeed, we have such functions for every %\\mu_p%.) I would be very interested to see a good understanding of density Sperner with small number of Poisson number of wild-cards. (I tried to do some computation and I do not see how to proceed right now even with Sperner.)"},{"username":"gowers","timestamp":"2009-02-19T16:35:00.000Z","contents":"Sperner obstructions Ryan, that is indeed a nice way of thinking about equal-slices measure, and thanks for reminding me about monotone functions, which, despite a number of your previous comments, had slipped out of my mind. I must try to think what the most general phenomenon is, of which monotone functions are a particularly nice manifestation. (For example, the intersection of an up-set and a down-set is likely to be highly non-quasirandom for Sperner.) Better still, it would be good to generalize that to subsets of %{}[3]^n,% where I don’t find it completely obvious what one should mean by a “monotone” set. (Of course, one can come up with definitions, but I’d like a definition with a proper justification.) In general, however, it seems to me that anything even remotely monotone-like in %{}[3]^n% ought to correlate with a set of complexity 1."},{"username":"gowers","timestamp":"2009-02-19T18:38:00.000Z","contents":"Equal-slices orthonormal basis Ryan, I completely understand your not being 100% convinced by equal slices probability. I myself am in the position of (i) very much wanting it to do something good and (ii) feeling that it would be a bit of a miracle if it did. I’ve been thinking about Gil’s question of finding a nice orthonormal basis of functions with respect to equal-slices density, and have just come up with a guess. In the true polymath spirit I will present the guess and only after that see whether it actually works. I’ve got a good feeling about it though (because I scientifically inducted from 2 to the general case). Here are my initial desiderata. I would like a set of functions %w_A% (sort of pseudo-Walsh functions), one for each subset %A\\subset[n]%, such that, regarding the elements of %{}[2]^n% as subsets of %{}[n]%, the value of %w_A(B)% depends only on the cardinality of %A\\cap B%. I would also like them to be orthogonal in the equal-slices measure. (I think this could be enough to nail the functions completely, but I’m just going to guess a solution. If it turns out to be rubbish then I’ll go away and try to solve some simultaneous equations.) There is more or less no choice about what to do with singletons: up to a constant multiple, I’ve basically got to define %w_i(B)% to be %1% if %i\\notin B% and %{}0% otherwise. Since the equal-slices weight of %B% is equal to that of its complement, this function averages zero in the equal-slices measure, and is therefore orthogonal to the constant function 1 (which is what we have to choose for %w_A% when %A% is empty). Things get more complicated with pairs. As I argued in an earlier comment, if you just choose the usual Walsh function %w_A(B)=(-1)^{|A\\cap B|}% then with equal-slices measure you get that the average of the function is %1/3% (when %|A|=2%). And that was basically because if you randomly choose two points in %{}[n]% and then choose a random %m% between %{}0% and %n%, then the probability that exactly one of your two random points is less than %m% is approximately %1/3% instead of %1/2%. So the nicest thing to do seems to be this (always assuming that %|A|=2% for this discussion): we define %w_A(B)% to be 1 if %|A\\cap B|% is 0 or 2 and %-2% if %|A\\cap B|=1.% Now let’s jump straight to the guess. I won’t bother normalizing anything so I’m aiming for orthogonality rather than orthonormality. If %|A|=k% then I’d like %w_A(B)% to be %\\binom kj (-1)^{|A\\cap B|}.% OK, now let me begin the probably painful process of checking this guess. FIrst of all, this set of functions is no longer closed under pointwise multiplication, but that was always going to be too much to ask, since otherwise the functions %w_i% would have determined everything and we’d have ended up with the usual Walsh functions. This has the annoying consequence that it’s not enough just to check that %w_A% has average zero when %A% is non-empty. And I have to admit that at this stage I’m just hoping for a big slice of luck (no cheesy pun intended). What, then, is the product %w_Aw_{A'}%? Hmm, I think it’s going to be beyond me to do this straight up on to the computer, so now I really am going to do an offline computation. I’ve got various things coming up so it may be an hour or two before I can report back. (Also, I want to check that the expectation of %w_A% really is zero. I’m satisfied that it is approximately zero, but it would be much nicer to have exact orthogonality.)"},{"username":"gowers","timestamp":"2009-02-19T20:52:00.000Z","contents":"Equal slices and orthonormality Oh dear, just noticed, before getting round to any calculation, that this idea is dead before it’s even got going. If for each %A,B% we want %w_A(B)% to depend on %|A\\cap B|% only, then as I pointed out above we need %w_A% to be the usual Walsh function when %A% is a singleton. But for two such functions to be orthogonal we then need their pointwise product to average zero as well. But their pointwise product is the usual Walsh function associated with a set of size 2, and the whole motivation for this line of enquiry was that those Walsh functions did _not_ average zero in the equal-slices measure. This shows that my two desiderata above are inconsistent. In my next comment I’ll give a suggestion for what to do."},{"username":"gowers","timestamp":"2009-02-19T22:00:00.000Z","contents":"Equal slices and orthonormality It seems to me that Ryan’s way of looking at equal-slices density is too good to ignore, so instead of asking what the most natural orthonormal basis is with respect to equal-slices density, let us ask instead what the most natural thing to do is if (i) we want orthonormality and (ii) we want to think of equal-slices density as the average of the weighted densitites on the cube. As Gil points out, for each fixed %p\\in[0,1]% you have a natural orthonormal basis. Indeed, if we choose a random point of %\\{0,1\\}^n% by letting the coordinates be independent Bernoulli random variables with probability %p% of being 1 and %1-p% of being 0, then the function that is %p-1% when %x_1=1% and %p% when %x_1=0% has mean 0\\. The expectation of the square of this function is %p(1-p)%, so if we divide by %p^{1/2}(1-p)^{1/2}% then we get a function %w_{1,p}% of mean 0 and variance 1\\. We can define %w_{i,p}% similarly for each %i%. Now for a general set %A% define %w_{A,p}% to be %\\prod_{i\\in A}w_{i,p}.% Then the independence of the random variables %w_{i,p}% implies that %\\mathbb{E}_Bw_{A,p}(B)=0% if %A% is non-empty (always assuming that the elements of %B% are chosen randomly and independently with probability %p%). Therefore, the %w_{A,p}% are orthogonal. If %p=1/2% then we get the usual Walsh functions. I think that there is just one natural thing to do at this point. For each %p\\in[0,1]% let us write %\\mu_pf% for the expectation of %f(X_1,\\dots,X_n)%, where each %X_i% is Bernoulli with probability %p%. Then the equal-slices measure of %f% is %\\mathbb{E}_p\\mu_p(f)%. How about inner products? Well, %\\mathbb{E}_xf(x)g(x)=\\mathbb{E}_p\\mu_p(fg)%. If we want to understand this in a Fourier way, it seems crazy (with the benefit of hindsight) to try to find a single orthonormal basis that will do the job. Instead, for each %p% one should expand %f% and %g% in terms of the weighted Walsh basis %(w_{A,p})% and then take the average. For each individual %p% there is a nice Parseval identity, since the %w_{A,p}% are orthonormal. To spell it out, if we write %\\hat{f}_p(A)% for %\\mu_p(fw_{A,p})%, then %\\mathbb{E}_xf(x)g(x)=\\mathbb{E}_p\\sum_A\\hat{f}_p(A)\\hat{g}_p(A).% This looks to me like a nice usable Parseval identity, even if it doesn’t have the form I was orginally looking for. So we’re taking our Fourier coefficients to belong to %C[0,1]% rather than to %\\mathbb{R}%."},{"username":"gowers","timestamp":"2009-02-19T22:48:00.000Z","contents":"Sperner obstructions. The next thing I’d like is a good measure on pairs %A\\subset B%. With the old way of thinking about equal-slices density this was easy: we took two initial segments of a randomly permuted %{}[n]%. But can we think of it in a Ryan-like way? Here’s a natural suggestion. First we randomly pick a pair %(p,q)% of real numbers with %0\\leq p\\leq q\\leq 1%. Next, we choose the set %B% by picking each element independently and randomly with probability %q%. And then we pick the set %A\\subset B% by picking each element of %B% independently and randomly with probability %p/q%. Note that the elements of %A% end up being chosen independently with probability %p%, so the marginal distributions of %A% and %B% are sensible. The next task is to try to sort out what we mean by %\\mathbb{E}_{A\\subset B}f(A)f(B)%. (We want to say that %f% is non-uniform if this expectation is not small.) Wait, that’s not quite what I meant to say. It’s clear what it means, since I’ve just specified the distribution on pairs %(A,B)%. What is not quite so clear is how to express it in terms of the vector-valued Fourier coefficients. I think I need a new comment for that."},{"username":"gowers","timestamp":"2009-02-20T00:55:00.000Z","contents":"Sperner-related calculations. Finally I’ve found something that I definitely couldn’t do straight up. But with a bit of paper and a couple of false starts I think I’ve got it down, or at least have made some calculational progress in that direction. The aim is to express the quantity %\\mathbb{E}_{U\\subset V}f(U)f(V)% (defined in the previous comment—it is of course essential that we are talking about an equal-slices-related joint distribution here) in terms of the vector-valued Fourier coefficients of %f%. Also, I’ve changed A and B to U and V because I want to use A and B to label some Walsh functions. For this calculation it will be more convenient to think in terms of Bernoulli random variables. Let %Y_1,\\dots,Y_n% be Bernoulli with mean %q%, let %Z_1,\\dots,Z_n% be Bernoulli with mean %p/q%, and let %X_i=Y_iZ_i%. All the %Y_i% and %Z_i% are of course independent, so the %X_i% are independent Bernoulli random variables with mean %p%. Later on we will average over %0\\leq p\\leq q\\leq 1% but for now let us regard them as fixed and see what happens. Later we will take %U% to be the set of %i% such that %X_i=1% and %V% to be the set of %i% such that %Y_i=1%. (What this will ultimately be doing is investigating the number of pairs %U\\subset V% in %\\mathcal{A}% with %U% of size roughly %pn% and %V% of size roughly %qn%.) Now we can expand %f% in terms of the functions %w_{A,p}% and also in terms of the functions %w_{A,q}%. If we do so, then our main task is to try to understand the quantity %\\mathbb{E}w_{A,p}(X_1,\\dots,X_n)w_{B,q}(Y_1,\\dots,Y_n),% which we are rewriting as %\\mathbb{E}w_{A,p}(Y_1Z_1,\\dots,Y_nZ_n)w_{B,q}(Y_1,\\dots,Y_n).% Here we find that independence helps us a lot. Let us begin by conditioning on %Y_1,\\dots,Y_n%. Since this fixes %w_{B,q}(Y_1,\\dots,Y_n)% we are now interested in %\\mathbb{E}w_{A,p}(Y_1Z_1,\\dots,Y_nZ_n).% For every %p% let us write %\\phi_p% for the function that takes %{}0% to %-(1-p)^{-1/2}p^{1/2}% and %{}1% to %p^{-1/2}(1-p)^{1/2}%. Then %w_{i,p}(x)=\\phi_p(x_i)% and %w_{A,p}(x)=\\prod_{i\\in A}\\phi_p(x_i).% Therefore, %\\mathbb{E}w_{A,p}(Y_1Z_1,\\dots,Y_nZ_n)% is equal to %\\mathbb{E}\\prod_{i\\in A}\\phi_p(Y_iZ_i)% which by independence is equal to %\\prod_{i\\in A}\\mathbb{E}\\phi_p(Y_iZ_i)%. I have to feed my son now so will continue with this calculation later."},{"username":"gowers","timestamp":"2009-02-20T01:30:00.000Z","contents":"Sperner-related calculations continued. While walking to a local grocer’s I realized that conditioning on the %Y_i% was not such a good idea. Instead, we should note that %w_{A,p}(Y_1Z_1,\\dots,Y_nZ_n)w_{B,q}(Y_1,\\dots,Y_n)% splits up as a product of three products, which I will write separately. They are %\\prod_{i\\in A\\setminus B}\\phi_p(Y_iZ_i)%, %\\prod_{i\\in B\\setminus A}\\phi_q(Y_i)% and %\\prod_{i\\in A\\cap B}\\phi_p(Y_iZ_i)\\phi_q(Y_i)%. Therefore, because of independence, the expectation that we’re interested in (or at least I am) also splits up as a product of three products, namely %\\prod_{i\\in A\\setminus B}\\mathbb{E}\\phi_p(Y_iZ_i)%, %\\prod_{i\\in B\\setminus A}\\mathbb{E}\\phi_q(Y_i)% and %\\prod_{i\\in A\\cap B}\\phi_p(Y_iZ_i)\\mathbb{E}\\phi_q(Y_i).% Thus our calculation becomes very simple, since each of these expectations in the products is a number that does not depend on %i%. Must go again."},{"username":"gowers","timestamp":"2009-02-20T02:59:00.000Z","contents":"Sperner-related calculations continued. There are three very finite calculations to do next. 1\\. Since %Y_iZ_i=X_i% is Bernoulli with mean %p%, %\\mathbb{E}\\phi_p(Y_iZ_i)=0%, by the way we defined %\\phi_p%. Wow, I wasn’t expecting that. 2\\. Similarly, %\\mathbb{E}\\phi_q(Y_i)=0%. 3\\. As for %\\mathbb{E}\\phi_p(Y_iZ_i)\\phi_q(Y_i)%, it works out to be some number %\\lambda_{p,q}%. I’ll calculate it later as I have to go. So the whole thing collapses to 0 unless %A=B% and then we get %\\lambda_{p,q}^{|A|}%. That looks pretty nice to me."},{"username":"gowers","timestamp":"2009-02-20T06:08:00.000Z","contents":"Sperner-related calculations continued. Right, %(Y_iZ_i,Y_i)% is %(0,0)% with probability %1-q%, %(0,1)% with probability %q-p% and %(1,1)% with probability %p%. Recall that %\\phi_p(0)=(1-p)^{1/2}p^{-1/2}% and %\\phi_p(1)=-p^{1/2}(1-p)^{-1/2}%, and similarly for %\\phi_q%. (I’ve changed these to minus what they were before, but this isn’t important.) To tidy things up a bit I’m going to make the substitution %p=\\sin^2\\alpha% and %q=\\sin^2\\beta%. So %\\phi_p(0)=\\cot\\alpha%, %\\phi_p(1)=\\tan\\alpha%, and similarly for %q% and %\\beta%. Therefore, %\\phi_p(Y_iZ_i)\\phi_q(Y_i)% is %\\cot\\alpha\\cot\\beta% with probability %\\cos^2\\beta%, %\\cot\\alpha\\tan\\beta% with probability %\\sin^2\\beta-\\sin^2\\alpha% and %\\tan\\alpha\\tan\\beta% with probability %\\sin^2\\alpha%. Therefore, the expectation is %\\cos\\alpha\\cos^3\\beta/\\sin\\alpha\\sin\\beta+\\dots% hmm, I’ve just tried it on a piece of paper and it doesn’t seem to simplify. That’s annoying, as I would very much like to get some idea of the average size of %\\lambda_{p,q}^{|A|}% as %p% and %q% vary."},{"username":"gowers","timestamp":"2009-02-20T06:43:00.000Z","contents":"Sperner-related calculations continued. Actually, I forgot the minus signs: %\\phi_p(1)% should have been %-\\tan\\alpha%. So we get %\\cot\\alpha\\cot\\beta\\cos^2\\beta-\\cot\\alpha\\tan\\beta(\\sin^2\\beta-\\sin^2\\alpha)+\\tan\\alpha\\tan\\beta\\sin^2\\alpha%. I’m going to have to do this a few times until I’m confident it’s right. But at the moment it looks as though %\\lambda_{p,q}% is sometimes bigger than 1, which I don’t like, but also as though there is a range where it is less than 1, which might mean that we were OK after some very modest localization. The reason I’d like %\\lambda_{p,q} is that it would then mean that the contribution to the sum from large %A% was fairly negligible, and I think we’d have a proof of at least some statement along the lines of incorrect number of %U\\subset V% implying large Fourier coefficient at some small %A%. And if that worked, it would be hard to resist the temptation to try something similar for DHJ(3), though the thought of it is rather daunting."},{"username":"jozsef","timestamp":"2009-02-20T08:28:00.000Z","contents":"Removal lemma for DHJ The main difficulty – at least for me – in applying a removal lemma to the set model (DHJ Version 4\\. in the Wiki) is the disjointness of sets as it makes the corresponding graph very sparse. Here I’ll try to relax the disjointness condition in a slightly different model. Instead of working inside of one dense cube showing that it contains a line, here we will consider now several cubes; The “original” cube %[3]^n%, n copies of %[3]^{n-1}% … %\\binom{n}{k}% copies of %[3]^{n-k}%, and so on. For any subset of [n] we have one cube. We can suppose that each cube is at least c-dense and line-free. For any pair of subsets of [n], say A and B there is a unique point assigned; it is in the cube over %[n]\\-(A\\cap B)% with 1-s in the %A-B% positions, 2-s in the %B-A% positions, and 0-s elsewhere. Every point represents a pair of subsets in [n]. This model allows us to state the following “corner-like” form of DHJ:  \nFor every %\\delta > 0 % there exists n such that every collection of pairs (A,B) of subsets of [n] of cardinality at least %\\delta 4^n% contains a “corner” %(A,B), (A \\cup D, B), (A, B \\cup D)%, where D is disjoint from A and B. (A, B, D are subsets of [n] )"},{"username":"ryan-odonnell","timestamp":"2009-02-20T11:56:00.000Z","contents":"Sperner. Tim, regarding your problem beginning in #553 and fixing %p% and %q% first… Is it correct that if we fix %p = 1/2 - \\epsilon% and %q = 1/2 + \\epsilon%, then we recover the scenario from #476? One reason I like to have %p% very close to %q% (“extreme localization”) is that the %p%-biased and %q%-biased measures are practically disjoint if %p% is even a little bit less than %q%. So then how a set %\\mathcal{A}% acts under the %p%-biased distribution has nothing to do with how it acts under the %q%-biased distribution. (Of course, this is only if you *fix* distant %p,q% in advance. If you pick them at random, some bets are off.)"},{"username":"ryan-odonnell","timestamp":"2009-02-20T12:00:00.000Z","contents":"Moser. Given that we feel a bit stuck on HJ, I feel tempted to think more about Moser instead. I mean, why not? — it’s a strictly easier problem. Is there an “easier” ergodic-theory proof of Density-Moser?"},{"username":"jozsef","timestamp":"2009-02-20T12:11:00.000Z","contents":"Removal lemma for DHJ (contd.) Now that a dense system is given, let’s check how can we use a removal lemma. Given a collection of pairs of subsets of [n], denoted by S, of cardinality %\\delta 4^n%. The family of pairs having the same symmetric difference set form a matching in this graph. By Ruzsa-Szemeredi there will be many edges from the same matching (symmetric difference) that are connected by another edge. More precisely, there will be at least %\\epsilon 8^n% quadruples A,B,C,D that (A,B), (C,D), and (A,D) are from S and %A\\Delta B = C\\Delta D%. Unfortunately it is not a combinatorial line in general. However it is a combinatorial line if %A\\cap B=C\\cup D=A\\cup D%, i.e. when the three pairs are representing points from the same cube."},{"username":"jozsef","timestamp":"2009-02-20T12:18:00.000Z","contents":"oops, here is a typo in the previous post; the correct statement is  \n%A\\cap B=C\\cap D=A\\cap D%"},{"username":"jozsef","timestamp":"2009-02-20T12:32:00.000Z","contents":"Moser Re:561\\. Ryan, I don’t think that there is a published ergodic proof for Moser’s problem and I think that Moser k=6 implies DHJ k=3\\. But I agree, there should be a simpler way to prove Moser k=3\\. Note that for any algebraic line L in %({\\Bbb Z}/3{\\Bbb Z})^n% one can find an x such that x+L is a geometric line."},{"username":"gil","timestamp":"2009-02-20T12:44:00.000Z","contents":"Dear Jozsef, how do we get DHJ for k=3 from Moser for k=6? (Exploring these type of reductions may also be a nice avenue of some potential.)"},{"username":"jozsef","timestamp":"2009-02-20T13:18:00.000Z","contents":"Dear Gil, We have to check, but my thought was that if you map %3^n% to %6^n% as 1 maps to 1 or 6, 2 maps to 2 or 5, and 3 maps to 3 or 4, then if DHJ k=3 was combinatorial line free then Moser k=6 is geometric line free. And if DHJ was dense, then Moser is dense too. I hope it’s right ."},{"username":"gowers","timestamp":"2009-02-20T14:08:00.000Z","contents":"Equal-slices measure. Ryan, my answer to your point in 560 is that it is indeed true that if you fix %p% and %q% that are some way apart, then the measures %\\mu_p% and %\\mu_q% are approximately disjointly supported. However, this is quite natural once you average over %p% and %q%. It’s a bit like saying that in order to understand Sperner we will look at pairs of layers and see what happens for those. Obviously, one cannot look at each pair in complete isolation if one wants to prove Sperner’s theorem (since the weight inside each layer might be less than 1/2, say) but it’s not so clear that one can’t get useful information about obstructions that way: for example, one might be able to understand when two layers don’t make the right contribution to the number of subset pairs, and one might then be able to put that together to understand when it happens after averaging over all pairs of layers. But whether or not you are convinced by this, your alternative way of thinking about equal-slices measure has been hugely useful!"},{"username":"gowers","timestamp":"2009-02-20T14:22:00.000Z","contents":"Sperner calculations continued. A quick remark re 558\\. I realized as I woke up this morning that %\\lambda_{p,q}% must be at most 1 by Cauchy-Schwarz because it’s the covariance of the random variables %\\phi_p(Y_iZ_i)% and %\\phi_q(Y_i)%, each of which has been carefully designed to have variance 1\\. (To put it another way, it follows easily from Cauchy-Schwarz.) So I think we’re in business here. I made some small slips in the calculations earlier, but I _think_ that we have the very nice expression %\\mathbb{E}_{p\\leq q}\\sum_{A\\subset[n]}\\lambda_{p,q}^{|A|}\\hat{f}_p(A)\\hat{f}_q(A)% for the total weight %\\mathbb{E}_{U\\subset V}f(U)f(V)%, where each %\\lambda_{p.q}% has modulus less than 1, with equality if and only if %p=q%. From this one can informally read off a few quite interesting things. For instance, the contribution from a set %A% of large cardinality will be tiny if %p% and %q% are not very close to each other, but that happens only rarely (Ryan won’t like this bit), so the total contribution from large %A% appears to be small. This fits in nicely with one’s intuition that by Kruskal-Katona the upper shadow of a layer of positive density will be huge if you go up a lot of layers, unless there is some simple explanation such as all sets containing 1\\. Perhaps at this point I’ll try to tidy up these calculations and put them on the wiki."},{"username":"gowers","timestamp":"2009-02-20T17:17:00.000Z","contents":"Fourier and DHJ(3). Before I go off and do actual calculations, I want to see how much I can easily generalize of the Sperner calculations to DHJ(3). The first step is to come up with a Ryan-style model for equal-slices measure. For points it is easy enough: first, pick %p+q+r=1% and for each coordinate %x_i% let it be 1 with probability p, 2 with probability 1 and 3 with probability r. Then average the resulting density over all such p,q,r. Now for %p+q+r+s=1% I’d like to pick a random combinatorial line in such a way that its 1-set is chosen according to (p+s,q,r) measure, its 2-set is chosen according to (p,q+s,r) measure, and its 3-set is chosen according to (p,q,r+s) measure. I think this too is easy: for each i you make it 1 with probability p, 2 with probability q, 3 with probability r, and a wildcard with probability s. The next task is to come up with analogues of the “biased Walsh functions” %w_{A,p}% that we used on %{}[2]^n%. So now we are looking for functions %\\tau_{A,p,q,r}% (where the %r% is redundant but there for symmetry). We very much want to use independence, so we would like %\\tau_{A,p,q,r}% to be %\\prod_{i\\in A}\\tau_{i,p,q,r}%, where %\\tau_{i,p,q,r}(x)% will be %\\phi_{p,q,r}(x_i)%. Amongst the properties we will want of the function %\\phi_{p,q,r}% is that its expectation (defined to be %p\\phi_{p,q,r}(1)+q\\phi_{p,q,r}(2)+r\\phi_{p,q,r}(3)%) is 0 and that its variance is 1\\. To achieve this, I plan to let %\\phi_{p,q,r}(1)=cp^{-1}\\omega%, %\\phi_{p,q,r}(2)=cq^{-1}\\omega^2% and %\\phi_{p,q,r}(3)=cr^{-1}%. Here %\\omega=\\exp(2\\pi i/3)% and %c% is a normalizing factor to make the variance 1\\. This seems not to be the unique thing one could do but it does seem to be quite a natural thing to do. I’m hoping, however, that the expectation and variance will give me most of the information I care about. In particular, the functions %\\tau_{A,p,q,r}% defined this way do indeed form an orthonormal basis for (complex) functions defined on %{}[3]^n%. Now let us think about the expression %\\mathbb{E}f(x)g(y)h(z)%, where the expectation is over all combinatorial lines %(x,y,z)% chosen according to the (p,q,r,s) measure. To do this, we expand %f% as %\\sum_A\\hat{f}_{p+s,q,r}(A)\\tau_{A,p+s,q,r}%, and similarly for %g% and %h%. So what we would like to understand is %\\mathbb{E}\\tau_{A,p+s,q,r}(x)\\tau_{B,p,q+s,r}(y)\\tau_{C,p,q,r+s}(z)% for fixed sets %A,B,C%. Again, the expectation is over all (p,q,r,s)-combinatorial lines. (Eventually we will average over p+q+r+s=1.) I want to be able to see this expression, so will start a new comment."},{"username":"gowers","timestamp":"2009-02-20T17:41:00.000Z","contents":"Fourier and DHJ(3). Now let us choose a sequence of random variables %U_1,\\dots,U_n%, where each %U_i% is 1 with probability p, 2 with probability q, 3 with probability r, and a wildcard with probability s. Then let %X_i% be %U_i% if %U_i=1,2% or %3%, and %1% if %U_i% is a wildcard, and define %Y_i% and %Z_i% similarly, so that the three sequences %(X_1,\\dots,X_n)%, %(Y_1,\\dots,Y_n)% and %(Z_1,\\dots,Z_n)% form a random combinatorial line %(x,y,z)% determined by %(U_1,\\dots,U_n)%. Now each term in the product %\\mathbb{E}\\tau_{A,p+s,q,r}(x)\\tau_{B,p,q+s,r}(y)\\tau_{C,p,q,r+s}(z)% itself splits up into a big product, the first over all %i\\in A%, the second over all %i\\in B% and the third over all %i\\in C%. For example, %\\tau_{A,p+s,q,r}(X_1,\\dots,X_n)=\\prod_{i\\in A}\\phi_{p,q,r}(X_i)%. Let us try to understand when the expectation of this product has a chance of not being zero. One case where it _is_ zero is if there exists i that belongs to A but not to B or C. In that case, the big product will include %\\phi_{p+s,q,r}(X_i)%, but it won’t involve %Y_i% or %Z_i%. Therefore, by independence and the fact that %\\mathbb{E}\\phi_{p,q,r}(X_i)=0%, the entire expectation is 0. What about if %i\\in A\\cap B% but %i\\notin C%? Now we want to know what %\\mathbb{E}\\phi_{p+s,q,r}(X_i)\\phi_{p,q+s,r}(Y_i)% is. We know that the probability that %X_i=Y_i=1,2,3% is %p,q,r%, and the probability that %X_i=1% and %Y_i=2% is %s%. I would very much like the resulting expectation to be 0, but at the moment I don’t see any reason for this. So I think I’ve got to the point where I need to do a back-of-envelope calculation. Here is where it may turn out that my guess for a good choice of functions %\\phi_{p,q,r}% was not a good guess. (I should say that what I’m eventually hoping for is something reminiscent of what one gets in %\\mathbb{Z}_3^n%, namely a sum over cubes of Fourier coefficients. This may be asking a bit much though.)"},{"username":"gowers","timestamp":"2009-02-20T18:55:00.000Z","contents":"Fourier and DHJ(3) A quick report back. The guess doesn’t work, but I’m still not quite sure whether there might be another choice of functions %\\phi_{p,q,r}% that do the job. If we forget normalizations then the only property that matters is %p\\phi_{p,q,r}(1)+q\\phi_{p,q,r}(2)+r\\phi_{p,q,r}(3)=0%. And then what we’d very much like is that if %\\phi_{p+s,q,r}(i)=z_i% and %\\phi_{p,q+s,r}(i)=w_i%, then %pz_1w_1+qz_2w_2+rz_3w_3+sz_1w_2=0%. But I think that’s not going to be achievable. If we set %s=0%, then it suggests that we should also have %p\\phi_{p,q,r}(1)^2+q\\phi_{p,q,r}(2)^2+r\\phi_{p,q,r}(3)^2=0%, which more or less pins down the function (I think it decides it up to a constant multiple and taking complex conjugates). But I’m getting a bit bogged down in algebra at the moment, nice though that condition is."},{"username":"ryan-odonnell","timestamp":"2009-02-20T21:03:00.000Z","contents":"Sperner. Tim, the expression in #567 is so elegant, how can I not be on board for equal-slices now? <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> Here are some questions that occur to me: 1\\. Can we get a handle on the meaning of the first term? What does %\\mathbb{E}_{p \\leq q}[\\hat{f}_p(\\emptyset)\\hat{f}_q(\\emptyset)]% represent? Well, it represents the average of %f%‘s %p%-biased measure times its %q%-biased measure under this way of choosing %p, q%, but what does that… mean? 2\\. Can we understand why this expression is miraculously always %\\delta% whenever %f% is a monotone function of equal-slices-mean %\\delta%? Being “monotone” is a fairly “non-Fourier-y” property. 3\\. Is this quantity always at least %\\delta^2%? 4\\. What exactly might we be hoping to prove in this Sperner case? Something like, “The extent to which you don’t have a %\\delta% fraction of Sperner pairs (under Tim’s distribution) is related to the extent to which you are not monotone.”?"},{"username":"gowers","timestamp":"2009-02-20T21:46:00.000Z","contents":"Sperner Ryan, I don’t have complete answers to your questions, needless to say, but they are interesting questions so let me give my first thoughts. 1\\. I haven’t checked that it’s exactly the same, but I’d have thought it would be something like what I was looking at before you came along with your %p%s and %q%s. That is, you randomly scramble %{}[n]%, then randomly choose two initial segments %U% and %V%, and calculate %f(U)f(V)%. And then you average over everything you’ve done. In the case where %f% is the characteristic function of a set %\\mathcal{A}%, you are calculating the expectation of the square of the density of initial segments of a random permutation of %{}[n]% that belong to %A%. So in that model at any rate it’s always at least %\\delta^2% since the average is %\\delta%. So this may possibly answer 3\\. 2\\. I don’t understand your question here. For example, if %f% is the characteristic function of all sets of size at least %n/2%, then the %p%-biased measure is basically 0 if %p. So the equal-slices mean is %1/2%. And if you choose a random pair %(p,q)% such that %0\\leq p\\leq q\\leq 1% then the probability that they both exceed %1/2% is 1/4\\. And this works if you replace %1/2% by any %\\delta%. In order to depart from %\\delta^2%, you have to find a monotone function that doesn’t look like a union of layers, of which the obvious example is the set of all sets that contain 1\\. 3\\. See 1. 4\\. At one point I was convinced by you that monotonicity was a key property. But I’ve gone back to thinking that what really matters is small sets with big bias. I think what I’d like to see (and I think it could be quite close) is a density-increment proof of Sperner. The proof would go something like this. By the expansion above, either get the expected number of pairs %U\\subset V% or we find a small %A% such that %\\hat{f}_p(A)% is large for a dense set of %p%, where %f% is the balanced function of %A% (suitably defined). In the latter case, we fix a small number of variables and give ourselves a density increase in a subspace of bounded codimension. The motivation for such an argument would be to find something that could be adapted to DHJ(3). There are at least two technical problems to sort out. The first is to work out %\\mathbb{E}_{p\\leq q}\\lambda_{p,q}^k%. The second is that restricting to subspaces is problematic when one is dealing with equal-slices measure. Or at least, at some point I persuaded myself that it was."},{"username":"gowers","timestamp":"2009-02-20T23:14:00.000Z","contents":"Fourier, DHJ(3), corners It’s just occurred to me that what I was trying to do in 568-570 is unlikely to work because we don’t have a direct Fourier expansion of this kind for the corners problem. So before I go any further I want to think about corners. Actually, I’ve now had a counterthought, which is that if by some miracle we ended up with an expression of the form %\\mathbb{E}_{p+q+r=1}\\sum\\lambda_{p,q,r,s}^{|u|}\\hat{f}_{p+s,q,r}(u)\\hat{f}_{p,q+s,r}(u)\\hat{f}_{p,q,r+s}(u)% for the number of combinatorial lines (I’ve also just realized that I was absent-mindedly trying to define a basis that consisted of only %2^n% vectors, so I’ve changed %A% to %u% and will go back and think about this), then we would not particularly expect to get a non-zero answer (if %f% was the characteristic function of our dense set) unless we could find a good selection %p,q,r,s% such that all the Fourier coefficients at %u=0% were reasonably large. This would be asking for corners in the triangle %p+q+r=1%. So it could conceivably be that the corners theorem would get us started and Fourier would take over from there—which is reminiscent of, though not quite the same as, what Terry suggested way back in [comment 77](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1770). Here, the idea would be that the corners theorem (suitably Varnavides-ized) would guarantee that there were many candidate triples of rich slices that could lead to combinatorial lines, and Fourier methods would show that you could differ from the expected number of combinatorial lines _given the slice-density function_ only if something showed up in the Fourier expansions. However, having written that optimistic sounding paragraph, I feel pessimism returning. In fact, I have a little test to apply. I think if one chooses a random set of complexity 1 (see [the discussion of DHJ(1,3) on this page](http://michaelnielsen.org/polymath1/index.php?title=Density_Hales-Jewett_theorem#DHJ.281.2C3.29) for a definition) then it may well not have any large Fourier coefficients, even though it will typically have the wrong number of combinatorial lines. To end this comment on an optimistic note, perhaps this strange form of Fourier analysis could be used to prove DHJ(1,3), and perhaps it could come in at the point where Shkredov needs it for his corners proof."},{"username":"gowers","timestamp":"2009-02-20T23:57:00.000Z","contents":"Fourier and DHJ(3) Here’s the test then. Suppose you take two random collections %\\mathcal{U}% and %\\mathcal{V}% of subsets of %{}[n]%. Now let %\\mathcal{A}% be the set of all points %x% with 1-set in %\\mathcal{U}% and 2-set in %\\mathcal{V}%. Let %\\phi% and %\\psi% be two functions from %{}[3]% to %\\mathbb{C}% such that %\\phi(1)+\\phi(2)+\\phi(3)=\\psi(1)+\\psi(2)=\\psi(3)=0%. Let %\\tau% be a product of functions %x\\mapsto\\sigma_i(x_i)% where each %\\sigma_i% is either 1, %\\phi% or %\\psi%. The question now is whether %\\tau% can possibly correlate with %\\mathcal{A}%. And the answer seems to me to be so obviously no that I think I can get away without actually checking it carefully. In fact, here’s a sketch of a proof: there are only %3^n-1% non-constant functions of the given type, the expectation of each one over the random set %\\mathcal{A}% is 0, and there will be very very strong concentration. (I haven’t checked this argument, but it convinces me.) The next step is to see whether it is possible to make up for this slight disappointment (slight because we had no right to expect things to work better for DHJ(3) than they do for corners) by finding something comparably nice about bipartite graphs and singular values, for possible use in a regularity lemma. I’ll start a fresh comment and then explain what I’m hoping for here."},{"username":"gowers","timestamp":"2009-02-21T00:24:00.000Z","contents":"Global quasirandomness Recall that associated with a subset %\\mathcal{A}% of %{}[3]^n% are three bipartite graphs, each with two copies of the power set of %{}[n]% as its vertex sets, with edges joining some pairs of disjoint sets. For instance, the 12-graph joins %U% to %V% if the sequence x with 1-set equal to %U% and 2-set equal to %V% belongs to %\\mathcal{A}%. Now the averaging-over-permutations technique looked as though it would yield a definition of quasirandomness that was sufficient to guarantee the correct number of combinatorial lines. However, we did not get out of that the kind of global correlations that would be needed for a regularity lemma. Basically, the reason was that the definition that seemed to be emerging was of the number-of-4-cycles kind rather than the correlates-with-bipartite-subgraph kind. However, the thoughts that led to the Sperner expression feel as though they could also give rise to a useful definition of the latter kind. So that is what I want to try next."},{"username":"gowers","timestamp":"2009-02-21T00:58:00.000Z","contents":"Global quasirandomness First, let me give a Ryan-style equal-slices measure for the bipartite graph we’re talking about, or more generally for functions %f% that take pairs %(U,V)% of disjoint sets to %{}[-1,1]%. As usual, I’ll discuss things for fixed %p% and %q% and then average later. So let %p% and %q% be positive real numbers that add up to less than 1\\. For each element of %{}[n]%, put it in %U% with probability %p% and in %V% with probability %q%, with all choices independent. Then we would like to find a natural decomposition of such functions, analogous to the singular value decomposition of a bipartite graph. In the graphs case we can derive the SVD variationally. The analogue here would be to maximize %\\mathbb{E}_{U,V}f(U,V)a(U)b(V)% over all functions %a% and %b% of %L_2% norm 1\\. Here, I imagine that we choose %U% and %V% according to the distribution specified above, and measure the %L_2% norms of %a% and %b% using the %\\mu_p% and %\\mu_q% measures, respectively. Then if %d% is a very small function orthogonal to %b%, then the above expectation must change by %o(\\|d\\|)%, which implies that the function %c(V)=\\mathbb{E}_{U,V}f(U,V)a(U)% is proportional to %b(V)%. Got to go. To be continued."},{"username":"gowers","timestamp":"2009-02-21T05:26:00.000Z","contents":"Global quasirandomness My basic thought here is to try to do linear algebra but with a different model of matrix multiplication: when we write %\\mathbb{E}_{U,V}f(U,V)a(U)% we are not allowing %U% and %V% to vary independently. Having decided on that, I would like to try to follow the usual arguments as similarly as possible. For example, I would like to be able to say that %f% is quasirandom if %\\mathbb{E}_{U,V}f(U,V)a(U)% is approximately the constant function %\\mu_p(a)% for every bounded %a%. And I’d like to have a counting lemma for quasirandom functions (this is likely to be less easy). With those two ingredients, there would be a serious chance of proving a triangle-removal lemma. From the argument in the previous comment, we have seen that if %\\mathbb{E}_{U,V}f(U,V)a(U)b(V)% is maximized over all unit vectors %a% and %b%, then %\\mathbb{E}_{U,V}f(U,V)a(U)% is proportional to %b(V)%. Therefore, if %\\langle b,b'\\rangle_q=0%, it follows that %\\mathbb{E}_{U,V}f(U,V)a(U)b'(V)=0.% By symmetry, if %\\langle a,a'\\rangle_p=0%, it follows that %\\mathbb{E}_{U,V}f(U,V)a'(U)b(V)=0.% In other words, the linear map defined by %f% takes the space orthogonal to %a% to the space orthogonal to %b%, so we can find orthonormal bases %(a_i)% and %(b_i)% of %{}[2]^n%, one with respect to the measure %\\mu_p% and one with respect to the measure %\\mu_q%, such that for every %i%, %\\mathbb{E}_{U,V}f(U,V)a_i(U)% is proportional to %b_i(V)%. Let’s write %\\mathbb{E}_{U,V}f(U,V)a_i(U)=\\lambda_ib_i(V)%. (Everything here depends on %p% and %q% but we are thinking of those as fixed for now.) Then if %a=\\sum_i\\theta_ia_i%, then %\\mathbb{E}_{U,V}f(U,V)a(u)=\\sum_i\\lambda_i\\theta_ib_i(V)%, which has %L_2% norm %\\sum_i\\lambda_i^2\\theta_i^2%. For this not to be small, we need some %\\lambda_i% to be large, I think, and this seems to give us %a_i% and %b_i% such that %\\mathbb{E}_{U,V}f(U,V)a_i(U)b_i(V)% is large. Obviously this needs to be checked pretty carefully, but it looks on the face of it as though we find something like a complexity-1 function that correlates with %f%. (However, when %p% and %q% vary, so does the complexity-1 function.) But I still have the local-to-global problem, but from the other side: this time there seems to be a global non-quasirandomness, but it’s not clear to me that global quasirandomness leads to the right number of triangles."},{"username":"terence-tao","timestamp":"2009-02-21T11:47:00.000Z","contents":"A new proof of DHJ(2) Hi; I’ve been busy with other things for a while, so am not caught up… but meanwhile, I think I found a genuinely different proof of DHJ(2) based on obstructions-to-uniformity (similar, incidentally, to my paper proving convergence for multiple commuting shifts) which may have some potential. It goes like this. Call a bounded function %f: [2]^n \\to {\\Bbb R}% _uniform at scale m_ if f(x) and f(y) are uncorrelated, whenever x is chosen randomly from %[2]^n% and y is formed from x by randomly flipping O(m) 0s to 1s. Call a bounded function f _basic anti-uniform at scale m_ if its mean influence is O(1/m), i.e. if x is chosen randomly from %[2]^n% and y is obtained from x by randomly flipping one 0 to a 1, then f(x)=f(y) with probability 1-O(1/m). **Lemma:** A bounded function f is either uniform at scale m, or has non-trivial correlation with a function g which is basic anti-uniform at scale m. **Proof:** Take %g(x) := {\\Bbb E} f(y)%, where y is formed from x by randomly flipping m 0s to 1s. %\\Box% At this point we have two possible strategies. The first is to observe that basic anti-uniform functions tend to be roughly constant on large subspaces. Because of this, any function f of mean zero which correlates with a basic anti-uniform function will have a density increment on some large subspace. If one then starts with $f := 1_A – \\delta$ and runs the density increment argument (and reduce m at each iteration), one can eventually get to the point where $1_A – \\delta$ is uniform at some scale m, at which point one can find lots of combinatorial lines with m wildcards. The other strategy is to use energy-increment methods instead of density-increment ones. Indeed if we do the usual “energy-increment” iteration (as for instance appears in my first paper with Ben), and decreasing the parameter m appropriately with each iteration, we obtain **Corollary:** Every bounded function f can be split into a function %f_U% uniform at scale m, plus a polynomial combination %f_{U^\\perp}% of functions anti-uniform at scale m’, plus a small error, where m’ is much larger than m. Also, if f is non-negative and has large mean, one can arrange matters so that %f_{U^\\perp}% is also non-negative and has large mean. Now on subspaces of size about m’, %f_{U^\\perp}% is close to constant, and on many subspaces, this quantity is going to be large. Meanwhile, %f_U% is uniform on most subspaces, and so most subspaces are going to contain lines (with about m wildcards). The corresponding obstructions to uniformity for DHJ(3) are more complex. Roughly speaking, I’m facing things like this: take a random x in %[3]^n%, let y be formed from x by randomly flipping a bunch of 0s to 1s, and then let x’, y’ be formed from x and y by randomly flipping a bunch of 0s to 2s (with the digits being flipped for x being the same as those for y). Then one needs to understand what it means if %{\\Bbb E} f(x) f(y) f(x') f(y')% is large. There seems to be a sense that f is somehow correlating with a “local complexity 1” set – i.e. the type of expression we already saw in DHJ(1,3), but somehow localised to a much smaller scale than n – but I don’t see clearly what it will be yet."},{"username":"terence-tao","timestamp":"2009-02-21T22:36:00.000Z","contents":"579. There looks like there is a good chance that the type of arguments in 578 can be combined with the basic strategy I sketched out (very vaguely) in 536 (as Ryan already suggested in 538). Basically, if there are many large slices of A which are not uniform (which means that their Fourier transform is concentrated in a Hamming ball of radius n/m or so centred at the origin) then that slice of A is correlating with an anti-uniform object of low influence; this object is roughly constant on medium-dimensional subslices and so we can locate a density increment. So we can assume that most slices of A are uniform, and I think this is getting close to what we need for a counting lemma for geometric lines in A where the middle point of the line is close to the centre (2,2,..,2). I’ll have to do some calculations to clarify all this."},{"username":"gowers","timestamp":"2009-02-21T22:40:00.000Z","contents":"Extreme localization and DHJ(3) Although I am still very interested in trying to find a “global” proof of DHJ(3) (though cheating first and changing the measure to equal-slices), Terry’s 578 prompts me to ask why we don’t already have a proof of DHJ(3). I’m not saying that I think we do, but rather that I want to understand where the gap is. In order to explain that, let me try to give some sort of proof sketch. Either it will look fairly complete, in which case we will need to examine its steps more carefully, or there will be some step that I don’t yet see how to carry out, even roughly. The broad plan is similar to things that both Terry and I have discussed in much earlier comments, and that Terry alludes to in his final paragraph above. I shall write my points as %(U,V,W)%, where these are the 1-set, 2-set and 3-set of a sequence in %{}[3]^n%. If %\\mathcal{A}% is dense, then there will be many choices of %W% for which the set of %(U,V)% with %U\\cup V=[n]\\setminus W% and %(U,V,W)\\in\\mathcal{A}% is dense (in the set of %(U,V)% such that %U\\cup V=[n]\\setminus W%). Therefore, by Sperner we can find a pretty large number of _pairs_ %(U,V),(U',V')% of such pairs with %U\\subset U'%. And for each such pair of pairs we know that the point %(U,V',[n]\\setminus(U\\cup V'))% does not belong to %\\mathcal{A}%. Now let’s rephrase that. This time I’ll write %(U,V)% for the point with 1-set equal to %U% and 2-set equal to %V%. Then for each %W% we find a set-system %\\mathcal{U}_W% such that %(U,V)\\notin\\mathcal{A}% whenever %U\\in\\mathcal{U}_W% and %{}[n]\\setminus(V\\cup W)\\in\\mathcal{U}_W%. Let us write %\\mathcal{V}_W% for the set of all %V% with the second property. Sperner should tell us that on average the set of all disjoint pairs %(U,V)\\in\\mathcal{U}_W\\times\\mathcal{V}_W% is reasonably dense in the set of all disjoint pairs that are disjoint from %W%. So we have something that could be described as a “local complexity 1” set of points that do _not_ belong to %\\mathcal{A}%. Indeed, we have lots of such sets. I’m going to gloss over a few technicalities such as whether we are thinking about equal-slices measure or insisting that our lines have very few wildcards or whatever. Or perhaps I’ll just go for the latter: let’s focus on lines that have at most %m% wildcards, where %1<. Now for the main question. Can we convert this local non-uniformity into a global non-uniformity by some random restriction? At the moment things look a bit strange because the lines we are considering are of the form %(U,V)%, where %U% and %V% are disjoint from each other and from a fixed set %W%, and all of %U%, %V% and %W% have size about %n/3.% But if we restrict to the subspace of all sequences that are %3% on %W%, then in this subspace we are looking at points where the 1-set and 2-set use up about half the coordinates and the 3-set uses up only a tiny number of coordinates. So the plan would be to choose random sets %U_0% and %V_0% of size something like %n/2-m% and restrict to the subspace of points that are 1 on %U_0% and %2% on %V_0%. Inside that subspace I think we’d still have correlation with a set of complexity 1 but now a typical point would have around %m% each of 1s, 2s and 3s (over and above the already fixed coordinates). Obviously there would be some details to check there, but at the moment I don’t see why it doesn’t give us correlation with a set of complexity 1 after restricting to a small (but not all that small) subspace. Where to proceed from there? Well, the set of complexity 1 is of a nice form in that whether or not you belong to it depends just on whether your 1-set belongs to %\\mathcal{U}_W% and your 2-set belongs to %\\mathcal{V}_W%. (In other words, there isn’t an additional condition about the 3-set. As shown on [section 2 on this wiki page](http://michaelnielsen.org/polymath1/index.php?title=DHJ%281%2C3%29), such sets are simpler to deal with than general complexity-1 sets. I will be interested to see whether anyone can spot a flaw in the above sketched argument. If not, then attention should turn to a problem that up to now I have assumed is doable, which is to show that if you correlate with a complexity-1 set then you can get a density increase on a subspace. I’ll post this comment and then think about that problem in my next one."},{"username":"terence-tao","timestamp":"2009-02-21T22:42:00.000Z","contents":"Metacomment: Unrelated to this thread, but I thought I would like to report a success of this collaborative environment: at the 700 thread we have now produced two proofs (one computer-assisted, one not) that %c_6=450%, i.e. that the largest line-free subset of %[3]^6% contains exactly 450 points. Six dimensions is already well into the range where the “curse of dimensionality” really begins to bite, so I find this a significant milestone. At least six people made significant contributions to one of the two proofs."},{"username":"gowers","timestamp":"2009-02-21T23:03:00.000Z","contents":"Multidimensional Sperner. I’ve just realized that multidimensional Sperner is easy. I must have been aware of this argument at some point in the past, and certainly I know lots of similar arguments, but this time round I had been blind. The question I’m interested in is this. Suppose that you have a dense subset of %{}[2]^n%. Sperner tells you that you can find a pair %A\\subset B% in %\\mathcal{A}%, and that gives us a 1-dimensional combinatorial subspace. But what if we want a k-dimensional subspace? Here’s how I think one can argue. Actually, the further I go the more I realize that it’s not completely easy after all. But I think it shouldn’t be too bad. I think for convenience I’ll use equal-slices measure. In that measure one obtains that if %\\mathcal{A}% is dense then the proportion of pairs %A\\subset B% that belong to %\\mathcal{A}% is also positive. So by averaging I think one can find a set %D% such that the set of all %A% disjoint from %D% such that both %A% and %A\\cup D% belong to %\\mathcal{A}% is itself dense (in %{}[2]^{[n]\\setminus D}%). By induction that set contains a %(k-1)%-dimensional combinatorial subspace and we are done. (This argument is unchecked and I’m not certain the details work out, but if they don’t then some version of it with repeated localizations certainly ought to.) What I’d now like to do is prove that a dense set %\\mathcal{A}\\subset[3]^n% of complexity 1 of the special form discussed in the previous comment contains a combinatorial subspace with dimension that tends to infinity with %n%. (Eventually the plan would be to prove more than this: we would like to cover %\\mathcal{A}% evenly with such subspaces so that a set that correlates with %\\mathcal{A}% has to correlate with one of the subspaces. But we should walk before we run.) That counts as a separate task so I’ll attempt it in the next comment."},{"username":"gowers","timestamp":"2009-02-21T23:32:00.000Z","contents":"Dealing with complexity-1 sets. Actually, I suddenly feel lazy, for the following reason. It seems as though the multidimensional Sperner proof generalizes rather straightforwardly, and because of that I temporarily can’t face checking the details (and thereby heaping vagueness on vagueness — it is probably soon going to be a good idea to do the Sperner argument carefully). Here’s the rough thought though. In [section 2 on this wiki page](http://michaelnielsen.org/polymath1/index.php?title=DHJ%281%2C3%29) I showed that a set of the given form contains a combinatorial line. If the Sperner idea above works, then it should work here too: we just find a set %D% such that for a dense set of points %x% in %{}[3]^{[n]\\setminus D}% we can fix %x%, let %D% be a wildcard set, and thereby obtain a combinatorial line. If so, then once again we should be in a position to say that we have a %(k-1)%-dimensional combinatorial subspace in the set of all such %x%, and hence a %k%-dimensional combinatorial subspace in %\\mathcal{A}% itself. Let me take a break now, but end with the following summary of the proof strategy. As I’ve already said, I would greatly welcome any feedback, so that I can get an idea of which bits look the dodgiest … 1\\. As in 580 (but it was closely modelled on arguments that have been discussed already in earlier comments such as [comment 413](https://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2042), part of [comment 439](https://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2086), and [comment 441](https://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2090)) we prove that if %\\mathcal{A}% contains no combinatorial lines then, locally at least, there is a dense complexity-1 set that is disjoint from %\\mathcal{A}%. By averaging, we get a dense complexity-1 set inside which %\\mathcal{A}% has a density increment. Moreover, that complexity-1 set is of the simple form where it depends just on the set-systems to which the 1-set and 2-set belong, with the 3-set free to be whatever it likes. 2\\. Every dense complexity-1 set of this form contains a multidimensional subspace (as optimistically sketched in 581 and earlier in this comment). Almost certainly this can be lifted up to a proof that a dense complexity-1 set of this kind can be uniformly covered by multidimensional subspaces. And then we get a density increment on a subspace. 3\\. Therefore, by the usual density-increment strategy, we can keep passing to subspaces until the density reaches 1 (or 2/3 if we feel particularly clever), by which point we must have found a combinatorial line. Done! If there is a problem with covering a special complexity-1 set uniformly with subspaces, then what we could do next is remind ourselves exactly how the Ajtai-Szemerédi proof of the corners theorem goes, since the averaging arguments they did there are likely to be helpful here."},{"username":"jozsef","timestamp":"2009-02-22T00:10:00.000Z","contents":"Multidimensional Sperner. Tim, isn’t it the same question that we considered in 135?"},{"username":"gowers","timestamp":"2009-02-22T00:39:00.000Z","contents":"Multidimensional Sperner Ah — I remembered only Gil’s comment 29\\. So the main point of what I’m saying is not so much that multidimensional Sperner is a new and exciting result — with a proof like that it couldn’t be — but that if %\\mathcal{A}% is a special set of complexity 1, then any 12-cube in %\\mathcal{A}% (by which I mean that you fix some coordinates and some wildcard sets that take values 1 or 2) automatically extends to a combinatorial subspace. That’s quite a nice way of thinking about why special complexity-1 sets are good things. So at least part of the argument sketched in 582 seems to be correct. But there’s still the question of finding not just one combinatorial subspace but a uniform covering by combinatorial subspaces. I would be very surprised if that turned out to be a major problem. (In fact, I think I am beginning to see a possible strategy for a proof.)"},{"username":"gowers","timestamp":"2009-02-22T00:52:00.000Z","contents":"Metacomment. This thread is going to run out soon. I have recently learned from Luca Trevisan that WordPress now allows threading — that is, indented replies to existing comments. I think the current numbering system has served us surprisingly well, and if we introduced threading then we would lose the easily viewed chronological order of comments. But perhaps there would be compensating advantages. If you want to express a view in the following poll, then by all means do, though I suspect that we may have a split vote … If I allowed threading, I would be able to choose how much nesting was allowed. One possibility would be to allow only one level, say, and to ask contributors to use the facility as little as possible (perhaps restricting it to short replies to existing comments that do not themselves invite further discussion). That is what I mean by option 3 below.<a name=\"pd_a_1390386\"></a>  \n\n<noscript><a href=\"http://polldaddy.com/poll/1390386\">Take Our Poll</a></noscript>\n\nOh. It seems that what I’ve just typed works only in a post and not in a comment. So I’d better do that."},{"username":"terence-tao","timestamp":"2009-02-22T02:10:00.000Z","contents":"Oops My lemma in 578 might still be true, but I realise now that the proof I supplied is rubbish. Still thinking about it…"},{"username":"gowers","timestamp":"2009-02-22T02:17:00.000Z","contents":"Oops All I can say is that it convinced me at the time …"},{"username":"jozsef","timestamp":"2009-02-22T03:43:00.000Z","contents":"Dimensions I’m trying to follow the uniformity/density increment arguments and I have the same feeling like with the planar square problem. (show that every dense subset of the integer grid contains four points of the form (x,y),(x+d,y),(x,y+d),(x+d,y+d) ) One can try to prove it directly, it is very difficult but probably possible, extending methods of Ajtai-Szemeredi or Shkredov. On the other hand, transferring the problem to dimension three from 2d, makes it more accessible. So, my question is the following: Is there a natural way to consider larger cubes, where a special configuration is (more) dense and a projection of it to the smaller cube, %3^n%, is a combinatorial line?"},{"username":"terence-tao","timestamp":"2009-02-22T13:02:00.000Z","contents":"Dimensions Given that the planar square theorem implies the k=4 version of Szemeredi’s theorem, I would imagine that Fourier or graph regularity based methods are insufficient for the task."},{"username":"gowers","timestamp":"2009-02-22T16:02:00.000Z","contents":"Dimensions I agree with Terry, in the sense that I don’t think that deducing squares from 3D corners makes the problem fundamentally easier (just ask Michael Lacey …), and that either problem would have to involve quadratic methods or 3-uniform hypergraphs or something of equivalent difficulty. Nevertheless, it does seem that the 3D corners problem is the natural way to solve the 2D squares problem, so one can still ask whether it is more natural to find a low-dimensional subspace with high diameter rather than a high-dimensional subspace with low diameter. Here I think there are two possible answers. Suppose, for simplicity, that we are thinking about a high-dimensional version of Sperner’s theorem (but I think what I say generalizes easily to DHJ(k)). That is, we would like not just one wildcard set but several. For further simplicity let’s suppose that all we want is two wildcard sets. Then one proof is to use averaging arguments of the kind I was outlining in 581\\. This is fairly easy. Another proof is to establish DHJ(4) and observe that if you write the numbers 0 to 3 in binary then a combinatorial line in %{}[4]^n% when written out gives you a 2-dimensional combinatorial subspace in %{}[2]^n%. The obvious reaction to this is that the second proof is _much_ harder. However, it also gives a much stronger result, because the two wildcard sets have the same cardinality. Furthermore, the stronger result can be seen to be more useful: if you look at unions of slices, then the first result gives you a quadruple %(a,a+d,a+d',a+d+d')%, which it is easy to get from Cauchy-Schwarz, whereas the second gives you an arithmetic progression %(a,a+d,a+2d)%. These simple observations raise obvious questions, which I think are close to the kinds of things Jozsef is wondering about. For example, it doesn’t seem to be possible to deduce Szemerédi’s theorem for progressions of length 4 from the existence of a 2D subspace with equal-sized wildcard sets, so does that mean that the true difficulty of that problem is roughly comparable to that of DHJ(3)? In particular, does it either imply or follow from DHJ(3)? I find this quite an interesting question (though questions often seem interesting when you haven’t yet thought about them)."},{"username":"gowers","timestamp":"2009-02-22T19:42:00.000Z","contents":"Dimensions Just realized that the question above was possibly slightly silly. The proof that you can get two wildcard sets of the same size actually gives that one consists of odd numbers and the other of even numbers. So if you look at the number of 1s in odd places plus twice the number of 1s in even places then you get an arithmetic progression of length 4\\. In other words, what you deduce from DHJ(4) implies Szemerédi’s theorem for progressions of length 4, exactly as it should. Strictly speaking, it is just about conceivable that DHJ(3) implies the weaker statement, but I doubt it somehow."},{"username":"ryan-odonnell","timestamp":"2009-02-23T05:29:00.000Z","contents":"Re Oops/#578. Hmm, I too was pretty sold on #578, but then I seemed to founder when working out the exact details. My plan was to look at the noise stability of %f% at some parameter %1 - \\gamma%. If this quantity is even a smidgen greater than %\\mathbb{E}[f]^2% then one can do a density increment. Otherwise, %f% is extremely noise-sensitive at %1 - \\gamma%. (Or, “uniform at scale %\\gamma n%” as Terry called it.) This strongly feels like it should imply lots of Sperner pairs. But I really ran into difficulties when trying to prove this via the Fourier methods of #476…#567…."},{"username":"ryan-odonnell","timestamp":"2009-02-23T05:30:00.000Z","contents":"Re Oops/#578. More precisely, it seemed hard to simultaneously control all of the quantities %\\hat{f}_p(A) - \\hat{f}_{1/2}(A)%, even for %A% smallish. (I managed to do it for %A = \\emptyset%, at least <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> .)"},{"username":"randall","timestamp":"2009-02-23T06:08:00.000Z","contents":"DHJ (2.7): I think I managed to remove all hint of ergodic reduction in a proof of DHJ (2.6)…of course if you look at the proof (Plain TeX code will be at the end of this msg.), it’s because the reduction (to sets, not to stationarity) is inside. Still, it’s pretty painless and the Ramsey theory used has been trimmed again, to just Ramsey’s theorem this time. Also it doesn’t use DHJ (2) as a lemma. I’ve actually called it DHJ (2.7) because it seems to be very slightly stronger than what Terry called (2.6). More generally, I have been looking carefully at how to do a “straight translation” of the proof of DHJ (3) and am stuck. The issue arises already in the ergodic proof of the Szemeredi corners theorem. Basically, a set A of positive density in Z^2 is used to generate a measure preserving action of Z^2, together with a positive measure set B; recurrence properties of B imply existence of configurations for A. The issue arises because once the system is fixed, one can approximate B by a “relatively almost periodic” function f…f will have a kind of compactness property over a certain factor, meaning that given e>0 one can find an e-net (composed of functions) of finite cardinality M such that, on the fibers over the factor in question, the entire orbit of f will be e-close to some member of the net (on most fibers for most of the orbit). The point is that this number M is completely fixed by the set B, which is fixed by the set A, but the ergodic proof doesn’t give any way to bound M as a function of the density of A. For all the (usual) ergodic proof says to the contrary, this M might go to infinity for a sequence of A’s having measure bounded away from zero. This seems to be a not-very-desirable artefact of the ergodic proof, one that I don’t understand very well at present. ********* \\magnification=\\magstep 1  \n\\noindent {\\bf DHJ (2.7):} For all $\\delta>0$ there exists $n=n(\\delta)$ such that if $A\\subset [3]^n$ with $|A|> \\delta 3^n$,  \nthere exists $\\alpha\\in {\\bf N}$, $|\\alpha|0$. Let $m>{3\\over  \n\\delta_0}$ and choose by Ramsey’s theorem an $r$ such that for any 8-coloring of the 2-subsets of $[r]$, there is an $m$-subset  \n$B\\subset [r]$ all of whose 2-subsets have the same color. Let $\\delta=\\delta_0-{\\delta_0\\over 4\\cdot 3^r}$ and put $n_1=n(  \n\\delta_0+{\\delta_0\\over 4\\cdot 3^r})$. Finally put $n=r+n_1$ and suppose $A\\subset [3]^n$ with $|A|>\\delta 3^n$. For each $v\\in [3]^r$,  \nlet $E_v=\\{w\\in [3]^{n_1}:vw\\in A\\}$. If $|E_v|> (\\delta_0+{\\delta_0\\over 4\\cdot 3^r})3^{n_1}$ for some $v$ we are done; otherwise  \n$|E_v| > {\\delta_0\\over 3}$ for {\\it every} $v\\in [3]^r$. \\medskip\\noindent Some notation: for $i\\in [r]$ and $xy\\in \\{10,21,02\\}$, let $v_i^{xy}\\in [3]^r$ be the word $z_1z_2\\cdots z_r$,  \nwhere $z_a=x$ if $0\\leq a\\leq i$ and $z_a=y$ otherwise. Color $\\{1,j\\}$, $0\\leq i<j<r$, according to whether or not the sets  \n$E_{v_i^{xy}}\\cap E_{v_j^{xy}}$ are empty or not, $xy\\in \\{10,21,02\\}$. (This is an 8-coloring.) By choice of $r$ we can find  \n$0\\leq k_1<k_2<\\cdots <k_m<r$ such that $\\big\\{\\{ k_i,k_j\\}: 0\\leq i<j<m \\big\\}$ is monochromatic for this coloring. By pigeonhole,  \nfor, say, $xy=10$, there are $i<j$ such that $E_{v_{k_i}^{xy}}\\cap E_{v_{k_j}^{xy}}\\neq \\emptyset$, hence non-empty for all $i,j$  \nby monochromicity and similarly for $xy=21,02$. Now for $xy=10,21,02$, pick $u_{xy}\\in E_{v_{k_1}^{xy}}\\cap E_{v_{k_2}^{xy}}$ and  \nput $q_{xy}=s_1s_2\\cdots s_r$, where $s_i=x$, $0\\leq i<k_1$, $s_i=3$, $k_1\\leq i<k_2$, and $s_i=y$, $k_2\\leq i<r$. Finally put  \n$w_{xy}=q_{xy}u_{xy}$. Then $w_{xy}(x)=v^{xy}_{k_2} u_{xy}$ and $w_{xy}(y)=v^{xy}_{k_1} u_{xy}$ are in $A$, $xy\\in \\{10,21,02\\}$.  \nHence $n=n(\\delta)$, contradicting $\\delta<\\delta_0$. \\end"},{"username":"ryan-odonnell","timestamp":"2009-02-23T10:00:00.000Z","contents":"Sperner calculations. Re #567: %\\lambda_{p,q} = \\sqrt{p/(1-p)}\\sqrt{(1-q)/q}% by the way (if I remember my calculation correctly). In particular, %(1-\\epsilon)/(1+\\epsilon)% if %p,q = 1/2 \\pm \\epsilon/2%."},{"username":"gowers","timestamp":"2009-02-23T15:18:00.000Z","contents":"Sperner calculations Ryan, thanks for doing that calculation! At some point soon I think I’ll write up the whole thing fairly completely and put it on the wiki, which would make it easier to assess what it teaches us, if anything. I’m also working on step 1 of 582, as I feel that I can get something rigorously proved here too. Again, anything I manage to do properly will go on the wiki. But for now I want to report on the failure of my first attempt to write it up, because it says something interesting (though in retrospect pretty obvious) about equal-slices measure. I had previously come to the conclusion that a defect of equal-slices measure was that it didn’t restrict nicely to equal-slices measure. The mini-realization I have come to is to understand what _does_ happen when you restrict it. Suppose a sequence x is generated Ryan-style: that is, you randomly choose p+q+r=1 and pick each coordinate independently to be 1,2,3 with probability p,q,r. And now suppose that you restrict to the set of all x that take certain values on a subset %E\\subset[n].% You then find yourself with a classic problem of Bayesian statistics: given the values of %x_i% with %i\\in E%, what is the new probability distribution on the triples p+q+r=1? And qualitatively the answer is clear: if E is at all large, then with immensely high probability the triple (p,q,r) is approximately proportional to the triple (number of 1s in E, number of 2s in E, number of 3s in E). In other words, as soon as you restrict a few coordinates, you end up with something pretty close to a weighted uniform measure (by which I mean that each coordinate is chosen independently with fixed probabilities that are not necessarily equal to 1/3). So it seems that equal-slices measure is useful for truly global arguments, but that it collapses to more obvious measures once one starts to restrict. What about if we restrict to other sorts of subspaces? For example, suppose we partition [n] into s wildcard sets of size t. If you are given that a sequence is constant on some large wildcard set, then by far the most likely way that that could have come about is if one of p,q and r is approximately equal to 1\\. So the measure is concentrated around sequences that are approximately constant. Going back to step 1 of 582, I still think it should be possible to do it with localization. If I manage, it will go straight up on the wiki and I will report on it here."},{"username":"gowers","timestamp":"2009-02-23T16:06:00.000Z","contents":"Metacomment. This 500s thread has nearly run out. I’ve created a new post for an 800s thread, so new comments should go there unless they are quite clearly more appropriate here. But once we get to 599 that’s it!"},{"username":"ryan-odonnell","timestamp":"2009-02-23T23:06:00.000Z","contents":"Re Oops/Sperner. I think I can correct Terry’s #578 and put it together with our Fourier-based calculations. The writeup is a bit long. I’m going to try to post it here in WordPress using Luca’s LaTeX converter. It’s my fault if this goes horribly awry; if it does, I’ll just post a link to the pdf…. The following alternate argument for Sperner still feels “wrong” to me (I will say why at the end), but I can’t see the “right” one yet so I’ll write this one up for now. Let %{(\\bx_1,\\by_1)}% be chosen jointly from %{\\{ 0,1 \\} \\times \\{ 0,1 \\}}% as follows (I use **boldface** to denote random variables): The bit %{\\bx_1}% is %{0}% or %{1}% with probability %{1/2}% each. If %{\\bx_1 = 1}% then %{\\by_1 = 1}%. If %{\\bx_1 = 0}% then %{\\by_1 = 0}% with probability %{1 - \\epsilon}% and %{\\by_1 = 1}% with probability %{\\epsilon}%. Finally, let %{(\\bx, \\by)}% be chosen jointly from %{\\{ 0,1 \\}^n \\times \\{ 0,1 \\}^n}% by using the single-bit distribution %{n}% times independently. Note that %{\\bx \\leq \\by}% always. This distribution is precisely the one Tim uses, with %{p = 1/2}%, %{q = 1/2 + \\epsilon/2}%. Let %{f : \\{ 0,1 \\}^n \\rightarrow [-1,1]}% and let’s consider %{\\mathop{\\mathbb E}[f(\\bx)f(\\by)]}%. (We are eventually interested in %{f}%‘s with range %{\\{0,1\\}}% and mean %{\\delta}%, but let’s leave it slightly more general for now.) As per Tim’s calculations, %\\displaystyle \\mathop{\\mathbb E}[f(\\bx)f(\\by)] = \\sum_{S \\subseteq [n]} \\hat{f}(S) \\hat{f_\\epsilon}(S) (1 - \\epsilon')^{|S|}, %  \nwhere %{\\hat{f_\\epsilon}}% denotes Fourier coefficients with respect to the %{(1/2 + \\epsilon/2)}%-biased measure and %{\\eps'}% is defined by %{1-\\epsilon' = \\sqrt{(1-\\eps)/(1+\\eps)}}% (AKA %{\\lambda_{1/2,1/2+\\eps/2}}%). Separate out the %{S = \\emptyset}% term here and use Cauchy-Schwarz on the rest to conclude <a>%\\displaystyle \\left|\\mathop{\\mathbb E}[f(\\bx)f(\\by)] - \\hat{f}(\\emptyset)\\hat{f_\\eps}(\\emptyset)\\right| \\leq \\sqrt{\\sum_{|S| \\geq 1} \\hat{f}(S)^2 (1 - \\epsilon')^{|S|}}\\sqrt{\\sum_{|S| \\geq 1} \\hat{f_\\epsilon}(S)^2 (1 - \\epsilon')^{|S|}}. \\ \\ \\ \\ \\ (1)%  \n</a>Let’s compare %{\\hat{f_\\eps}(\\emptyset)}% to %{\\hat{f}(\\emptyset)}%. Write %{\\pi}% and %{\\pi_\\eps}% for the density functions of %{\\bx}%, %{\\by}% respectively. Then %\\displaystyle \\hat{f_\\eps}(\\emptyset) - \\hat{f}(\\emptyset) = \\mathop{\\mathbb E}[f(\\by)] - \\mathop{\\mathbb E}[f(\\bx)] = \\mathop{\\mathbb E}\\left[\\frac{\\pi_\\eps(\\bx)}{\\pi(\\bx)}f(\\bx)\\right] - \\mathop{\\mathbb E}[f(\\bx)] = \\mathop{\\mathbb E}\\left[\\left(\\frac{\\pi_\\eps(\\bx)}{\\pi(\\bx)}-1\\right)f(\\bx)\\right]. %  \nBy Cauchy-Schwarz, the absolute value of this is upper-bounded by %\\displaystyle \\sqrt{\\mathop{\\mathbb E}\\left[\\left(\\frac{\\pi_\\eps(\\bx)}{\\pi(\\bx)}-1\\right)^2\\right]}\\cdot \\|f\\|_2, %  \nwhere %{\\|f\\|_2}% denotes %{\\sqrt{\\mathop{\\mathbb E}[f(\\bx)^2]}}%. One easily checks that <a>%\\displaystyle \\mathop{\\mathbb E}\\left[\\left(\\frac{\\pi_\\eps(\\bx)}{\\pi(\\bx)}-1\\right)^2\\right] = \\mathop{\\mathbb E}\\left[\\frac{\\pi_\\eps(\\bx)^2}{\\pi(\\bx)^2}\\right] - 1, \\ \\ \\ \\ \\ (2)%  \n</a>and since %{\\pi_\\eps}% and %{\\pi}% are product distributions the RHS of [(2)](#eqndiff) is easy to compute. One can check explicitly that %\\displaystyle \\mathop{\\mathbb E}[\\pi_\\eps(\\bx_1)^2/\\pi(\\bx_1)^2] = 1+\\eps^2 %  \nand therefore [(2)](#eqndiff) is %{(1+\\eps^2)^n - 1}%. Naturally we will be considering %{\\eps \\ll 1/\\sqrt{n}}%, and in this regime the quantity is bounded by, say, %{4\\eps^2n}%. Hence we have shown %\\displaystyle |\\hat{f_\\eps}(\\emptyset) - \\hat{f}(\\emptyset)| \\leq 2\\eps\\sqrt{n} \\cdot \\|f\\|_2 %  \nand in particular if %{f}% has range %{\\{0,1\\}}% and mean %{\\mu}% (AKA %{\\delta}%) then <a>%\\displaystyle |\\hat{f_\\eps}(\\emptyset) - \\mu| \\leq 2 \\eps \\sqrt{n} \\cdot \\sqrt{\\mu}. \\ \\ \\ \\ \\ (3)%  \n</a>This is not very interesting unless %{2 \\eps \\sqrt{n} \\cdot \\sqrt{\\mu} \\leq \\mu}%, so let’s indeed assume %{\\eps \\leq \\sqrt{\\mu}/\\sqrt{n}}% and then we can also use %{\\hat{f_\\eps}(\\emptyset) \\leq 2 \\mu}%. We now these deductions in [(1)](#eqnmain). Note that the second factor on the RHS in [(1)](#eqnmain) is at most the square-root of %\\displaystyle \\sum_{S} \\hat{f_\\eps}(S)^2 = \\mathop{\\mathbb E}[f(\\by)^2] = \\mathop{\\mathbb E}[f(\\by)] = \\hat{f_\\eps}(\\emptyset) \\leq 2\\mu \\leq 4\\mu. %  \nAlso, using [(3)](#eqnmeans) for the LHS in [(1)](#eqnmain) we conclude <a>%\\displaystyle |\\mathop{\\mathbb E}[f(\\bx)f(\\by)] - \\mu^2| \\leq 2\\mu^{3/2} \\cdot \\eps\\sqrt{n} + 2\\sqrt{\\mu} \\sqrt{\\mathbb{S}_{1-\\eps'}(f) - \\mu^2}, \\ \\ \\ \\ \\ (4)%  \n</a>where %\\displaystyle \\mathbb{S}_{1-\\eps'}(f) = \\sum_{S} \\hat{f}(S)^2(1-\\eps')^{|S|}. %  \nLet’s simply fix %{\\eps = (1/8)\\sqrt{\\mu}/\\sqrt{n}}% at this point. Doing some arithmetic, it follows that _if_ we can bound <a>%\\displaystyle \\mathbb{S}_{1-\\eps'}(f) - \\mu^2 \\leq \\mu^3/64\\ (?) \\ \\ \\ \\ \\ (5)%  \n</a>(AKA %{f}% is “uniform at scale %{\\eps' n}%” as Terry might say) then [(4)](#eqnfinal) implies %\\displaystyle \\mathop{\\mathbb E}[f(\\bx)f(\\by)] \\geq \\mu^2/2\\. %  \nSo long as %{\\mathop{\\mathbb P}[\\bx = \\by] < \\mu^2/2}% we’ve established existence of a Sperner pair (AKA non-degenerate combinatorial line). Since this probability is %{(1-\\eps/2)^n \\leq \\exp(-\\eps n/2) = \\exp(-\\Omega(\\sqrt{\\mu}\\sqrt{n}))}%, we’re done assuming <a>%\\displaystyle n \\geq O(\\log^2(1/\\mu)/\\mu). \\ \\ \\ \\ \\ (6)%  \n</a>Thus things come down to showing [(5)](#eqngoal). Now in general, there is absolutely no reason why this should be true. The idea, though, is that if it’s _not_ true then we can do a density increment. More precisely, it is very easy to show (one might credit this to an old result of Linial-Mansour-Nisan) that %{\\mathbb{S}_{1-\\eps'}(f)}% is precisely %{\\mathop{\\mathbb E}_{\\bV}[\\mathop{\\mathbb E}[f|_\\bV]^2]}%, where %{\\bV}% is a “random restriction with wildcard probability %{\\eps'}%” (and the inner %{\\mathop{\\mathbb E}[\\cdot]}% is with respect to the uniform distribution). In other words, %{\\bV}% is a combinatorial subspace formed by fixing each coordinate randomly with probability %{1 - \\eps'}% and leaving it “free” with probability %{\\eps'}%. Hence if [(5)](#eqngoal) _fails_ then we have %\\displaystyle \\mathop{\\mathbb E}_{\\bV}[\\mathop{\\mathbb E}[f|_\\bV]^2] \\geq \\mu^2 + \\mu^3/64\\. %  \nIn particular, since %{f}% is bounded it follows that %{\\mathop{\\mathbb E}[f|_\\bV]^2 \\geq \\mu^2 + \\mu^3/128}% with probability at least %{\\mu^3/128}% over the choice of %{\\bV}%. It’s also very unlikely that %{\\bV}% will have fewer than, say, %{(\\eps'/2)n}% wildcards; a large-deviation bound shows this probability is at most %{\\exp(-\\Omega(\\eps' n))}%. Since %{\\eps' \\approx \\eps = (1/8)\\sqrt{\\mu}/\\sqrt{n}}%, by choosing the constant in [(6)](#eqnassume) suitably large we can make this large-deviation bound strictly less than %{\\mu^3/128}%. Thus we conclude that there is a positive probability of choosing some %{\\bV = V}% which both has at least %{(\\eps'/2)n = \\Omega(\\sqrt{\\mu}\\sqrt{n})}% free coordinates and also has %\\displaystyle \\mathop{\\mathbb E}[f|_V]^2 \\geq \\mu^2 + \\mu^3/128 \\Rightarrow \\mathop{\\mathbb E}[f|_V] \\geq \\mu + \\mu^2/500\\. %  \nI.e., we can achieve a density increment. If I’m not mistaken, this kind of density increment (gaining %{\\mu^2/C}% at the expense of going down to %{c \\sqrt{\\mu} \\sqrt{n}}% coordinates, with [(6)](#eqnassume) as the base case) will ultimately show that we need the initial density to be at least %{1/\\log \\log n}% (up to %{\\log \\log \\log n}% factors?) in order to win. Only a couple of exponentials off the truth <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> The incorrect quantitative aspect here isn’t quite the reason I feel this argument is “wrong”. Rather, I believe that no density increment should be necessary. (Actually, we probably know this is the case, by Sperner’s proof of Sperner.) In other words, I believe that %{\\mathop{\\mathbb E}[f(\\bx)f(\\by)] \\geq \\Omega(\\mu^2)}% **for any** %{f}%, assuming %{\\eps \\ll \\sqrt{\\mu}/\\sqrt{n}}%."},{"username":"ryan-odonnell","timestamp":"2009-02-23T23:09:00.000Z","contents":"Wow, indeed that didn’t go so great. I should have studied the output more carefully before posting. Sorry about that. I’ll post the pdf in Tim’s new thread…"},{"username":"gowers","timestamp":"2009-02-23T23:19:00.000Z","contents":"Metacomment (so as not to use precious numbers): Ryan, I’ve had a look at what you wrote and I’m afraid it’s beyond my competence to fix — it’s full of strange html-ish stuff that I don’t understand."},{"username":"ryan-odonnell","timestamp":"2009-02-24T10:31:00.000Z","contents":"Here is the corrected version. I should have read Luca’s documentation better… I think I can correct #578 and put it together with the Fourier-based calculations. The following alternate argument for Sperner still feels “wrong” to me (I will say why at the end), but I can’t see the “right” one yet so I’ll write this one up for now. Let %{({\\bf x}_1,{\\bf y}_1)}% be chosen jointly from %{\\{ 0,1 \\} \\times \\{ 0,1 \\}}% as follows (I use **boldface** to denote random variables): The bit %{{\\bf x}_1}% is %{0}% or %{1}% with probability %{1/2}% each. If %{{\\bf x}_1 = 1}% then %{{\\bf y}_1 = 1}%. If %{{\\bf x}_1 = 0}% then %{{\\bf y}_1 = 0}% with probability %{1 - {{\\epsilon}}}% and %{{\\bf y}_1 = 1}% with probability %{{{\\epsilon}}}%. Finally, let %{({\\bf x}, {\\bf y})}% be chosen jointly from %{\\{ 0,1 \\}^n \\times \\{ 0,1 \\}^n}% by using the single-bit distribution %{n}% times independently. Note that %{{\\bf x} \\leq {\\bf y}}% always. This distribution is precisely the one Tim uses, with %{p = 1/2}%, %{q = 1/2 + {{\\epsilon}}/2}%. Let %{f : \\{ 0,1 \\}^n \\to [-1,1]}% and let’s consider %{\\mathop{\\bf E}[f({\\bf x})f({\\bf y})]}%. (We are eventually interested in %{f}%‘s with range %{\\{0,1\\}}% and mean %{\\delta}%, but let’s leave it slightly more general for now.) As per Tim’s calculations, %\\displaystyle \\mathop{\\bf E}[f({\\bf x})f({\\bf y})] = \\sum_{S \\subseteq [n]} \\hat{f}(S) \\hat{f_{{\\epsilon}}}(S) (1 - {{\\epsilon}}')^{|S|}, %  \nwhere %{\\hat{f_{{\\epsilon}}}}% denotes Fourier coefficients with respect to the %{(1/2 + {{\\epsilon}}/2)}%-biased measure and %{{\\epsilon}'}% is defined by %{1-{{\\epsilon}}' = \\sqrt{(1-{\\epsilon})/(1+{\\epsilon})}}% (AKA %{\\lambda_{1/2,1/2+{\\epsilon}/2}}%). Separate out the %{S = \\emptyset}% term here and use Cauchy-Schwarz on the rest to conclude %\\displaystyle \\left|\\mathop{\\bf E}[f({\\bf x})f({\\bf y})] - \\hat{f}(\\emptyset)\\hat{f_{\\epsilon}}(\\emptyset)\\right| %  \n<a>%\\displaystyle \\leq \\sqrt{\\sum_{|S| \\geq 1} \\hat{f}(S)^2 (1 - {{\\epsilon}}')^{|S|}}\\sqrt{\\sum_{|S| \\geq 1} \\hat{f_{{\\epsilon}}}(S)^2 (1 - {{\\epsilon}}')^{|S|}}. \\ \\ \\ \\ \\ (1)%  \n</a>Let’s compare %{\\hat{f_{\\epsilon}}(\\emptyset)}% to %{\\hat{f}(\\emptyset)}%. Write %{\\pi}% and %{\\pi_{\\epsilon}}% for the density functions of %{{\\bf x}}%, %{{\\bf y}}% respectively. Then %\\displaystyle \\hat{f_{\\epsilon}}(\\emptyset) - \\hat{f}(\\emptyset) = \\mathop{\\bf E}[f({\\bf y})] - \\mathop{\\bf E}[f({\\bf x})] %  \n%\\displaystyle = \\mathop{\\bf E}\\left[\\frac{\\pi_{\\epsilon}({\\bf x})}{\\pi({\\bf x})}f({\\bf x})\\right] - \\mathop{\\bf E}[f({\\bf x})] = \\mathop{\\bf E}\\left[\\left(\\frac{\\pi_{\\epsilon}({\\bf x})}{\\pi({\\bf x})}-1\\right)f({\\bf x})\\right]. %  \nBy Cauchy-Schwarz, the absolute value of this is upper-bounded by %\\displaystyle \\sqrt{\\mathop{\\bf E}\\left[\\left(\\frac{\\pi_{\\epsilon}({\\bf x})}{\\pi({\\bf x})}-1\\right)^2\\right]}\\cdot \\|f\\|_2, %  \nwhere %{\\|f\\|_2}% denotes %{\\sqrt{\\mathop{\\bf E}[f({\\bf x})^2]}}%. One easily checks that <a>%\\displaystyle \\mathop{\\bf E}\\left[\\left(\\frac{\\pi_{\\epsilon}({\\bf x})}{\\pi({\\bf x})}-1\\right)^2\\right] = \\mathop{\\bf E}\\left[\\frac{\\pi_{\\epsilon}({\\bf x})^2}{\\pi({\\bf x})^2}\\right] - 1, \\ \\ \\ \\ \\ (2)%  \n</a>and since %{\\pi_{\\epsilon}}% and %{\\pi}% are product distributions the RHS of [(2)](#eqndiff) is easy to compute. One can check explicitly that %\\displaystyle \\mathop{\\bf E}[\\pi_{\\epsilon}({\\bf x}_1)^2/\\pi({\\bf x}_1)^2] = 1+{\\epsilon}^2 %  \nand therefore [(2)](#eqndiff) is %{(1+{\\epsilon}^2)^n - 1}%. Naturally we will be considering %{{\\epsilon} \\ll 1/\\sqrt{n}}%, and in this regime the quantity is bounded by, say, %{4{\\epsilon}^2n}%. Hence we have shown %\\displaystyle |\\hat{f_{\\epsilon}}(\\emptyset) - \\hat{f}(\\emptyset)| \\leq 2{\\epsilon}\\sqrt{n} \\cdot \\|f\\|_2 %  \nand in particular if %{f}% has range %{\\{0,1\\}}% and mean %{\\mu}% (AKA %{\\delta}%) then <a>%\\displaystyle |\\hat{f_{\\epsilon}}(\\emptyset) - \\mu| \\leq 2 {\\epsilon} \\sqrt{n} \\cdot \\sqrt{\\mu}. \\ \\ \\ \\ \\ (3)%  \n</a>This is not very interesting unless %{2 {\\epsilon} \\sqrt{n} \\cdot \\sqrt{\\mu} \\leq \\mu}%, so let’s indeed assume %{{\\epsilon} \\leq \\sqrt{\\mu}/\\sqrt{n}}% and then we can also use %{\\hat{f_{\\epsilon}}(\\emptyset) \\leq 2 \\mu}%. We now these deductions in [(1)](#eqnmain). Note that the second factor on the RHS in [(1)](#eqnmain) is at most the square-root of %\\displaystyle \\sum_{S} \\hat{f_{\\epsilon}}(S)^2 = \\mathop{\\bf E}[f({\\bf y})^2] = \\mathop{\\bf E}[f({\\bf y})] = \\hat{f_{\\epsilon}}(\\emptyset) \\leq 2\\mu \\leq 4\\mu. %  \nAlso, using [(3)](#eqnmeans) for the LHS in [(1)](#eqnmain) we conclude <a>%\\displaystyle |\\mathop{\\bf E}[f({\\bf x})f({\\bf y})] - \\mu^2| \\leq 2\\mu^{3/2} \\cdot {\\epsilon}\\sqrt{n} + 2\\sqrt{\\mu} \\sqrt{\\mathbb{S}_{1-{\\epsilon}'}(f) - \\mu^2}, \\ \\ \\ \\ \\ (4)%  \n</a>where %\\displaystyle \\mathbb{S}_{1-{\\epsilon}'}(f) = \\sum_{S} \\hat{f}(S)^2(1-{\\epsilon}')^{|S|}. %  \nLet’s simply fix %{{\\epsilon} = (1/8)\\sqrt{\\mu}/\\sqrt{n}}% at this point. Doing some arithmetic, it follows that _if_ we can bound <a>%\\displaystyle \\mathbb{S}_{1-{\\epsilon}'}(f) - \\mu^2 \\leq \\mu^3/64\\ (?) \\ \\ \\ \\ \\ (5)%  \n</a>(AKA %{f}% is “uniform at scale %{{\\epsilon}' n}%” as Terry might say) then [(4)](#eqnfinal) implies %\\displaystyle \\mathop{\\bf E}[f({\\bf x})f({\\bf y})] \\geq \\mu^2/2\\. %  \nSo long as %{\\mathop{\\bf P}[{\\bf x} = {\\bf y}] < \\mu^2/2}% we’ve established existence of a Sperner pair (AKA non-degenerate combinatorial line). Since this probability is %{(1-{\\epsilon}/2)^n \\leq \\exp(-{\\epsilon} n/2) = \\exp(-\\Omega(\\sqrt{\\mu}\\sqrt{n}))}%, we’re done assuming <a>%\\displaystyle n \\geq O(\\log^2(1/\\mu)/\\mu). \\ \\ \\ \\ \\ (6)%  \n</a>Thus things come down to showing [(5)](#eqngoal). Now in general, there is absolutely no reason why this should be true. The idea, though, is that if it’s _not_ true then we can do a density increment. More precisely, it is very easy to show (one might credit this to an old result of Linial-Mansour-Nisan) that %{\\mathbb{S}_{1-{\\epsilon}'}(f)}% is precisely %{\\mathop{\\bf E}_{{\\bf V}}[\\mathop{\\bf E}[f|_{\\bf V}]^2]}%, where %{{\\bf V}}% is a “random restriction with wildcard probability %{{\\epsilon}'}%” (and the inner %{\\mathop{\\bf E}[\\cdot]}% is with respect to the uniform distribution). In other words, %{{\\bf V}}% is a combinatorial subspace formed by fixing each coordinate randomly with probability %{1 - {\\epsilon}'}% and leaving it “free” with probability %{{\\epsilon}'}%. Hence if [(5)](#eqngoal) _fails_ then we have %\\displaystyle \\mathop{\\bf E}_{{\\bf V}}[\\mathop{\\bf E}[f|_{\\bf V}]^2] \\geq \\mu^2 + \\mu^3/64\\. %  \nIn particular, since %{f}% is bounded it follows that %{\\mathop{\\bf E}[f|_{\\bf V}]^2 \\geq \\mu^2 + \\mu^3/128}% with probability at least %{\\mu^3/128}% over the choice of %{{\\bf V}}%. It’s also very unlikely that %{{\\bf V}}% will have fewer than, say, %{({\\epsilon}'/2)n}% wildcards; a large-deviation bound shows this probability is at most %{\\exp(-\\Omega({\\epsilon}' n))}%. Since %{{\\epsilon}' \\approx {\\epsilon} = (1/8)\\sqrt{\\mu}/\\sqrt{n}}%, by choosing the constant in [(6)](#eqnassume) suitably large we can make this large-deviation bound strictly less than %{\\mu^3/128}%. Thus we conclude that there is a positive probability of choosing some %{{\\bf V} = V}% which both has at least %{({\\epsilon}'/2)n = \\Omega(\\sqrt{\\mu}\\sqrt{n})}% free coordinates and also has %\\displaystyle \\mathop{\\bf E}[f|_V]^2 \\geq \\mu^2 + \\mu^3/128 \\Rightarrow \\mathop{\\bf E}[f|_V] \\geq \\mu + \\mu^2/500\\. %  \nI.e., we can achieve a density increment. If I’m not mistaken, this kind of density increment (gaining %{\\mu^2/C}% at the expense of going down to %{c \\sqrt{\\mu} \\sqrt{n}}% coordinates, with [(6)](#eqnassume) as the base case) will ultimately show that we need the initial density to be at least %{1/\\log \\log n}% (up to %{\\log \\log \\log n}% factors?) in order to win. Only a couple of exponentials off the truth <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> The incorrect quantitative aspect here isn’t quite the reason I feel this argument is “wrong”. Rather, I believe that no density increment should be necessary. (Actually, we probably know this is the case, by Sperner’s proof of Sperner.) In other words, I believe that %{\\mathop{\\bf E}[f({\\bf x})f({\\bf y})] \\geq \\Omega(\\mu^2)}% **for any** %{f}%, assuming %{{\\epsilon} \\ll \\sqrt{\\mu}/\\sqrt{n}}%."},{"username":"terence-tao","timestamp":"0006-02-20T05:00:00.000Z","contents":"Initial impressions of paper #1 Paper #1 outlines a proof of DHJ(3): given any %\\delta > 0%, that any subset A of %{}[3]^n = \\{0,1,2\\}^n% of density at least %\\delta% contains a combinatorial line if n is sufficiently large depending on %\\delta%. (The paper refers to combinatorial lines as “HJ sequences”, and %{}[3]^n% as %\\Omega_n%, but I am translating to be more compatible with the notation already used in other threads.) It proceeds by first reducing the problem to one concerning a family of measurable sets on a probability space, indexed by words, and for which there is a family of (non-commuting!) measure-preserving transformations relating these sets to each other. This reduction is performed in Proposition 3.1. A key point in this reduction is that a certain “stationarity” property is obtained on this family of sets. To obtain this stationarity, one needs a version of the Carlson-Simpson theorem, which seems to be an infinitary strengthening of the colouring Hales-Jewett theorem. (It looks vaguely reminiscent of Hindman’s theorem; presumably there is a relation.) With the above reductions, the game is now to show that a certain triple intersection of three sets has positive measure (Theorem B). This is not actually done in paper #1, but is covered in #2 or #3\\. In #1, the simpler (but still non-trivial) result is shown that all pairwise intersections of the three sets can simultaneously have positive measure (Theorem 4.1). (It would be interesting to see what the combinatorial analogue of this weaker statement is. Getting just one of the pairs to have positive measure sounds similar to the “DHJ(2.5)” I formulated in [my comment 130](http://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1853), which turns out to be equivalent to DHJ(2) and thus an easy consequence of Sperner’s theorem.) The proof of Theorem 4.1 is established from some soft functional analysis results about IP-systems of unitary operators in Section 5. In Section 6, there is a discussion of what is needed to establish the full strength of Theorem B, and I guess this is completed in #2. My initial plan is to go to Section 3 and start understanding the equivalence between the combinatorial problem and the ergodic theory one. There seems to be several stages to this equivalence; a relatively easy correspondence between a combinatorial problem and an ergodic problem without stationarity, and then the reduction to the stationary case which requires the Carlson-Simpson type theorems."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"Proposition 3.1 Proposition 3.1 of paper #1 equates the combinatorial statement DHJ(3) with various more ergodic-flavoured versions of the same statement. There are three main statements in this equivalence, (a), (b), and (c). (There are also minor variants (a*), (b*), (c*) of (a), (b), and (c), but I think we won’t need to focus on them too much.) DHJ(3)(a) is the combinatorial version of DHJ(3), i.e. for any %\\delta > 0% and for n sufficiently large depending on %\\delta%, every subset of %{}[3]^n% of density at least %\\delta% contains a combinatorial line. DHJ(3)(b) is a preliminary ergodic version of DHJ(3). Define a _%{}[3]^n%-system_ to be a collection of subsets %(E_w)_{w \\in [3]^n}% in some probability space %(X, {\\mathcal X}, \\mu)%, indexed by strings in %{}[3]^n%. **DHJ(3)(b)**: If %\\delta > 0% and n is sufficiently large depending on %\\delta%, and %(E_w)_{w \\in [3]^n}% is a %{}[3]^n%-system with all sets %E_x% having measure %\\mu(E_w)% greater than or equal to %\\delta%, then there exists a combinatorial line %w^0, w^1, w^2% such that %E_{w^0} \\cap E_{w^1} \\cap E_{w^2} \\neq \\emptyset%.) This can be stated in a probabilistic language: if A is a _random_ subset of %{}[3]^n% with the property that each element w of %{}[3]^n% belongs to A with probability at least %\\delta%, then with positive probability, A contains a combinatorial line. (Indeed, just take A to be the set of those %w \\in [3]^n% for which %E_w% holds.) To deduce DHJ(3)(b) from DHJ(3)(a), observe from linearity of expectation that the expected density of A is at least %\\delta%, and so A has density at least %\\delta/2% (say) with positive probability. To deduce DHJ(3)(a) from DHJ(3)(b), one can use random averaging arguments (as discussed for instance in [comment 460](http://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2120)). [Paper #1 uses a somewhat different derivation of this implication which I will skip here.] To recap that argument briefly: we pick a medium integer m between 1 and n, and pick a random m-dimensional subspace inside %{}[3]^n%. Pulling back the dense subset A of %{}[3]^n% to %{}[3]^m%, we get a random subset A’ of %{}[3]^m% with each point lying in A’ with probability at least %\\delta/2% (say), if n is large enough depending on m. The claim then follows. DHJ(3)(c) is DHJ(3)(b) with an additional stationarity hypothesis thrown in, and will be discussed subsequently."},{"username":"gowers","timestamp":"0002-02-20T05:00:00.000Z","contents":"602. Dear Terry, I think it’s a very interesting idea to have an online reading seminar. Of course, it requires us to do the reading, which is quite a big investment to make, and presumably why you envisage a sedate affair. I think what would be most useful to me is basically what you encourage: rather than reading the paper very carefully and then explaining bits I understand here, I would prefer to try to use this post and its associated comments to read the paper sloppily and present my very partial understanding, or even complete misunderstanding, in the hope that others will be able to say, “Yes, you’re getting there — what you need to grasp next is this,” or “No, that’s not quite it — if that were the case then so would this be but it clearly isn’t,” and things like that. So when I get a moment, I’ll try to skim-read Randall’s paper and give some kind of initial reaction to it. I haven’t yet looked at it at all, but my eventual aim would be to obtain some kind of non-ergodic understanding of the proof. (Here I mean non-ergodic in the shallow sense of trying to explain things without getting completely into the ergodic language, rather than in the deep sense of genuinely doing without ergodic theory.)"},{"username":"zugzwang-trebuchet","timestamp":"0004-02-20T05:00:00.000Z","contents":"603. Hi gowers,  \nWhy not work out the ergodic details here?"},{"username":"david-speyer","timestamp":"0008-02-20T05:00:00.000Z","contents":"604. I’m going to try to restate proposition 3.1 to see if I understand it. If I get it right, this will probably be redundant with commment 601 above, but if I get it wrong, it will be one of those naive quesitons Terry asked for. Prop. 3.1 says that six things are equivalent, called (a), (a*), (b), (b*), (c) and (c*). In each case, the unstarred version is a uniform variant on the starred version. So the starred version will say “If we have a sequence %A_n% where lim sup (some measure of the size of %A_n%)%>0% then…” while the unstarred version will say “For every %\\epsilon >0%, there is an integer %N% such that, if %n>N% and (some measure of the size of %A_n%) %> \\epsilon% then…”. We’re going to concentrate on the unstarred versions. (a) is what we’re trying to prove. For every %\\epsilon%, there is an %N% such that any subset of %{}[3]^N% of cardinatliy more that %\\epsilon 3^N% contains a combinatorial line. (b) says that we can’t break (a) by a random construction. Suppose we had some random process, like drawing ping pong balls out of a bag. The set of balls is called %X%, and the probaility that an individual ball %x \\in X% is drawn will be %\\mu(x)%. For each ball %x%, our process produces a particular subset of %{}[3]^n%, which I will denote %A(x)%. Confusingly (to me), H and K use the opposite indexing; for each %w \\in [3]^N%, they write %B_{w}% for the set of all %x% such that %w \\in A(x)%, but they don’t have a notation for %A(x)%. For fixed %\\epsilon%, if %N% is large enough, the following holds: is long as the probability of any particular element %w% of %{}[3]^N% being in %A(x)% is at least %\\epsilon%, then there will be some particular combiantorial line which occurs in %A(x)% with nonzero probablity. Some comments: Notice that (b)’s framework is general enough to destroy any probabilistic construction I might attempt. In particular, there is no hypothesis that the events “%w_1% is in %A(x)%” and “%w_2% is in %A(x)%” are independent in any way. The only kind of probabilistic construction I could imagine which would violate the hypotheses of (a) is a process which, although they produce sets of density %\\geq \\epsilon%, have much lower probability of including certain particular elements of %{}[3]^N%. However, the problem is so symmetric that I see no reason that this would help. Presumably, the proof of (b)–>(a) is to make this idea rigorous. Finally, I described things above with a finite collection of ping pong balls, each with a positive probability. H and K phrase things in terms of a general measure space. I assume this generality will be helpful, although it seems pointless so far. Whew, that’s long! I’ll save condition (c) for later, asI need to get going now."},{"username":"terence-tao","timestamp":"2011-02-20T05:00:00.000Z","contents":"605. Dear Tim: Well, I don’t really have an organised plan as to where this will go, but I would like to get as many contributions (from as many backgrounds) as possible, to take full advantage of [Metcalfe’s law](http://en.wikipedia.org/wiki/Metcalfe%27s_law). So if some people read slowly and carefully, others comment loosely and freely, and yet others translate from ergodic theory to combinatorics or back, I think that will be an instructive mix. (One advantage of an internet-based seminar, as opposed to a physical one, is that one can have multiple concurrent discussions on different aspects of a paper without too much confusion.) Incidentally, the McCutcheon notes rely heavily on paper #1, so I think it is best to start with that paper first, though it may still be valuable to skim through #2 at this preliminary stage nevertheless. Dear David: Welcome! I’m glad you took the initiative to jump in (there is no roster or anything for this seminar :-). Please don’t worry about redundancy – in fact, I think the more perspectives we have on an individual component of the argument, the better. Regarding the implication of (b) from (a), I can give you a simpler version of it, in which we are not looking for combinatorial lines in %{}[3]^n%, but rather algebraic lines %(x,x+r,x+2r)% in %{}({\\Bbb Z}/3{\\Bbb Z})^n%; the analogue of DHJ(3) here is called “Roth’s theorem in %({\\Bbb Z}/3{\\Bbb Z})^n%“. Given a (deterministic) set A in %({\\Bbb Z}/3{\\Bbb Z})^n% of density at least %\\delta%, we can define a random set A(x) in %({\\Bbb Z}/3{\\Bbb Z})^n% by setting %A(x) := A+x% where x is a random element of %({\\Bbb Z}/3{\\Bbb Z})^n%. Note that each element w of %({\\Bbb Z}/3{\\Bbb Z})^n% will lie in A(x) with probability at least %\\delta%, and if A(x) contains an algebraic line with positive probability, then A certainly contains an algebraic line also. So in this case the deduction of (a) from (b) is pretty simple. This doesn’t work in the DHJ setting because combinatorial lines are not translation invariant, but one can do a somewhat different trick, embedding %{}[3]^m% randomly in %{}[3]^n% in a combinatorial line-preserving manner, rather than by translating %{}[3]^n% randomly back to %{}[3]^n%."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"Stationarity In Proposition 3.1(b), the %{}[3]^n%-system %(E_w)_{w \\in [3]^n}% (which is called %(B_w)_{w \\in \\Omega_n}% in the notation of #1, and is equivalent to the notion of a random subset A(x) of %{}[3]^n% as discussed in Speyer.604), has no structure other than having every set %E_w% be large. But the deepest part of Proposition 3.1 is a reduction to a special type of %{}[3]^n%-system, namely a _stationary_ %{}[3]^n%-system. This is a system where the sets %E_w \\subset X% take the form %E_{w} = U_w^{-1} E_\\emptyset%, (1) where %U_{w_1 \\ldots w_n} := U_1^{w_1} U_2^{w_2} \\ldots U_n^{w_n}%, (2) %E_\\emptyset \\subset X% is a set, and the %U_i^j: X \\to X% for %1 \\leq i \\leq n% and %j=0,1,2% are measure-preserving invertible transformations on X. [These transformations are _not_ assumed to commute with each other, and this is apparently a significant technical difficulty in the rest of the argument.] Clearly, if (1) holds, and %E_\\emptyset% has measure at least %\\delta%, then all the %E_w% have measure at least %\\delta%. So DHJ(3)(b) certainly implies **DHJ(3)(c):** If %\\delta > 0% and n is sufficiently large depending on %\\delta%, and %(E_w)_{w \\in [3]^n}% is a stationary %{}[3]^n%-system with %E_\\emptyset% having measure at least %\\delta%, then there exists a combinatorial line %(w^0, w^1, w^2)% such that %E_{w^0} \\cap E_{w^1} \\cap E_{w^2}% has positive measure. The non-trivial fact is that DHJ(3)(c) also implies DHJ(3)(b). I guess I’ll try to discuss that in future comments. It is tempting to try to describe stationarity in terms of a measure-preserving action on X by the free group %F_3% on three generators %e^0, e^1, e^2%, but this isn’t quite right, because the %U_i^j% are not constant in i. But if one adds a discrete time variable i, thus replacing X with %{\\Bbb Z} \\times X%, then one can recover a measure-preserving action %(T_g: {\\Bbb Z} \\times X \\to {\\Bbb Z} \\times X)_{g \\in F_3}% of %F_3%, with each generator %e^j%, %j=0,1,2% of %F_3% mapping %(i,x)% to %(i-1, U_i^j x)% for %i \\in {\\Bbb Z}% and %x \\in X% (here we have to define %U_i^j% for non-positive i in some arbitrary fashion, e.g. setting these transformations to be the identity). With this action, and embedding %{}[3]^n% inside %F_3% by identifying a string %w_1 \\ldots w_n% with the group element %e^{w_1} \\ldots e^{w_n}%, one can rewrite (1), (2) as %\\{n\\} \\times E_w = T_w^{-1} ( \\{0\\} \\times E_\\emptyset )% (3) and so the task is to show that there exists a combinatorial line %(w^0, w^1, w^2)% such that %T_{w^0}^{-1} ( \\{0\\} \\times E_\\emptyset ) \\cap T_{w^1}^{-1} ( \\{0\\} \\times E_\\emptyset ) \\cap T_{w^2}^{-1} ( \\{0\\} \\times E_\\emptyset ) \\neq \\emptyset%. (4) At this stage I do not know whether this group action perspective is useful, although it does make the problem look a little bit like simpler ergodic recurrence theorems. For instance, Roth’s theorem is equivalent to the assertion that if T acts in a measure-preserving way on a probability space X, then for every set E of positive measure, there exists an arithmetic progression %(w^0, w^1, w^2)% of integers such that %(T^{w^0})^{-1}(E) \\cap (T^{w^1})^{-1}(E) \\cap (T^{w^2})^{-1}(E) \\neq \\emptyset%. (5)"},{"username":"kristal-cantwell","timestamp":"0001-02-20T05:00:00.000Z","contents":"I would be interested in this seminar. I have downloaded the three papers and will be looking at them in the next few days along with the comments here."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"Stationarity cont. Welcome, Kristal! Feel free to jump in and contribute with any questions, comments, or suggestions, even if (or especially if) they overlap with those already in this post. One small comment, continuing 606: it appears that the stationarity gives quite strong constraints on the %E_w%. Most obviously, they are all forced to have the same measure, but there are other constraints too. For instance, if n=2, then %E_{00} \\cap E_{10}% has to have the same measure as %E_{02} \\cap E_{12}%, since the latter set can be obtained from the former set by applying the measure-preserving %(U_2^2)^{-1} U_2^0%. To put it another way, the random set %A(x) \\subset [3]^2% is just as likely to contain the pair (0,0), (1,0) as it is to contain the pair (0,2), (1,2). More generally, stationarity seems to be saying that if one looks at a slice %A(x) \\cap ([3]^{n-m} \\times \\{ w_{n-m+1} \\ldots w_n \\})% of %A(x)% formed by freezing the last m positions of A(x), that the distribution of that slice (or more precisely, the identification of that slice with a subset of %{}[3]^{n-m}%) is in fact independent of the choice of the frozen coordinates  \n%w_{n-m+1},\\ldots,w_n%. (This would explain the terminology “stationary”.) I do not know yet whether this property already captures the full strength of stationarity, or whether stationarity in fact says something more."},{"username":"terence-tao","timestamp":"0002-02-20T05:00:00.000Z","contents":"Symmetry breaking I wanted to repeat a remark of [Austin.54](http://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1729) here: the original formulation DHJ(3)(a) of the problem, together with its probabilistic variant DHJ(3)(b), are invariant under permutation of the indices in [n], but the notion of stationarity is not invariant (being dependent on the ordering of the indices), and so DHJ(3)(c) has “broken” the permutation symmetry. There is nothing particularly harmful about this – the standard proofs of colouring Hales-Jewett also break symmetry, for instance – but it seems like a noteworthy point nonetheless."},{"username":"david-speyer","timestamp":"0006-02-20T05:00:00.000Z","contents":"My first dumb question: In (b) and (c), would it add any generality to allow the measure space %X% to depend on %n%? When I think of sampling methods I might use, it seems natural to me to use different sample spaces %X_n% for different %n%. But I suspect that I could use %\\prod_{n=1}^{\\infty} X_n% as my sample space, and thus reduce to H and K’s set up."},{"username":"randall","timestamp":"0008-02-20T05:00:00.000Z","contents":"Fixed some typos I found on an airplane today and put a better version up. The most potentially confusing of these were in Lemma 1 (continued), the first display on page 4 and in the definition of \\delta on page 6\\. There may be others so please speak up if you see anything that looks incongruous. The notion of stationarity is more familiar for (linear) processes. If (X_i) are random variables taking values say in {a,b,…,z}, then we call (X_i) a process and we say that the process is stationary if the probability that X_1X_2X_3=”cat” is the same as the probability that X_17X_18X_19=”cat” (for all cats and all shifts). In the current setup, you can view the sets B_w as random variables X_w taking values in {0,1}. Stationarity says: take any finite set of words. Call that set I. The probability of seeing a certain pattern of 0s and 1s in the random variables X_w, w in I, is exactly the same as the probability of seeing the same pattern of 0s and 1s in the random variables X_wv, w in I, where v is any word. Here “v” plays the role 16 did in the cat example, the pattern of 0s and 1s plays the role of “cat”, and I plays the role of {1,2,3}. I can certainly sympathize with the sentiment that it seems quite unclear what 3.1 can possibly do to help. It gives you some measure preserving transformations, but nothing resembling honest ergodic theory, really, because although there are measure preserving transformations around, they don’t even constitute an action (as Terry pointed out). I guess the thing to do at this early stage is just have faith…."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"the measure space X David, I think you’re right, the actual choice of space X is more or less irrelevant. In a lot of these problems one can take the Cantor space %\\{0,1\\}^{\\Bbb Z}% (or the unit interval) as a universal space (note that any countably generated probability space can be lifted up to either of these two spaces). One of the philosophies of probability theory, by the way, is to try to hide the underlying space X as much as possible; thus one talks about events E and their probabilities, random variables and their expectations and correlations, etc. without explicit mention of the probability space and probability measure, etc. It’s like studying a manifold or variety through its ring of functions rather than by looking at its constituent points. It has the advantage that one can concatenate any number of random processes together (e.g. flip infinitely many coins, and then pull balls out of urns infinitely often, etc.) without having to think about what happens to the underlying probability space. In ergodic theory, though, one often takes almost the opposite view: one starts with an abstract measure space X (with some groups acting on it) and then creates some factors Y of that space which often have a very explicit algebraic structure (e.g. they are the homogeneous space for some Lie group). One then studies the geometry and dynamics of that space Y from a very concrete point of view, thus moving from the category of abstract measure spaces and measure preserving transformations to something much more algebraic."},{"username":"kristal-cantwell","timestamp":"2011-02-20T05:00:00.000Z","contents":"Hindman’s theorem is a special case of one of their versions of the Carlson-Simpson theorem according to the paper."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"Stationarity II DHJ(3)(c) finds combinatorial lines in stationary %{}[3]^n%-systems %(U_w^{-1} E)_{w \\in [3]^n}%. It turns out to be convenient to eliminate the role of n, and instead work with stationary %{}[3]^\\omega%-systems %(U_w^{-1} E)_{w \\in [3]^\\omega}%, where %{}[3]^\\omega := \\bigcup_{n=0}^\\infty [3]^n% is the space of all words of finite length with alphabet {0,1,2}. (Paper #1 uses the notation %\\Omega^f% for %{}[3]^\\omega%.) It is clear that DHJ(3)(c) implies **DHJ(3)(c*)**: If %(U_w^{-1} E)_{w \\in [3]^\\omega}% is a stationary %{}[3]^\\omega%-system, with E having positive measure, then there is a combinatorial line %(w^0,w^1,w^2)% such that %U_{w^0}^{-1} E \\cap U_{w^1}^{-1} E \\cap U_{w^2}^{-1} E \\neq \\emptyset%. In a similar spirit, DHJ(3)(b) is easily seen to be equivalent to **DHJ(3)(b*)**: If %(E_w)_{w \\in [3]^\\omega}% is a %{}[3]^\\omega%-system, with the %E_w% having measure uniformly bounded away from zero, then there is a combinatorial line %(w^0,w^1,w^2)% such that %E_{w^0} \\cap E_{w^1} \\cap E_{w^2} \\neq \\emptyset%. To finish the proof of Proposition 3.1, we need to deduce DHJ(3)(b*) from DHJ(3)(c*). Broadly speaking, the idea is to start with a %{}[3]^\\omega%-system and repeatedly use Carlson-Simpson-type Ramsey theorems to pass to subsystems in which become increasingly stationary. A %{}[3]^\\omega%-system can be viewed as a collection of random sets %A_n(x) \\subset [3]^n%, one for each n. As noted already in 610, stationarity implies that %A_n(x) \\times \\{v\\}% has the same distribution as %A_{n+m}(x) \\cap ([3]^n \\times \\{v\\})% for all n and all words %v \\in [3]^m%, where of course we identify %{}[3]^{n+m}% with %{}[3]^n \\times [3]^m% in the obvious manner."},{"username":"terence-tao","timestamp":"2010-02-20T05:00:00.000Z","contents":"A possible simplification It appears to me that one can in fact get stationarity for free (and avoid invoking the Carlson-Simpson theorem here). The basic observation is that when one deduces DHJ(3)(b) from DHJ(3)(a) by using a random m-dimensional subspace inside %{}[3]^n%, the random set obtained in this fashion already is essentially stationary by construction. I’ll write up the details on the wiki and report back here when I’m done. (This does not completely eliminate the need for Carlson-Simpson, though, as such results are also used later in the paper.)"},{"username":"ryan-odonnell","timestamp":"0009-02-20T05:00:00.000Z","contents":"Let me try to recap Terry’s points, passing from DHJ(3)(a) to DHJ(3)(c), in my own language. Let %A \\subseteq \\{0,1,2\\}^n% be a given set of density %\\delta%. Let %m = m(\\delta, n)% be some moderate integer; perhaps %n^{1/3}% when %\\delta% is a “constant”. Let %S \\subseteq [n]% be a random subset of cardinality %m%. Imagine choosing a “random restriction” %x% to the coordinates %\\overline{S} = [n] \\setminus S%. I.e., %x \\in \\{0,1,2\\}^{\\overline{S}}% is chosen uniformly at random. Then %A% induces a probability distribution on subsets of %\\{0,1,2\\}^S%. Call the induced subset %A_x \\subseteq \\{0,1,2\\}^S%. Let %y% denote a string in %\\{0,1,2\\}^S%. I will use the notation “%(x,y)%” for the string in %\\{0,1,2\\}^n% gotten by appropriately piecing %x% and %y% together. We claim that for *each* string %y \\in \\{0,1,2\\}^S%, the probability (over choice of %x%) that %y \\in A_x% is at least %\\delta/2%. On one hand, note that %(x,y)% is *not* a uniformly distributed string in %\\{0,1,2\\}^n%. For example, if %y% is the all-0’s string, then the distribution on %(x,y)% is slightly biased towards those strings with more 0’s. Still if %m% is “small enough” then the distribution will be very close to uniform (except around the “extreme slices” of %\\{0,1,2\\}^{n}%, but %A% has almost all its density outside these slices anyway). And this should prove the claim with a minimum of calculations. At this point we’re at DHJ(3)(b); on to DHJ(3)(c). “Stationarity” here would seem to be that for any further restriction %z% to coordinates %S' \\subseteq S%, the resulting distribution on the set %A_{(x,z)}% (a subset of %\\{0,1,2\\}^{S \\setminus S'}%) is the same. Again, this is not true, but it should be “almost” true. And it seems like we’re only going for “almost stationarity”, not “stationarity” anyway. In fact, it seems to me that whereas “(almost) stationarity” importantly involves an *ordering* on the coordinates %S%, it seems this argument doesn’t really. I.e., %S'% need not be a “suffix” of %S% or anything."},{"username":"terence-tao","timestamp":"2010-02-20T05:00:00.000Z","contents":"616. Thanks Ryan, this fleshes out what I was trying to say. I’ve started a running “combinatorial translation” of paper #1 on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Furstenberg-Katznelson_argument](http://michaelnielsen.org/polymath1/index.php?title=Furstenberg-Katznelson_argument) It’s likely to evolve in unstable ways over time (in particular, I am likely to overhaul the notation as I understand more of the big picture) but hopefully will eventually settle down. As you note, the random sampling construction gives additional stationarity properties beyond those used in Paper #1\\. In particular, the random sets %A_m \\subset [3]^m% are also stationary wrt permutations of the index set [m]. It’s not clear whether this additional symmetry could be useful though (it might even be harmful, because later parts of the proof may need to break this symmetry and so trying to hold on to it may be counterproductive)."},{"username":"terence-tao","timestamp":"2010-02-20T05:00:00.000Z","contents":"Recap I’m now going to try to recapitulate the previous discussion in light of the new simplification, thus moving away from the presentation in Paper #1\\. (I guess I am changing the rules a little bit here from a “reading seminar” to a “revisionist seminar”, but I said that the rules would be made up as we went along. :-) ) I will try to keep the wiki page above current with the latest revision of thinking. Let %A^{(n)}% be a dense line-free subset of %[3]^n%, where n is very large. For every small integer m, we can then form a random line-free subset %A^{(n)}_m \\subset [3]^m% by randomly embedding %[3]^m% inside %[3]^n% (by picking m random positions for the wildcards, and then picking the frozen positions randomly), and then pulling %A^{(n)}% back by this map. One can think of %A^{(n)}_m% as a random variable taking values in the power set %X_m := 2^{[3]^m}%, so its probability distribution is a probability measure %\\mu^{(n)}_m% on %X_m%. By construction, %A^{(n)}_m% is stationary with respect to the permutation group %S_m% of [m], i.e. %(X_m,\\mu^{(n)}_m)% is invariant wrt the action of %S_m%. Also, for any word %w \\in [3]^r%, the _slicing map_ %U_{m+r}^w: X_{m+r} \\to X_m% defined by %U_{m+r}^w(A_{m+r}) := \\{ v \\in [3]^m: vw \\in A_{m+r} \\}% is _almost stationary_ in the sense that for fixed m, r, and w, the random variables %A^{(n)}_m% and %U_{m+r}^w(A^{(n)}_{m+r})% have asymptotically the same distribution in the limit %n \\to \\infty%. Taking a subsequence limit as %n \\to \\infty% (and using the Helly selection principle, a.k.a. the vague sequential compactness of probability measures), we can then find random line-free sets %A_m \\subset [3]^m% (or equivalently, a probability measure %\\mu_m% on %X_m%) which is stationary with respect to the permutation groups %S_m% acting on %X_m%, and also with respect to the slicing maps %U_{m+r}^w: X_{m+r} \\to X_m%. Also, if the %A^{(n)}% all have density at least %\\delta%, then so do the %A_m%: in particular, %A_0% will be non-empty with probability at least %\\delta%, which implies that for any %v \\in [3]^m%, that %v\\in A_m% with probability at least %\\delta% also. This is all very well and good, but there is one minor technical problem which will cause difficulty in later parts of paper #1: the slicing maps %U_{m+r}^w: X_{m+r} \\to X_m% are not invertible. We would like to upgrade them to be invertible (thus upgrading the semigroupoid of transformations %U_{m+r}^w% to a groupoid), thus giving us DHJ(3)(c*) in its full strength. This can be done, but requires extending the measure spaces %X_m% a bit (or, in probabilistic language, adding a whole bunch of “dice” or “random number generators”). I plan to talk about this in a later comment."},{"username":"terence-tao","timestamp":"2011-02-20T05:00:00.000Z","contents":"Inversion of maps There is a simple lemma that says that any finite probability-preserving surjective map can be inverted if one throws in a random number generator that draws uniformly from [0,1]. More precisely: **Lemma** Let %U: X \\to Y% be a surjective map from one finite probability space to another which preserves the probability measure (in the sense that U pushes the probability measure on X forward to the probability measure on Y). Then one can lift this map to an _invertible_ measure-preserving map %\\tilde U: X \\times[0,1] \\to Y \\times[0,1]%, with product measure. I prove this lemma in the wiki notes, but let’s just illustrate this with the simplest example: the non-injective map from the two-element space {a,b} (with probability 1/2 of each) to the one-element space {c}. One invertible lift of this map is the map %\\tilde U: \\{a,b\\} \\times [0,1] \\to \\{c\\} \\times [0,1]% which maps (a,t) to (c,t/2) and (b,t) to (c,(t+1)/2). Many other choices are possible; this lift is not unique. Applying this lift to all of the generators %U_{m+1}^0, U_{m+1}^1, U_{m+1}^2: X_{m+1} \\to X_m%, we can now find probability spaces %\\tilde X_m := X_m \\times [0,1]% and invertible measure-preserving maps %\\tilde U_{m+1}^i: \\tilde X_{m+1} \\to \\tilde X_m% that commute with the slicing maps %U_{m+1}^i: X_{m+1} \\to X_m%. Concatenating these maps then give us more invertible measure-preserving maps %\\tilde U_{m+r}^w: \\tilde X_{m+r} \\to \\tilde X_m% for all words w of length r. We are now basically in the situation of DHJ(3)(c*), which the rest of Paper #1 aims to prove. (It’s slightly different because we’ve “graded” the underlying probability space X, replacing it with a sequence %X_m% of probability spaces, but I don’t think this will make much of a difference.) [Remark: the lemma here is my interpretation of Lemma 3.2 in Paper #1; the statement there is false as stated, but can be repaired by throwing in the [0,1] as is done above.]"},{"username":"terence-tao","timestamp":"2012-02-20T05:00:00.000Z","contents":"619. A frivolous comment, for the combinatorially minded: the Lemma in 618 is in some sense a continuous version of (a special case of) Hall’s marriage theorem, and serves much the same purpose (viz. conjuring up a bunch of artificial but useful perfect matchings out of thin air)."},{"username":"ryan-odonnell","timestamp":"0002-02-20T05:00:00.000Z","contents":"Inversion of maps. I’m getting slightly lost in notation and also slightly lost because I don’t quite know where this is going. However it seems to me that we might not eventually need a completely general Lemma like the one in Terry.618\\. Some natural randomized-inverse map might work in our context; e.g., tack on a random substring from %{}[3]^r%."},{"username":"terence-tao","timestamp":"0002-02-20T05:00:00.000Z","contents":"Inversion of maps Sorry about the notation shifting – I also don’t fully know where we are going either, and this I think is reflected in the notation… We’re not inverting a map from %[3]^{m+r}% to %[3]^m%, but rather a map from %2^{[3]^{m+r}}% to %2^{[3]^m}% (which, I guess, is induced by an embedding of %[3]^m% in %[3]^{m+r}%). Given a word w of length r, and a set %A_{m+r} \\subset [3]^{m+r}%, we have the slice %U_{m+r}^w(A_{m+r}) \\subset [3]^m%, defined as %U_{m+r}^w(A_{m+r}) := \\{ x \\in A_m: xw \\in A_{m+r} \\}%. Stationarity tells us that if we take the random set %A_{m+r} \\subset [3]^{m+r}% and look at a slice %U_{m+r}^w(A_{m+r})% of it (think of w as fixed), one gets back something with the same distribution as the random set %A_m \\subset [3]^m%. But one cannot initially invert this procedure: starting with the random set %A_m \\subset [3]^m%, one cannot _deterministically_ lift it back to a set in %[3]^{m+r}% that has the same distribution as %A_{m+r}% (and which agrees with to %A_m% on the slice) – there is not enough entropy in the domain and too much in the range. But one can do this once one has a random number generator, which I am modeling by a uniform distribution on [0,1] – this gives us an infinite amount of entropy on both sides and removes the obstruction to invertibility, as per “Hilbert’s hotel”. (One has to take care that one only performs “reversible computations” in order to maintain invertibility, but this is a minor technical detail.)"},{"username":"ryan-odonnell","timestamp":"0008-02-20T05:00:00.000Z","contents":"Understood, I’m just thinking that since we got to %{}[3]^m% in the first place by making a large random restriction, perhaps we can “remember” this and “unrestrict” %r% coordinates later. Just guessing… I’m probably being too speculative right now. Best to see where the next parts of the arguments go."},{"username":"terence-tao","timestamp":"2011-02-20T05:00:00.000Z","contents":"Oh, that’s an interesting idea. The artificial nature of the transformations used in the Furstenberg-Katznelson argument bothered me a bit, and this would be a much more natural way to do it. I don’t quite see how to set it up so that everything is completely reversible (i.e. the relevant maps %U_{m+r}^w% are invertible), so that entropy is neither created nor destroyed, but I can imagine that it is possible. I’ve read a little further into the paper (an interesting experiment, reading a paper collaboratively a small fragment at a time). With the above revisionist notation, we now have a sequence %(X_m,\\mu_m)% of probability spaces and invertible measure-preserving maps %U_{m+r}^w: X_{m+r} \\to X_m% for all words w, obeying a semigroup law. The claim is now that for every set %E \\subset X_0% of positive measure, that there exists a combinatorial line %w^0,w^1,w^2% in some cube %[3]^n% for some n such that the triple intersection of %(U_n^{w^0})^{-1}(E)%, %(U_n^{w^1})^{-1}(E)%, and %(U_n^{w^2})^{-1}(E)% has positive measure (or equivalently, that the random set %A_n \\subset [3]^n% contains the line %w^0,w^1,w^2% with positive probability). In Paper #1, a somewhat weaker result is proven, namely there exists a line %w^0, w^1, w^2%, such that the three _pairwise_ intersections of %(U_n^{w^0})^{-1}(E)%, %(U_n^{w^1})^{-1}(E)%, and %(U_n^{w^2})^{-1}(E)% have positive measure (thus the random set %A_n% will contain any two of the points on this line with positive probability). The notes in #2 finish the job to get all three points on the line. This weaker statement is already not obvious to me combinatorially – it’s some sort of triple version of the DHJ(2.5) statement we discussed back around 130 or so. (I think it is asserting the existence of a positive r for any dense %A \\subset [3]^n% with r wildcards such that for each ij=01,12,20, there exists a combinatorial line which intersects A in the i and j position.) I think I’ll ask it at the 500 thread to see if anyone can come up with a combinatorial proof."},{"username":"terence-tao","timestamp":"0004-02-20T05:00:00.000Z","contents":"Compactification of the string space I’ve scanned through the rest of the paper, and it seems like one has no choice but to start understanding the statement of the Carlson-Simpson theorem (and its variants) before progressing much further, although one can at least take these statements as “black boxes”. This theorem is an infinite-dimensional version of the colouring Hales-Jewett theorem, but to state it properly, we need some notation. I’m using %[3]^n := \\{0,1,2\\}^n% for the strings of length n, and %[3]^\\omega := \\bigcup_{n=1}^\\infty% for the finite length strings. We can give %[3]^\\omega% a “3-adic” metric by defining the distance between two distinct strings x, y (possibly of different length) to be %d(x,y) := 3^{-l}%, where l is the first digit at which x and y differ (or terminate). The metric space %[3]^\\omega% is totally bounded but incomplete. One can complete it by attaching the space %[3]^\\infty% of infinite strings. The space %[3]^\\omega \\cup [3]^\\infty% is totally bounded and complete, with %[3]^\\omega% as a dense subspace, thus %[3]^\\omega \\cup [3]^\\infty% is a compactification of %[3]^\\omega%. (Roughly, one can think of %[3]^\\omega% as being like the terminating decimals base 3 in %[0,1]%, and %[3]^\\omega \\cup [3]^\\infty% as being like the entire interval %[0,1]%.) [Paper #1 uses %\\Omega_n, \\Omega^f, \\Omega% in place of %[3]^n, [3]^\\omega, [3]^\\infty%.] Let X be a compact metric space. Observe that a map %F: [3]^\\omega \\to X% is uniformly continuous if and only if it extends to a continuous map %F: [3]^\\omega \\cup [3]^\\infty \\to X%. One can think of a uniformly continuous F as an operation that takes a string as input and spits out a point in X as output, but is increasingly insensitive to fluctuations in the %n^{th}% and higher digits as %n \\to \\infty%. Of course, not every map on %[3]^{\\omega}% is continuous. (A good example of a discontinuous map: the map %F: [3]^{\\omega} \\to \\{0,1\\}% that counts the parity of the number of 1s in the string.) But the Carlson-Simpson theorem is an assertion that every map F from %[3]^\\omega% to a compact set (or a finite set) becomes uniformly continuous when restricted to an “infinite-dimensional combinatorial subspace”. This allows one to extend F at some key points %\\omega% of %[3]^\\infty%. The proof is going to proceed by taking the shift maps %\\tilde U^{w^i}_r% (viewed as unitary operators) and taking a suitable limit (in the weak operator topology) as %w^i \\to \\omega% and working with the limiting operators (which turn out to be orthogonal projections). I don’t know yet why this is going to be a useful thing to do. In a subsequent comment I will state the Carlson-Simpson theorem properly, both in the finite colouring version (which is the standard formulation) and in the compact version (which is the version preferred by paper #1)."},{"username":"ryan-odonnell","timestamp":"0001-02-20T05:00:00.000Z","contents":"In the analysis of boolean functions there is general intuition of continuous/measurable functions vs. discontinuous/nonmeasurable functions. Roughly, the former are those functions which are “asymptotically noise stable” (in the sense of Benjamini-Kalai-Schramm: [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.2927](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.2927) ) or have “analogue” functions on Gaussian space or the sphere. I wonder if this viewpoint will be helpful."},{"username":"terence-tao","timestamp":"0002-02-20T05:00:00.000Z","contents":"Colouring theorems I: finitary theorems In this post and the next I want to describe a square of four Ramsey theorems: two finitary theorems, and two infinitary theorems. It is the latter which is used in Paper #1. The first finitary theorem is the **colouring Hales-Jewett theorem**. The k=3 version of this theorem asserts that if [3]^n is partitioned into c colour classes, and n is sufficiently large depending on c, then one of the colour classes contains a combinatorial line. Iterating this theorem, we conclude that [3]^n is partitioned into c colour classes, and n is sufficiently large depending on c, m, then one of the colour classes contains an m-dimensional combinatorial subspace. The second finitary theorem is the **Graham-Rothschild theorem**. The k=3 version of this theorem asserts that if, instead of colouring the vertices of [3]^n as in Hales-Jewett, one colours the _combinatorial lines_ in [3]^n, partitioning the space of all such lines into c colours, then (if n is sufficiently large depending on c, m) one can find an m-dimensional combinatorial subspace in which all lines in that space have the same colour. Recall that a combinatorial line can be viewed as a string of 0s, 1s, 2s, and a wildcard *, with the wildcard appearing at least once (e.g. 01*2* is a combinatorial line). Identifying the wildcard with the symbol 3, we can thus view a combinatorial line as a point in %[4]^n% containing at least one 4\\. Thus one can rephrase the k=3 Graham-Rothschild theorem in another way: **Graham-Rothschild theorem**. If the vertices of [4]^n are partitioned into c colour classes, then (if n is sufficiently large depending on m, c) there is an m-dimensional subspace (with none of the fixed coordinates equal to 3) such that all points in this subspace (with at least one coordinate equal to 3) have the same colour. Written this way, Graham-Rothschild looks a lot like colouring Hales-Jewett, and it is in fact possible to deduce the latter from the former (the precise deduction escapes me right now, though). The k=1 version of the Graham-Rothschild theorem implies Folkman’s theorem: if the positive integers are finitely coloured, then for any m, one can find m positive integers such that all finite sums of those integers (other than the trivial sum 0) have the same colour. This can be shown by considering the colouring on %[2]^n \\backslash \\{0\\}% that colours a vertex by the colour given to the number of 1s on that vertex. In the next comment I describe the infinitary strengthenings of the colouring Hales-Jewett and the Graham-Rothschild theorems, which are the Carlson-Simpson theorem and Carlson’s theorem respectively."},{"username":"terence-tao","timestamp":"0003-02-20T05:00:00.000Z","contents":"Colouring theorems II. Infinitary theorems The infinitary strengthening of the Hales-Jewett theorem is the Carlson-Simpson theorem. To define it, I need some notation. Define an _infinite-dimensional combinatorial subspace_ of %[3]^\\omega% to be an object indexed by an infinite string of 0s, 1s, 2s, and an infinite number of wildcards %*_1, *_2, *_3, \\ldots%, where each wildcard %*_i% appears in a non-empty consecutive block %I_i%, with each %I_i% appearing to the left of %I_{i+1}%. For instance, the following string describes an infinite-dimensional combinatorial subspace: %2110*_1 *_1 03012 *_2 *_2 *_2 2221111 *_3 201 *_4 *_4 \\ldots% This gives an embedding of %[3]^\\omega% into %[3]^\\omega% by mapping any n-digit string into the string formed from the above infinite string by substituting these digits into the first n wildcards %*_1,\\ldots,*_n%, and then truncating everything after %I_n%. For instance, in the above example, 20 maps to %21102203012000%. **Carlson-Simpson theorem** If %[3]^\\omega% is finitely coloured, then one of the colour classes contains an infinite-dimensional combinatorial subspace. This theorem implies the colouring Hales-Jewett theorem but is significantly stronger, in particular it does not seem to be deducible solely from the colouring Hales-Jewett theorem (much as the infinite pigeonhole principle cannot be deduced from its finite counterpart). It implies a compact version: **Carlson-Simpson theorem, compact version** If %F: [3]^\\omega \\to X% is a map from %[3]^\\omega% into a compact metric space X, then there exists a infinite-dimensional combinatorial subspace %\\phi: [3]^\\omega \\to [3]^\\omega% such that %F \\circ \\phi: [3]^\\omega \\to X% is uniformly continuous (i.e. it extends continuously to the compactification %[3]^\\omega \\cup [3]^\\infty%). It is not hard to see that the two versions are equivalent (one has to cover the compact metric space by an increasingly fine mesh of small balls, which can then be used to colour %[3]^\\omega%). In paper #1 it is remarked that the compact case is in fact slightly easier to prove than the finite colouring case (I suppose that the induction is somehow cleaner.) In a similar spirit, the Graham-Rothschild theorem has the following infinitary generalisation: **Carlson’s theorem** Suppose that the combinatorial lines in %[3]^\\omega% are finitely coloured. Then there is an infinite-dimensional combinatorial subspace on which all lines have the same colour. Or equivalently: **Carlson’s theorem** Suppose that %[4]^\\omega% is finitely coloured. Then there is an infinite-dimensional combinatorial subspace with no fixed coordinate equal to 3, such that any element of this space with at least one 3 has the same colour. Again, there is a compact version: **Carlson’s theorem** Suppose that %F: [4]^\\omega \\to X% is a map into a compact metric space X. Then there is an infinite-dimensional combinatorial subspace with no fixed coordinate equal to 3, such that the restriction of F to this space is uniformly continuous on those points with at least one 3. As I understand it, Carlson’s theorem was discovered independently by Carlson and by Furstenberg-Katznelson. The k=1 case of Carlson’s theorem is essentially Hindman’s theorem (the infinitary generalisation of Folkman’s theorem). Given that the cleanest proof of Hindman’s theorem involves idempotent ultrafilters, I would imagine that this is how Carlson’s theorem is also proved. Needless to say, such theorems are not easily finitisable."},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Convergence to an orthogonal projection Recall that our objective is to show that, given any system of measure-preserving transformations %U^w_{m+|w|}: X_{m+|w|} \\to X_m% obeying a groupoid law, and any set %E \\subset X_0% of positive measure, that there exists a combinatorial line %w = (w^0, w^1, w^2)% in some %[3]^n% such that %(U^{w^i}_n)^{-1}(E) \\cap (U^{w^j}_n)^{-1}(E)% has positive measure for ij=01,12,20. The measure of %(U^{w^i}_n)^{-1}(E) \\cap (U^{w^j}_n)^{-1}(E)% can also be written as %\\langle 1_E, U^{w^i}_n (U^{w^j}_n)^{-1} 1_E \\rangle_{L^2(X_0)},% (1) where we let measure-preserving transformations %T: X \\to X% act on functions %f: X \\to {\\Bbb R}% in the usual manner, %Tf := f \\circ T^{-1}%. The key claim is **Claim** There exists a sequence of combinatorial lines %w_m = (w_m^0, w_m^1, w_m^2) \\in [3]^{n_m}% such that for each ij=01,12,20, the unitary operators %U^{w^i_m}_{n_m} (U^{w^j_m}_{n_m})^{-1}% converge in the weak operator topology to orthogonal projections %P_{ij}%. Indeed, if this claim held, then (1) for the word %w_m% would converge as %m \\to \\infty% to %\\langle 1_E, P_{ij} 1_E \\rangle = \\| P_{ij} 1_E \\|_{L^2}^2.% On the other hand, since %\\langle 1, U^{w^i_m}_{n_m} (U^{w^j_m}_{n_m})^{-1} 1_E \\rangle = \\mu_0(E),% we must have %\\langle 1, P_{ij} 1_E \\rangle = \\mu_0(E) > 0% and hence %\\|P_{ij} 1_E \\|_{L^2} > 0%, and so (1) is positive for sufficiently large m. The main tool in proving the claim is the Carlson-Simpson theorem. This is a somewhat infinitary argument; at present I do not see what the combinatorial analogue of it will be."},{"username":"terence-tao","timestamp":"2011-02-20T05:00:00.000Z","contents":"The invertible maps %U_{m+1}^j% and group structure I am beginning to understand a bit better the strange collection of invertible measure-preserving transformations %U_{m+1}^j: X_{m+1} \\to X_m% that appear in the papers 1, 2, 3, which in turn generate the compound maps %U_{m+|w|}^w: X_{m+|w|} \\to X_m%. Firstly, as Ryan proposed in 622, there is a nice probabilistic interpretation of these transformations that does not require the artificial device of introducing a new source of randomness, such as the uniform probability measure on [0,1]. Roughly speaking, one can view each %X_m% as the space of random m-dimensional combinatorial subspaces in %[3]^n%, where each of the m wildcards %*_1, \\ldots, *_m% appear exactly once, plus some additional data which is a bit tricky to describe cleanly (details are [on the wiki](http://michaelnielsen.org/polymath1/index.php?title=Furstenberg_correspondence_principle)). The transformation %U_{m+1}^j: X_{m+1} \\to X_m% for j=1,2,3 is the map that takes the random m+1-dimensional subspace to an m-dimensional slice, formed by replacing the final wildcard %*_{m+1}% with j. To invert this transformation (i.e. extending an m-dimensional subspace with an m+1-dimensional one), what one would like to do is start with the random m-dimensional subspace, search the fixed coordinates randomly for an instance of j, and replace this j with a wildcard %*_{m+1}%. Of course, this is a random map rather than a deterministic one; so what one has to do is to “roll the dice in advance” and store the choices of fixed coordinates one would use to extend the transformation as part of the data used to define an element of %X_m%. In fact one needs to roll the dice once for every element of the free group %F_3% on three generators, representing all the future possibilities of extending and contracting the space. The construction actually gives more than the invertible maps %U_{m+|w|}^w : X_{m+|w|} \\to X_m%; it also gives a permutation action on each %X_n%, and one can multiply all the %X_n% to get an honest-to-goodness measure-preserving group action %U_w: X \\to X% of the free group %F_3%, rather than this funny groupoid action that Furstenberg and Katznelson use. However (as was pointed out all the way back in Austin.6), all this additional structure is actually a hindrance to the proof, because one has to let go of it in order to take advantage of the powerful Ramsey theorems available such as Carlson’s theorem. I still feel that there should be some clean algebraic substitute for the concept of a group action that can clarify conceptually the dynamics of this system, but I do not yet know what it should be (something like a semi-groupoid action). The key thing seems to be that the structure one has generates lots of “IP-systems”, which I will discuss next."},{"username":"terence-tao","timestamp":"2011-02-20T05:00:00.000Z","contents":"An IP convergence lemma Let %U_1, U_2, \\ldots \\in G% be a sequence of group elements, then we can define %U_\\alpha \\in G% for any finite set %\\alpha% of natural numbers by the formula %U_{\\{a_1,\\ldots,a_n\\}} := U_{a_1} \\ldots U_{a_n}% when %a_1 < \\ldots < a_n%. The %U_\\alpha% form an _IP-system_, which means that they have the restricted multiplicativity property %U_\\alpha U_\\beta = U_{\\alpha \\cup \\beta}% whenever %\\alpha% is to the left of %\\beta%. (This is an example of the “semi-groupoid action” concept I am struggling to formalise in a clean algebraic fashion.) If G has a topology on it, we say that the IP-system %U_\\alpha% converges to a limit %U \\in G% if every neighbourhood of U contains the %U_\\alpha% “for sufficiently large %\\alpha%“, which means that the least element %\\min(\\alpha)% of %\\alpha% is sufficiently large. Hindman’s theorem is equivalent to the claim that any IP-system in a compact metrisable space has a convergent sub-IP-system (I will not define what a sub-IP-system is here; again, I do not feel that I have that the correct algebraic formalism for all this yet.). The following lemma is the key in Paper #1 (together with Carlson’s theorem, which has a similar flavour to Hindman’s theorem) to proving the claim in 628: **Lemma.** Let %U_\\alpha% be an IP system of unitary operators on a separable Hilbert space H that converges (in the weak operator topology) to a limit P. Then P is an orthogonal projection. [Note that the weak operator topology on the operators with norm at most 1 on H is compact metrisable.] Proof. Since the %U_\\alpha% have operator norm at most 1, P has norm at most 1 as well. Since the only idempotents of norm at most 1 are the orthogonal projections, it suffices to show that %P^2 = P%. Let %v \\in H%. Then %U_\\alpha^* v% converges weakly to %P^* v% as %\\alpha \\to \\infty%. But for fixed %\\alpha%, $U^*_\\beta U_\\alpha^* v$ converges weakly to %P^* U_\\alpha^* v% as %\\beta \\to \\infty%; thus %\\lim_{\\alpha \\to \\infty} \\lim_{\\beta \\to \\infty} U^*_\\beta U^*_\\alpha v = (P^*)^2 v%, where the limits are in the weak sense. On the other hand, using the IP property, the limit on the left is %P^* v%. Thus %P^* = (P^*)^2% and so P is idempotent."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"An informal combinatorial translation of the ergodic proof of DHJ(2) I’m beginning to understand (though still in a somewhat fuzzy, informal way) what the ergodic argument in Paper #1 would translate to in purely combinatorial terms. For simplicity I’ll start with DHJ(2) – the claim that any dense subset %A% of %[2]^n% contains a combinatorial line. Let m be an integer that grows slowly with n (think of m as being like %\\lfloor \\log \\log n \\rfloor%, for instance), and let V be a random m-dimensional combinatorial subspace of %[2]^n%, where I will be deliberately vague as to what distribution V is drawn from here. One can use this random subspace V to create various real-valued random variables f(V) which depend only on what A is doing on V. For instance, one could consider the indicator random variable %1_{E_0}(V)%, which equals 1 when the bottom corner of V (formed by sending all m wildcards to 0) lies in A; one could consider the random variable %d(V)% that measures the density of A in V; and so forth. Now, the random subspace V has a lot of fixed coordinates, and so with very high probability it’s going to have a lot of fixed 0s and fixed 1s (about n/2 of each). Let V’ be the random subspace parallel to V formed by taking one of the fixed 0s at random and flipping it to a 1\\. Then every random variable f(V) that is associated to V has a “shifted” version f(V’). Note that we expect V’ to have almost the same distribution as V, and so f(V) and f(V’) should also have about the same distribution; this is a _stationarity_ property. However, this does not mean that f(V) and f(V’) are necessarily close to each other. There are two fundamental types of random variable: * A random variable f(V) is _periodic_ if f(V’) and f(V) are close, thus %{\\Bbb E} |f(V) - f(V')|% is small. * A random variable f(V) is _mixing_ if f(V’) and f(V) are uncorrelated, thus %{\\Bbb E} f(V) f(V')% is small. Examples: any constant random variable is periodic. If the set A has “low influence” (which means that flipping a bit from 0 to 1 is unlikely to affect membership in A) then every random variable f(V) is in fact periodic. At the other extreme, if A is a random set of a fixed density %\\delta%, then any random variable f(V) of mean zero will be mixing, and only the constants will be periodic. Now, here is the basic fact (modulo some lies): **Fact**. Every random variable f(V) can be decomposed (uniquely) into a periodic random variable Pf(V) and a mixing random variable (1-P)f(V). Furthermore, the periodic random variables and mixing random variables are always uncorrelated (i.e. orthogonal to each other). This fact is a heavily disguised version of the lemma in 630, and can also be viewed as a sort of “IP mean ergodic theorem”. It is not quite true as stated; to be true, one has to redefine V’ to not be formed by flipping just one fixed coordinate of V, but by flipping an unspecified but bounded number of fixed coordinates. (The number of digits to flip will be determined by a Ramsey-type theorem, such as Folkman’s theorem, Graham-Rothschild, etc. The Ramsey theorem will also tell us what distribution V is to be drawn from, and in particular how many times each wildcard will appear.). Assume this fact. Now we can prove DHJ(2). If we let %1_{E_0}(V)% be the event that the bottom left corner of V lies in A, it suffices to show that %{\\Bbb E} 1_{E_0}(V) 1_{E_0}(V')% is large. Observe that the claim would be easy if %1_{E_0}(V)% was periodic, since then %1_{E_0}(V) \\approx 1_{E_0}(V')%. The claim would also be easy if %1_{E_0}(V)% was mixing, since then the two random variables %1_{E_0}(V), 1_{E_0}(V')% would be roughly independent. In general, the Fact tells us that we are in a combination of the two situations. Splitting both %1_{E_0}(V)% and %1_{E_0}(V')% into periodic and mixing components, we get four terms, three of which are small, leaving us with %{\\Bbb E} P 1_{E_0}(V) P 1_{E_0}(V') \\approx {\\Bbb E} P 1_{E_0}(V)^2%. (1) But observe that the mixing function %(1-P) 1_{E_0}(V)% is orthogonal to the periodic function 1, and thus has zero expectation. Since %1_{E_0}(V)% has expectation %\\delta%, we conclude that %P 1_{E_0}(V)% has expectation %\\delta%, and so (by Cauchy Schwarz) the expression (1) is large, and we are done. In my next comment I will try to indicate how DHJ(3) works in some special cases. Here there is not just one projection P, but now three separate projections P, reflecting interchange between 0 and 1, 1 and 2, or 2 and 0\\. Things are relatively easy when one of the projections is extreme (either everything is periodic, or only the constants are periodic), but the case in which all projections are intermediate (so some random variables are periodic and others are mixing) is quite tricky."},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Some special cases of DHJ(3) Now we move from k=2 to k=3\\. Again, we consider an m-dimensional random subspace V of %[3]^n%. Instead of a single companion subspace V’, we now have three spaces %V_{0 \\to 1}, V_{1 \\to 2}, V_{2 \\to 0}%, formed from V by flipping a fixed 0 to a 1, or a fixed 1 to a 2, or a fixed 2 to a 0\\. We can then define the notion of a random variable f(V) being 01-periodic or 01-mixing (resp. 12-periodic or 12-mixing, etc.) by comparing f(V) with %f(V_{0 \\to 1})%, etc. We then get decompositions %f(V) = P_{01} f(V) + (1-P_{01})f(V)% into 01-periodic and 01-mixing components, etc. What makes life difficult here is that the three orthogonal projections %P_{01}, P_{12}, P_{20}% can be in general position with respect to each other (in particular, they need not commute). But things are simpler when one of the orthogonal projections is trivial. At one extreme, consider the case when %P_{12}% (say) is the identity – in other words, that _every_ random variable f(V) is 12-periodic. This is basically saying that the original set A is essentially insensitive to flipping a 1 to 2 or vice versa. Because of this, we can basically identify 1 and 2 together and reduce the k=3 problem to the k=2 problem. A more non-trivial case is the other extreme, in which %P_{12}% is the expectation operator – i.e. every random variable f(V) of expectation zero is 12-periodic, or equivalently, f(V) and g(V_{1 \\to 2}) are independent for every f, g. Now let f(V), g(V), h(V) be random variables. We claim that if f is 01-mixing OR if g is 20-mixing, then %{\\Bbb E} h(V) f(V_{0 \\to 1}) g(V_{0 \\to 2}) \\approx 0%, (1) where %V_{0 \\to 2}% is defined by taking the _same_ digit that was flipped from 0 to 1 in the definition of %V_{0 \\to 1}%, and flipping it to 2 instead. To show this, it suffices (by a certain “IP van der Corput lemma”, which one should think of as a variant of Cauchy-Schwarz) to eliminate h and show that %{\\Bbb E} f(V_{00 \\to 01}) g(V_{00 \\to 02}) f(V_{00 \\to 11}) g(V_{00 \\to 22}) \\approx 0% (2) where the %V_{00 \\to ij}% are formed by choosing two of the fixed 0s of V and flipping them to i and j (and it is the same two digits being flipped for all four of the %V_{00 \\to ij}%). The triviality of %P_{12}% (or more precisely, a variant of this with m replaced by m+1) can then be used to show that the random variables %f(V_{00 \\to 01}) f(V_{00 \\to 11})% and %g(V_{00 \\to 02}) g(V_{00 \\to 22})%, because these random variables involve parallel m+1-dimensional subspaces which differ from each other by flipping a single 1 to a 2\\. Assuming this, we can simplify the expression in (2) to %[{\\Bbb E} f(V_{00 \\to 01}) f(V_{00 \\to 11})] [{\\Bbb E} g(V_{00 \\to 02}) g(V_{00 \\to 22})]%. But one of these factor vanishes since f is 01 mixing or g is 20 mixing, and we are done. For general f, g, we can now use the Fact from the previous comment to obtain the “generalised von Neumann theorem”. %{\\Bbb E} h(V) f(V_{0 \\to 1}) g(V_{0 \\to 2}) \\approx {\\Bbb E} h(V) P_{01} f(V) P_{02} g(V)%. Now we can prove DHJ(3). It suffices to show that %{\\Bbb E} 1_{E_0}(V) 1_{E_0}(V_{0 \\to 1}) 1_{E_0}(V_{0 \\to 2}) % is large. Using the generalised von Neumann theorem, this expression can be rewritten as %{\\Bbb E} 1_{E_0}(V) P_{01} 1_{E_0}(V) P_{20} 1_{E_0}(V)%. But, as we saw in 631, %P_{01} 1_{E_0}% has positive inner product with %1_{E_0}%. Furthermore %P_{01} 1_{E'}% has positive inner product with %1_{E'}% for any subset E’ of %E_0% of positive measure. One can check that %P_{01}% is monotone, and so %P_{01} 1_{E_0}% has positive inner product with %1_{E'}% for any %E'% of positive measure; thus %P_{01} 1_{E_0}% is positive almost surely on %E_0% (indeed, %P_{01} 1_{E_0}% is the conditional expectation of %E_0% to a certain factor), and similarly for %P_{20} 1_{E_0}%, and we are done."},{"username":"terence-tao","timestamp":"2011-02-20T05:00:00.000Z","contents":"Thread moving I have now got an informal combinatorial translation of Paper #2 on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Furstenberg-Katznelson_argument](http://michaelnielsen.org/polymath1/index.php?title=Furstenberg-Katznelson_argument) and it is now being discussed on the 800 thread at [http://gowers.wordpress.com/2009/02/23/brief-review-of-polymath1/](http://gowers.wordpress.com/2009/02/23/brief-review-of-polymath1/) so I am now moving discussion of this paper over to that thread."},{"username":"thomas-sauvaget","timestamp":"0007-02-20T05:00:00.000Z","contents":"bound on %c_5%, then on %c_n% in order not to burden this thread I’m inviting any other attempts along the lines of 275 to be discussed at: [http://thomas1111.wordpress.com/2009/02/14/a-proof-that-c_5154/](http://thomas1111.wordpress.com/2009/02/14/a-proof-that-c_5154/) and if things work out we’ll report a brief summary back here and in the wiki, after the (possibly longish) trial/error period. Currently I’ve found a new pattern for %c_5% which requires independent confirmation/infirmation."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"bound on %c_5%. By the way, one cautionary note: I think any line-free subset of the 162-element set %D_5 = \\{ x_1 x_2 x_3 x_4 x_5: x_1+\\ldots+x_5 \\neq 0 \\hbox{ mod } 3 \\}% must have at most 150 points. Proof: one can view %D_5% as %{}[3]^5% with %\\Gamma_{(a,b,c)}% deleted for the triples %(a,b,c) = (0,0,5), (1,1,3), (2,2,1), (0,3,2), (1,4,0), (3,0,2), (4,1,0)%. From the triples %(5,0,0), (2,0,3), (2,3,0)% one more point needs to be deleted, and similarly for %(0,5,0), (2,0,3), (0,3,2)%. Meanwhile, from the triples %(4,0,1), (1,0,4), (1,3,1)% five more points need to be deleted, and similarly for %(0,4,1), (0,1,4), (3,1,1)%, leading to at most 150 points overall. at least one point needs to be deleted from"},{"username":"michael-peake","timestamp":"2010-02-20T05:00:00.000Z","contents":"Lower bound for Moser’s cube The following set gives a lower bound of %n(n+1)2^{n-3}% points for Moser’s cube: Take all points with two 2s, and all those with one 2 and an odd number of 1s."},{"username":"michael-peake","timestamp":"2010-02-20T05:00:00.000Z","contents":"Lower bound for Moser’s cube Should the asymptotic lower bound for Moser’s cube be %12^{n/3}/\\sqrt{n}% ? Now that I’ve read the page on Moser’s cube, you can take all points with q 2s, and all those with q-1 2s and an odd number of 1s, to give %\\binom{N+1}{q}2^{N-q}% points, where q is the nearest integer to N/3"},{"username":"terence-tao","timestamp":"2010-02-20T05:00:00.000Z","contents":"Lower bound for Moser’s cube Dear Michael, I think Stirling’s formula will show that %\\binom{N+1}{q} 2^{N-q}% is roughly %3^N / \\sqrt{N}%. (One can also see this from the binomial formula %3^{N+1} = \\sum_{q=0}^{N+1} \\binom{N+1}{q} 2^{N-q+1}% and the law of large numbers, which shows that the bulk of the sum comes from the region %q = N/3 + O(\\sqrt{N})%.)"},{"username":"michael-peake","timestamp":"2012-02-20T05:00:00.000Z","contents":"Bounds for %c'_n% Oh, I see. Thanks. I think I can squeeze in some more points with N/6-1 1s, and N/6 2s,  \nbut they will be relatively few. BTW it must be a quarter century since we last met, at Vern’s classes.  \nI’m delighted to be in this, which is sort of one of your classes."},{"username":"klas-markstrom","timestamp":"0007-02-20T05:00:00.000Z","contents":"The value of c_5 is 150 I formulated the problem as an integer programing instance and solved it using some of my linear programming tools. The program quickly finds a solutions with 150 points and then reduces the upper bound to 150 in less than 2 minutes My program correctly gives the known values for the lower c_n and the correct number of solutions for them as well. There are 12 different extremal configurations for c_5. I have started a run for c_6"},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"%c_5% Klas, that’s quite impressive! (For comparison, the fact that %c''_6 = 112% took some time to be done by hand.) It would be great if some data or source code for the run could be made available somewhere (e.g. on the wiki). The Moser number c’_5 might also be in reach now (it is somewhere between 120 and 129, according to the spreadsheet). [By the way, to the anonymous contributor of the lower bounds for the Moser problem: if you could describe the examples either here or on the wiki page for the Moser problem, that would be great!]"},{"username":"kristal-cantwell","timestamp":"2011-02-20T05:00:00.000Z","contents":"If c_5 is 150 than if the spreadsheet is correct c_6 is 450\\. Clearly  \nc_6 is less than or equal to 3 times c_5 and we have a lower bound  \nof 450 so that forces c_6 to be 450\\. c_7 is between 1302 and 1350 since we have a lower bound of 1302 and by the same reasoning 3 times 450 is  \n1350."},{"username":"kristal-cantwell","timestamp":"2011-02-20T05:00:00.000Z","contents":"A052979 was the last sequence in the OEIS which might have been equal to the sequence c_n but the two sequences diverge if c_5 equals 150."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"I’ve provisionally updated the spreadsheet to reflect the upper bound for %c_5%. (It seems consistent with the difficulties Thomas has been experiencing in trying to improve the lower bound below 150.) If we have a complete list of all the extremisers for %c_5%, then it looks likely that one will be able to shave a couple points off of the upper bound 1350 from %c_7%, though it seems rather unlikely that we will be able to close the gap with the lower bound of %1302% without a massive computational effort. Still, this is much more of the sequence %c_n% computed than I would have initially thought possible, if it all checks out…"},{"username":"klas-markstrom","timestamp":"0002-02-20T05:00:00.000Z","contents":"c_5 Terry, I can upload the file with the linear program in a standard format which can be read by e.g. the open source linear programing solver glpk from gnu.  \nThe integer programing formulation is quite simple. One 0/1-variable for each point in the cube, and one linear inequality for each combinatorial line. I have a file with the 12 extremal sets for c_5 which I can also upload. I am running the c_6 case to see if I can find the extremal sets there too, but it is considerably harder and I have moved it to a linux cluster where it will run over night. (It’s now late in the evening in Sweden) I did a quick check for the Moser-problem and it seem less well suited for integer programing. I could quickly verify the value of 43 for n=4, but for n=5 the integer gap is much larger than it was for c_5, and that means that it will take a lot longer to run. But I can give it a try later."},{"username":"terence-tao","timestamp":"0002-02-20T05:00:00.000Z","contents":"%c_6% If %c_5 = 150%, then as noted in 708, %c_6=450% and so every %c_6% extremiser is formed from three %c_5% extremisers. If there are only 12 %c_5% extremisers, then one should be able to fairly quickly exhaust the %12^3=1728% combinations to find all the %c_6% extremisers. (There are also some symmetries of the problem that might be deployable to cut down the search space by a factor of 6 or so.) It may also be possible to get the computer to classify near-extremisers, e.g. 149-element line-free subsets of %[3]^5%. This may then be able to lower the upper bound of %c_7 \\leq 1350% somewhat."},{"username":"michael-peake","timestamp":"0004-02-20T05:00:00.000Z","contents":"Lower bounds for Moser cubes Dear Terry.707, I am sorry, that was me who entered the lower bounds. I forgot to log into Google before I did so. My logic was as I described in 702 and 703\\. You can include all points with q 2s and half of those with q-1 2s. Any combinatorial line can only have one entry vary. In one of the endpoints, it has an odd number of 1s. So all combinatorial lines are excluded."},{"username":"jozsef","timestamp":"0005-02-20T05:00:00.000Z","contents":"Lower bound for Moser’s cube We would like to find some restriction on the number of 0-s, 1-s, and 2-s that makes the appearance of a geometric line impossible, but still allows many points. If %a % denotes the number of 0-s and %c % denotes the number of 2-s, then fixing the number of 1-s means that we select a,b pairs that a+c = constant. (this is our best solution so far) There is another way to avoid lines; If A and C denote the sets of a-s and c-s, then if the difference set of A, A-A , and the difference set C-C are disjoint then triples (a,*,c) won’t form geometric lines. Now, if %A , C \\subset [m]% then it is easy to choose them being “difference-disjoint” and that the product |A||C| is about m. This gives the same bound as the a+c=constant trick. But I’m not sure that one can’t find two larger difference-disjoint sets A and C. That would improve our lower bound instantly."},{"username":"terence-tao","timestamp":"0005-02-20T05:00:00.000Z","contents":"Lower bound for Moser’s problem Dear Jozsef, if %A, C \\subset [m]% are such that %|A| |C| > 2m%, then by the pigeonhole principle there must be a collision in the sumset A+C, i.e. a non-trivial solution to a+c=a’+c’ with %a,a' \\in A% and %c,c' \\in C%. Then a-a’=c’-c, and so A-A and C-C have non-trivial intersection. So I think this method may not give a substantial improvement over the current example."},{"username":"terence-tao","timestamp":"0005-02-20T05:00:00.000Z","contents":"Genetic algorithms This is perhaps not something to pursue too seriously here (it would take a non-trivial programming effort, unless someone just happens to have an off-the-shelf piece of GA software lying around), but it would be amusing to see how well a [genetic algorithm](http://en.wikipedia.org/wiki/Genetic_algorithm) would fare with trying to maximise the size of a line-free set. It’s tempting to use blocks of, say, %[3]^2% as “genes” that can be swapped around between competitor sets, possibly with some [transposon](http://en.wikipedia.org/wiki/Transposon) genes to mix the blocks up a little. (The evolution of the “phenotype” in [Thomas’s blog post](http://thomas1111.wordpress.com/2009/02/14/a-proof-that-c_5154/) might give an illustration of how such an algorithm would develop, sans the “intelligent designer” of course ;-) .) More generally, this problem might serve as a reasonable one for benchmarking a number of standard algorithms (we’ve discussed greedy algorithms, integer programming, genetic algorithms, SAT solvers, and quadratic programming in previous comments)."},{"username":"jozsef","timestamp":"0006-02-20T05:00:00.000Z","contents":"Lower bound Terry, re. 715, yes you are right, however we might handle such cases by considering only some a, c pairs. In general, we can ask what is the max number of edges in a bipartite graph between A and B which still avoids lines. (i.e. by taking (a,*,c) points only where a,c is an edge can’t form lines) If it true that the number of edges of such graph is always at most m?"},{"username":"jozsef","timestamp":"0006-02-20T05:00:00.000Z","contents":"Lower bound Well, it seems that any bipartite graph between %A\\subset [m]% and %C\\subset [m]% which avoids lines has at most m edges. A graph avoids lines iff there are no parallel edges; one edge between a and c and another connecting a+d and c+d. This shows that simply considering the number of 0-s, 1-s, and 2-s can’t give us a better bound than what we have."},{"username":"michael-peake","timestamp":"0004-02-20T05:00:00.000Z","contents":"Lower bound for Moser, k=4 A straightforward lower bound for Moser’s cube k=4 (values 0,1,2,3) is:  \nq entries are 1 or 2; or q-1 entries are 1 or 2 and an odd number of entries are 0. %\\binom{N}{N/2}2^N + \\binom{N}{N/2-1}2^{N-1}% I think this is %O(4^N/\\sqrt{N})%"},{"username":"jason-dyer","timestamp":"0005-02-20T05:00:00.000Z","contents":"Multi-dimensional tic-tac-toe Busy weekend! Are we ready for the OEIS, or should we try a proof by hand that c_5 = 150 first? I haven’t done much other than realize there might be something about our problem in the literature relating to multi-dimensional tic-tac-toe, and indeed the very first to explore it were Hales and Jewett. (Was this how DHJ was started, or did this occur later?) [This article](http://books.google.com/books?hl=en&lr=&id=PZV6yhwa_ZQC&oi=fnd&pg=PA93) from _Contemporary Combinatorics_ seems to be the most recent treatment."},{"username":"jozsef","timestamp":"0009-02-20T05:00:00.000Z","contents":"Lower bound for Moser, k=4 Michael, re. 719, you have a better bound by selecting elements where the total number of 1-s and 2-s is n/2\\. Similar to the k=3 case, one can argue that this is the best possible bound if you only consider the numbers of 0-s,1-s,2-s, and 3-s. For k=5 the situation is better, there we have an arithmetic progression-like sequence just by considering the sizes of digits."},{"username":"jozsef","timestamp":"0009-02-20T05:00:00.000Z","contents":"Lower bound for Moser, k=4 contd. It seems that this is practically the same bound as yours, Michael. Sorry for not noticing. I’ll have a morning coffee before writing anything more …"},{"username":"jozsef","timestamp":"0009-02-20T05:00:00.000Z","contents":"Lower bound for Moser, k=5 If A, B, C, D, and E denote the numbers of 0-s, 1-s, 2-s, 3-s, and 4-s then the first three points of a geometric line form a 3-term arithmetic progression in A+E+2(B+D)+3C. So, for k=5 we have a similar lower bound for the Moser’s problem as for DHJ k=3."},{"username":"klas-markstrom","timestamp":"0009-02-20T05:00:00.000Z","contents":"Files for c_5=150  \nI’m a bit short on time today but I want to make the data for c_5 available. I have put the two files on my webserver, temporarily.  \nIf someone is willing to put them in a suitable place in the wiki then that would be great. This file contains the extermisers. One point per line and different extermisers separated by a line with “—”  \n[http://abel.math.umu.se/~klasm/extremal-c5](http://abel.math.umu.se/~klasm/extremal-c5) This is the linear program, readable by Gnu’s glpsol linear programing solver, which also quickly proves that 150 is the optimum.  \n[http://abel.math.umu.se/~klasm/linprog-d=5-t=3.lpt](http://abel.math.umu.se/~klasm/linprog-d=5-t=3.lpt)  \nEach variable corresponds to a point in the cube, numbered according to their lexicografic ordering. If a variable is 1 then the point is in the set, if it is 0 then it is not in the set.  \nThere is one linear inequality for each combinatorial line, stating that at least one point must be missing from the line."},{"username":"klas-markstrom","timestamp":"0009-02-20T05:00:00.000Z","contents":"c_6  \nI’m aware that the value of c_6 follows from c_5 and that the extermisers can be constructed from the c_5 extermisers. However when doing computer work for these kinds of things I try to follow the cautious strategy that if one can get the same answer in two different ways then it is best to try them both. This gives a good insurance against mistakes and other errors. It would be good if someone else would be willing to combine the c_5 extermisers into c_6 extermisers. That way we would have an independent construction by that method as well as the pure integer programming run I am doing for the problem now."},{"username":"terence-tao","timestamp":"2010-02-20T05:00:00.000Z","contents":"726. I’ve put the recent progress on c_n and Moser’s problem on the wiki. Please feel free to edit. I think as soon as we have a second confirmation of the 150 result (either by hand or by computer) we can send the sequence (up to 450) to the OEIS. I doubt we will pin down %1302 \\leq c_7 \\leq 1350% exactly any time soon, though we may be able to narrow the bounds by a bit."},{"username":"sune-kristian-jakobsen","timestamp":"2012-02-20T05:00:00.000Z","contents":"727. Have anyone tried to generalize the construction in 220? (Michael’s answer to Terry.219)  \nDo there exist similar examples when a, b and c are not equal? or when r>1?"},{"username":"jason-dyer","timestamp":"0002-02-20T05:00:00.000Z","contents":"Fujimura’s problem Thinking about the multi-dimensional tic-tac-toe led to attempting this problem as an n-queens variant. Imagine each removed point as a “queen” that attacks in all six possible directions (and is able to pass over other queens). For there to be no equilateral triangles, it is necessary but not sufficient for each point without a queen to be attacked by at least n queens. This corresponds with the removal of all n triangles from that point. To be a sufficient condition, each available distance and direction must contain at least one queen. (The three directions are (a+k,b,c) (a,b+k,c) (a,b,c+k) with k>0.) There is also a limit to how many queens any individual queen may be attacked by, but I haven’t gone through the calculations yet."},{"username":"dima","timestamp":"0006-02-20T05:00:00.000Z","contents":"729.Geometric hyperplanes. (studied mostly in a different context, for geometries related to finite simple groups, buildings, etc) As the name suggests, a geometric hyperplane is a subset such that every line (combinatorial line, that is, in our case) either lies in it, or intersects it in a point (i.e. an %n-%word over {1,2,3}, in our case). So the complement of a geometric hyperplane does not contain a combinatorial line. The opposite need not be true, as a combinatorial line can intersect the complement of a line-free set in 2 points (the latter looks like a “waste of lines”, though, and one would hope (too optimistically, perhaps) that in the extremal examples of line-free sets this does not happen) A geometric hyperplane of any “partial linear space with line size 3” (we have our combinatorial lines as an example) is obtained from a so-called universal embedding into %\\mathbb{F}_2 P^{N-1},% where %N% is the number of points, %\\mathbb{F}_2% are integers mod 2, and in our case %N=3^n,% provided certain natural condition is satisfied. This is a result due to M.Ronan (“Embeddings and hyperplanes of discrete geometries”, Europ. J. Combin. 8 (1987), 179–185.)  \nAs far as I can recall (I can’t easily lay my hands on this text here), for our examples this condition trivially holds. The universal embedding is the following natural contruction: take the vectorspace %\\mathbb{F}_2^N% modulo the relations %x_i+x_j+x_k=0,% for each line %\\{i,j,k\\}.% This maps each point the geometry to a nonzero vector of %\\mathbb{F}_2^M,% and each line to a 2-space of %\\mathbb{F}_2^M% (here %M% is the dimension of the quotient space).  \nThe geometric hyperplanes then are just hyperplanes of the space %\\mathbb{F}_2^M.% One way or another, this is a nice and sometimes useful construction, although I am not too optimistic for it being useful here. Perhaps it would lead to better understanding of construtions of particular line-free sets."},{"username":"michael-peake","timestamp":"0006-02-20T05:00:00.000Z","contents":"List of %c_5% solutions The twelve solutions are in three types. In the notation of Kristal.247 and Terry.248, all twelve can be formed by removing points from yzx zxy xyz  \nxyz yzx zxy  \nzxy xyz yzx or a cyclic permutation of that. The number of points remaining in each cube is either 3 solutions from permutations of this  \n17 17 18  \n18 14 17  \n14 18 17 6 solutions from permutations of this  \n17 17 15  \n18 14 17  \n17 18 17 3 solutions from permutations of this  \n17 17 12  \n18 17 17  \n17 18 17 In each solution, one number on the rising diagonal is a multiple of 3;  \nthat cube was xyz before the removal of points. All twelve solutions are formed from complete %\\Gamma_{a,b,c}% slices. If my program is right, there is only one %c_6% solution, the one we know, and it is built from the last three solutions above."},{"username":"terence-tao","timestamp":"0006-02-20T05:00:00.000Z","contents":"%c_5 \\leq 152% I was able to show by hand that %c_5 \\leq 152%, basically by using a precise description of the 52 and 51-point line-free sets in %[3]^4%, followed by a rather tedious case analysis. See [http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds](http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds) The key point seems to be that extremal sets have a very strong tendency to be trapped in one of the sets %D_{n,j} := \\{ (x_1,\\ldots,x_n): x_1+\\ldots+x_n = j \\hbox{ mod } 3 \\}% for j=0,1,2, and any attempt to deviate from these sets by adding a point or two outside of the set tends to be “punished” by the forcible removal of many points from inside that set. Much of the analysis seems like it can be pushed to 150\\. For instance, I know that if two parallel slices of the set have 51 points or more, then the set as a whole can have at most 150 points. One still has to rule out 52-50-49 and 51-50-50 slice patterns, though, if one is to get all the way to 150. Michael: If there is only one %c_6% solution, then we have %c_7 \\leq 3c_6 - 2 = 1348%, by the same argument that gave us %c_4 \\leq 3 c_3 - 2 = 52%."},{"username":"dima","timestamp":"0007-02-20T05:00:00.000Z","contents":"Symmetries. Klas, Terence,  \nwhen you talk about cutting down the search space, it can be cut down by the full automorphism group of the set of combinatorial lines, that is by the direct product of %\\mathbb{Z}_3% and %S_n,% the symmetric group on %n% letters. (It is easy to see that adding 1 to each coordinate, thinking of coordinates being in %\\mathbb{Z}_3,% maps a combinatorial line to a combinatorial line, and gives %\\mathbb{Z}_3,% whereas permuting the coordinates gives %S_n.% That there are no more symmetries, can be seen by observing that there are different numbers of combinatorial lines on points with different numbers of symbols equal to each other.)"},{"username":"michael-peake","timestamp":"0009-02-20T05:00:00.000Z","contents":"Moser, k=5, equals DHJ, k=3 joszef.723 noted that we get the same density for the Moser cube with k=5 as we got for DHJ with k=3\\. That is because the restriction of the k=5 Moser’s cube to %{}[0,1,2]^N% is the same problem as DHJ. It also explains why Moser’s k=3 cube gets the same density as Spurner’s lemma."},{"username":"michael-peake","timestamp":"2010-02-20T05:00:00.000Z","contents":"Moser, k=5 doesn’t equal DHJ, k=3 No, I don’t think my previous comment is correct, although the densities do match."},{"username":"jozsef","timestamp":"0008-02-20T05:00:00.000Z","contents":"Re:734\\. Moser, k=5 doesn’t equal DHJ, k=3 Michael, the k=6 case is the first one where I see some simple correlation between the Moser and DHJ problems. By the map P: P(0)=P(5)=0, P(1)=P(4)=1, P(2)=P(3)=2 a line-free grid maps to a combinatorial-line-free cube. So, Moser k=6 is at least %2^n% times DHJ k=3\\. For k=5 we would need an asymmetric map, I’m not sure if one can get a relation between the two problems similar to the k=6 case."},{"username":"ks-chua","timestamp":"0003-02-20T05:00:00.000Z","contents":"%c'_5 \\ge 124% Using the optimization method described in 229\\. I found a geometric line free set of size 124\\. (%k=3,n=5%). The program is implemented in MATLAB using the builtin minimization routine (which does not use the fact that the objective function is a cubic polynomial). Because there is no apparent structure to the set found, I list the 124 integers below (whose ternary expansions give the set) so that it may be checked independently. 1 , 4 , 5 , 9 , 10 , 12 , 14 , 16 , 17 , 21 , 22 , 25 , 28 , 29 , 30 , 32 , 33 , 34 , 36 , 38 , 42 , 44 , 45 , 46 , 48 , 50 , 52 , 53 , 57 , 58 , 61 , 62 , 64 , 65 , 66 , 68 , 69 , 70 , 73 , 76 , 77 , 80 , 81 , 82, 84 , 86 , 88 , 89 , 90 , 92 , 96 , 98 , 100 , 101 , 102 , 104 , 105 , 106 , 108 , 110 , 114 , 116 , 126 , 128 , 132 , 134, 136 , 137 , 138 , 140 , 141 , 142 , 144 , 146 ,150 , 152 , 153 , 154 , 156 , 158 , 160 , 161 , 164 , 165 , 166 , 169 , 172 , 173,  \n174 , 176 , 177 , 178 , 181 , 184 , 185 , 186 , 189 , 190 , 192 , 194 , 196 , 197, 198 , 200, 204 , 206 , 208 , 209 , 210 , 212,  \n213 , 214 , 217 , 220 , 221 , 225 , 226 , 228 , 230 , 232 , 233 , 237, 238 , 241."},{"username":"michael-peake","timestamp":"0006-02-20T05:00:00.000Z","contents":"c5′ Chua.736’s solution contains all points with two 1s, or one 1 and an even number of 0s.  \nIt also contains four points with no 1s and two or three 0s. Any two of these four points differ in three places, except for one pair of points that differ in one place.  \nFor these reasons, I agree that this is a solution to Moser’s problem."},{"username":"michael-pea","timestamp":"0008-02-20T05:00:00.000Z","contents":"Moser’s cube, k=3 This suggests further solutions for %c'_N% q 1s, all points from %A(N-q,1)%  \nq-1 1s, points from %A(N-q+1,2)%  \nq-2 1s, points from %A(N-q+2,3)%  \netc. where %A(m,d)% is a subset of %[0,2]^m% for which any two points differ from each other in at least d places. Mathworld’s entry on error-correcting codes suggests it might be NP-complete to find the size of A(m,d) in general. %|A(m,1)| = 2^m% because it includes all points in %[0,2]^m%  \n%|A(m,2)| = 2^{m-1}% because it can include all points in %[0,2]^m% with an odd number of 0s"},{"username":"klas-markstrom","timestamp":"0001-02-20T05:00:00.000Z","contents":"c_6  \nMy integer programing run has now finished and as expected it found the optimum to be c_6=450 and found a unique one solution. This case required a lot more CPU-time than c_5\\. I had to split the problem into many subcases and ran them in parallell ona linux cluster. My guess would that it used around 1000 CPU-hours."},{"username":"michael-peake","timestamp":"2010-02-20T05:00:00.000Z","contents":"That’s a huge effort, and I’m guessing we won’t be able to find %c_7% by this method. In 738, I can at least estimate the size of A(m,d) by sphere packing. In A(m,3), each selected point and its m immediate neighbours form disjoint sets, so %|A(m,3)|  \nAlso %|A(m,4)| In A(m,5) and A(m,6), each selected point and two layers of neighbours form disjoint sets, so %|A(m,5)|(1+m+\\binom{m}{2}) \\leq 2^m%. Using these inequalities, it seems that only a tiny fraction of points from lower layers are included in the method described in 738\\. So this method still only has %O(3^N/\\sqrt{N})% points."},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Moser It occurs to me that if we have a good enough understanding of the %c'_4=43% result, and in particular we can classify 43-point Moser sets, then we may be able to improve the upper bound on %c'_5% from 129 to %127 = 43 + 42 + 42% as was done for the %c_4% and %c_7% upper bounds, bringing us closer to the lower bound of 124\\. If we also can classify the 42-point Moser sets then in principle we can hit %124 = 42 + 41 + 41% exactly. (Similarly, if we get a good classification of 50-point line-free subsets of %[3]^4%, then we should be able to finish off a non-computer-assisted proof that c_5 = 150.)"},{"username":"klas-markstrom","timestamp":"0002-02-20T05:00:00.000Z","contents":"Moser.  \nI ran my program for the Moser problem. For n=4 I get 43 as the optimum and the program found 1552 extremisers. However a quick heuristic test suggest that most might be isomorphic.  \nThe solutions can be found here  \n[http://abel.math.umu.se/~klasm/extremal-moser-n=5-t=3](http://abel.math.umu.se/~klasm/extremal-moser-n=5-t=3)  \nThe file format is the same as before."},{"username":"michael-peake","timestamp":"0006-02-20T05:00:00.000Z","contents":"Moser At a first glance, there are four types of solution 512 solutions: 24 points contain two 2s, 16 contain one 2, 3 contain no 2s  \n768 solutions: 23 points contain two 2s, 16 contain one 2, 4 contain no 2s  \n256 solutions: 24 points contain two 2s, 15 contain one 2, 4 contain no 2s  \n16 solutions: 18 points contain two 2s, 20 contain one 2, 5 contain no 2s"},{"username":"kristal-cantwell","timestamp":"0008-02-20T05:00:00.000Z","contents":"If the following: 512 solutions: 24 points contain two 2s, 16 contain one 2, 3 contain no 2s  \n768 solutions: 23 points contain two 2s, 16 contain one 2, 4 contain no 2s  \n256 solutions: 24 points contain two 2s, 15 contain one 2, 4 contain no 2s  \n16 solutions: 18 points contain two 2s, 20 contain one 2, 5 contain no 2s are the only possible values for c_4′ = 43\\. Then I think that c_5′  \nmust be 128 or less. There are only 24 points possible with two 2’s  \nso if there are three in a row the first one must have at least 18 of these  \npoints the second cube must also have at least 18 so there must be at least  \n12 in both final the third must have at least 18 so there must be  \nat least 6 in all three which results in a line. So the maximum value for  \nc_5′ is 128 or less."},{"username":"terence-tao","timestamp":"2010-02-20T05:00:00.000Z","contents":"%c_4% I’ve managed to classify all 52-point, 51-point, and 50-point line-free subsets of %[3]^4%; the details are somewhat tedious but they are on the wiki, see Lemma 2 of [http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds#n.3D4](http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds#n.3D4) At the 50-point level we see for the first time a set which is not contained in one of the %D_{4,j}% sets, which I have called X. In principle, it should now be possible to establish the upper bound %c_5 \\leq 150%, using Lemmas 2, 3, 4 on the wiki, though I will not have the time to verify this in the near future."},{"username":"christian-elsholtz","timestamp":"0004-02-20T05:00:00.000Z","contents":"746, Mosers problem in dimension 4 and 5. From Klas’ list in 742 it appears to me (just by a visual inspection, not by computer count) that possibly the distributions of the 43 vectors in the 3 planes are like  \n(13, 16,14) and (14,15,14) (only?)  \nThis would mean that the middle layer is quite full,  \nso, when constructing a 5 dimensional array out of the 4 dimensional configurations,  \nlike  \n(a b c)  \n(d e f)  \n(g h i), where %a+b+c \\leq 43%, etc, and also  \n%b +e+h \\leq 43%  \none should indeed be able to get a better upper bound.  \nPerhaps one of you can count the number of ones, twos and threes in the first column, for each solution and report the distributions? Given that for dimension 3 the extremal example is of type (6,4,6), i.e. middle layer is not dense, in dimension 4 we have (possibly) the opposite, so that for dimension 5 one could expect again the middle layer is less dense,  \nThe example by KS Chua in 736 has a distribution of  \n{{42, 40, 42}, {42, 40, 42}, {42, 40, 42}, {41, 40, 43}, {41, 40, 43}}  \nin the 5 parallel layers, so is in the spirit of this philosophy."},{"username":"klas-markstrom","timestamp":"0005-02-20T05:00:00.000Z","contents":"Moser n=4 Over night I ran the program to find all solutions for Moser’s problem for n=4 and 42 points.  \nThe solutions can be found here  \n[http://abel.math.umu.se/~klasm/moser-n=4-t=3-p=42.gz](http://abel.math.umu.se/~klasm/moser-n=4-t=3-p=42.gz)  \nThere are of course a large number of solutions, 86232, but again it seems that the number of non-isomorphic solutions might be quite small. A question for those analysing the solutions. Are all the 42 point solutions subsets of the 43 point extremisers or are there other maximal solutions as well?"},{"username":"christian-elsholtz","timestamp":"0008-02-20T05:00:00.000Z","contents":"Moser  \nThe observation made by Michael, in 738 and 740 has also been made  \nby V Chvatal, Canadian Math Bulletin, Vol 15, 1972, 19-21\\. “Remarks on a problem of Moser”.  \nThat paper, however, is more difficult to read, and rewriting that observation in modern notation, as done by Michael, was certainly useful. That paper also comments that the construction only gives %O( \\frac{3^n}{\\sqrt{n}})%, the reason being indeed that %A(n,d)\\leq 2^{n-d+1}% It can also be observed that the method gives for dimension 4 only 42 points, so that one should not expect that construction to be optimal, since %c_4'=43%."},{"username":"ks-chua","timestamp":"0008-02-20T05:00:00.000Z","contents":"%c_n^{\\mu}% by optimization and LYM The optimization approach can be applied to the weighted version  \n%c_n^{\\mu}%. The argument in 229 (minus the confusing  \nindexing) is : Let %G% be a uniform k graph with vertex set %V=\\{v_1,...,v_m\\}% and edge set %E_k% together with a weight  \nfunction %0 \\le w_i \\le 1% for each vertex %v_i% and set %f(x)= \\sum_{i=1}^m x_i w_i-\\sum_{(i_1,...,i_k) \\in E_k} x_{i_1}...x_{i_k} .% Then the maximal total weight %\\alpha% of an independent set  \nof vertices (no k of which is an edge) in %G% is given by %\\alpha=MAX_{x \\in \\{0,1\\}^n}f(x)=MAX_{x \\in [0,1]^n}f(x).% Applied to subset relation , with variables %x_I, I \\subset [n]% weighted by %1/\\begin{pmatrix} n \\cr |I| \\end{pmatrix}% and the known %c_n^{\\mu}=1% for %k=2%, it gives the  \nfollowing interpolation of the LYM equality. For all %x \\in [0,1]^{2^n}%, %\\sum_{I \\subset [n]} \\frac{x_I}{\\begin{pmatrix} n \\cr |I| \\end{pmatrix}}-\\sum_{I \\subset J \\;\\; or \\;\\; J \\subset I} x_I x_J \\le 1.% Choosing a 0-1 %x% corresponding to an antchain gives the usual LYM."},{"username":"kristal-cantwell","timestamp":"2012-02-20T05:00:00.000Z","contents":"If there are 1552 43-point solutions and 86232 42-point solutions  \nthen there must be one 42-point solution which is not a subset of any of the 43-point solutions, since each 43-point solution has 43 42-point subsets and 43*1552 is less than 86232."},{"username":"michael-peake","timestamp":"0008-02-20T05:00:00.000Z","contents":"%c_5 \\leq 151% by hand I used Terry’s classification, in comment 745, of 50pt, 51pt and 52pt subsets of %{}[3]^4% to show %c_5 \\leq 151%. My proof  \nis in the wiki. In 743, one group of 16 solutions had five points with no 2s.  \nOne of these solutions is the union of the following, and the other fifteen are reflections of it in one or more of the axes. %\\Gamma_{0,0,4} + \\Gamma_{0,1,3} + \\Gamma_{0,2,2} + \\Gamma_{1,2,1} + \\Gamma_{2,1,1} + \\Gamma_{3,0,1} + \\Gamma_{3,1,0}%"},{"username":"seva","timestamp":"0005-02-20T05:00:00.000Z","contents":"An algebraic DHJ(3) Not quite following the mainstream of this thread, and perhaps already discussed elsewhere — but anyway. There seems to be a nice version of the DHJ(3), which may be easier to attack and which looks (almost) equally exciting to me. Let’s say that a vector in %F_3^r% is _simple_ if either %1%, or %2% does not appear among its coordinates; say, %(1,0,0,1,0)% and %(2,2,2,0,2)% are simple vectors, while %(1,2,1,1,1)% is not. Furthermore, call a three-term progression in %F_3^r% simple if its difference is simple. Thus, a simple progression is a triple %a,b,c% of distinct elements of %F_3^r% with %a+b+c=0% and %a-b,b-c,c-a% all simple. The DHJ(3) implies that any positive density subset of %F_3^r% contains a simple progression. Is there any combinatorial or Fourier-theoretic proof of this fact? Notice that by the integer Roth theorem, any set %A\\subseteq[N]% of positive density contains a three-term progression with the difference, say, at most %N^{0.1}%; however, the corresponding argument does not seem to extend onto simple progressions in %F_3^r%."},{"username":"michael-peake","timestamp":"0005-02-20T05:00:00.000Z","contents":"%c_5 \\leq 150% by hand I think I have finished the long-winded proof that %c_5 \\leq 150%, in the wiki."},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"753. Michael: Excellent! Now we have two independent confirmations that %c_5=150% and thus %c_6=450%, and also quite a good understanding of the extremisers and near-extremisers (though probably not enough to completely close the gaps in the bounds %1302 \\leq c_7 \\leq 1348%). Incidentally, I placed this sequence on the OEIS at [http://www.research.att.com/~njas/sequences/A156989](http://www.research.att.com/~njas/sequences/A156989) and c”_n at [http://www.research.att.com/~njas/sequences/A156990](http://www.research.att.com/~njas/sequences/A156990) Seva: Thanks for the question, which I have added to [the wiki](http://michaelnielsen.org/polymath1/index.php?title=Tidy_problem_page). The same question was also posed, incidentally, by Randall in 480\\. I think the problem is similar to that of finding arithmetic progressions of length three in dense sets of [N] whose difference is a square. Ben observed once that such a result could be deduced from known results in “quadratic Fourier analysis” (in particular, the U^3 inverse theorem) and so there is a chance that something similar can be done here. Re: Moser: it may be helpful to try to find a non-computer-assisted proof of the upper bound %c'_4 \\leq 43%, as this may help us understand the structure of the extremisers and near-extremisers better (given that this strategy seems to have been quite successful for %c_n%). Actually, do we even have a human proof of %c'_3 \\leq 16%?"},{"username":"christian-elsholtz","timestamp":"2010-02-20T05:00:00.000Z","contents":"Human proofs of Moser’s problem in dimension 3 and 4  \n(with regard to terry’s question in 753) 1) L. Moser, Problem P.170 in Canad. Math. Bull. 13 (1970), 268.  \nOrigin of problem. 2) Komlos, solution to problem P.170 by Leo Moser,  \nCanad. Math.. Bull. vol (??check) (1972), 312-313, 1970. gives a solution like fixing n/3 coefficients, i.e. a lower bound of %\\frac{3^n}{\\sqrt{n}}%. 3) V. Chvatal, Remarks on a problem of Moser, Canad. Math. Bull. 15 (1972),  \n19-21. improves the constant in Komlos construction, essentially taking a union of slices,  \ngives a lower bound of %c'_4\\geq 42% and implicitly $c’_5\\geq 124$ etc,  \nby. relating the problem to a coding theory problem.  \nbut is aware of the solution with 43 points in dimension 4. 4) V. Chvatal,  \nEdmonds polytopes and a hierarchy of combinatorial problems  \nDiscrete Math. 4 (1973) 305-337.  \nReprinted: Discrete Mathematics, 306, 2006, 886-904. gives a human proof of %c'_3=16%, it’s actually a linear programming method, with some geometrically motivated clever scalings. perhaps this can be generalized. 5) A.K. Chandra, On the solution of Moser’s problem in four dimensions,  \nCanad. Math. Bull. 16 (1973), 507-511. gives a human proof of %c'_4=43% with some useful info on number of points in certain sub-configurations."},{"username":"jozsef","timestamp":"2010-02-20T05:00:00.000Z","contents":"Exact bounds It is great that you find the exact values of %c_5% and %c_6% !  \nI was wondering if you could check if the extremal configurations are “corner-free” or not. By corner-free I meant that if for every point you assign a 3-dimensional point (a,b,c) where a is the number of 0-s, b is the number of 1-s, and c is the number of c-s then there is no triple of the form (x+d,y,z), (x,y+d,z),(x,y,z+d). It would be also interesting to know if there is at least one such extremal configuration exists for k=6\\. It is for checking if our present lower bound strategy is optimal or not. (Probably not)"},{"username":"kristal-cantwell","timestamp":"2012-02-20T05:00:00.000Z","contents":"I tried these two links [http://www.research.att.com/~njas/sequences/A156989](http://www.research.att.com/~njas/sequences/A156989) [http://www.research.att.com/~njas/sequences/A156990](http://www.research.att.com/~njas/sequences/A156990) and the I didn’t get a sequence for the second one which  \nI think is for %c''_n% even though I tried it twice."},{"username":"christian-elsholtz","timestamp":"0001-02-20T05:00:00.000Z","contents":"Moser, n=5 Comment on the example that shows %c'_5 \\geq 154%: If one shifts the digits from {1,2,3} to {-1,0,1}, then one observes that  \nthe example has all 80 vectors with “sum of coordinate entry squares=3,  \ni.e. two zeros and three times %\\pm 1%  \nall 40 vectors with sum of entry squares=4, i.e. one zeros and four %\\pm 1% entries, and four more vectors with all entries being %\\pm 1 %. In other words, this is a union of two Behrend spheres and a third partial sphere."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"Dear Kristal, the sequence got moved to [http://www.research.att.com/~njas/sequences/A090245](http://www.research.att.com/~njas/sequences/A090245) because of a duplication."},{"username":"sune-kristian-jakobsen","timestamp":"2011-02-20T05:00:00.000Z","contents":"Contains on %(|A|, |B|, |C|)%.  \nLet %S := \\Gamma_{a+r,b,c} \\cup \\Gamma_{a,b+r,c} \\cup \\Gamma_{a,b,c+r} \\subset [3]^n%, where %r>0%, and let %A \\cup B \\cup C% be a (combinatorial-)line-free subset of %S%, such that %A \\subset \\Gamma_{a+r,b,c}, B \\subset \\Gamma_{a,b+r,c}, C \\subset \\Gamma_{a,b,c+r}%. Now we want to find the contains on %(|A|, |B|, |C|)%. It is possible to show that %\\frac{|A|}{\\binom n{a+r,b,c}}+\\frac{|B|}{\\binom n{a,b+r,c}}+\\frac{|C|}{\\binom n{a,b,c+r}}\\leq 2% (216). That is: The equal-slice measure of %A \\cup B \\cup C% is at most %2%. I thought this inequality was sharp when none of A, B, and C are empty, but this was disproved in Michael.220 in the case %a=b=c=r=1%. The following is a general counterexample except for the case where %a=b=c=0% where it is trivially true. Assume WLOG that %c \\neq 0%, and let M be a proper non-empty subset of %\\binom {[n]}{c}% (such a set M exist since %n\\geq 2%, because %c,r>0%). Now let %C=\\Gamma_{a,b,c+r}%, let A be the set of elements %x\\in\\Gamma_{a+r,b,c}% where the set of i’s such that %x_i=3% is an element in M, and let B be the set of elements %x\\in\\Gamma_{a,b+r,c}% where the set of i’s such that %x_i=3% is _not_ an element in M. Now %A \\cup B \\cup C% contains 2 elements of every line, and thus %\\frac{|A|}{\\binom n{a+r,b,c}}+\\frac{|B|}{\\binom n{a,b+r,c}}+\\frac{|C|}{\\binom n{a,b,c+r}}= 2%. Notice that the example in 220 is not a special case of the above example."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"Moser(3) I found the %c'_4 = 43% paper online at [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/72/286/CS-TR-72-286.pdf](ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/72/286/CS-TR-72-286.pdf) I also reconstructed the %c'_3 \\leq 16% argument as follows. For every %0 \\leq m \\leq n%, let %\\alpha_m% be the density of A inside the set of points with exactly m 2s. Thus for instance when n=3 we have %|A| = 8 \\alpha_0 + 12 \\alpha_1 + 6 \\alpha_2 + \\alpha_3%. We also have the linear inequalities %2\\alpha_i + \\alpha_j \\leq 2% for %i < j%, as can be seen by double-counting those geometric lines with j-i wildcards and i fixed 2s. There is also the refinement %2 \\alpha_0 + \\alpha_1 + \\alpha_2/2 \\leq 2%, formed by checking this inequality by hand in the two-dimensional case, which then implies the general case by an averaging argument. In three dimensions, one can get %|A| \\leq 16% by a suitable linear combination of these inequalities. The same argument only seems to give %c'_4 \\leq 48% in four dimensions. I haven’t yet digested the paper above to see how to do better."},{"username":"terence-tao","timestamp":"0002-02-20T05:00:00.000Z","contents":"Moser(3) There is an additional inequality in 3 and higher dimensions, namely %\\alpha_0 + \\alpha_2 \\leq 4/3%. This can be proven by first verifying it inside the six-element set {333, 322, 311, 223, 113, 212} and then averaging over all the symmetries of the Moser cube. This seems to lower the upper bound for %c'_4% to 46 2/3 (and hence to 46, by integrality)."},{"username":"terence-tao","timestamp":"0007-02-20T05:00:00.000Z","contents":"Moser(3) In 743 it was remarked that the %c'_4 = 43% extremisers were distributed in the patterns (3,16,24,0,0), (4,15,24,0,0), (4,16,23,0,0), or (5,20,18,0,0), where (a,b,c,d,e) means that there are a numbers with 0 2s, b numbers with 1 2, etc. (For comparison, the entire cube %[3]^4% has distribution (16,32,24,8,1)). It is not so surprising that the d and e slots are empty (putting a point in there tends to be very expensive with regard to the higher slices), but it is interesting that there is such a huge bulge in the c position; as noted in 744, this already shows that one cannot have three parallel slices of 43-point sets, leading to the bound %c'_5 \\leq 3 c'_4 - 1 = 128%. It should be straightforward to compute the distributions of the 42-point sets provided in 747\\. If the bulge persists,then there should not be any parallel slices of three 42-point sets either, which would let us shave the upper bound from 128 to 127 and put significant constraints on any 126-point set. Eventually, one may want to work in (a,b,c,d,e) space. Let %\\Omega% be the convex hull of all (a,b,c,d,e) that are attainable from Moser sets in %[3]^4%, then the cardinality of a Moser set A in %[3]^5% is equal to %3(a'+b'+c'+d'+e')%, where %(a',b',c',d',e')% is the average of three points %(a_1,\\ldots,e_1), (a_2,\\ldots,e_2), (a_3,\\ldots,e_3)% in %\\Omega% (corresponding to the 3 slices of %A%) and thus also lies in %\\Omega%. On the other hand, by considering vertical lines (which can each intersect at most two of the three slices of A) we see that %(a',b',c',d',e') \\leq \\frac{2}{3} (16,32,24,8,1)%, in the sense that each component on the left is less than the corresponding component on the right. Linear programming should then give us some bound on %3(a'+b'+c'+d'+e')%, but first we need to understand the shape of %\\Omega%. As a first approximation we might make the simplifying assumption that d=e=0 (so that %\\Omega% is some three-dimensional polyhedron), given that this is what happens in the 43-point extremisers."},{"username":"christian-elsholtz","timestamp":"0001-02-20T05:00:00.000Z","contents":"Moser, in dimension 2,3,4,5  \nWe can assume that e=0 (in Terry’s notation in 761) for a very simple reason:  \nSuppose that e=1, meaning the central points is used, then there can be at most  \n%\\frac{3^n-1}{2}+1 % points in the set, since for each non-central point there is a forbidden point by symmetry.  \nIn dimension 2,3,4,5 the lower bounds are larger than the bound above, so that the central point is not used."},{"username":"klas-markstrom","timestamp":"0001-02-20T05:00:00.000Z","contents":"Moser(3)  \nThe statistics for the number of 2s in the point of the 42 point sets can be found here  \n[http://abel.math.umu.se/~klasm/Moser-42-stat.pdf](http://abel.math.umu.se/~klasm/Moser-42-stat.pdf) As you can see there is a small group of solutions with only 12 2s, but the rest have at least 17\\. There is also a group of solutions with 2 in the d slot. The solutions with 12 2s can be found here  \n[http://abel.math.umu.se/~klasm/moser-n=4-42-12](http://abel.math.umu.se/~klasm/moser-n=4-42-12)  \nand the solutions with non-zero d slot here  \n[http://abel.math.umu.se/~klasm/moser-n=4-42-e=2](http://abel.math.umu.se/~klasm/moser-n=4-42-e=2) _[“e” corrected to “d” – T.]_"},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Moser(3) Klas: thanks, this is very interesting! It means in particular that the only 43+43+42 = 128 point solutions in %[3]^5% can come when the 42-point slice is one of the eight c=12 solutions, and one of the 42-point slices is one of the six c=18 point solutions. This looks like enough reduction in the entropy to eliminate this case by computer search, and maybe even by hand if one is clever enough. (It also suggests that we should understand the structure of these anomalously low-c solutions better. Given how few they are, it is likely that there is just one solution up to isomorphism in both cases). That should lower the upper bound for %124 \\leq c'_5 \\leq 128% to 127. Presumably the number of 41-point solutions is too enormous to list explicitly… but perhaps the number of low-c solutions is small, and could be worth searching for by an integer program. Note for instance in a 42+42+41=125 type solution, if the 42-point solutions are not from the c=12 class, then they each have c >= 17, and so the 41 point solution has c <= 14\\. So perhaps it is worth trying to enumerate the 41-point solutions with c <=14? From the 42-point and 43-point data one may hope that there are relatively few of these. [It would be good, though, to have some explanation of why c tends to be high. Note that there is a 40-point solution with (a,b,c,d,e) = (8,32,0,0,0), so this phenomenon may well be very transient.] Christian: It is true that if one has e=1 in a Moser set in %[3]^4% then the number of points is at most %\\frac{3^4-1}{2}+1 = 41%, which is less than %c_4 = 43%. But this does not yet rule out the possibility that one of the 3-slices of, say, a 125-point Moser set in %[3]^5% could have e=1, since the distribution could be (say) 42+42+41 or 43+43+39\\. (The argument does tell us, though, that the _middle_ slice has e=0). But presumably we can improve on the upper bound, which leads to a subquestion: what’s the largest size of a Moser set in %[3]^4% that contains the centre 2222? If we can improve the easy upper bound 41 to 38 then we can rule out this case completely, or failing that if we get a reasonable description of the 39+ point sets with the centre then we should be able to eliminate it by hand. [Also, if we know that every slice of a Moser set has e=0, this implies after rotation of the cube that the middle slice of the Moser set has d=0\\. Similarly, if we learn that every slice of a Moser set has to have small d, then this should imply that at least one middle slice has small c, which by the previous discussion should be helpful in limiting the number of possibilities.]"},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Moser(3) In fact, I can now rule out 128-point sets as follows. If one had a 128-point set, then the set must slice as 43+43+42 (or a permutation thereof) in every direction. But all 43-point slices have d=0 and all 42-point slices have d <= 2\\. Thus, there are at most two points amongst the sixteen points x222y, x22y2, x2y22, xy222 with x,y = 1,3 that lie in the set. Averaging this over all orientations we see that at most one eighth of all points with exactly 3 2s lie in the set. Thus, by the pigeonhole principle and a double counting argument, there exists a middle slice of the set with this property, i.e. a slice with c <= 24/8 = 3, but we know from the 43-point and 42-point distribution data that this is impossible. To push this argument further, it would be helpful to classify the large-d 41-point solutions (as well as the large-e and low-c); presumably there are not very many of these."},{"username":"ks-chua","timestamp":"0005-02-20T05:00:00.000Z","contents":"Moser(3) From 762, there are at most 2^13=8192 subsets of size 14 in [3]^3 containing (222) which may be line free. This can be checked and they all contain at least 2 lines. So all such set has size <=13\\. Doing the n=4 case this way requires checking 2^41."},{"username":"terence-tao","timestamp":"0006-02-20T05:00:00.000Z","contents":"Moser(3) Dear KS: Thanks! Actually, knowing that 3-dimensional Moser sets with 222 have at most 13 points automatically implies that 4-dimensional Moser sets with 2222 have at most 40 points, because if they had 41 points, then every pair of antipodal points in the cube have exactly one representative in the set, which implies that every middle slice of the set has 14 points, which you have just ruled out. In fact, if a 4D Moser set contains 2222, we now know that every middle slice must be missing at least one antipodal pair of points. There are four middle slices, and every antipodal pair belongs to at most three of them, so we must miss at least two antipodal pairs and so Moser sets with 2222 can have at most 39 points. This is right on the edge of what we need to rule them out for 125 = 43 + 43 + 39-point 5D Moser sets; perhaps a slightly more sophisticated analysis can eliminate them completely. If so, this would imply that a 125-point 5D Moser set cannot contain any point with 4 or 5 2s. In any event, we have already established this for 126-point sets."},{"username":"terence-tao","timestamp":"0009-02-20T05:00:00.000Z","contents":"Moser(3) I can almost eliminate points with four 2s completely from the 5D Moser problem. By 767, we know that any 4D Moser set with 2222 has at most 39 points. Hence, the only way a 5D Moser can have a point (say 12222) with four 2s is if the 1****, 2****, 3**** slices have 39+43+43=125 points between them. But by 743, the 43-point slices have c = 24, 23, or 18\\. Meanwhile, the 39-point slice must have c at least 10, because it is allowed to miss at most two of the 12 antipodal pairs in c (and all antipodal pairs have at most one point in the set). Looking at vertical lines, we see that the total of the three c’s from the three slices is at most 2*24\\. This is only possible if the two 43-point slices have c=18. So we are now in a position where the 2**** and 3**** slices are one of the 16 solutions with (a,b,c,d,e) = (5,20,18,0,0). This looks like a lot of information available, but to proceed further we would need to understand the structure of these 16 exceptional solutions (presumably they are all isomorphic to each other)."},{"username":"jason-dyer","timestamp":"0009-02-20T05:00:00.000Z","contents":"Counting problems There are %4^n-3^n% sets of 3 distinct points x, y, z in %[3]^n% such that they form a combinatorial line. Therefore there are %3^n(3^n-1)(3^n-2)-4^n+3^n% sets that do **not** form a combinatorial line. Take those sets, and consider the pairs x and y, y and z, and x and z. Counting Problem A: How many of those sets have all three pairs form partial combinatorial lines? (I’m using “partial combinatorial line” to mean given a pair there’s a third point in %[3]^n% that will make it a combinatorial line.) Counting Problem B: How many of those sets have exactly two pairs form partial combinatorial lines? Counting Problem C: How many of those sets have exactly one pair form a partial combinatorial line? Counting Problem D: How many of those sets have no pairs form a partial combinatorial line? I have made progress on B by making a “fake wildcard” (using the symbol #). For example, take a sequence like *#212\\. Given x = both wildcards set at 1 (11212) then y = 21312 or 31312 and z = 12312 or 13312\\. Repeat this for all possible wildcard settings, remove all duplicates, and all sequences ..212 are accounted for. Then it just is a matter of counting how many possible settings it is with both * and # wildcards and removing duplicates from there (*#212 will yield the same results as #*212), and problem B is solved. But that’s a messy calculation so I wanted to share here my current progress before I went any farther. (Also someone might point out where we have the solution to this already 600 posts back.)"},{"username":"kristal-cantwell","timestamp":"0002-02-20T05:00:00.000Z","contents":"The 18 points with 2 2’s in the 2**** slot will produce 18 points  \nwith 3 2’s in the total configuration moreover they will all have  \n2’s in one coordinate x(the one in which the point with 4 2’s has  \na coordinate not equal to 2\\. These 18 points will have 36  \ncoordinates not equal to 2 among the four coordinates not equal  \nto x but that means there must be at least 9 coordinates  \nnot equal to 2 at one of these coordinates. A majority of these coordinates  \nmust be equal to a value not equal to 2 without loss of generality say 1  \nthen at most six of these points can be equal to 1 since the ramaining  \ncoordinate not equal to 2 can take one of two values at the remaining  \n3 coordinates giving a total of 6\\. thus we have one coordinate with  \nthree 1’s and three 3’s among the 18 triples. Then we slice at this coordinate  \nthe 3 triples of two in the one and three block makes these slices have  \nsize 41 the point with four twos becomes a point with three twos in  \nthe 2 slice thus forcing it to have value 42 or less so the sum is 124.  \nThus if there is one point with four 2’s the value is 124 or less."},{"username":"terence-tao","timestamp":"0003-02-20T05:00:00.000Z","contents":"Moser(3) Kristal: thanks! I wrote up your argument on the [wiki page](http://michaelnielsen.org/polymath1/index.php?title=Moser%27s_cube_problem). In fact the argument shows that any 125-point Moser set cannot have 43 points in its 2**** slice, and in fact cannot have 42 points either unless it is one of the eight anomalous (a,b,c,d,e) = (6,24,12,0,0) solutions at that level. If we can eliminate those anomalous solutions, it implies that 125-point Moser sets must always have their thin slice in the middle, i.e. they must slice as 43+39+43, 43+40+42, 42+40+43, or 42+41+42."},{"username":"michael-peake","timestamp":"0006-02-20T05:00:00.000Z","contents":"Moser(3) One solution of the (6,24,12,0,0) set is made of the following slices.  \nthe other solutions would be reflections in one or more axes %\\Gamma_{220}+\\Gamma_{202}+\\Gamma_{022}+\\Gamma_{112}+\\Gamma_{211}%"},{"username":"michael-peake","timestamp":"0001-02-20T05:00:00.000Z","contents":"Moser(3) Terry, in your writeup of Kristal’s proof, you said that any solution with d>2 had at most 40 points. From reading this blog and the wiki, I can only tell that such a solution has at most 41 points. What have I missed?"},{"username":"terence-tao","timestamp":"0007-02-20T05:00:00.000Z","contents":"Oops, you’re right, which means that my strengthening of Kristal’s argument doesn’t quite work (at least for the problem of getting the bound 124 on the nose; it still works if one just wants to prove %c'_5 \\leq 125%). I’ve readjusted the wiki appropriately."},{"username":"terence-tao","timestamp":"0002-02-20T05:00:00.000Z","contents":"Strategy for Moser(3) I think we have a decent shot at pushing the upper bound for %c'_5% (currently at 127) all the way down to the lower bound of 124\\. The idea is to exploit the numerically observed phenomenon that large four-dimensional slices (particularly those with 42 and 43 points) tend to have very large “c” values (points with two 2s) but very low “d” values (points with three 2s). In particular, middle slices such as 2**** (if large) tend to have large “c”, and side slices such as 1**** or 3**** tend to have small “d”. But (as implicitly observed in Kristal’s argument) one middle slice’s “c” is another side slice’s “d”. For instance, if the 2xxxx slice contains an element with two additional 2s, e.g. 21223, then this contributes a “d” point to the side slices *1*** and ****3\\. More generally, double counting tells us that the total “c”‘s of all the five middle slices put together, is equal to 3/2 times the total “d”s of all the ten side slices put together. By the pigeonhole principle, there must therefore exist one coordinate such that the “c” of the middle slice of that coordinate is at most 3/2 the sum of the “d” of the two side slices of that coordinate. Without loss of generality we may assume this is the first coordinate; thus c(2****) <= 3/2 [ d(1****) + d(3****) ] (1) where I use c(2****) to denote the “c” count of the 2**** slice (i.e. the number of points in this slice with two additional 2s), etc. The listing of 43 and 42-point slices tells us that for such slices, c is at least 12 and d is at most 2\\. Comparing this with (1) we already see that not all slices can have 42 or more points, which already recovers our existing upper bound of 127=43+43+41\\. But I think we can do better. Firstly, it seems that having a lot of “d”s really does push down the total size of a slice. In the extreme case, if d is equal to its maximal value of 8, then it is easy to see that c is at most 12, b is at most 8 (note for instance that at most one of 2111, 2133, 2313, 2331 can lie in the set if every point with 3 2s lie in the set), and a is at most 8, leading to only 36 points. Conversely, if c is really small (and d=e=0, which we know to be true for the middle slice), this also pushes down the size of a slice. For instance, in the extreme case when c=0, the bound b+4a <= 64 (coming from double counting lines such as 1111, 1112, 1113) and the trivial bound b <= 32 gives at most 40 points in the slice. I’m hopeful that if we get good enough upper bounds on the size of slices for various small values of c or large values of d, we should be able to solve the problem. (For comparison, in the 124-point examples given in 736, the middle slice is (8,32,0,0,0) (thus c=0) and the side slices are (1,16,24,0,0), (2,16,24,0,0) or (3,16,24,0,0) (thus d=0).)"},{"username":"christian-elsholtz","timestamp":"0007-02-20T05:00:00.000Z","contents":"Moser:  \nI have typed some notes (to use Latex) and have put them here:  \n[http://www.ma.rhul.ac.uk/~elsholtz/WWW/blog/mosertablogv01.pdf](http://www.ma.rhul.ac.uk/~elsholtz/WWW/blog/mosertablogv01.pdf) This contains two parts:  \n1) A human proof that a Moser set in dimension 3 with centre point 222 contains at most 13 points. Earlier proved by KS Chua 766 (by computer). 2) A sketch to prove that in dimension 4 a Moser set with centre 2222 contains at most 39 points. Proved earlier by Terry in 767.  \nEssentially I proved that the two missing pairs of antipodal points must be included  \nin sets of the form 22xy or xy22, which means I have localized them somewhat. possibly one can squeeze more of the argument. Is there actually a set with 39 points? (those computer programmes that classified the  \nMoser sets with 43 points can possibly be adapted). Perhaps the arguments prove useful for the discussions on dimension 5."},{"username":"kristal-cantwell","timestamp":"0001-02-20T05:00:00.000Z","contents":"We can show that any line free configuration  \nHas 126 or less points If there is a 42 or 43 in the center  \nThen the value is 125  \nCase 1  \nThere are 17 or more points with 3 2’s and two 1 or 3’s one coordinate has all  \n2’s the 34 or more non two coordinates are divided among  \nfour coordinates one must contain 9 or more but at most six can have  \nthe one value so there will be at least three points with value one and  \nthree with value 3 at that coordinate we slice at that coordinate  \nand get the slices associated with one and three  \nhaving three or more points with three 2’s and hence  \nhaving 41 points or less thus the value is 41 + 41 +43  \nCase 2 there is one remaining anamolous case which from  \nNote 771 contains  \nall points with two coordinates equal to  \n2 and two points equal to one of the values 1 or three  \nthen the set of points 1122 1212 1221 3322 3232 3223 which is in every configuration of this case has three points  \nwith value 1 and three with value 2 which have two twos in the center slice  \nand hence three 2’s overall we can slice at this coordinate and get by similar reasoning  \nto the above the two noncenter cubes having 41 points or less and hence  \nthe entire configuration having 125 points or less Now we assume we have a line free configuration with 127 points  \nCenter must be 41  \nThen other two edges must be 43  \nNext we note that if the center slice  \ncontains no point with 2 or more 2’s it must have 40  \npoints or less and we are done  \nwe use the inequality  \n%2\\alpha_i + \\alpha_j \\leq 2%  \nfor i less than j  \nmentioned in 759 for the center cube for i = 0  \nand j =1\\. Then it implies that a center slice  \nwith no points with 2 or more twos can have at most  \n40 points. Now using the above  \nany configuration with 127 or more points  \nmust have a slice in the center with 41 points or less  \nand also its center slice must contain at least one point with  \ntwo twos in the coordinates of the slice and three overall  \n(or else we have 43 + 43 +40 points and that is less than 127  \nand we are done) that slice must have one coordinate not equal  \nto 2 or else the configuration will have less than 127 points  \nwe can slice along that coordinate and get one of the non-center  \nslices having a point with three coordinates equal to 2 and thus  \nhaving value less than or equal 42 now by the above the center slice  \nis 41 or less and we have less than 127 points at most 41 + 42  \n+43 is 126 points"},{"username":"terence-tao","timestamp":"0008-02-20T05:00:00.000Z","contents":"Moser(3) Thanks, Kristal! Now we are down to %124 \\leq c'_5 \\leq 126%. I’ve written up your proof on the wiki. Christian: Thanks for the human proof of these facts; it is good to have independent confirmation of these things. I’ve also linked your notes to the wiki."},{"username":"ks-chua","timestamp":"2012-02-20T05:00:00.000Z","contents":"Moser(3) Here is a Moser set containing (2222) of size 36.  \n0,2,3,5,7,9,11,15,17,19,21,23,24,26,27,29,33,35,36,38,40,  \n41,46,48,50,52,55,58,60,62,64,66,68,70,76,79 It seems the upper bound of 39 is too high.  \nOne can classify 13-point Moser containg (222) and 12 points.  \nCall a point odd if the sum of coordinates in (123-form) is odd and even  \notherwise. Then the omitted antipodal must be odd ie. the omitted point is  \neither the corner or center of a side slice or the complement of this in the  \nmiddle slice. If there are only 2 omitted points in [3]^4 Moser with center. They must together account for the four 2 in the center slices. The above rule out  \nthe case when both have two 2."},{"username":"klas-markstrom","timestamp":"2011-02-20T05:00:00.000Z","contents":"Moser(3)  \nAfter waiting for some fre time on a Linux-cluster I have now constructed the 41 point solutions. There are quite a few of them, 2 765 200.  \nHere are the statistics for the number of 2:s  \n[http://abel.math.umu.se/~klasm/Moser-41-stat.pdf](http://abel.math.umu.se/~klasm/Moser-41-stat.pdf)  \nand here are the solutions.  \n[http://abel.math.umu.se/~klasm/solutions-4-t=3-41-moser.gz](http://abel.math.umu.se/~klasm/solutions-4-t=3-41-moser.gz)  \nIn order to make thing more compact each point is now given by its number in the lexicografic ordering and solutions are separated by a line with a single “Y”."},{"username":"terence-tao","timestamp":"0001-02-20T05:00:00.000Z","contents":"Moser (3). Thanks Klas! Scanning through the statistics, I see two things which look quite encouraging: 1\\. d is always small; the largest value of d among any 41-point set is at most 3 (and in fact is at most 2, with just one family of 256 exceptions). 2\\. c is usually large. There are 16 anomalous solutions with c=6 (which are worth taking a closer look at), but everyone else has c at least 11. Inserting this into my equation (1) from 774, we see that not all of the three slices here can have 41 or more points, unless the middle slice is one of the 16 anomalous c=6 solutions, and in the latter case the other two slices can’t have 43 points (because then they won’t contribute enough d’s to (1)). Unfortunately this is not enough to improve the 126 upper bound because of the 43+40+43 and 40+43+43 cases, but we understand the 43 point slices pretty well and so perhaps we can do something else to eliminate these cases also. For instance, if (1) holds and we are in the 43+40+43 case then d(1****)=d(3****)=0 and so c(2****)=0, which forces b(2****)=32 and a(2****)=8, which is looking pretty incompatible with the 43-point slices, though I cannot rule them out completely yet."},{"username":"klas-markstrom","timestamp":"0003-02-20T05:00:00.000Z","contents":"Moser(3)  \nI let a script run during the evening to pick out some of the more interesting solutions. The flies are in the original format. Here are the solutions with c=6  \n[http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-c=6.gz](http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-c=6.gz) Here are the solutions with d=1  \n[http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-d=1.gz](http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-d=1.gz)  \nthe solutions with d=2  \n[http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-d=2.gz](http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-d=2.gz)  \nthe solutions with d=3  \n[http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-d=3.gz](http://abel.math.umu.se/~klasm/moser-n=3-t=3-41-d=3.gz)"},{"username":"klas-markstrom","timestamp":"2010-03-20T04:00:00.000Z","contents":"Did my posting of the low-c solutions just get stuck in the spam filter or did it disappear? _[It got stuck in the spam filter – I think any comment which has too many hyperlinks in it gets flagged as spam. I’ve fished it out. -T.]_"},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"Any line free configuration has 125 or less points. The center slice is less than 42 because we have shown that a center slice of 42 or more means a total of 125 points or less. The center slice is 40 or more since if it is 39 or less the total is at most 125. Then the center is 40 or 41 and we have two cases with the center 40 or 41 and the sum 125  \n43 40 43  \n43 41 42  \nlet us first look at the case in which the center is 41. Now this must have a point p with 3 2’s in the center slice since 4a + b must be less than  \n64 and b must be 32 or less so if it is not then there are only 40 points in the center  \nslice. Then we can slice on one of the coordinates of p not equal to two  \nand get a non-center cube with 42 points with a point with with three twos or a point a non-center  \ncube with 41 points which together with the center cube in the new slice  \nwhich must have 41 points gives a total of 125 or less 41 43 or 41 41 43 but this 125 and we are done.  \nSo we must have a side cube with 42 points and two points with three two’s  \nand I have checked all cases of these 42 point configurations  \nthey are available at [http://abel.math.umu.se/~klasm/moser-n=4-42-e=2](http://abel.math.umu.se/~klasm/moser-n=4-42-e=2)  \nand in each case the two points with three twos have a coordinate at which one point has a one and the other a three  \nso we can slice at this coordinate and we get two noncenter cubes that must have at least one point with  \nthree coordinates equal to 2 and hence the these two cubes  \nmust have value 42 or less and since the center cube has value  \n41 we have a total of 125 and we are done with this case. the only case left is one in which the center is 40 and the remaining cubes  \nare 43 now we divide this into two cases  \nfirst the center contains a point with two threes  \nthen we can cut at a coordinate at which this point is not two  \nand force the case 43 41 42 which we have just done. now it must a 43 40 43 division must occur in every slice  \nor we are back to the previous case  \nsince 4a + b must be less than  \n64 and b must be 32 or less  \nwe must have all 32 points with one two must be in the center slice  \nand all 32 points must be in all center slices so  \nwe must have all points with exactly two coordinates equal to 2 in the configuration. Now we note that in the division of the configuration into cubes with each cube corresponding to a fixed choice  \nof the first two coordinates we have the following all the points with first two coordinates equal to  \none of 1 or 3( there are four of these) must have the center cube occupied hand hence by must have 13 or less see post 775  \npoints the center cube can only have 8 points as the remaining points have 3 or more twos, the remaining cubes  \ncan have at most 16 points this gives a total of 4*13 + 8 +4*16 = 124 and we are done."},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"Moser(3) Kristal, I’ve written up most of your proof on the wiki, but I had trouble with the last paragraph. I agree that the only case left is when all slices are 43+40+43, and that the 126-point set contains every point with two 2s, and no point with three or more 2s. But I don’t think that this implies that the corner slices 11***, 13***, 31***, 33*** contain their center. Instead, they seem to contain all six points with two 2s, and perhaps half of the 12 points with one 2 and at most 2 of the 8 points with no 2s (because we can divide these 8 points into two quadruplets ***=111,133,313,331 and ***=333,311,131,113, each of which can have at most one point), leading to as many as 14 points which is not quite good enough. We are beginning to brush up against the sphere packing problem that Michael observed back in 737, 738\\. Once we have all of the 80 points in %[3]^5% with exactly two 2s, we have at most 40 of the 80 points with one 2, and we have to show that there are at most 4 points (or 5 points, if one is willing to settle for the 125 upper bound) remaining amongst the 32 points with no 2s. Note that any two points in this final component cannot differ in exactly two positions, but they may possibly differ in one position, so it isn’t quite a classical sphere packing problem here."},{"username":"michael-peake","timestamp":"0006-03-20T04:00:00.000Z","contents":"Moser(3) If all 80 points with two 2s are included, then at most 4 points from %{}[1,3]^5% can be chosen.  \nTwo of these have an even number of 1s, and two have an odd number. Suppose 11111 is one of the points. All points with two 3s are excluded, so the only points allowed with an odd number of 1s are those with four 3s. But all those points differ from each other in two positions, so at most  \none of them is allowed."},{"username":"terence-tao","timestamp":"0007-03-20T04:00:00.000Z","contents":"Moser(3) Thanks Michael! I’ve completed the proof of the 125 upper bound on the wiki. Only one more improvement left before we pin down %c'_5% exactly…"},{"username":"jason-dyer","timestamp":"0008-03-20T04:00:00.000Z","contents":"%\\overline{c}^\\mu_6 < 18% First note that there are eight extremal solutions to %\\overline{c}^\\mu_3%: Solution I: remove 300, 020, 111, 003  \nSolution II: remove 030, 111, 201, 102  \nSolution III (and 2 rotations): remove 030, 021, 210, 102  \nSolution III’ (and 2 rotations): remove 030, 120, 012, 201 Also consider the same triangular lattice with the point 020 removed, making a trapezoid. Solutions based on I-III are  \nSolution IV: remove 300, 111, 003  \nSolution V: remove 201, 111, 102  \nSolution VI: remove 210, 021, 102  \nSoltuion VI’: remove 120, 012, 201 Suppose we can remove all equilateral triangles on our 7x7x7 triangular lattice with only 10 removals. The triangle 141-411-114 must have at least one point removed. Remove 141, and note because of symmetry any logic that follows also applies to 411 and 114\\. (1 removal total) There are three disjoint triangles 060-150-051, 240-231-330, 042-132-033, so each must have a point removed. (Now only six removals remaining.) The remainder of the triangle includes the overlapping trapezoids 600-420-321-303 and 303-123-024-006.  \nIf the solutions of these trapezoids come from V, VI, or VI’, then 6 points have been removed.  \nSuppose the trapezoid 600-420-321-303 uses the solution IV (by symmetry the same logic will work with the other trapezoid). Then there are 3 disjoint triangles 402-222-204, 213-123-114, and 105-015-006\\. Then 6 points have been removed.  \nTherefore the remaining six removals must all come from the bottom three rows of the lattice. Note this means the “top triangle” 060-330-033 must have only four points removed so it must conform to solution either I or II, because of the removal of 141. Suppose the solution of the trapezoid 600-420-321-303 is VI or VI’. Both solutions I and II on the “top triangle” leave 240 open, and hence the equilateral triangle 240-420-222 remains. So the trapezoid can’t be VI or VI’. Suppose the solution of the trapezoid 600-420-321-303 is V. This leaves an equilateral triangle 420-321-330 which forces the “top triangle” to be solution I. This leaves the equilateral triangle 201-321-222\\. So the trapezoid can’t be V. Therefore the solution of the trapezoid 600-420-321-303 is IV. Since the disjoint triangles 402-222-204, 213-123-114, and 105-015-006 must all have points removed, that means the remaining points in the bottom three rows (420, 321, 510, 501, 312, 024) must be left open. 420 and 321 force 330 to be removed, so the “top triangle” is solution I. This leaves triangle 321-024-051 open, and we have reached a contradiction."},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"Any Moser set with at least 42 points in the middle slice has at most 124 points. There are two cases one the c value of the middle slice is 17 or more.  \nThen these points have 34 or more coordinate values not equal to 2 divided  \namong four coordinates one of these must have 9 of these coordinates  \nat most six of these can have value 1 or 3 thus at least 3 must be 3 and three must be one and also there must be more than 5 of either one or three. Now we slice at this coordinate then both slices have three points with three coordinates equal to 2\\. Then they must have at most 41 points.  \nNow since the most points with three coordinates equal to 2 in a slice  \nof 41 is 4 and we have one slice with 5 or more of these points that slice  \nmust have at most 40 points. Now since one slice is of size 40 or less the other  \n41 or less and the remain slice trivially has 43 or less we have at most 124 points."},{"username":"klas-markstrom","timestamp":"0003-03-20T04:00:00.000Z","contents":"HOC(3)  \nI have now found the extremal solutions for the weighted problem in the hyper-optimistic conjecture, again using integer programming.  \nThe first few values are  \nc^{\\mu}_2=4 with 3 solutions  \nc^{\\mu}_3=6 with 9 solutions  \nc^{\\mu}_4=9 with 1 solution  \nc^{\\mu}_5=12 with 1 solution The solution for n=5 is here  \n[http://abel.math.umu.se/~klasm/solutions-n=5-k=3-HOC](http://abel.math.umu.se/~klasm/solutions-n=5-k=3-HOC) The solution for n=4 is here  \n[http://abel.math.umu.se/~klasm/solutions-n=4-k=3-HOC](http://abel.math.umu.se/~klasm/solutions-n=4-k=3-HOC) I’ll see if I can do n=6 as well."},{"username":"klas-markstrom","timestamp":"0003-03-20T04:00:00.000Z","contents":"HOC(3) The solutions for n=3 are here  \n[http://abel.math.umu.se/~klasm/solutions-n=3-k=3-HOC](http://abel.math.umu.se/~klasm/solutions-n=3-k=3-HOC) The solutions for n=2 are here  \n[http://abel.math.umu.se/~klasm/solutions-n=2-k=3-HOC](http://abel.math.umu.se/~klasm/solutions-n=2-k=3-HOC)"},{"username":"jason-dyer","timestamp":"0006-03-20T04:00:00.000Z","contents":"A super-optimist conjecture For %n > 2%, %c^\\mu_n = \\overline{c}^\\mu_n = 3(n-1)%"},{"username":"marc","timestamp":"0008-03-20T04:00:00.000Z","contents":"%\\bar{c}^\\mu_{10} \\ge 29% 028,046,055,064,073,118,172,181,190,208,217,235,262,  \n316,334,352,361,406,433,442,541,550,604,613,622,  \n721,730,901,1000"},{"username":"jason-dyer","timestamp":"0009-03-20T04:00:00.000Z","contents":"%\\bar{c}^\\mu_10% Well, *that* was a short-lived conjecture. (re: Marc.791 in response to Dyer.790 — I just confirmed, and changed the spreadsheet) Was that done with random fiddling or was there a method? Also I would like to remind people of the optimist conjecture in [Jakobsen.813](http://gowers.wordpress.com/2009/02/23/brief-review-of-polymath1/#comment-2405), which seems quite (well, moreso than other things) reasonable to prove."},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Various A lot of progress! I will have to start a new thread soon, this one is being used up. Kristal: it seems the second case (when c(2****) < 17) is missing from your 787\\. I’ve tentatively updated Lemma 2 on the wiki to reflect this improved bound, but we’ll need to check that second case. Assuming this improvement, it means that 125-point Moser sets have at most 41 points in the middle slice, which forces them to have at least 41 points in the side slices. This is good news, because we’ve got a computer list of all of these points. In fact I have the following strategy to propose. We know that a 125-point Moser set does not contain 22222, and so all the points in the Moser set are contained in at least one side slice. More precisely, points with four 2s are contained in one side slice, points with three 2s are contained in two side slices, and so forth up to points with no 2s, which are contained in five side slices. Double-counting this fact, we see that 125 = sum_S a(S)/5 + b(S)/4 + c(S)/3 + d(S)/2 + e(S) where S ranges over the 10 side slices. Dividing the side slices into five pairs (1****,3****), etc., and using the pigeonhole principle, we may thus assume without loss of generality that a(1****) + 5 b(1****)/4 + 5 c(1****) / 3 + 5 d(1****) / 2 + 5 e(1*****) and a(3****) + 5 b(3****)/4 + 5 c(3****) / 3 + 5 d(3****) / 2 + 5 e(3*****) sum to at least 125\\. Now, we know (contingent on Kristal’s lemma) that the 1**** and 3**** slices have at least 41 points. On the spreadsheet [http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuqNcxJ171Bbw&hl=en](http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuqNcxJ171Bbw&hl=en) I’ve computed a+5b/4+5c/3+5d/2+5e for the 43 and 42-point solutions. Encouragingly, the largest this quantity is is 63, and almost all of these numbers are less than 62; only the (4,16,23,0,0), (3,16,24,0,0), (4,15,24,0,0), (2,16,24,0,0) configurations are 62 or higher. I haven’t entered in the 41-point data but I would assume the quantity is smaller than 62 in all those cases. If so, that pins down the 1**** and 3**** slices quite precisely, and I would hope that this gets us close to finishing off the problem. Klas: Great! I’ve updated the hyper-optimistic conjecture page and the spreadsheet to reflect your new data. Thus far the HOC is holding up very nicely, we’ve verified it up to n=5\\. If we get n=6 for either c^\\mu_n or \\overline{c}^\\mu_n then it might be time to send this sequence to the OEIS also. (The latter sequence is an integer sequence, and if HOC is true, the former is also.) Marc: I’ve added your example to the wiki and spreadsheet too. Looks like Jason’s super-optimistic conjecture didn’t make it, unfortunately."},{"username":"klas-markstrom","timestamp":"0009-03-20T04:00:00.000Z","contents":"HOC  \nFrom the comment in the wiki on using Behrend sets we have a super-linear lower bound for Fujimuras problem. So the nice pattern behind the super-optimistic conjecture is a small n effect. The HOC for n=6 seems a lot harder than the previous cases but I’ll see what I can do."},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Moser(3) I’ve updated the Moser spreadsheet, [http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuqNcxJ171Bbw&hl=en](http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuqNcxJ171Bbw&hl=en) to input the 41-point data, and as expected the “score” a+5b/4+5c/3+5d/2+5e never reached 62 (though the anomalous d=3 solution got close, at 61.5). Assuming Kristal’s lemma, this implies that there is a coordinate whose two side slices have one of the following statistics: * (4,16,23,0,0) [score: 62 1/3]  \n* (3,16,24,0,0) [score: 63]  \n* (4,15,24,0,0) [score: 62 3/4]  \n* (2,16,24,0,0) [score: 62] and furthermore the total score must add up to at least 125\\. (For comparison, in the known 124 point example, the side slices have statistics (3,16,24,0,0), (2,16,24,0,0), or (1,16,24,0,0), with scores 63, 62, and 61 respectively.) This looks like quite a bit of information with which one can work with."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"We have already shown that if  \nthe center slice is 42 or 43 and it has  \n17 or more points then there at most 124. Case 2 The center slice is 42 c less that 17  \nThen we must have c is 12  \nAnd we must have on of the 8  \n(6, 24, 12, 0, 0) sets.  \nwithout less of generality  \nwe can assume it to be %\\Gamma _{220}+\\Gamma _{202}+\\Gamma _{022}+\\Gamma _{112}+\\Gamma _{211}%  \nIn particular the middle slice  \nContains the points  \n2122 21212 21221  \n23322 23232 23223 and the *1*** and *3*** slices  \nhave a d value of at least three and so have at most  \n41 points now if the center slice is 42 we have 124 points  \nand we are done so it must be 43 but then it has value 18 or more  \nand and by case 1 we have 124 points or less so we are done."},{"username":"terence-tao","timestamp":"0001-03-20T04:00:00.000Z","contents":"Moser(3) Thanks Kristal! Looks like we are in very good shape now. In particular we now have the situation described in 795; I’ll put that argument on the wiki now. As this thread is now basically full, I am closing it and we will be moving to the 900 thread at [https://terrytao.wordpress.com/2009/03/04/dhj3-900-999-density-hales-jewett-type-numbers/](https://terrytao.wordpress.com/2009/03/04/dhj3-900-999-density-hales-jewett-type-numbers/)"},{"username":"ryan-odonnell","timestamp":"2009-02-23T23:12:00.000Z","contents":"Here is a writeup of a Fourier/density-increment argument for Sperner, implementing Terry’s #578 using some of the Fourier calculations Tim and I were doing: [http://www.cs.cmu.edu/~odonnell/wrong-sperner.pdf](http://www.cs.cmu.edu/~odonnell/wrong-sperner.pdf) Hope it’s mostly bug-free!"},{"username":"jozsef","timestamp":"2009-02-23T23:14:00.000Z","contents":"Tim – First of all I have to say that I did enjoy to read the blog and to contribute a bit. On the other hand it became less and less comfortable to me posting any comments. You wrote that  \n“Meanwhile, the discussion itself has become somewhat fragmented, in the sense that various people, or smaller groups of people, are pursuing different approaches and commenting only briefly if at all on other people’s approaches.”  \nLet me tell you what is my experience. I’m reading posts using notations from to ergodic theory or Fourier analysis with great interest, but I will rarely make any comment as I’m not expert on that field. Some post are still accessible to me and some others aren’t. There is no way that I will understand posts referring to noise stability unless I learn something more about it, which I’m not planning to do in the near future unless it turns out to be crucial for the project. Maybe the blog is still accessible for the majority of readers but I’m finding it more and more difficult to follow. For the same reasons I feel a bit awkward about posting notes. It’s tempting to work on the problems alone without sharing it with the others but this is clearly against the soul of the project."},{"username":"gowers","timestamp":"2009-02-23T23:28:00.000Z","contents":"Metacomment. Jozsef, in the light of what you say, I invite others to give their reactions to how things are going for them. My feeling, as I said in the post, is that we are entering uncharted territory (or rather, a different uncharted territory from the initial one) and it is not clear that the same rules should still apply. What we have done so far has, in my view, been a wonderful way of doing very quickly and thoroughly the initial exploration of the problem, and it feels as though the best way of making further progress will be if we start dealing with technicalities and actually write up some statements formally. I am hoping to do that in the near future, Ryan is doing something along those lines, Terry recently put a Hilbert-spaces lemma on the wiki, etc. The obvious hazard with this is that people may end up writing things that others do not understand, or can understand only if they are prepared to invest a lot of time and effort. So if we want to continue to operate as a collective unit, so to speak, it is extremely important for people who write formal proofs to give generous introductions explaining what they are doing, roughly how the proof goes, why they think it could be useful for DHJ(3) (or something related to DHJ(3)), and so on. This is perhaps an area where people who have been following the discussion without actually posting comments could be helpful—just by letting us know which bits you find hard to understand. For example, if you see something on the wiki that is clearly meant to be easily comprehensible but it in fact isn’t, then it would be helpful to know. (However, some of the wiki articles are over-concise simply because we haven’t had time to expand them yet.) Do others have any thoughts about where we should go from here?"},{"username":"gowersgowersryan-odonnell","timestamp":"2009-02-23T23:47:00.000Z","contents":"Fourier/Sperner Ryan, re the remark at the end of the thing you wrote up, if we do indeed have an expression of the form %\\mathbb{E}_{p\\leq q}\\sum_{A\\subset[n]}\\lambda_{p,q}^{|A|}\\hat{f}_p(A)\\hat{f}_q(A)% for the total weight %\\mathbb{E}_{U\\subset V}f(U)f(V)% for %\\mathbb{E}_{U\\subset V}f(U)f(V)% when everything is according to equal-slices density, then it seems to me not completely obvious that one couldn’t prove some kind of positivity result for the contribution from each %A%, especially given your calculation that %\\lambda_{p,q}% splits up as a product. But probably you’ve thought about that—I can see that it’s a problem that %\\lambda_{p,q}% is not symmetric in p and q, but can one not make it symmetric by talking instead about pairs of disjoint sets? Or perhaps it’s false that you have positivity and the disjoint-pairs formulation is what you need to get a similar result where it becomes true. I hope this vague comment makes some sense to you. I’ll see if I can think of a counterexample. In fact, isn’t this a counterexample: let n=1, and let f(0)=-1, f(1)=1\\. Then the expectation of f(x)f(y) over combinatorial lines (x,y) is -1 and the expectation of f is 0. 801.1 Please ignore the last paragraph of this comment — for analytic purposes we need to consider degenerate lines, and as you say the usual proof of Sperner proves positivity. Hmm — not sure that the threading adds much here. 801.2: Yes, it’s not clear to me why %\\mathbb{E}_{p \\leq q}[\\hat{f}_p(A) \\hat{f}_q(A)]% needs to be nonnegative. This is a shame, because: a) we’d be extremely done if we could show this; b) as %p, q \\to 1/2% the quantity approaches %\\hat{f}(A)^2%, which is of course nonnegative. Unfortunately, if you make %p, q% extremely close to %1/2% so as to force it nonnegativity, you’ll probably be swamped with degenerate lines."},{"username":"gowers","timestamp":"2009-02-24T00:25:00.000Z","contents":"801.1 Please ignore the last paragraph of this comment — for analytic purposes we need to consider degenerate lines, and as you say the usual proof of Sperner proves positivity. Hmm — not sure that the threading adds much here."},{"username":"ryan-odonnell","timestamp":"2009-02-24T01:05:00.000Z","contents":"801.2: Yes, it’s not clear to me why %\\mathbb{E}_{p \\leq q}[\\hat{f}_p(A) \\hat{f}_q(A)]% needs to be nonnegative. This is a shame, because: a) we’d be extremely done if we could show this; b) as %p, q \\to 1/2% the quantity approaches %\\hat{f}(A)^2%, which is of course nonnegative. Unfortunately, if you make %p, q% extremely close to %1/2% so as to force it nonnegativity, you’ll probably be swamped with degenerate lines."},{"username":"ryan-odonnell","timestamp":"2009-02-24T01:20:00.000Z","contents":"Metacomment: I sympathise with Jozsef’s position. Luckily, some of the directions we’re working on are in areas I’m familiar with, so I can comment there. But to be honest, I know nothing about ergodic methods, am extremely shaky on what triangle-removal is, and am only kind of on top of Szemeredi’s regularity lemma. So indeed it takes me quite a while just to read comments on these topics. That’s fine with me though. Thing is, even in the areas I’m familiar with it’s awfully tough to keep up with a certain pair of powerhouses who post regularly to the project <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> In a perfect world I’d post more definitions, add more intuition to my comments, update the wiki with stuff about noise sensitivity and so forth — but it already takes me many hours just to quasi-keep up with Tim and Terry and generate non-nonsensical comments. I’m fine with this too, though. Finally, I agree that it’s also gotten to the point where it’s sort of impossible to explore certain directions — especially ones with calculations — “online”. You probably won’t be surprised to learn I spent a fair bit of time working out the pdf document in #800 by myself on paper. I hope this isn’t too much against the spirit of the project, but I couldn’t find any way to do it otherwise. I would feel like I were massively spamming people if I tried to compute like this online, with all the associated wrong turns, miscalculations, and crazy mistakes I make."},{"username":"ryan-odonnell","timestamp":"2009-02-24T01:25:00.000Z","contents":"Sperner/Fourier. By the way, I’m pretty sure one can also do Roth’s Theorem (or at least, finding “Schur triples” in %\\mathbb{F}_2^n%) in this way. It might sound ridiculous to say so, since Roth/Meshulam already gave a highly elegant density-increment/Fourier-analysis proof. But the point is that:  \n. it works in a non-“arithmetic”/”algebraic” way  \n. it works by doing density increments that restrict just to *combinatorial* subpaces  \n. it demonstrates that the method can sometimes work in cases where one needs to find *three* points in the set satisfying some relation. I can supply more details later but right now I have to go to a meeting…"},{"username":"gilryan-odonnell","timestamp":"2009-02-24T01:54:00.000Z","contents":"Metacomment.1 (If this .1 will automatically creat a threading I will consider it a miracle.) I think the nature of this collaboration via different people writing blog remarks is somewhat similar to the massive collaboration in ordinary math where people are writing papers partially based on earlier papers. Here the time scale is quicker and the nature of contributions is more tentative. Half baked ideas are encourged. It is not possible to follow everything or even most of the things or even a large fraction of them. (Not to speak about understanding and digesting and remembering). I think it can be useful that people will not assume that other people read earlier things and from time to time repeat and summarize and overall be clear as possible. (I think comments of the form: “please clarify” can be useful.) One the other hand, like in usual collaborations people often write things mainly for themselves (e.g. the Hardy-Littlewood rules) and this is very fine. (A question: is Ryan’s write-up a correct new proof of (weak) Sperner using density incement and Fourier? (The file is called wrong-Sperner but reading it I realize that this means the proof is right but not yet the “Fourier/Sperner proof from the book” or something like that. Please clarify.) There are various avenues which I find very interesting even if not directly related to the most promising avenues towards DHJ proof and even some ideas I’d like to share and explore. (One of these is a non-density-decreasing Fourier strategy.) One general avenue is various reductions (ususally based on simple combinatorial reasonings) : Joszef mentioned Moser(k=6)-> DHK(k=3) and some reductions to Szemeredi (k=4) were considered. Summarizing those and pushing for further reductions can be fruitfull. Also the equivalence of DHJ for various measures (I supose this is also a reduction business) is an interesting aspect worth further study. When there is group acting such equivalences are easy and of general nature. But for DHJ I am not sure how general they are. Gil wrote: “(The file is called wrong-Sperner but reading it I realize that this means the proof is right but not yet the “Fourier/Sperner proof from the book” or something like that. Please clarify.)” Yes — I forgot to mention that. I think/hope the writeup’s correct, but I called it “wrong-Sperner” because I still feel there’s something not-from-the-Book about it."},{"username":"ryan-odonnell","timestamp":"2009-02-24T04:32:00.000Z","contents":"Gil wrote: “(The file is called wrong-Sperner but reading it I realize that this means the proof is right but not yet the “Fourier/Sperner proof from the book” or something like that. Please clarify.)” Yes — I forgot to mention that. I think/hope the writeup’s correct, but I called it “wrong-Sperner” because I still feel there’s something not-from-the-Book about it."},{"username":"terence-tao","timestamp":"2009-02-24T03:07:00.000Z","contents":"Metacomment.2 It certainly does feel that the project is developing into a more mature phase, where it resembles the activity of a highly specialised mathematical subfield, rather than a single large collaboration, albeit at very different time scales than traditional activity. (The wiki is somewhat analogous to a “Journal of Attempts to Prove Density Hales Jewett”, and the threads are analogous to things like “4th weekly conference on Density Hales Jewett” <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":-)\">:-)</span> .) Also we are hitting the barrier that the number of promising avenues of research seems to exceed the number of people actively working on the project. But I think things will focus a bit more (and become more “polymathy”) once we identify a particularly promising approach to the problem (I think we already have a partially assembled skeleton of such, and a significant fraction of the tools needed have already been sensed). This is what is going on in the 700 thread, by the way; we are focusing largely on one subproblem at a time (right now, it’s getting a good Moser(3) bound for n=5) and we seem to be using the collaborative environment quite efficiently. My near-term plan is to digest enough of the ergodic theory proofs that I can communicate in finitary combinatorial language a formal proof of DHJ(2) that follows the ergodic approach, and a very handwavy proof of DHJ(3). (The Hilbert space lemma I’m working on is a component of the DHJ(2) analysis.) The finitisation of DHJ(2) looks doable, but is already quite messy (more “wrong” than Ryan’s “wrong” proof of DHJ(2)), and it seems to me that formally finitising the ergodic proof of DHJ(3), while technically possible, is not the most desirable objective here. But there does seem to be some useful ideas that we should be able to salvage from the ergodic proof that I would like to toss out here once I understand them properly. (For instance, there seems to be an “IP van der Corput lemma” that lets one “win” when one has 01-pseudorandomness (which roughly means that if one takes a medium dimensional slice of A and then flips one of the fixed digits from 0 to 1, then the pattern of A on the shifted slice is independent of the pattern of A on the original slice). I would like to understand this lemma better. The other extreme, that of 01-insensitivity, is tractable by Shelah’s trick of identifying 0 and 1 into a single letter of the alphabet, and the remaining task is to apply a suitable structure theorem to partition arbitrary sets into 01-structured and 01-pseudorandom components, analogously to how one would apply the regularity lemma to one part of the tripartite graph needed to locate triangles.)"},{"username":"gowersryan-odonnellgowersgowers","timestamp":"2009-02-24T03:34:00.000Z","contents":"Sperner/Fourier Re the discussion in 801, this is a proposal for getting positivity. I haven’t checked it, and it could just obviously not work. The reason I make it is that I just can’t believe the evidence in front of me: the trivial positivity in “physical space” just _must_ be reflected by equally trivial positivity in “frequency space”. I wonder if the problem, looked at from the random-permutation point of view, is that to count pairs of initial segments it is not completely sensible to count them with the smaller one always first — after all, the proof of positivity goes by saying that that is actually half the set of pairs of initial segments. So maybe we should look at a sum of the form %\\mathbb{E}_{A:B}f(A)f(B)%, where %A:B% is a hastily invented notation for “%A\\subset B% or %B\\subset A%.” How could we implement this Ryan style? Well, obviously we would average over all pairs of probabilities (p,q). And if we choose a particular pair, then the obvious thing to do would be to choose independent random variables %(X_1,\\dots,X_n)% and %(Y_1,\\dots,Y_n)% in such a way that the resulting set will give you a probability p of being in A, a probability q of being in B, and a certainty that A:B. To achieve that in a unified way, for each %i% we choose a random %t\\in [0,1]% and set %X_i=1% if and only if %t\\leq p% and %Y_i=1% if and only if %t\\leq q%. I haven’t even begun to try any calculations here, but I would find it very strange if one didn’t get some kind of positivity over on the Fourier side. Having said that, the fact that one expands f in a different way for each p does make me not quite as sure of this positivity as I’m pretending to be. 803.1 Positivity: In my experience, obvious positivity on the physical side doesn’t always imply obvious positivity on the Fourier side. Here is an example (although it goes in the reverse direction). Let %f : \\{0,1\\}^n \\to \\mathbb{R}% be arbitrary and consider %\\mathbb{E}[f(x) f(y)]%, where %x% is uniform random and %y% is formed by flipping each bit of %x% with probability %1/5%. (Note that %(x,y)% has the same distribution as %(y,x)%, so there’s no asymmetry here.) Now %x% and %y% are typically quite far apart in Hamming distance, and since %f% can have positive and negative values, big and small, it seems far from obvious that %\\mathbb{E}[f(x) f(y)]% should be nonnegative. But this is obvious on the Fourier side: it’s precisely %\\sum_S (3/5)^{|S|} \\hat{f}(S)^2%. 803.2 Positivity That’s interesting. The non-obvious assertion is that a certain matrix (where the value at (x,y) is the probability that you get y when you start from x and flip) is positive definite. And the Fourier transform diagonalizes it. And that helps to see why it’s genuinely easier on the Fourier side: given a quadratic form, there is no reason to expect it to be easy to see that it is positive definite without diagonalizing it. (However, I am tempted to try with your example …) 803.3 Positivity Got it! Details below in an hour’s time (after my lecture). It will be comment 812 if nobody else has commented by then."},{"username":"ryan-odonnell","timestamp":"2009-02-24T04:43:00.000Z","contents":"803.1 Positivity: In my experience, obvious positivity on the physical side doesn’t always imply obvious positivity on the Fourier side. Here is an example (although it goes in the reverse direction). Let %f : \\{0,1\\}^n \\to \\mathbb{R}% be arbitrary and consider %\\mathbb{E}[f(x) f(y)]%, where %x% is uniform random and %y% is formed by flipping each bit of %x% with probability %1/5%. (Note that %(x,y)% has the same distribution as %(y,x)%, so there’s no asymmetry here.) Now %x% and %y% are typically quite far apart in Hamming distance, and since %f% can have positive and negative values, big and small, it seems far from obvious that %\\mathbb{E}[f(x) f(y)]% should be nonnegative. But this is obvious on the Fourier side: it’s precisely %\\sum_S (3/5)^{|S|} \\hat{f}(S)^2%."},{"username":"gowers","timestamp":"2009-02-24T05:05:00.000Z","contents":"803.2 Positivity That’s interesting. The non-obvious assertion is that a certain matrix (where the value at (x,y) is the probability that you get y when you start from x and flip) is positive definite. And the Fourier transform diagonalizes it. And that helps to see why it’s genuinely easier on the Fourier side: given a quadratic form, there is no reason to expect it to be easy to see that it is positive definite without diagonalizing it. (However, I am tempted to try with your example …)"},{"username":"gowers","timestamp":"2009-02-24T16:52:00.000Z","contents":"803.3 Positivity Got it! Details below in an hour’s time (after my lecture). It will be comment 812 if nobody else has commented by then."},{"username":"terence-taorandall","timestamp":"2009-02-24T04:15:00.000Z","contents":"DHJ(2.7) Randall: I tried to wikify your notes from 593 at [http://michaelnielsen.org/polymath1/index.php?title=DHJ(2.7)](http://michaelnielsen.org/polymath1/index.php?title=DHJ(2.7)) Unfortunately due to a wordpress error, an important portion of your Tex (in particular, part of the **statement** of DHJ(2.7)) was missing (the portion between a less than sign and a greater than sign – unfortunately this was interpreted as HTML and thus eaten). So I unfortunately don’t understand the proof. Could you possibly take a look at the page and try to restore the missing portion? Thanks! Sorry about that. After several missteps, I may have managed to correct it in full…."},{"username":"randall","timestamp":"2009-02-24T10:04:00.000Z","contents":"Sorry about that. After several missteps, I may have managed to correct it in full…."},{"username":"terence-taoterence-tao","timestamp":"2009-02-24T04:22:00.000Z","contents":"Sperner (Previous comment should be 804.) Ryan’s notes (which, incidentally, might also be migrated to the wiki at some point) seem to imply something close to my Lemma in 578 that had a bogus proof, namely that if f is non-uniform in the sense that %{\\Bbb E} f(x) g(y)% or %{\\Bbb E} g(x) f(y)% is large for some g, then f has significant Fourier concentration at low modes (i.e. the energy %\\sum_{|S| \\leq \\varepsilon n} |\\hat f(S)|^2% is large), and hence f correlates with a function of low influence (e.g. f correlates with %T_\\varepsilon f%, where %T_\\varepsilon% is the Fourier multiplier that multiplies the S fourier coefficient by %(1-1/\\varepsilon n)^{|S|}%). If this is the case, then (because polynomial combinations of low influence functions are still low influence) one can iterate this in the usual energy-increment manner (or as in the proof of the regularity lemma) to obtain a decomposition %1_A = f_{U^\\perp} + f_U%, where %f_U% is uniform and %f_{U^\\perp}% has low influence, is non-negative and has the same density as %1_A%. (This is an oversimplification; there are error terms, and one has to specify the scales at which uniformity or low influence occurs, but ignore these issues for now.) If this is the case, then the Sperner count %{\\Bbb E} 1_A(x) 1_A(y)% should be approximately equal to %{\\Bbb E} f_{U^\\perp}(x) f_{U^\\perp}(y)%. But if %f_{U^\\perp}% is low influence, this should be approximately %{\\Bbb E} f_{U^\\perp}(x)^2%. Meanwhile, we have %{\\Bbb E} f_{U^\\perp}(x) = \\delta%, so by Cauchy-Schwartz we get at least %\\delta^2%, as desired. 805.1 p.s. Something very similar goes on in the ergodic proof of DHJ(2). What I call %f_{U^\\perp}% here will be called %P 1_A%, where P is a certain “weak limit” of shift operators. The key point is that P is going to be an orthogonal projection in L^2 (this is the upshot of a Hilbert space lemma that I am working on on the wiki.)"},{"username":"terence-tao","timestamp":"2009-02-24T04:24:00.000Z","contents":"805.1 p.s. Something very similar goes on in the ergodic proof of DHJ(2). What I call %f_{U^\\perp}% here will be called %P 1_A%, where P is a certain “weak limit” of shift operators. The key point is that P is going to be an orthogonal projection in L^2 (this is the upshot of a Hilbert space lemma that I am working on on the wiki.)"},{"username":"ryan-odonnellterence-tao","timestamp":"2009-02-24T05:01:00.000Z","contents":"Sperner. Re #805, Terry, one thing I had trouble with was — roughly speaking — showing that indeed %\\mathbb{E}[f_{U^\\bot}(x) f_{U^\\bot}(y)]% is close to %\\mathbb{E}[f_{U^\\bot}(x)^2]%. It seemed logical that this would be true but I had technical difficulties actually making the switch from %y% to %x%. In some sense that’s why I wrote the notes, to really convince myself that one could do something. Unfortunately, that something was to define %f_{U^\\bot}% all the way down to just the lowest mode; i.e., to look at density rather than energy. Perhaps we could try to think about, at a technical level, how to really pass from %(x,y)% to %(x,y)% for low-influence functions… 806.2 Sperner Well, I would imagine that %f_{U^\\perp}% would have Fourier transform concentrated in sets S of size %\\varepsilon n%, and this should mean that the average influence of %f_{U^\\perp}%, i.e. {\\Bbb E} |f_{U^\\perp}(x) – f_{U^\\perp}(y)|^2, where x and y differ by just one bit, should be small (something like %O(\\varepsilon)%). This should then be able to be iterated to extend to x and y being several bits apart rather than just one bit. (In order to do this properly, one will probably need to have a whole bunch of different scales in play, not just a single scale %\\varepsilon n%. There are various pigeonhole tricks that let one find a whole range of scales at which all statistics are the same [I call this the “finite convergence principle” in my blog], so as a first approximation let’s pretend that concentration at one scale %\\epsilon n% is the same as concentration at other small scales such as %\\epsilon^2 n%. The point is that while the L^2 energy of f could be significant in the range %\\epsilon^2 n \\leq |S| \\leq \\epsilon n% for a single %\\varepsilon%, the pigeonhole principle prevents it from being significant for all such choices of %\\varepsilon%])"},{"username":"terence-tao","timestamp":"2009-02-24T06:49:00.000Z","contents":"806.2 Sperner Well, I would imagine that %f_{U^\\perp}% would have Fourier transform concentrated in sets S of size %\\varepsilon n%, and this should mean that the average influence of %f_{U^\\perp}%, i.e. {\\Bbb E} |f_{U^\\perp}(x) – f_{U^\\perp}(y)|^2, where x and y differ by just one bit, should be small (something like %O(\\varepsilon)%). This should then be able to be iterated to extend to x and y being several bits apart rather than just one bit. (In order to do this properly, one will probably need to have a whole bunch of different scales in play, not just a single scale %\\varepsilon n%. There are various pigeonhole tricks that let one find a whole range of scales at which all statistics are the same [I call this the “finite convergence principle” in my blog], so as a first approximation let’s pretend that concentration at one scale %\\epsilon n% is the same as concentration at other small scales such as %\\epsilon^2 n%. The point is that while the L^2 energy of f could be significant in the range %\\epsilon^2 n \\leq |S| \\leq \\epsilon n% for a single %\\varepsilon%, the pigeonhole principle prevents it from being significant for all such choices of %\\varepsilon%])"},{"username":"ryan-odonnellgil","timestamp":"2009-02-24T05:08:00.000Z","contents":"Fourier/density-increment. In #802, I guess I’m getting the terminology all wrong. I should say that another problem I think the same method will work for is the problem in %\\mathbb{F}_2^n% where you’re looking for “lines” of length 3 where in the wildcard coordinates you’re allowed either (0,1,1), (1,0,1), or (1,1,0). I don’t know if this problem has a name. It’s not quite a Schur triple. I personally would call it the “Not-Two” problem because in every coordinate you’re allowed (0,0,0), (1,1,1), (0,1,1), (1,0,1), or (1,1,0): anything where the number of 0’s is “Not Two”. 807.1 A question and 3 remarks:  \nQ: I remember when we first talked about it there were obvious difficulties for why fourier+increasing density strategy wont work. At the end what is the basic change that make it works is it changing the measure? R1: sometimes is is equally convenient (and even more) not to work with %\\mu_p% measures and their associated Fourier transform but to consider functions on [0,1]%n% and use the Walsh orthonormal basis even for every [0,1]. So given p we represent our set by a subset of the solid cube and use the “fine” Walsh transform all the time. R2: If changing the measure can help when small Fourier coefficients do not suffice for quasirandomness, is there hope it will help for case k=4 of Szemeredi? R3: there are some problems regarding influences threshold phenomena where better understanding of the relation between equal sliced density and specific %\\mu_p% densities are needed. Maybe some of the tools from here can help."},{"username":"gil","timestamp":"2009-02-24T05:59:00.000Z","contents":"807.1 A question and 3 remarks:  \nQ: I remember when we first talked about it there were obvious difficulties for why fourier+increasing density strategy wont work. At the end what is the basic change that make it works is it changing the measure? R1: sometimes is is equally convenient (and even more) not to work with %\\mu_p% measures and their associated Fourier transform but to consider functions on [0,1]%n% and use the Walsh orthonormal basis even for every [0,1]. So given p we represent our set by a subset of the solid cube and use the “fine” Walsh transform all the time. R2: If changing the measure can help when small Fourier coefficients do not suffice for quasirandomness, is there hope it will help for case k=4 of Szemeredi? R3: there are some problems regarding influences threshold phenomena where better understanding of the relation between equal sliced density and specific %\\mu_p% densities are needed. Maybe some of the tools from here can help."},{"username":"gil","timestamp":"2009-02-24T06:13:00.000Z","contents":"Another Fourier/Sperner approach Let me mention briefly a non density-increasing approach to Sperner. You first note that not having lines with 1 wild card implies that %\\sum \\hat f^2 (S)|S| = \\hat f^2(S) (n-|S|)%. If you can show that the non empty fourier coefficients are concentrated (in terms of |S|) then I think you can conclude that the density is small. You want to control expression of the from %\\hat f^2(S) {{n-|S|} \\choose {k}}%. Those are related to the number of elements in the set when we fixed the value of n-k variables and look over all the rest. These numbers are controlled by the density of Sperner families for subsets of {1,2,…,k}. So this looks like circular thing but maybe it allows some bootstrapping."},{"username":"ryan-odonnellterence-tao","timestamp":"2009-02-24T10:42:00.000Z","contents":"Response to 806.2 (Sperner). Terry wrote, “This should then be able to be iterated to extend to x and y being several bits apart rather than just one bit.” Hmm, but one source of annoyance is that y is not just x with %\\epsilon n% or so bits flipped — it’s x with %\\epsilon n% or so 0’s flipped to 1’s. As you get more and more 1’s in there, the intermediate strings are less and less distributed as uniformly random strings. So it’s not 100% clear to me that you can keep appealing to average influence, since influence as defined is a uniform-distribution concept. Overall I agree that this is probably more of a nuisance then a genuine problem, but it was stymieing me a bit so I thought I’d ask. 809.2 Sperner and influence Ryan, I think the trusty old triangle inequality should deal with things nicely. (I would need a binomial model of bit-flipping rather than a Poisson model, but I’m sure this makes very little difference.) Suppose %f_{U^\\perp}% has average influence %\\mu%; thus flipping a bit from a 0 to 1 or vice versa would only affect things by %\\mu% on the average. Conditioning, we conclude that if we flip just one bit from a 0 to 1, we expect %f_{U^\\perp}% to change by at most %2 \\mu% on the average. Now we iterate this process %\\varepsilon n% times. As long as %\\varepsilon n% is much less than %\\sqrt{n}% (as in your notes), there is no significant distortion of the underlying probability distribution and we see that if y differs by %\\varepsilon n% 0->1 bits from x, then %f_{U^\\perp}(y)% and %f_{U^\\perp}(x)% will differ by %O( \\varepsilon n \\mu )% on the average. This will be enough for our purposes as long as the influence %\\mu% is a really small multiple of %1/\\varepsilon n%. One can’t quite get this by working at a single scale – one gets %O( 1/\\varepsilon n )% instead – but one can do so if one works with a whole range of scales and uses the energy increment argument. I’ve sketched out the details on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Fourier-analytic_proof_of_Sperner](http://michaelnielsen.org/polymath1/index.php?title=Fourier-analytic_proof_of_Sperner)"},{"username":"terence-tao","timestamp":"2009-02-24T11:07:00.000Z","contents":"809.2 Sperner and influence Ryan, I think the trusty old triangle inequality should deal with things nicely. (I would need a binomial model of bit-flipping rather than a Poisson model, but I’m sure this makes very little difference.) Suppose %f_{U^\\perp}% has average influence %\\mu%; thus flipping a bit from a 0 to 1 or vice versa would only affect things by %\\mu% on the average. Conditioning, we conclude that if we flip just one bit from a 0 to 1, we expect %f_{U^\\perp}% to change by at most %2 \\mu% on the average. Now we iterate this process %\\varepsilon n% times. As long as %\\varepsilon n% is much less than %\\sqrt{n}% (as in your notes), there is no significant distortion of the underlying probability distribution and we see that if y differs by %\\varepsilon n% 0->1 bits from x, then %f_{U^\\perp}(y)% and %f_{U^\\perp}(x)% will differ by %O( \\varepsilon n \\mu )% on the average. This will be enough for our purposes as long as the influence %\\mu% is a really small multiple of %1/\\varepsilon n%. One can’t quite get this by working at a single scale – one gets %O( 1/\\varepsilon n )% instead – but one can do so if one works with a whole range of scales and uses the energy increment argument. I’ve sketched out the details on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Fourier-analytic_proof_of_Sperner](http://michaelnielsen.org/polymath1/index.php?title=Fourier-analytic_proof_of_Sperner)"},{"username":"terence-tao","timestamp":"2009-02-24T11:27:00.000Z","contents":"DHJ(2.7) Randall, thanks for cleaning up the file! It is a very nice proof, and I’d like to try to describe it in my own words here. DHJ(2.7) is the strengthening of DHJ(2.6) that gives us three _parallel_ combinatorial lines (i.e. they have the same wildcard set), the first of which hits the dense set A in the 0 and 1 position, the second hits it in the 1 and 2 position, and the third in the 2 and 0 position. To describe Randall’s argument, I’d first like to describe how Randall’s argument gives yet another proof of DHJ(2) which is quite simple (and gives civilised bounds). It uses the density increment argument: we want to prove DHJ(2) at some density %\\delta% and we assume that we’ve already proven it at any significantly bigger density. Now let A be a subset of %[2]^n% of density %\\delta% for some n. We split n into a smallish r and a biggish n-r, thus viewing %[2]^n% as a whole bunch of (n-r)-dimensional slices, each indexed by a word in %[2]^r%. If any of the big slices has density significantly bigger than %\\delta%, we are done; so we can assume that all the big slices have density not much larger than %\\delta% (e.g. at most %\\delta + \\delta^2 / 2^r%). Because the total density is %\\delta%, we can subtract and conclude that all the big slices have density close to %\\delta%. Now we look at the r+1 slices indexed by the words %0^r, 0^{r-1} 1, 0^{r-2} 1^2, \\ldots, 0 1^{r-1}, 1^r%. Each of these slices intersects the set A with density about %\\delta%. Thus by the pigeonhole principle, if r is much bigger than %1/\\delta%, then two of the A-slices must share a point in common, i.e. there exists %w \\in [2]^{n-r}% and i,j such that %0^i 1^{r-i} w, 0^j 1^{r-j} w% both lie in A. Voila, a two-dimensional line. The same argument gives DHJ(2.7). Now there are three different graphs on r+1 vertices, the 01 graph, the 12 graph, and the 20 graph. i and j are connected on the 01 graph if the %0^i 1^{r-i}%-slice and %0^j 1^{r-j}%-slice of A have a point in common; similarly define the 12 graph and the 20 graph. Together, this forms an 8-coloured graph on r+1 vertices. By Ramsey’s theorem, if r is big enough there is a monochromatic subgraph of size %\\gg 1/\\delta%. But by the preceding pigeonhole argument, we see that none of the 01 graph, the 12 graph, or the 20 graph can vanish completely here, so they must instead all be complete, and we get three parallel combinatorial lines, each intersecting A in two of the three positions."},{"username":"ryanworldwide","timestamp":"2009-02-24T11:53:00.000Z","contents":"Prelude to some more density-increment arguments. I was hoping to give an illustration of the #578 technique for a problem in %{[3]^n}%. But it got too late for me to finish writing it, so I’ll just give the “background info” I managed to write. This probably ought to go in the wiki rather than in a post, but having spent a bit of time to get Luca’s converter working for me, I didn’t have the energy to also convert to the third, wiki format. More on the #578 method tomorrow. Here, then, some basics on noise and correlated product spaces; for more, see e.g. [this paper of Mossel](http://arxiv.org/abs/math/0703683). Let %{\\Omega}% be a small finite set; for example, %{\\Omega = [3]}%. Let %{\\mu}% be a probability distribution on %{\\Omega}%. Abusing notation, write also %{\\mu = \\mu^{\\otimes n}}% for the corresponding product distribution on %{\\Omega^n}%. For %{0 \\leq \\rho \\leq 1}%, define the _noise operator_ %{T_\\rho^\\mu}%, which acts on functions %{\\Omega^n \\to {\\mathbb R}}%, as follows: %\\displaystyle (T_\\rho^\\mu f)(x) = \\mathop{\\bf E}_{{\\bf y} \\sim^\\mu_\\rho x}[f({\\bf y})], %  \nwhere %{{\\bf y} \\sim^\\mu_\\rho x}% denotes that %{{\\bf y}}% is formed from %{x}% by doing the following, independently for each %{i \\in [n]}%: with probability %{\\rho}%, set %{{\\bf y}_i = x_i}%; with probability %{1 - \\rho}%, draw %{{\\bf y}_i}% from %{\\mu}% (i.e., “rerandomize the coordinate”). (I use **boldface** for random variables.) Here are some simple facts about the noise operator: Fact 1: %{\\mathop{\\bf E}_{{\\bf x} \\sim \\mu}[T_\\rho^\\mu f({\\bf x})] = \\mathop{\\bf E}_{{\\bf x} \\sim \\mu}[f({\\bf x})]}%. Fact 2: For %{p \\geq 1}% we have %{\\|T_\\rho^\\mu f\\|_{\\mu,p} \\leq \\|f\\|_{\\mu,p}}%, where %{\\|f\\|_{\\mu,p}}% denotes %{\\mathop{\\bf E}_{{\\bf x} \\sim \\mu}[|f({\\bf x})|^p]^{1/p}}%. Fact 3: The %{T^\\mu_\\rho}%‘s form a semigroup, in the sense that %{T^\\mu_{\\rho_1} T^\\mu_{\\rho_2} = T^\\mu_{\\rho_1 \\rho_2}}%. Fact 4: %{\\mathop{\\bf E}_{{\\bf x} \\sim \\mu}[f({\\bf x}) \\cdot T_\\rho^\\mu f({\\bf x})] = \\mathop{\\bf E}_{{\\bf x} \\sim \\mu}[(T_{\\sqrt{\\rho}}^\\mu f({\\bf x}))^2]}%. We use the notation %{\\mathbb{S}_\\rho^\\mu(f)}% for this quantity. Fact 5: %{\\mathbb{S}_{1-\\gamma}^\\mu(f) = \\mathop{\\bf E}_\\mu[(T_{\\sqrt{1 - \\gamma}}^\\mu f)^2] = \\mathop{\\bf E}_{{\\bf V}}[\\mathop{\\bf E}_\\mu[f|_{{\\bf V}}]^2]}%, where: a) %{{\\bf V}}% is a randomly chosen combinatorial subspace, with each coordinate being fixed (from %{\\mu}%) with probability %{1 - \\gamma}% and left “free” with probability %{\\gamma}%, independently; b) %{f|_{{\\bf V}}}% denotes the restriction of %{f}% to this subspace, and %{\\mathop{\\bf E}_\\mu[f|_{{\\bf V}}]}% denotes the mean of this restricted function, under %{\\mu}% (the product distribution on the free coordinates of %{{{\\bf V}}}%)."},{"username":"gowersryanworldwideryanworldwide","timestamp":"2009-02-24T18:32:00.000Z","contents":"Positivity This concerns Ryan’s example of a quantity that is easily seen to be non-negative on the Fourier side, and not so easily seen to be non-negative in physical space. Except that I now have an easy argument in physical space. I don’t know how relevant this is, but there is some potential motivation for it, which is that perhaps the simple idea behind the proof could help in Ryan’s quest for a “non-wrong” proof of Sperner. The problem, laid out in 803.1, was this. Let %f% be a function defined on %\\{0,1\\}^n%. Now choose two random points %x,y% in %\\{0,1\\}^n% as follows: %x% is uniform, and %y% is obtained from %x% by changing each bit of %x% independently with probability 1/5\\. Now show that %\\mathbb{E}f(x)f(y)% is non-negative for all real functions %f%. There are various ways of describing a proof of this. Here is the one I first thought of, and after that I’ll give a slicker version. I’m going to do a different calculation. Again I’ll pick a random %x%, but this time I’ll create _two_ new vectors %y% and %z% out of %x%, which will be independent of each other (given %x%) and obtained from %x% by flipping coordinates with probability %\\lambda%. And I’ll choose %\\lambda% so that %2\\lambda(1-\\lambda)=1/5%, which guarantees that the probability that the %i%th coordinates of %y% and %z% are equal is %1/5%. Then I’ll work out %\\mathbb{E}f(y)f(z)%. It’s obviously positive because it is equal to %\\mathbb{E}_x(\\mathbb{E}[f(y)|x])^2%. But also, the joint distribution of %y% and %z% is equal to the joint distribution of %x% and %y%, since %y% is uniform and the digits of %z% are obtained from %y% by flipping independently with probability %1/5%. (Proof: the events %y_i=z_i% are independent and have probability 4/5.) Now for the second way of seeing it. This time I’ll generate %y% from %x% in two stages. First I’ll flip each digit with probability %\\lambda%. And then I’ll do that again. Now each time you flip with probability %\\lambda% you are multiplying %f% by the symmetric matrix %A% defined by %A_\\lambda(x,y)=\\binom n{|x-y|}\\lambda^{|x-y|}(1-\\lambda)^{n-|x-y|}% (which is the same as convolving %f% by the function %w(z)=\\binom n{|z|}\\lambda^{|z|}(1-\\lambda)^{n-|z|}.)% So we end up with %\\langle f,A_\\lambda^2f\\rangle%, which, since %A_\\lambda% is symmetric, is %\\langle A_\\lambda f,A_\\lambda f\\rangle.% Note that this proof fails if you flip with probability p greater than 1/2, because then you can’t solve the equation %2\\lambda(1-\\lambda)=p%. But that’s all to the good, because the result is false when %p>1/2%. When %p=1/2% it is “only just true” since flipping with probability 1/2 gives you the uniform distribution for y, so if %f% averages zero then %\\mathbb{E}f(x)f(y)% is zero too. I’m sure there’s yet another way of looking at it (and in fact these thoughts led to the above proof) which is to think of the random flipping operation as the exponential of an operation where you flip with a different probability. (To define that, I would flip N times, each time with probability %c/N%, and take limits.) I would then expect some general nonsense about exponentials to give the positivity. In the end I have taken a square root instead. 812.1\\. Thanks Tim! I believe this proof is in fact Fact 4 from #811 (with its %\\rho = 3/5%). 812.2\\. I was pleased with myself for figuring out the analogous trick in our equal-slices setting… until I realized it was exactly what you said way back in [#572](https://gowers.wordpress.com/2009/02/13/dhj-possible-proof-strategies/#comment-2309), first paragraph. Drat."},{"username":"ryanworldwide","timestamp":"2009-02-24T21:06:00.000Z","contents":"812.1\\. Thanks Tim! I believe this proof is in fact Fact 4 from #811 (with its %\\rho = 3/5%)."},{"username":"ryanworldwide","timestamp":"2009-02-24T21:27:00.000Z","contents":"812.2\\. I was pleased with myself for figuring out the analogous trick in our equal-slices setting… until I realized it was exactly what you said way back in [#572](https://gowers.wordpress.com/2009/02/13/dhj-possible-proof-strategies/#comment-2309), first paragraph. Drat."},{"username":"sune-kristian-jakobsen","timestamp":"2009-02-24T18:50:00.000Z","contents":"An optimistic conjecture. The hyper-optimistic conjecture says that %c_n{\\mu}\\leq \\overline{c}_n{\\mu}%. Here I would like to suggest an “optimistic conjecture”:  \nThere exist a number %C% such that %c_n{\\mu}\\leq C\\overline{c}_n{\\mu}% for all %n%. The hyper-optimistic conjecture implies the optimistic conjecture and the optimistic conjecture implies DHJ. Let %A_{\\epsilon}% be the set of elements x in A, such that A has measure at least %\\epsilon% in the slice x belong to.  \nWe know that if A is a line-free set, the sum of the measure of A in the slices %\\Gamma_{a+r,b,c}, \\Gamma_{a,b+r,c}, \\Gamma_{a,b,c+r}% is at most 2\\. In particular A cannot contain more than 2/3 of each of three slices in an equilateral triangle. Thus, the slices where A has a density greater than 2/3 forms a triangle-free subset of %\\Delta_n:= \\{ (a,b,c) \\in {\\Bbb Z}_+^3: a+b+c=n \\}%. So, if the optimistic conjecture is false, we know that for biggest line-free set A, the density of %A_{2/3+\\epsilon}% in A go to 0 as $n\\to \\infty$. I think the following would imply DHJ:  \nFor every %\\epsilon% there exist a C such that for every set A with %A=A_{\\epsilon}% (no element in A is “epsilon-lonesome” in its slice) there exist a equilateral triangle-free set %D\\subset \\Delta_n % with more than %\\mu(A)/C% elements."},{"username":"ryanworldwideryanworldwideryanworldwide","timestamp":"2009-02-24T21:11:00.000Z","contents":"Re Sperner & Influences & 809.2. Hi Terry, not to be ridiculously nitpicky, but could I ask one more question? It seems to me that:  \na) %f_{U^\\bot}% might end up having range %[0,1]% rather than %\\{0,1\\}%;  \nb) average influence is defined in terms of the *squared* change in %f_{U^\\bot}%;  \nc) we don’t have a triangle inequality for %\\ell_2^2%. (It wouldn’t be a problem if %f_{U^\\bot}% had range %\\{0,1\\}% since %\\ell_2^2% is a metric on $\\{0,1\\}$.) 814.1\\. Maybe one gets around it by playing with the scales & using triangle inequality for %\\ell_2%; I’m thinking about it… 814.2\\. Oops. That was a dumb question, as it turns out. Assuming %f_{U^\\bot}% is bounded, squared-differences are bounded by 4 times absolute-value-differences, and then use the triangle inequality. Got it."},{"username":"ryanworldwide","timestamp":"2009-02-24T21:33:00.000Z","contents":"814.1\\. Maybe one gets around it by playing with the scales & using triangle inequality for %\\ell_2%; I’m thinking about it…"},{"username":"ryanworldwide","timestamp":"2009-02-24T22:10:00.000Z","contents":"814.2\\. Oops. That was a dumb question, as it turns out. Assuming %f_{U^\\bot}% is bounded, squared-differences are bounded by 4 times absolute-value-differences, and then use the triangle inequality. Got it."},{"username":"ryanworldwideterence-tao","timestamp":"2009-02-24T22:56:00.000Z","contents":"Re Sperner. I think I can answer my own question from #814\\. I’ll say what Terry was saying in #809.2, being a little bit imprecise. Let %{T \\ll \\sqrt{n}}% be an integer. Define random variables %{x_0, \\dots, x_T}% as follows: %{x_0}% is a uniformly random string; %{x_{t}}% is formed from %{x_{t-1}}% by picking a random coordinate and, if that coordinate is %{0}% in %{x_{t-1}}%, changing it to a %{1}%. Now the distribution on %{(x_0, x_T)}% is pretty much like the distribution on %{(x,y)}% from before, with %{{\\epsilon} = T/n}%. Probably, as Terry says, one should ultimately wrap a Poisson choice of %{T}% around this entire process. Anyway, for fixed %{T}%, by telescoping we have <a>%\\displaystyle \\mathop{\\bf E}[f(x_0)g(x_T)] = \\mathop{\\bf E}[f(x_0)g(x_0)] + \\sum_{t=1}^T \\mathop{\\bf E}[f(x_0)(g(x_t) - g(x_{t-1}))]. \\ \\ \\ \\ \\ (1)%  \n</a>To bound the “error term” here, use Cauchy-Schwarz on each summand. For a given %{1 \\leq t \\leq T}% we have <a>%\\displaystyle |\\mathop{\\bf E}[f(x_0)(g(x_t) - g(x_{t-1}))]| \\leq \\|f\\|_2 \\sqrt{\\mathop{\\bf E}[(g(x_t) - g(x_{t-1}))^2]}. \\ \\ \\ \\ \\ (2)%  \n</a>Let %{Mg}% be the function defined by %{Mg(y) = \\mathop{\\bf E}_{y'}[(g(y') - g(y))^2]}%, where %{y'}% is formed from %{y}% by taking one step in the Markov chain we used in defining the %{x_t}%‘s. Hence the expression inside the square-root in [(2)](#eqnerr) is %{\\mathop{\\bf E}[Mg(x_{t-1})]}%. We would like to say that this expectation is close to %{\\mathop{\\bf E}[Mg(x_0)]}% because the distributions on %{x_0}% and %{x_{t-1}}% are very similar. We can use the argument in my “wrong-Sperner” notes for this. Using %{t \\leq T \\ll \\sqrt{n}}%, I think that’ll give something like %\\displaystyle |\\mathop{\\bf E}[Mg(x_{t-1})] - \\mathop{\\bf E}[Mg(x_{0})]| \\leq O(t/\\sqrt{n}) \\|Mg(x_0)\\|_2 \\leq O(t/\\sqrt{n}) \\sqrt{\\mathop{\\bf E}[Mg(x_0)]}, %  \nwhere on the right I’ve now assumed that %{g}% is bounded, hence %{Mg}% is bounded, hence %{(Mg)^2 \\leq Mg}% pointwise. But %{\\mathop{\\bf E}[Mg(x_0)]}% is precisely (well, maybe up to a factor of %{2}% or something) the “energy” or “average influence” of %{g}%; write it as %{\\mathcal{E}(g)}%. So we get %\\displaystyle \\mathop{\\bf E}[(g(x_t) - g(x_{t-1}))^2] = \\mathcal{E}(g) \\pm O(t/\\sqrt{n}) \\sqrt{\\mathcal{E}(g)}. %  \nLet’s assume now that in fact %{T \\leq \\sqrt{\\mathcal{E}(g)} \\sqrt{n}}%. Then the error in the above is essentially negligible. Now plugging this into [(2)](#eqnerr) we get that each of the %{T}% error terms in [(1)](#eqnmain) is at most %{O(1) \\|f\\|_2 \\sqrt{\\mathcal{E}(g)}}%. So overall, we conclude <a>%\\displaystyle \\mathop{\\bf E}[f(x_0)g(x_T)] = \\mathop{\\bf E}[f(x_0)g(x_0)] \\pm O(T \\cdot \\|f\\|_2 \\cdot \\sqrt{\\mathcal{E}(g)}). \\ \\ \\ \\ \\ (3)%  \n</a>Just to repeat, the assumptions needed to deduce [(3)](#eqnfinal) were that %{g}% is bounded and that %{T \\leq \\sqrt{\\mathcal{E}(g)} \\sqrt{n}}%. 815.1 Sperner Ryan, thanks for fleshing in the details and sorting out the l^1 vs l^2 issue. I just wanted to point out that we may have located the “DHJ(0,2)” problem that Boris brought up a while back – namely, DHJ(0,2) should be DHJ(2) for “low influence” sets. I think we now have a satisfactory understanding of the DHJ(2) problem from an obstructions-to-uniformity perspective, namely that arbitrary sets can be viewed as the superposition of a low influence set (or more precisely, a function %f_{U^\\perp}%) plus a DHJ(2)-uniform error. DHJ(1,3) may now need to be tweaked to generalise to functions f which are polynomial combinations of 01-low influence, 12-low influence, and 20-low influence functions, where 01-low influence means that f(x) and f(y) are close whenever y is formed from x by flipping a random 0 bit to a 1 or vice versa, etc. (With our current definition of DHJ(1,3), we are considering products of indicator functions which have zero 01-influence, zero 12-influence, and zero 20-influence respectively.)"},{"username":"terence-tao","timestamp":"2009-02-25T01:10:00.000Z","contents":"815.1 Sperner Ryan, thanks for fleshing in the details and sorting out the l^1 vs l^2 issue. I just wanted to point out that we may have located the “DHJ(0,2)” problem that Boris brought up a while back – namely, DHJ(0,2) should be DHJ(2) for “low influence” sets. I think we now have a satisfactory understanding of the DHJ(2) problem from an obstructions-to-uniformity perspective, namely that arbitrary sets can be viewed as the superposition of a low influence set (or more precisely, a function %f_{U^\\perp}%) plus a DHJ(2)-uniform error. DHJ(1,3) may now need to be tweaked to generalise to functions f which are polynomial combinations of 01-low influence, 12-low influence, and 20-low influence functions, where 01-low influence means that f(x) and f(y) are close whenever y is formed from x by flipping a random 0 bit to a 1 or vice versa, etc. (With our current definition of DHJ(1,3), we are considering products of indicator functions which have zero 01-influence, zero 12-influence, and zero 20-influence respectively.)"},{"username":"gowers","timestamp":"2009-02-24T23:47:00.000Z","contents":"Obstructions. I hope I’m going to have time to do some serious averaging arguments this evening. They will be private calculations (though made public as soon as they work), but let me give an outline of what I am hoping to make rigorous. The broad aim is to prove that a set with no combinatorial lines has a significant local correlation with a set of complexity 1\\. This is a statement that has been sort of sketched in various comments already (by Terry and by me and possibly by others too), but I now think that writing it out in a serious way will be a very useful thing to do and should get us substantially closer to a density-increment proof of DHJ(3). Here is a rough description of the steps. 1\\. A rather general argument to prove that whenever we feel like restricting to a combinatorial subspace, we can always assume that it has density at most %\\delta-\\eta%, where %\\eta% is an arbitrary function of %\\delta%. This kind of argument is standard, and has already been mentioned by Terry: basically if you can find a subspace with increased density then you happily pass to that subspace and you’ve already completed your iteration. If you can’t do that, then the proportion of subspaces (according to any reasonable distribution of your convenience) with substantially smaller density is tiny. The minor (I hope) technical challenge is to get a version of this principle that is sufficiently general that it can be used easily whenever it is needed. 2\\. Representing combinatorial lines as (U,V,W), we know that for an average W (each element chosen with probability 1/3 — I’m going for uniform measure here) the density of (U,V) such that (U,V,W) is in %\\mathcal{A}% is %\\delta%. 3\\. Also, by 1, for almost all W we find that the set of points %(U,V,W\\cup W')% where %|W'|\\leq m% (for some %m% with %1<) has density almost %\\delta%. 4\\. Combining 2 and 3, we obtain a W such that the density of (U,V) with (U,V,W) in %\\mathcal{A}% is at least %\\delta/2%, say, _and_ the density of %(U,V,W\\cup W')\\in\\mathcal{A}% is almost as large as %\\delta% (at least). 5\\. But the points of the latter kind have to avoid those %U% and %V% for which the point %(U,V,W)\\in\\mathcal{A}%, which is a dense complexity-1 obstruction in the “unbalanced” set of %(U,V,W')% that have union %{}[n]\\setminus W% and have %W'% of size at most %m%. 6\\. Randomly restricting, we can get a similar statement, but this time for a “balanced” set—that is, one where %U',V',W'% have comparable sizes. 7\\. From that it is straightforward to get a (local) density increment on a special set of complexity 1 (as defined [here](http://michaelnielsen.org/polymath1/index.php?title=Complexity_of_a_set)). Now I think I’ve basically shown that a special set of complexity 1 contains large combinatorial subspaces, though I need to check this. But what we actually need is a bit stronger than that: we need to cover sets of complexity 1 uniformly by large combinatorial subspaces, or perhaps do something else of a similar nature. (This is where the Ajtai-Szemerédi proof could come in very handy.) But if everything from 1 to 7 works out—I’m not sure how realistic a hope that is but even a failure would be instructive—then we’ll be left with a much smaller-looking problem to solve. I’ll report back when I either get something working or see where some unexpected difficulty lies."},{"username":"ryanworldwide","timestamp":"2009-02-25T01:27:00.000Z","contents":"Sperner. Just to clarify, the thing I wrote in #800 says that if you don’t mind restricting to combinatorial subspaces (which we usually don’t, unless we’re really trying to get outstanding quantitatives), then the decomposition %f = f_{U^\\bot} + f_U% we seek can be achieved trivially: you just take %f_{U^\\bot} = \\mathbb{E}[f]%."},{"username":"terence-taorandallterence-tao","timestamp":"2009-02-25T07:35:00.000Z","contents":"Ergodic proof of DHJ(3) I managed to digest Randall’s lecture notes on the completion of the Furstenberg-Katznelson proof of DHJ(3) (the focus of the 600 thread) to the point where I now have an informal combinatorial translation of the argument at [http://michaelnielsen.org/polymath1/index.php?title=Furstenberg-Katznelson_argument](http://michaelnielsen.org/polymath1/index.php?title=Furstenberg-Katznelson_argument) that avoids any reference to infinitary concepts, at the expense of rigour and precision. Interestingly, the argument is morally based on a reduction to something resembling DHJ(1,3), but more complicated to state. We are trying to get a lower bound for %{\\Bbb E} f(\\ell(0)) f(\\ell(1)) f(\\ell(2))% (1) where f is non-negative, bounded, and has positive density, and %\\ell% ranges over all lines with “few” wildcards (and I want to be vague about what “few” means). The first reduction is to eliminate “uniform” or “mixing” components from the second two factors and reduce to %{\\Bbb E} f(\\ell(0)) f_{01}(\\ell(1)) f_{20}(\\ell(2))% (2) where %f_{01}%, %f_{20}% are certain “structured” components of f, analogous to %f_{U^\\perp}% from the Sperner theory. They have positive correlation with f, and in fact are positive just about everywhere that f is positive. What other properties do %f_{01}, f_{20}% have? In a perfect world, they would be “complexity 1” sets, and in particular one would expect %f_{01}% to be describable as some simple combination of 01-low influence sets and 12-low influence sets (and similarly %f_{20}% should be some simple combination of 20-low influence sets and 12-low influence sets). Here, ij-low influence means that the function does not change much if an i is flipped to a j or vice versa. Unfortunately, it seems (at least from the ergodic approach) that this is not easily attainable. Instead, %f_{01}% obeys a more complicated (and weaker) property, which I call “01-almost periodicity relative to 12-low influence”, with %f_{20}% obeying a similar property. Very roughly speaking, this is a “relative” version of 01-low influence: flipping digits from 0 to 1 makes %f_{01}% change, but the way in which it changes is controlled entirely by functions that have low 12-influence. (This is related to the notion of “uniform almost periodicity” which comes up in my paper on the quantitative ergodic theory proof of Szemeredi’s theorem.) It is relatively painless (using Cauchy-Schwarz and energy-increment methods) to pass from (1) to (2). To deal with (2) we need some periodicity properties of %f_{01}, f_{20}% on small “monochromatic” spaces (the existence of which is ultimately guaranteed by Graham-Rothschild) which effectively let us replace %f_{01}(\\ell(1))% with %f_{01}(\\ell(0))% and %f_{20}(\\ell(2))% with %f_{20}(\\ell(0))% on a large family of lines %\\ell% (and more importantly, a large _12-low influence_ family of lines). From this fact, and the previously mentioned fact that %f_{01}, f_{20}% are large on f, we can get DHJ(3). The argument as described on the wiki is far from rigorous at present, but I am hopeful that it can be translated into a rigorous finitary proof (though it is not going to be pleasant – I would have to deploy a lot of machinery from my quantitative ergodic theory paper). Perhaps a better approach would be to try to export some of the ideas here to the Fourier-type approaches where there is a better chance of a shorter and more quantitatively effective argument. 818.1 I haven’t read this closely enough to have even an initial impression, however, much of it looks (somewhat) familiar.  \nFirst, I notice you removed your discussion of stationarity…instead (tell me if I misread), and in multiple settings, you seem to do a Cesaro average, over lines, rather than employing something like Graham-Rothschild to get near-convergence along lines restricted to a subspace. Most striking of these are instances of using the so-called IP van der Corput lemma. Looking at its proof, this does indeed look kosher, but it’s rather surprising to me all the same; assuming I’m understanding this at least somewhat correctly, did you give any thought to whether the ergodic proof itself could be tidily rewritten to accommodate this averaging method? Modulo the above, the main part of the argument I still don’t (in principle) understand is how you bound h, the number of functions used to approximate the almost periodic component of f, independently of n. (This is the part of the argument I got stuck on in my own thoughts.) I see now that you solved this issue in your quantitative ergodic proof of Szemeredi, which I printed last night as well, though I haven’t read deeply enough yet to see how. Am I to assume that something similar happens here, or is the answer different in the two cases? 818.2 Yes, the argument is distilled from your notes, though as you see I messed around with the notation quite a bit. The stationarity is sort of automatic if you work with random subspaces of a big space %[3]^n%, and I am implicitly using it all over the place when rewriting one sort of average by another. I am indeed hoping that Cesaro averaging may be simpler to implement than IP averaging, and may save me from having to use Graham-Rothschild repeatedly. There are a lot of things hidden in the sketch that may cause this to bubble up. For instance, I am implicitly using the fact that certain shift operators %U_\\alpha% converge (in some Cesaro or IP sense) to an orthogonal projection, and this may require a certain amount of Graham-Rothschild type trickery (I started writing some separate notes on a finitary Hilbert space IP Khintchine recurrence theorem which will be relevant here.) I admit I’m a bit sketchy on how to deal with h not blowing up. A key observation here is that of statistical sampling: if one wants to understand an average %{\\Bbb E}_{h \\in H} c_h% of bounded quantities over a very large set H, one can get quite a good approximation to this expression by picking a relatively small number %h_1,\\ldots,h_m% of representatives of H at random and looking at the local average %{\\Bbb E}_{j=1,\\ldots,m} c_{h_j}% instead. (This fact substitutes for the fact used in the Furstenberg approach that Volterra integral operators are compact and hence approximable by finite rank operators; or more precisely, the Furstenberg approach needs the relative version of this over some factor Y.) I haven’t worked out completely how this trick will mesh with the IP-systems involved, but I’m hoping that I can throw enough Ramsey theorems at the problem to make it work out. Perhaps one thing that helps out in the finitary setting that is not immediately available in the ergodic setting is that there are more symmetries available; in particular, the non-commutativity of the IP systems that makes the ergodic setup so hard seems to be less of an issue in the finitary world (the operations of flipping a random 0 to a 1 and flipping a random 0 to a 2 essentially commute since there are so many 0s to choose from). There is a price to pay for this, which is that certain Ramsey theorems may break the symmetry and so one may have to choose to forego either the Ramsey theorem or the symmetry. This could potentially cause a problem in my sketch; as I said, I have not worked out the details (given the progress on the Fourier side of things, I had the vague hope that maybe just the concepts in the sketch, most notably the concept of almost periodicity relative to a low influence factor, could be useful to assist the other main approach to the problem, as I am not particularly looking forward to rewriting my quantitative ergodic theory paper again.)"},{"username":"randall","timestamp":"2009-02-25T22:06:00.000Z","contents":"818.1 I haven’t read this closely enough to have even an initial impression, however, much of it looks (somewhat) familiar.  \nFirst, I notice you removed your discussion of stationarity…instead (tell me if I misread), and in multiple settings, you seem to do a Cesaro average, over lines, rather than employing something like Graham-Rothschild to get near-convergence along lines restricted to a subspace. Most striking of these are instances of using the so-called IP van der Corput lemma. Looking at its proof, this does indeed look kosher, but it’s rather surprising to me all the same; assuming I’m understanding this at least somewhat correctly, did you give any thought to whether the ergodic proof itself could be tidily rewritten to accommodate this averaging method? Modulo the above, the main part of the argument I still don’t (in principle) understand is how you bound h, the number of functions used to approximate the almost periodic component of f, independently of n. (This is the part of the argument I got stuck on in my own thoughts.) I see now that you solved this issue in your quantitative ergodic proof of Szemeredi, which I printed last night as well, though I haven’t read deeply enough yet to see how. Am I to assume that something similar happens here, or is the answer different in the two cases?"},{"username":"terence-tao","timestamp":"2009-02-26T01:50:00.000Z","contents":"818.2 Yes, the argument is distilled from your notes, though as you see I messed around with the notation quite a bit. The stationarity is sort of automatic if you work with random subspaces of a big space %[3]^n%, and I am implicitly using it all over the place when rewriting one sort of average by another. I am indeed hoping that Cesaro averaging may be simpler to implement than IP averaging, and may save me from having to use Graham-Rothschild repeatedly. There are a lot of things hidden in the sketch that may cause this to bubble up. For instance, I am implicitly using the fact that certain shift operators %U_\\alpha% converge (in some Cesaro or IP sense) to an orthogonal projection, and this may require a certain amount of Graham-Rothschild type trickery (I started writing some separate notes on a finitary Hilbert space IP Khintchine recurrence theorem which will be relevant here.) I admit I’m a bit sketchy on how to deal with h not blowing up. A key observation here is that of statistical sampling: if one wants to understand an average %{\\Bbb E}_{h \\in H} c_h% of bounded quantities over a very large set H, one can get quite a good approximation to this expression by picking a relatively small number %h_1,\\ldots,h_m% of representatives of H at random and looking at the local average %{\\Bbb E}_{j=1,\\ldots,m} c_{h_j}% instead. (This fact substitutes for the fact used in the Furstenberg approach that Volterra integral operators are compact and hence approximable by finite rank operators; or more precisely, the Furstenberg approach needs the relative version of this over some factor Y.) I haven’t worked out completely how this trick will mesh with the IP-systems involved, but I’m hoping that I can throw enough Ramsey theorems at the problem to make it work out. Perhaps one thing that helps out in the finitary setting that is not immediately available in the ergodic setting is that there are more symmetries available; in particular, the non-commutativity of the IP systems that makes the ergodic setup so hard seems to be less of an issue in the finitary world (the operations of flipping a random 0 to a 1 and flipping a random 0 to a 2 essentially commute since there are so many 0s to choose from). There is a price to pay for this, which is that certain Ramsey theorems may break the symmetry and so one may have to choose to forego either the Ramsey theorem or the symmetry. This could potentially cause a problem in my sketch; as I said, I have not worked out the details (given the progress on the Fourier side of things, I had the vague hope that maybe just the concepts in the sketch, most notably the concept of almost periodicity relative to a low influence factor, could be useful to assist the other main approach to the problem, as I am not particularly looking forward to rewriting my quantitative ergodic theory paper again.)"},{"username":"ryanworldwideterence-taoryanworldwide","timestamp":"2009-02-25T09:53:00.000Z","contents":"#816 and #818 look quite exciting; I plan to try to digest them soon. Meanwhile, here is a Moser-esque problem I invented for the express purpose of being solvable. I hope it might give a few tricks we can use (but it might not be of major help due to the PS of #528). Let’s define a combinatorial _bridge_ in %{[3]^n}% to be a triple of points %{(x,y,z) \\in \\{-1,0,1\\}^n}% formed by taking a string with zero or more wildcards and filling in the wildcards with either %{(0, -1, 0)}% or %{(0, 1, 0)}%. If there are zero wildcards we call the bridge degenerate. I think I can show, using the ideas from #800, that if %{f : \\{-1,0,1\\}^n \\to \\{0,1\\}}% has mean %{\\delta > 0}% and %{n}% is sufficiently large as function of %{\\delta}%, then there is a nondegenerate bridge %{(x,y,z)}% with %{f(x) = f(y) = f(z) = 1}%. Roughly, we first use a density-increment argument to reduce to the case when %{f}% is extremely noise sensitive; i.e., %{\\|T_{1-\\gamma} f\\|_2^2}% is only a teeny bit bigger than %{\\delta^2}%. Here %{\\gamma}% is something very small to be chosen later. Next, we pick a suitable distribution on combinatorial bridges %{(x,y,z)}%; basically, choose a random one where the wildcard probability is %{{\\epsilon} \\ll \\sqrt{\\delta/n}}%. Now the key is that under this distribution, there is “imperfect correlation” between the random variable %{(x,y)}% and the random variable %{z}% — and similarly, between %{x}% and %{(y,z)}%. Here I use the term in the sense of the Mossel paper in #811\\. Because of this (see Mossel’s Lemma 6.2), %{\\mathop{\\bf E}[f(x)f(y)f(z)]}% is practically the same as %{\\mathop{\\bf E}[T_{1-\\gamma}f(x)\\cdot T_{1-\\gamma}f(y)\\cdot T_{1-\\gamma}f(z)]}%, when %{\\gamma \\leq \\delta^{O(1)} {\\epsilon}}%. But this is extremely close to %{\\delta^3}% because %{\\mathop{\\bf E}[T_{1-\\gamma}f(x)] = \\mathop{\\bf E}[T_{1-\\gamma}f(y)] = \\mathop{\\bf E}[T_{1-\\gamma}f(z)] = \\delta}% and because the error can be controlled with H\\”{o}lder in terms of %{\\|T_{1-\\gamma} f - \\delta\\|_2^2}%, which is teeny by the density-increment reduction. More details here: [http://www.cs.cmu.edu/~odonnell/bridges.pdf](http://www.cs.cmu.edu/~odonnell/bridges.pdf). I can try to port this to the wiki soon. Ryan, I’m not sure what you mean by “fill the wildcards by either (0,-1,0) or (0,+1,0)”. Wouldn’t this always make x and z equal in a bridge? Perhaps an example would clarify what you mean here. 819.2: Er, whoops. You’re right. Even easier then I thought <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> To make this problem more interesting, I think it will work if the triples allowed on wildcards are, say: (-,0,-), (-,0,0), (+,0,0), (+,0,+). But: a) this is not as nice-looking, and b) I think it’ll actually take slightly more work. So, um, never mind for now."},{"username":"terence-tao","timestamp":"2009-02-26T01:38:00.000Z","contents":"Ryan, I’m not sure what you mean by “fill the wildcards by either (0,-1,0) or (0,+1,0)”. Wouldn’t this always make x and z equal in a bridge? Perhaps an example would clarify what you mean here."},{"username":"ryanworldwide","timestamp":"2009-02-26T03:09:00.000Z","contents":"819.2: Er, whoops. You’re right. Even easier then I thought <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> To make this problem more interesting, I think it will work if the triples allowed on wildcards are, say: (-,0,-), (-,0,0), (+,0,0), (+,0,+). But: a) this is not as nice-looking, and b) I think it’ll actually take slightly more work. So, um, never mind for now."},{"username":"gowersryanworldwidegowers","timestamp":"2009-02-25T23:44:00.000Z","contents":"Density increment. I’ve now finished a wiki write-up that is supposed to establish (and does unless I’ve made a mistake, which is possible) that if %\\mathcal{A}% is a line-free set of uniform density %\\delta% then you can pass to a combinatorial subspace of dimension %m%, as long as %m=o(n^{1/2})%, and find a special subset of complexity 1 in that subspace of density at least %c\\delta^2%, such that the equal-slices density of %\\mathcal{A}% inside that special subset is at least %\\delta+c\\delta^2%. To be less precise about it, I think I’ve shown that if %\\mathcal{A}% contains no combinatorial line, then you get a density increase on a nice set. I start with uniform density and switch to equal-slices density, but that is deliberate, and explained in the write-up, which, by the way, is [here](http://michaelnielsen.org/polymath1/index.php?title=Line-free_sets_correlate_locally_with_complexity-1_sets#Remarks) So my attention is now going to turn to trying to copy either the Ajtai-Szemerédi proof, or Shkredov’s proof, of the corners theorem. I feel optimistic that this can be done, given that special sets of complexity 1 can be shown to contain combinatorial subspaces—though that on its own is not enough. 820.1\\. Hi Tim — nice. I read through the wiki proof and agree that it should be correct. It is quite interesting how the passage between uniform and equal-slices seems necessary. On one hand I worry a bit that the different measures might get out of control; on the other hand, the optimistic way to look at it is that we may eventually get so proficient at passing between measures that it’ll be a very useful tool. 820.2 One observation that makes me feel slightly less worried about passing from one measure to another is that you can start with equal-slices measure instead. Then as a first step you pass to uniform measure on a subspace. Then you carry out the argument as given (passing to a subspace of that subspace). And now you’ve ended up with the same measure that you started with."},{"username":"ryanworldwide","timestamp":"2009-02-26T06:02:00.000Z","contents":"820.1\\. Hi Tim — nice. I read through the wiki proof and agree that it should be correct. It is quite interesting how the passage between uniform and equal-slices seems necessary. On one hand I worry a bit that the different measures might get out of control; on the other hand, the optimistic way to look at it is that we may eventually get so proficient at passing between measures that it’ll be a very useful tool."},{"username":"gowers","timestamp":"2009-02-26T15:14:00.000Z","contents":"820.2 One observation that makes me feel slightly less worried about passing from one measure to another is that you can start with equal-slices measure instead. Then as a first step you pass to uniform measure on a subspace. Then you carry out the argument as given (passing to a subspace of that subspace). And now you’ve ended up with the same measure that you started with."},{"username":"ryanworldwide","timestamp":"2009-02-26T03:11:00.000Z","contents":"Here is (I believe) a simple proof of Terry’s most basic structure theorem. The setting is as follows: %{\\Omega}% is a finite set and %{\\mu}% is a probability distribution on %{\\Omega}%. We also write %{\\mu = \\mu^{\\otimes n}}% for the product distribution on %{\\Omega^n}%. We work in the space of functions %{f : \\Omega^n \\to {\\mathbb R}}% with inner product %{\\langle f, g \\rangle = \\mathop{\\bf E}_{x \\sim \\mu}[f(x)g(x)]}%. Henceforth, all probabilities are wrt %{\\mu}% unless specified. **Theorem:** Let %{f :\\Omega^n \\to {\\mathbb R}}% have %{\\mathop{\\bf Var}[f] \\leq 1}%. Let %{0 < {\\epsilon} < 1}%. Let %{\\frac{1}{n} \\leq \\sigma_1 \\leq \\tau_1 \\leq \\sigma_2 \\leq \\tau_2 \\leq \\cdots \\leq \\sigma_k \\leq \\tau_k \\leq {\\epsilon}}% be a sequence of “scales” (or “times”) satisfying the following conditions: (a) %{\\sigma_{i}/\\tau_{i-1} \\geq 3\\ln(1/{\\epsilon})/{\\epsilon}}% for all %{i}%; (b) %{k \\geq 1/{\\epsilon}^2}%. Then there exists %{i}% such that %{f}% can be written as %{f = f_{\\text{stab}} + f_{\\text{sens}}}%, where: 1\\. %{\\mathop{\\bf E}[f_{\\text{stab}}] = \\mathop{\\bf E}[f]}%. 2\\. %{\\|f_{\\text{stab}}\\|_p \\leq \\|f\\|_p}% for all %{1 \\leq p \\leq \\infty}%; hence %{f_{\\text{stab}}}% has range %{[0,1]}% if %{f}% does (and then %{f_{\\text{sens}}}% has range %{[-1,1]}%). 3\\. %{0 \\leq \\langle f_{\\text{stab}}, f_{\\text{sens}} \\rangle \\leq 3{\\epsilon}}%. 4\\. %{\\mathcal{E}(f_{\\text{stab}}) \\leq 1/(\\tau_{i} n)}%, where %{\\mathcal{E}(g)}% denotes the “average influence of %{g}%“: i.e., %{\\mathop{\\bf E}[(g(x) - g(x'))^2]}%, where %{x \\sim \\mu}% and %{x'}% is formed by rerandomizing a random coordinate of %{x}%. 5\\. %{\\mathcal{S}_{1 - \\sigma_{i}}(f_{\\text{sens}}) \\leq 3{\\epsilon}^2}%, where %{\\mathcal{S}_{1 - \\sigma}(g)}% denotes the “noise stability of %{g}% at %{1 - \\sigma}%“: i.e., %{\\mathop{\\bf E}[g(x)g(x')]}%, where %{x \\sim \\mu}% and %{x'}% is formed by rerandomizing each coordinate of %{x}% with probability %{\\sigma}%. Proof in next post."},{"username":"ryanworldwideterence-taoryanworldwide","timestamp":"2009-02-26T03:12:00.000Z","contents":"_Proof:_ For simplicity I present the proof when %{\\Omega = \\{0,1\\}}% and %{\\mu}% is the uniform distribution; to get the full version, replace the Fourier transform with the “Hoeffding AKA Efron-Stein orthogonal decomposition”. For an interval %{J = [a,b]}% of natural numbers, let %{\\mathcal{W}_I(f)}% denote %{\\sum_{a \\leq |S| \\leq b} \\hat{f}(S)^2}%. The condition %{\\mathop{\\bf Var}[f] \\leq 1}% is equivalent to %{\\mathcal{W}_{[1,n]} \\leq 1}%, and we will use this frequently. Now consider the intervals of the form %{J_i = [{\\epsilon}/\\tau_i, 2\\ln(1/{\\epsilon})/\\sigma_i]}%. By hypothesis (a) these intervals are disjoint, and by hypothesis (b) there are at least %{1/{\\epsilon}^2}% of them. By Pigeonhole, we can fix a particular %{i}% such that %{\\mathcal{W}_{J_i} \\leq {\\epsilon}^2}%. Henceforth write %{\\sigma = \\sigma_i}%, %{\\tau = \\tau_i}%. Now set %{f_{\\text{stab}} = T_{1-\\tau} f}% and %{f_{\\text{sens}} = f - f_{\\text{stab}}}%. Conditions 1 and 2 are satisfied. Condition 4 holds because %\\displaystyle n\\mathcal{E}(f_{\\text{stab}}) = \\sum_S |S|\\hat{f}_{\\text{stab}}(S)^2 = \\sum_S |S| (1-\\tau)^{2|S|}\\hat{f}(S)^2 %  \nand this is at most %{1/\\tau}% because %{|S| (1-\\tau)^{2|S|} \\leq 1/\\tau}% for all %{S}%. For Condition 3, %\\displaystyle \\langle f_{\\text{stab}}, f - f_{\\text{stab}} \\rangle = \\sum_S a(|S|) \\hat{f}(S)^2, %  \nwhere %\\displaystyle a(s) = (1-\\tau)^{s} - (1-\\tau)^{2s}. %  \nWe have %{a(0) = 0}% so we can drop that term. We have %{a(s) \\leq s \\tau}% always so we get a contribution of at most %{{\\epsilon}}% from the terms with %{|S| \\leq {\\epsilon}/\\tau}%. We also have %{a(s) \\leq (1-\\tau)^s \\leq \\exp(-\\tau s)}% always so we get a contribution of at most %{{\\epsilon}^2}% from the terms with %{|S| \\geq 2\\ln(1/{\\epsilon})/\\tau}%. And the remaining terms have %{|S| \\in [{\\epsilon}/\\tau, \\ln(1/{\\epsilon})/\\tau] \\subset J_i}%, so we get a contribution of at most %{{\\epsilon}^2}% from them. The proof of Condition 5 is similar. We have %\\displaystyle \\mathcal{S}_{1 - \\sigma}(f_{\\text{sens}}) = \\sum_S (1-\\sigma)^{|S|} \\hat{f}_{\\text{sens}}(S)^2 = \\sum_S b(|S|) \\hat{f}(S)^2, %  \nwhere %\\displaystyle b(s) = (1-\\sigma)^{s} (1-(1-\\tau)^s)^2\\. %  \nAgain, %{b(0) = 0}%. We have %{b(s) \\leq (\\tau s)^2}% always so we get a contribution of at most %{{\\epsilon}^2}% from the terms with %{|S| \\leq {\\epsilon}/\\tau}%. We also have %{b(s) \\leq (1-\\sigma)^s}% always so we get a contribution of at most %{{\\epsilon}^2}% from the terms with %{|S| \\geq 2\\ln(1/{\\epsilon})/\\sigma}%. And the remaining terms have %{|S| \\in J_i}% so we get a contribution of at most %{{\\epsilon}^2}% from them. %\\Box% 822.1 This looks about right to me. The game is always to first use pigeonhole to find a nice big gap in the energy spectrum, and then use that gap to split f into opposing pieces. (Sometimes one also needs a (small) third term to deal with the small amount of energy stuck inside the gap.) One thing that works in your favour here is that the averaging operators %T_{1-\\tau}% are positivity preserving, so if f is positive then so is %f_{stab}% (and related to this is a useful comparison principle: if f is pointwise bigger than g, then %f_{stab}% is pointwise bigger than %g_{stab}%.) Things get more tricky when one doesn’t have this positivity preserving property, because it is absolutely essential later on that %f_{stab}% be non-negative. In my paper with Ben we had to introduce the machinery of partitions and conditional expectations to deal with this. One side effect of this is that it can force the scales to be exponentially separated (e.g. %1/\\sigma_i \\gg \\exp( 1 / \\sigma_{i+1} )%) rather than just separated by a large constant depending only on %\\varepsilon%). This leads to the type of tower-exponential bounds which are familiar from the regularity lemma. Once we get to the fancier structure theorems, we may start seeing these sorts of bounds emerge, but I agree that in the simple case here we don’t have to deal with all that because of the handy %T_{1-\\tau}% operators. Combining this with your notes that control the Sperner count by the noise stability, it looks like we have a pretty solid Fourier-analytic proof of DHJ(2), which was one of our early objectives (suggested first, I believe, by Gil). 822.2: Yes, I think with slightly more work one can get an “Adequate-Sperner” Fourier proof; I think it will require density %\\Omega(1/\\sqrt{\\log n})%. This is still not quite right, but is closer."},{"username":"terence-tao","timestamp":"2009-02-26T03:48:00.000Z","contents":"822.1 This looks about right to me. The game is always to first use pigeonhole to find a nice big gap in the energy spectrum, and then use that gap to split f into opposing pieces. (Sometimes one also needs a (small) third term to deal with the small amount of energy stuck inside the gap.) One thing that works in your favour here is that the averaging operators %T_{1-\\tau}% are positivity preserving, so if f is positive then so is %f_{stab}% (and related to this is a useful comparison principle: if f is pointwise bigger than g, then %f_{stab}% is pointwise bigger than %g_{stab}%.) Things get more tricky when one doesn’t have this positivity preserving property, because it is absolutely essential later on that %f_{stab}% be non-negative. In my paper with Ben we had to introduce the machinery of partitions and conditional expectations to deal with this. One side effect of this is that it can force the scales to be exponentially separated (e.g. %1/\\sigma_i \\gg \\exp( 1 / \\sigma_{i+1} )%) rather than just separated by a large constant depending only on %\\varepsilon%). This leads to the type of tower-exponential bounds which are familiar from the regularity lemma. Once we get to the fancier structure theorems, we may start seeing these sorts of bounds emerge, but I agree that in the simple case here we don’t have to deal with all that because of the handy %T_{1-\\tau}% operators. Combining this with your notes that control the Sperner count by the noise stability, it looks like we have a pretty solid Fourier-analytic proof of DHJ(2), which was one of our early objectives (suggested first, I believe, by Gil)."},{"username":"ryanworldwide","timestamp":"2009-02-26T06:00:00.000Z","contents":"822.2: Yes, I think with slightly more work one can get an “Adequate-Sperner” Fourier proof; I think it will require density %\\Omega(1/\\sqrt{\\log n})%. This is still not quite right, but is closer."},{"username":"ryan-odonnellryan-odonnell","timestamp":"2009-02-26T11:54:00.000Z","contents":"Structure Theorem. A small note: Unless I’m mistaken, the structure theorem around Lemma 1 in Terry’s wiki notes on Furstenberg-Katznelson — which decomposes functions on %[3]^n% into a 12-stable and a 12-sensitve part — can be proved in the same way as in #821\\. Specifically, employing Tim’s %(U,V,W)% notation, I think you just need to look at the expectation over %U% of the Fourier weight of %f_U% on the various intervals %J_i%. Here %f_U% denotes the restricted function, with 0’s fixed into %U%, on binary inputs (1 and 2). Everything seems to go through the same, using the fact that restricting functions with $U$ commutes with flipping 1’s and 2’s, and that %\\mathbb{E}_U[\\mathrm{Var}[f_U]] \\leq \\mathrm{Var}[f]%. 823.1\\. The weird %\\mathbb{Var}% at the end here is meant to be “Var”. _Fixed now — Tim._"},{"username":"ryan-odonnell","timestamp":"2009-02-26T23:56:00.000Z","contents":"823.1\\. The weird %\\mathbb{Var}% at the end here is meant to be “Var”. _Fixed now — Tim._"},{"username":"gowers","timestamp":"2009-02-26T16:48:00.000Z","contents":"DHJ(3) general strategy I have a hunch, after looking at [Van Vu’s quantitative version of Ajtai-Szemerédi](http://www.math.rutgers.edu/~vanvu/papers/numbertheory/gower.pdf), that it may be possible to strengthen the argument on the wiki to give the whole thing. If this were not a polymath project I would of course go away and try to make the idea work, keeping quiet if it didn’t and giving full details if it did. But it is polymath, so let me say in advance roughly what I hope will happen. In the corners problem, it is fairly easy to prove that a corner-free set has a density increment on a Cartesian product of two dense sets. The problem is what to do then. Ajtai and Szemerédi’s strategy is to use a more sophisticated averaging argument, making heavy use of Szemerédi’s theorem, to obtain a density increment on a highly structured Cartesian product: it is a product of two arithmetic progressions with the same common difference. Initially, it looks very discouraging that they have to use Szemerédi’s theorem, but I now believe that that may just be an artificial byproduct of the fact that they are working in the grid and want to get another grid at the next stage of the induction. But in the Hales-Jewett world, you don’t actually need long arithmetic progressions anywhere, and I think the right analogue will in fact turn out to be multidimensional subspaces of %{}[2]^n%. My actual idea is slightly more precise than this, but hard to explain. Basically, I want to revisit the argument on the wiki, but aim straight for a density increase on a subspace, modelling the argument as closely as I can on the Ajtai-Szemerédi argument for the corners problem. If that works, then it will probably give a tower-type bound. A follow-up project would then be to try to do a more powerful Shkredov-type argument to get a bounded number of exponentials. But I’m getting way ahead of myself by even discussing this. Let me also slip in a metacomment. I find it slightly inconvenient that, now that we have modest threading, I can’t just scroll to the bottom of the comments and see what has been added. As Jason Dyer pointed out, there is a trade-off here. On the one hand, it makes it slightly harder if you are trying to keep up with the discussion in real time, but on the other it probably means that the comments are better organized if you want to come back to them later. In the end, the second consideration probably trumps the first, because it’s a long-term convenience at the cost of a mild short-term inconvenience. But I still think that we should be quite careful about when and how much we use the threading."},{"username":"gowers","timestamp":"2009-02-26T22:32:00.000Z","contents":"Ajtai-Szemerédi approach. No time to justify this, or at least not for a few hours (and possibly not before tomorrow morning) but I am now very optimistic indeed that the Ajtai-Szemerédi approach to corners can be modified to give DHJ(3). As usual, this surge of optimism may be followed by a realization that there are some serious technical obstacles, but I think that the existing write-up of the complexity-1 correlation contains methods for dealing with the technicalities that will arise. I plan to write up (i) a sketch proof of Ajtai-Szemerédi on the wiki, (ii) an outline of an argument here on the blog (which will include the AS-to-DHJ(3) dictionary I will be using), and (iii) a detailed sketch of an argument on the wiki. Those are in decreasing order of probability that I can actually finish them without getting stuck."},{"username":"terence-taoterence-tao","timestamp":"2009-02-27T05:45:00.000Z","contents":"Structure theorem I now think there is a way to do the DHJ(2) structure theory without explicit mention of the Fourier transform (at the cost of worse quantitative bounds), which may be important when trying to extend the arguments to the DHJ(3) structure theory sketched in my wiki notes. The point is to replace the self-adjoint operators %T_{1-\\tau}% that Ryan uses by a “one-sided” counterpart. Given a function %f: [2]^n \\to {\\Bbb R}%, and a parameter %\\lambda > 0%, let %S_\\lambda f: [2]^n \\to {\\Bbb R}% be defined by %S_\\lambda f(x) := {\\Bbb E} f(y)%, where y is formed from x by allowing each coordinate of x, with probability %\\lambda/n%, of being overwritten by a 1\\. (This is in contrast to Ryan’s rerandomisation, which would cause x to be overwritten by a random 0/1 bit rather than by 1.)  \nInformally, one can think of y as taking about %\\lambda/2% of the 0-bits of x (the exact number basically obeys a Poisson distribution) and flipping them to 1s. Observe that (x,y) always forms a combinatorial line. Thus lower bounds on %{\\Bbb E} f(x) S_\\lambda f(x)% imply lots of combinatorial lines in the support of f. The key observation is that the %S_\\lambda% are basically a semigroup (for %\\lambda% much smaller than n): %S_\\lambda S_{\\lambda'} \\approx S_{\\lambda+\\lambda'}%. In particular, if %\\lambda \\ll \\lambda'%, we have the absorption property %S_\\lambda S_{\\lambda'}, S_{\\lambda'} S_\\lambda \\approx S_{\\lambda'}%. Because of this, one can show that the %S_\\lambda% converge (in the strong operator topology) to an orthogonal projection P; to see this finitarily, one would inspect the energies %\\| S_\\lambda f \\|_2^2%, which are basically monotone decreasing in %\\lambda%, and locate a long gap in the energy spectrum in which these energies plateau in %\\lambda%. I think this gives us the same type of structure/randomness decomposition as before. Note that for large %\\lambda%, %S_\\lambda f% is approximately %S_1%-invariant, which is basically the same thing as having low influence. correction: (x,y) form a combinatorial line most of the time; there is a degenerate case when x=y (analogous to the case of a degenerate arithmetic progression). But for large lambda, this case is very rare and can be ignored."},{"username":"terence-tao","timestamp":"2009-02-27T05:46:00.000Z","contents":"correction: (x,y) form a combinatorial line most of the time; there is a degenerate case when x=y (analogous to the case of a degenerate arithmetic progression). But for large lambda, this case is very rare and can be ignored."},{"username":"gowers","timestamp":"2009-02-27T06:12:00.000Z","contents":"Ajtai-Szemerédi Just to say that the easy step of my three-step plan is now done: an informal write-up of the Ajtai-Szemerédi proof of the corners theorem can now be found [on this wiki page](http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemerédi%27s_proof_of_the_corners_theorem). I have to go to bed now so parts (ii) and (iii) will have to wait till tomorrow."},{"username":"gowersryan-odonnellgowersterence-taoterence-taoryan-odonnell","timestamp":"2009-02-27T18:44:00.000Z","contents":"Progress report Part (ii) is, perhaps predictably, giving me a bit more of a headache. But it’s an interesting headache in that I am stuck on something that feels as though it shouldn’t be too hard. I don’t know how much time I’ll have to think about it, so let me say what the problem is. Call a subset %\\mathcal{B}\\subset[3]^n% _simple_ if whether or not %x% belongs to %\\mathcal{B}% depends only on the 1-set of %x%. In set language, there is a set-system %\\mathcal{U}% such that %\\mathcal{B}=\\{(U,V,W):U\\in\\mathcal{U}\\}.% The question I’m struggling with is this. Let %\\mathcal{A}% be a dense subset of %[3]^n% that correlates with a dense simple set %\\mathcal{B}%. Does that imply that there is a combinatorial subspace with dimension tending to infinity on which %\\mathcal{A}% has a density increment? I cannot believe that this is a truly hard problem, but I haven’t yet managed to solve it. I think a solution would be very useful. Oh, and I really don’t mind what measure is used in any solution … 828.1: What if %\\mathcal{A} = \\mathcal{B} = % strings with an even number of 1’s? 828.2: Then take all sequences x such that %x_1=x_2,% %x_3=x_4,% etc., and you have a combinatorial subspace of dimension %n/2% where the density has gone all the way up to 1\\. Were you imagining that wildcard sets had to have size 1? That is certainly not intended to be a requirement in my question. 828.3 I can answer your question as “yes”, but in a totally unhelpful way: DHJ(3) implies that _any_ dense set has a density increment on a large combinatorial subspace. Of course, the whole point is to find a DHJ(3)-free proof of this fact… I don’t currently have time to think about it right now, but I might look at it later. 828.4 Hmm, actually thinking about it a little bit there may be an energy-increment type proof. Firstly, one should be able to show that a 1-set contains lots of combinatorial subspaces where it is denser, by concatenating the 2 and 3-sets together and applying DHJ(2). Next, one should be able to apply some sort of structure theorem to decompose an arbitrary set (or function) into a 1-set plus something orthogonal to 1-sets. Actually one needs to mess around with scales and get a decomposition into a 1-set %f_{U^\\perp}% at some large scale plus something %f_U% orthogonal to 1-sets at finer scales (in particular, it should have mean zero at finer scales). (Let me be vague about what “scale” means, but every time you pass to a large combinatorial subspace, you’re supposed to descend one scale.) Now take the guy %f_{U^\\perp}% which is a 1-set at a large scale and find lots of combinatorial subspaces in it at the finer scale where it is denser. The guy %f_U% which is orthogonal to 1-sets at finer scale should not disrupt this density increment. Have to run, unfortunately… may flesh this out later 828.5\\. Re 828.2: Ah, no, I guess I was misunderstanding what a combinatorial subspace is. I always think of this as, “fixing some of the coordinates and leaving the rest of them ‘free’.” It had not occurred to me that one is also allowed to insist on things like “x_i = x_j”, although I now see that there is no reason not to."},{"username":"ryan-odonnell","timestamp":"2009-02-27T21:26:00.000Z","contents":"828.1: What if %\\mathcal{A} = \\mathcal{B} = % strings with an even number of 1’s?"},{"username":"gowers","timestamp":"2009-02-27T21:50:00.000Z","contents":"828.2: Then take all sequences x such that %x_1=x_2,% %x_3=x_4,% etc., and you have a combinatorial subspace of dimension %n/2% where the density has gone all the way up to 1\\. Were you imagining that wildcard sets had to have size 1? That is certainly not intended to be a requirement in my question."},{"username":"terence-tao","timestamp":"2009-02-27T22:29:00.000Z","contents":"828.3 I can answer your question as “yes”, but in a totally unhelpful way: DHJ(3) implies that _any_ dense set has a density increment on a large combinatorial subspace. Of course, the whole point is to find a DHJ(3)-free proof of this fact… I don’t currently have time to think about it right now, but I might look at it later."},{"username":"terence-tao","timestamp":"2009-02-27T22:33:00.000Z","contents":"828.4 Hmm, actually thinking about it a little bit there may be an energy-increment type proof. Firstly, one should be able to show that a 1-set contains lots of combinatorial subspaces where it is denser, by concatenating the 2 and 3-sets together and applying DHJ(2). Next, one should be able to apply some sort of structure theorem to decompose an arbitrary set (or function) into a 1-set plus something orthogonal to 1-sets. Actually one needs to mess around with scales and get a decomposition into a 1-set %f_{U^\\perp}% at some large scale plus something %f_U% orthogonal to 1-sets at finer scales (in particular, it should have mean zero at finer scales). (Let me be vague about what “scale” means, but every time you pass to a large combinatorial subspace, you’re supposed to descend one scale.) Now take the guy %f_{U^\\perp}% which is a 1-set at a large scale and find lots of combinatorial subspaces in it at the finer scale where it is denser. The guy %f_U% which is orthogonal to 1-sets at finer scale should not disrupt this density increment. Have to run, unfortunately… may flesh this out later"},{"username":"ryan-odonnell","timestamp":"2009-02-27T22:52:00.000Z","contents":"828.5\\. Re 828.2: Ah, no, I guess I was misunderstanding what a combinatorial subspace is. I always think of this as, “fixing some of the coordinates and leaving the rest of them ‘free’.” It had not occurred to me that one is also allowed to insist on things like “x_i = x_j”, although I now see that there is no reason not to."},{"username":"gowers","timestamp":"2009-02-27T23:37:00.000Z","contents":"Ajtai-Szemerédi Let me try to explain the motivation for my simple-sets question. As I said earlier, I am trying to model a proof of DHJ(3) on the Ajtai-Szemerédi corners proof (an account of which is given [here on the wiki](http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemerédi%27s_proof_of_the_corners_theorem)). I have the following rough dictionary in mind. 1\\. A vertical line corresponds to a set of sequences (U,V,W) where you fix the 1-set U. 2\\. A horizontal line corresponds to a set of sequences (U,V,W) where you fix the 2-set V. 3\\. A line of slope -1, or diagonal, corresponds to a set of sequences (U,V,W) where you fix the 3-set W. 4\\. An arithmetic progression in [n] with length tending to infinity corresponds to a combinatorial subspace in %{}[2]^n% with dimension tending to infinity. 5\\. A subgrid with width tending to infinity corresponds to a combinatorial subspace of %{}[3]^n% with dimension tending to infinity. 6\\. A corner corresponds to a combinatorial line. That’s about it. So now let me say where the first problem arises when one tries to translate the AS proof to the DHJ(3) context. Obviously step 1 is fine: with any sensible model of a random combinatorial subspace you can assume that the density of A in that subspace is almost always at least %\\delta-\\eta%, where %\\delta% is the density of A. Step 2 says that if you have a positive density of vertical lines inside which A has density noticeably less than %\\delta%, then you also have a positive density of vertical lines inside which A has density noticeably _greater_ than %\\delta%. If we let %B% be the set of x-coordinates associated with these vertical lines, then we can use Szemerédi’s theorem to get an arithmetic progression P of these “over-full” vertical lines, with small common difference. Then we can partition the horizontal lines into arithmetic progressions %Q_i%, each of which is a translate of %P%, and by averaging we find a subgrid %P\\times Q_i% inside which A has density noticeably larger than %\\delta%, which gives us a density increment and we’re done. (Of course, then one has to go on and say what happens if we don’t have a positive density of sparse vertical lines — this is, as I say, just the first step of the argument.) Let me gloss over the question of which measure I want to use: I feel that that is just a technicality and the main point is to get the conceptual argument working. If you accept item 1 in the dictionary, then the obvious first step is to define U to be underfull if the density of pairs (V,W) (out of all pairs such that (U,V,W) is a point in %{}[3]^n%) is noticeably less than %\\delta%. Then if there is a positive density of underfull U, then we have a positive density of overfull U as well. Now I don’t have a problem finding a nice big combinatorial subspace consisting entirely of overfull Us (which is what item 4 suggests I should be looking for) but I don’t then have a nice analogue of the fact that a Cartesian product of two APs with the same common difference is a grid. Or rather, maybe I do, but I’m not sure how to get at it. The analogue should be something like that if I have such a combinatorial subspace (by which I mean a system of sets %U_0,U_1,\\dots,U_k% such that the elements of the system are all sets of the form %\\bigcup_{i\\in E}U_i% with 0 belonging to E), then on average the combinatorial subspaces that fix %U_0% and treat %U_1,\\dots,U_k% as wildcards should be slightly too dense. But for a fixed combinatorial subspace this doesn’t seem to have to be true, so I need somehow to build a combinatorial subspace such that it is true. I can feel this explanation getting less clear as I proceed, but perhaps it’s at least clear globally what I want to do: the “simple set” is the set of all (U,V,W) such that U is overfull. So trivially A intersects it too densely. If we can get a density increment on a subgrid, then we’re done, which means that Step 3 is complete and we may assume that virtually no U are underfull. (Given the difficulties associated with this step, it’s not altogether clear that this statement is strong enough, but it would be encouraging. I might assume it and see what happens if I try to press on with the later steps.)"},{"username":"ryan-odonnell","timestamp":"2009-02-28T00:50:00.000Z","contents":"Combinatorial subspaces; 828.4 vs. 828.2. It seems to me that an argument along the lines in 828.4 at some point has to find combinatorial subspaces which include these %x_i = x_j% constraints, as in Tim’s 828.2\\. If the combinatorial subspaces are just of the “fixing-some-coordinates” type then I’m not sure (yet) how to get around 828.1\\. But will arguments as in 828.4 actually produce such subspaces?"},{"username":"gowers","timestamp":"2009-02-28T01:14:00.000Z","contents":"Finding combinatorial subspaces Ryan, that’s an interesting point. The kind of arguments I had in mind are different though: they are inductive ones where you build a subspace up dimension by dimension, using Sperner’s theorem (or similar results) at each stage. A prototype for such an argument is the proof I sketched on the wiki of the [basic multidimensional Sperner theorem](http://michaelnielsen.org/polymath1/index.php?title=Sperner%27s_theorem). This argument leads naturally to larger wildcard sets."},{"username":"randallterence-taorandallterence-tao","timestamp":"2009-02-28T03:28:00.000Z","contents":"1-sets I was trying to think about Tim’s question from the ergodic perspective; if I am translating correctly the outlook is not encouraging. Perhaps though I am overgeneralizing. A 1-set is just a set, membership in which is impervious to switchings of 0s with 2s, right? Okay, so, if I understand Terry’s translations from the ergodic world and back again, the functions that are measurable with respect to 1-sets would be in some sense analogous to the $\\tau\\rho^{-1}$ rigid factor…if that’s right, then in order to think about the question ergodically, one should think whether it would help at all to know that 1_A had non-zero projection onto this factor. And I don’t see how it would. In particular, it isn’t the case that things orthogonal to this factor don’t contribute to the averages you’re interested in, so I don’t see how decomposing would help. Yes, it is easy to see from DHJ (2) what the positive contribution would be restricted to this rigid factor, but you lose positivity of the relative AP components; it would not seem an easy task to show that they have non-negative contribution without positivity, especially given that the argument as it now stands uses positively very heavily (it throws a lot away because it can). On the other hand, a non-zero projection onto this factor is sufficient to get (what Terry would call, I think) both 01 over 02 and 12 over 02 almost periodic components, and in a special way, so it might in theory be easier to get a density increment for these guys in the combinatorial world, but at least from an ergodic perspective, I can’t see any strategy that might simplify the proof from what it already is. Short, that is, of a detailed analysis of what is going on in terms of finite-dimensional modules instead of “almost periodicity”, but that is potentially a step in the direction of harder, not easier. Of course, all I say depends on this analogy between 1-sets and the 02-rigid factor being what I suggest it is, and I don’t really see that part well at all (I’m mostly just guessing because I don’t understand the picture very well from the combinatorial side). (Also of course someone may be posting a proof simultaneously with my posting my doubts.) 832.1 Yeah, a 1-set is what I would call a 02-low influence (in fact, a 02-zero influence) set, and it would correspond to a tau rho^{-1} invariant set in the ergodic theory language. Tim’s approach actually deals with intersections of 1-sets and 2-sets (which we have been calling “complexity 1 sets”), which would be a combination of a 02-low influence and 01-low influence set, or a set in the factor generated by both the tau rho^{-1}-invariants and the sigma rho^{-1}-invariants. (One may also want to throw in the 12-invariant sets for good measure.) But never mind this distinction for now. Perhaps the place where Tim’s approach and the ergodic approach differ, though, is that Tim can always stop the argument as soon as a density increment is found on a large subspace, whereas this trick is not available in the ergodic world. So I don’t think Tim needs the strong (and presumably false) claim that “functions orthogonal to complexity 1 sets have negligible impact on the combinatorial line count”. He only needs the weaker claim that “if a set has no combinatorial lines, then it must have non-trivial correlation with a complexity 1 set”, combined with the (as yet not fully established) claim that “a set with non-trivial correlation with a complexity 1 set has a density increment on a large combinatorial subspace”. Hmm. Maybe one thing to put on my “to do” list is to find an ergodic translation of Tim’s argument on the wiki that line-free sets correlate with complexity 1 sets. (If you haven’t already noticed, I’m a big fan of translating back and forth between these two languages.) 832.2 Actually I just got on here again to retract everything I said (I should think more carefully before posting). I now think, from the ergodic perspective, you trivially get a “measure increment” if 1_A correlates projects non-trivially onto the 02-rigid factor, for the reason that on fibers, the measure is not constant. Just pick a bunch of fibers on which the measure is too big and then a subspace on which you have intersection of those fibers (DJH (2)). 832.2 Hmm, I tried my hand at translating Tim’s argument to ergodic theory, and it came out weird… in order to describe it, I have to leave the measure space X and work in a certain extension of it, vaguely analogous to the Host-Kra parallelopiped spaces %X^{[k]}% (or the Furstenberg self-joinings used in Tim Austin’s proof of convergence for multiple commuting shifts). It’s something like this: consider the space of all quadruples %(x, \\sigma(\\alpha) \\rho^{-1}(\\alpha) x, \\sigma(\\alpha \\beta) \\rho^{-1}(\\alpha \\beta) x, \\sigma(\\alpha) \\tau(\\beta) \\rho^{-1}(\\alpha \\beta) x)% in %X^4%, where %\\alpha% is a word to the left of %\\beta% (or maybe to the right; I may have the signs messed up). There should somehow be a natural measure %\\tilde \\mu% associated to these quadruples, which I currently do not know how to define properly; each of the four projections of this measure to X should be the original measure %\\mu%. The last three points of this quadruple form a combinatorial line (I may have messed up the order of operations a bit). So if A is line-free, the function %1_A(x_4)% on %X^4% will have strong negative correlation with %1_A(x_2) 1_A(x_3)%. But the function %1_A(x_2)% has a “02-invariance property” in the sense that it is insensitive to changes to %\\beta%, while the function %1_A(x_3)% has a certain “01-invariance property” in the sense that it is insensitive to interchanges between %\\alpha% and %\\beta%. So A is correlating with a combination of a 02-invariant set and a 01-invariant set, but the correlation only takes place in the extension %(X^4, \\tilde \\mu)% of %(X,\\mu)% and the invariance seems to be a bit fancier than just %\\tau \\rho^{-1}%-invariance or %\\sigma \\rho^{-1}%-invariance."},{"username":"terence-tao","timestamp":"2009-02-28T03:55:00.000Z","contents":"832.1 Yeah, a 1-set is what I would call a 02-low influence (in fact, a 02-zero influence) set, and it would correspond to a tau rho^{-1} invariant set in the ergodic theory language. Tim’s approach actually deals with intersections of 1-sets and 2-sets (which we have been calling “complexity 1 sets”), which would be a combination of a 02-low influence and 01-low influence set, or a set in the factor generated by both the tau rho^{-1}-invariants and the sigma rho^{-1}-invariants. (One may also want to throw in the 12-invariant sets for good measure.) But never mind this distinction for now. Perhaps the place where Tim’s approach and the ergodic approach differ, though, is that Tim can always stop the argument as soon as a density increment is found on a large subspace, whereas this trick is not available in the ergodic world. So I don’t think Tim needs the strong (and presumably false) claim that “functions orthogonal to complexity 1 sets have negligible impact on the combinatorial line count”. He only needs the weaker claim that “if a set has no combinatorial lines, then it must have non-trivial correlation with a complexity 1 set”, combined with the (as yet not fully established) claim that “a set with non-trivial correlation with a complexity 1 set has a density increment on a large combinatorial subspace”. Hmm. Maybe one thing to put on my “to do” list is to find an ergodic translation of Tim’s argument on the wiki that line-free sets correlate with complexity 1 sets. (If you haven’t already noticed, I’m a big fan of translating back and forth between these two languages.)"},{"username":"randall","timestamp":"2009-02-28T04:06:00.000Z","contents":"832.2 Actually I just got on here again to retract everything I said (I should think more carefully before posting). I now think, from the ergodic perspective, you trivially get a “measure increment” if 1_A correlates projects non-trivially onto the 02-rigid factor, for the reason that on fibers, the measure is not constant. Just pick a bunch of fibers on which the measure is too big and then a subspace on which you have intersection of those fibers (DJH (2))."},{"username":"terence-tao","timestamp":"2009-02-28T04:39:00.000Z","contents":"832.2 Hmm, I tried my hand at translating Tim’s argument to ergodic theory, and it came out weird… in order to describe it, I have to leave the measure space X and work in a certain extension of it, vaguely analogous to the Host-Kra parallelopiped spaces %X^{[k]}% (or the Furstenberg self-joinings used in Tim Austin’s proof of convergence for multiple commuting shifts). It’s something like this: consider the space of all quadruples %(x, \\sigma(\\alpha) \\rho^{-1}(\\alpha) x, \\sigma(\\alpha \\beta) \\rho^{-1}(\\alpha \\beta) x, \\sigma(\\alpha) \\tau(\\beta) \\rho^{-1}(\\alpha \\beta) x)% in %X^4%, where %\\alpha% is a word to the left of %\\beta% (or maybe to the right; I may have the signs messed up). There should somehow be a natural measure %\\tilde \\mu% associated to these quadruples, which I currently do not know how to define properly; each of the four projections of this measure to X should be the original measure %\\mu%. The last three points of this quadruple form a combinatorial line (I may have messed up the order of operations a bit). So if A is line-free, the function %1_A(x_4)% on %X^4% will have strong negative correlation with %1_A(x_2) 1_A(x_3)%. But the function %1_A(x_2)% has a “02-invariance property” in the sense that it is insensitive to changes to %\\beta%, while the function %1_A(x_3)% has a certain “01-invariance property” in the sense that it is insensitive to interchanges between %\\alpha% and %\\beta%. So A is correlating with a combination of a 02-invariant set and a 01-invariant set, but the correlation only takes place in the extension %(X^4, \\tilde \\mu)% of %(X,\\mu)% and the invariance seems to be a bit fancier than just %\\tau \\rho^{-1}%-invariance or %\\sigma \\rho^{-1}%-invariance."},{"username":"ryan-odonnellterence-taojason-dyerjason-dyerjason-dyerryan-odonnell","timestamp":"2009-02-28T03:36:00.000Z","contents":"Following up on 830, perhaps Randall or Terry would be able to answer: Is there an analogue in the ergodic proof of restrictions to these subspaces with more than one “type” of wildcard? (In other words, subspaces with these “%x_i = x_j%” constraints?) In my limited understanding of the ergodic proofs, the operators used therein seemed to be more aligned with just the simpler combinatorial subspaces. Ryan, I’m not sure I understand your question. A k-dimensional combinatorial subspace is described by a word with k wildcards, but each wildcard can be used more than once. For instance, xxyyzz21 would describe the three-dimensional subspace of %[3]^8% in which %x_1=x_2, x_3=x_4, x_5=x_6, x_7=2, x_8=1%. A combinatorial line is the case when k=1, so we have one wild card x, which can be used many times over. Are you meaning like how Moser’s has a wildcard that goes 1-2-3 while the other goes 3-2-1? So, generalizing, there would be six types of wildcards? (123, 132, 213, 312, 231, 321 — obviously certain combinations would be equivalent to Moser) To clarify further, any pair of distinct wildcards from those six (123, 132, 213, 312, 231, 321) will fix one element and permute the other two, so they are all equivalent to Moser. However, we could take all the way up to six different wildcards, resulting in lines I believe have never been named. For example, taking x = 123, y = 321 and z= 213, then a line formed with 11xyz would be 11132, 11221, 11313. Argh. There are pairs that don’t fix any of the elements, (123-231 for example) which would also be something different than Moser’s. 832.x. Re Terry’s reply: Right, what I’m saying is, in my limited experience, I hadn’t noticed any ergodic-inspired finitary arguments that involved combinatorial subspaces with more than one type of wildcard."},{"username":"terence-tao","timestamp":"2009-02-28T03:59:00.000Z","contents":"Ryan, I’m not sure I understand your question. A k-dimensional combinatorial subspace is described by a word with k wildcards, but each wildcard can be used more than once. For instance, xxyyzz21 would describe the three-dimensional subspace of %[3]^8% in which %x_1=x_2, x_3=x_4, x_5=x_6, x_7=2, x_8=1%. A combinatorial line is the case when k=1, so we have one wild card x, which can be used many times over."},{"username":"jason-dyer","timestamp":"2009-02-28T04:06:00.000Z","contents":"Are you meaning like how Moser’s has a wildcard that goes 1-2-3 while the other goes 3-2-1? So, generalizing, there would be six types of wildcards? (123, 132, 213, 312, 231, 321 — obviously certain combinations would be equivalent to Moser)"},{"username":"jason-dyer","timestamp":"2009-02-28T04:26:00.000Z","contents":"To clarify further, any pair of distinct wildcards from those six (123, 132, 213, 312, 231, 321) will fix one element and permute the other two, so they are all equivalent to Moser. However, we could take all the way up to six different wildcards, resulting in lines I believe have never been named. For example, taking x = 123, y = 321 and z= 213, then a line formed with 11xyz would be 11132, 11221, 11313."},{"username":"jason-dyer","timestamp":"2009-02-28T04:29:00.000Z","contents":"Argh. There are pairs that don’t fix any of the elements, (123-231 for example) which would also be something different than Moser’s."},{"username":"ryan-odonnell","timestamp":"2009-03-01T02:34:00.000Z","contents":"832.x. Re Terry’s reply: Right, what I’m saying is, in my limited experience, I hadn’t noticed any ergodic-inspired finitary arguments that involved combinatorial subspaces with more than one type of wildcard."},{"username":"gowers","timestamp":"2009-02-28T06:40:00.000Z","contents":"Fourier and Sperner On a slightly different topic, I still haven’t given up hope of a “right” Fourier proof of Sperner’s theorem. On the wiki I have now written up [an argument that almost gets there](http://michaelnielsen.org/polymath1/index.php?title=A_second_Fourier_decomposition_related_to_Sperner%27s_theorem). The good news is that it gives a Fourier-type decomposition with the property that a suitable equal-slice-weighted sum of f(A)f(B) over all pairs %A\\subset B% transforms into an expression where the contribution from every Fourier mode (when this claim is suitably interpreted) is positive. The bad news is that so far the only way I have of proving this positivity, which boils down to proving that a certain explicitly defined integral kernel is positive definite, is using an inverse Fourier argument, so I can’t quite claim that the proof is a Fourier proof of Sperner. But now that I know that this kernel is positive definite, I think it will be just a matter of time before one of us comes up with a direct proof, and then there will be a nice clean Fourier proof of Sperner (or at least, I think it would deserve to be called that)."},{"username":"terence-tao","timestamp":"2009-02-28T10:07:00.000Z","contents":"Density increment I think I can now prove Tim’s claim in 828, namely that a set that correlates with a 1-set has a density increment on a large dimensional subspace. First observe, by the greedy algorithm and DHJ(2), that if n is large enough depending on m and %\\varepsilon%, then any dense subset A of %[2]^n% can be partitioned into m-dimensional subspaces, plus a residual set of density at most %\\varepsilon%. Indeed one just keeps using DHJ(2) to locate and then delete m-dimensional subspaces from the set. Similarly, the complement of A can also be partitioned into m-dimensional spaces plus a residual set of density at most density at most %\\varepsilon%. Now, one should be able to pull this statement to %[3]^n% and show that if A is a dense 1-set of %[3]^n%, then one can partition both A and its complement into m-dimensional subspaces, plus a residual of small density. I’m not sure what “density” should mean here; equal-slices may be a better bet than uniform at this point. Now, if B is a dense set which correlates with a dense 1-set A, then by picking a moderately large m, picking %\\varepsilon% tiny compared to 1/m, and then n really large, we should be able to use the above partition and the pigeonhole principle to get a density increment of B on one of these m-dimensional subspaces."},{"username":"gowersgowersjason-dyergowersrandallterence-taoterence-taojason-dyer","timestamp":"2009-02-28T14:35:00.000Z","contents":"Density increment Terry, that’s similar to things I was trying, and I’m wondering whether it will run into similar difficulties. The point where I get stuck is where you say, “Now, one should be able to pull this statement to %{}[3]^n%.” How might one do this? Well, given any combinatorial subspace in %{}[2]^n% you can easily define an associated combinatorial subspace in %{}[3]^n% by simply allowing the wildcards to take value 2 as well. (For convenience I am taking %{}[2]=\\{0,1\\}% and %{}[3]=\\{0,1,2\\}.)% And the result will be a collection of disjoint subspaces. But they won’t anything like partition %{}[3]^n% because so far all the fixed coordinates are 0 or 1\\. We can correct that by allowing the fixed coordinates that used to be 0 to be either 0 or 2, but I think that doesn’t get us out of the woods. What one really needs to answer is the following question: given a sequence %x% in %{}[3]^n%, to what combinatorial subspace does it belong? Here’s an example. Suppose %n=5% and you are presented with %x=110022.% And suppose that one of the combinatorial subspaces you took in %{}[2]^n% was %V=aabcbc%. Then you’d be forced to put %x% into %V%, since its 1-set is contained in %V%. But if you now allow the wildcards to take the value 2 as well, you can’t get %x%. 835.1 Maybe one could use a variant of the argument where having chosen the first wildcard set you then inductively cover evenly everything else. Sorry — that’s a bit vague but I have no time to clarify it just yet. In the above example, having chosen aa you might e.g. then partition into things like bcbc, bbcc, bccb, cbbb, etc. Except that even that wouldn’t be right because once you’d chosen your second wildcard set you’d play the same game with the third, fourth, etc. Sorry, that’s probably still completely unclear. I haven’t had time to test this yet, but would it help to triplicate the original set into a 9-uniform hypergraph (with subgraphs A, B, and C) such that subgraph A uses 0 or 2, B uses 0 or 1, and C uses 1 or 2? Then somehow before the last step everything would need to be recombined into the original graph. 835.3 Jason, I’m not sure I follow what you are saying here. Hopefully this issue will be resolved in a simpler way soon anyway, but could the “measure increment” observation I made in my last post be fashioned into a proof? In the regular F-K conversion they only cared about combinatorial lines, but presumably one could arrange that for any set I of words, if the intersection over I of T_w^{-1}C is non-empty, then you see a subspace-homomorphic image of I in the original set A you were dealing with. You could do this for 2 sets simultaneously, a 1-set A and a correlated set B. I am guessing that A would wind up associated with a set in your measure space that would be measurable with respect to the \\tau\\rho^{-1} rigid factor, and that B would wind up associated with a set that had non-zero projection onto that factor, and you would then wrap up the proof as indicated above. Of course you’d have to convert this to a combinatorial proof, which presumably Terry is good at (energy incrementation?) But again, one hopes it’s not that hard, or that something already suggested can be made to work…. Huh, so the pullback of a combinatorial subspace of %[2]^n% to %[3]^n% is not a combinatorial subspace. That’s a bit weird. I agree that my previous comment doesn’t quite work as stated then. However, it does seem that the pullback of a combinatorial subspace of %[2]^n% is an “average” of combinatorial subspaces of %[3]^n% (or what Gil would call a “fractional cover”). Basically, there is a random way to convert a 2-dimensional subspace e.g. 01aa01bbb10ccc to a 3-dimensional subspace by randomly converting 0s to 0s or 2s, and a wildcard such as a to a, 0, or 2\\. One has to carefully select the probability measures here (in particular, to make choices between equal slices and uniform) to make everything work, but perhaps it can be pulled off. (One may also want to ensure that each wildcard appears a lot of times so that when one pulls back, one (with very high probability) doesn’t accidentally erase all copies of any given wildcard.) As Gil observed, if you can fractionally cover most of a set A and its complement by large combinatorial subspaces, and B has a large correlation with A, then B has a density increment on one of these subspaces. No, that’s rubbish, I take it back; the pullback of, say, aa, is the set {00, 11, 12, 21, 22}, and I can’t cover that space by lines. Strange. 835.5 I was meaning based on “allowing the fixed coordinates that used to be 0 to be either 0 or 2” you have two more subspaces where: coordinates that used to be a 0 are either 1 or 2, and coordinates that used to be a 0 are a 1  \nand  \ncoordinates that used to be a 0 are either 0 or 1, and coordinates that used to be a 1 are a 2. However, I have not been able to get anything useful out of this."},{"username":"gowers","timestamp":"2009-02-28T16:37:00.000Z","contents":"835.1 Maybe one could use a variant of the argument where having chosen the first wildcard set you then inductively cover evenly everything else. Sorry — that’s a bit vague but I have no time to clarify it just yet. In the above example, having chosen aa you might e.g. then partition into things like bcbc, bbcc, bccb, cbbb, etc. Except that even that wouldn’t be right because once you’d chosen your second wildcard set you’d play the same game with the third, fourth, etc. Sorry, that’s probably still completely unclear."},{"username":"jason-dyer","timestamp":"2009-02-28T21:23:00.000Z","contents":"I haven’t had time to test this yet, but would it help to triplicate the original set into a 9-uniform hypergraph (with subgraphs A, B, and C) such that subgraph A uses 0 or 2, B uses 0 or 1, and C uses 1 or 2? Then somehow before the last step everything would need to be recombined into the original graph."},{"username":"gowers","timestamp":"2009-02-28T22:04:00.000Z","contents":"835.3 Jason, I’m not sure I follow what you are saying here."},{"username":"randall","timestamp":"2009-02-28T22:24:00.000Z","contents":"Hopefully this issue will be resolved in a simpler way soon anyway, but could the “measure increment” observation I made in my last post be fashioned into a proof? In the regular F-K conversion they only cared about combinatorial lines, but presumably one could arrange that for any set I of words, if the intersection over I of T_w^{-1}C is non-empty, then you see a subspace-homomorphic image of I in the original set A you were dealing with. You could do this for 2 sets simultaneously, a 1-set A and a correlated set B. I am guessing that A would wind up associated with a set in your measure space that would be measurable with respect to the \\tau\\rho^{-1} rigid factor, and that B would wind up associated with a set that had non-zero projection onto that factor, and you would then wrap up the proof as indicated above. Of course you’d have to convert this to a combinatorial proof, which presumably Terry is good at (energy incrementation?) But again, one hopes it’s not that hard, or that something already suggested can be made to work…."},{"username":"terence-tao","timestamp":"2009-03-01T00:30:00.000Z","contents":"Huh, so the pullback of a combinatorial subspace of %[2]^n% to %[3]^n% is not a combinatorial subspace. That’s a bit weird. I agree that my previous comment doesn’t quite work as stated then. However, it does seem that the pullback of a combinatorial subspace of %[2]^n% is an “average” of combinatorial subspaces of %[3]^n% (or what Gil would call a “fractional cover”). Basically, there is a random way to convert a 2-dimensional subspace e.g. 01aa01bbb10ccc to a 3-dimensional subspace by randomly converting 0s to 0s or 2s, and a wildcard such as a to a, 0, or 2\\. One has to carefully select the probability measures here (in particular, to make choices between equal slices and uniform) to make everything work, but perhaps it can be pulled off. (One may also want to ensure that each wildcard appears a lot of times so that when one pulls back, one (with very high probability) doesn’t accidentally erase all copies of any given wildcard.) As Gil observed, if you can fractionally cover most of a set A and its complement by large combinatorial subspaces, and B has a large correlation with A, then B has a density increment on one of these subspaces."},{"username":"terence-tao","timestamp":"2009-03-01T00:42:00.000Z","contents":"No, that’s rubbish, I take it back; the pullback of, say, aa, is the set {00, 11, 12, 21, 22}, and I can’t cover that space by lines. Strange."},{"username":"jason-dyer","timestamp":"2009-03-01T00:44:00.000Z","contents":"835.5 I was meaning based on “allowing the fixed coordinates that used to be 0 to be either 0 or 2” you have two more subspaces where: coordinates that used to be a 0 are either 1 or 2, and coordinates that used to be a 0 are a 1  \nand  \ncoordinates that used to be a 0 are either 0 or 1, and coordinates that used to be a 1 are a 2. However, I have not been able to get anything useful out of this."},{"username":"gowersgowers","timestamp":"2009-02-28T22:24:00.000Z","contents":"Density increment I’m going to return to full polymath mode and think online rather than offline, concentrating on the problem of getting a density increment on a subspace if you already have one on a simple set. (Recall that I am defining a simple set to be a set %\\mathcal{A}% of sequences x of the form %\\{(U,V,W):U\\in\\mathcal{U}\\}.)% Since I’ll be talking about %{}[2]^n% quite a bit, it will be natural to take %{}[3]=\\{0,1,2\\}% and %{}[2]=\\{0,1\\}.% Set theorists would no doubt approve. I am aiming for an inductive construction. That is, I want to fix some coordinates and choose a wildcard set in such a way that I am not dead. To say that more precisely, the assumption we are given to start with is that for every %U\\in\\mathcal{U}% the density of points %(U,V,W)\\in\\mathcal{A}% is at least %\\delta+\\eta%. I would now like to find a disjoint quadruple %(U_1,V_1,W_1,X_1)% that does _not_ partition %{}[n]%, or even come close to doing so, and I would like the following to be the case. Let %Z_1% be the complement of %U_1\\cup V_1\\cup W_1\\cup X_1%. Then there are many %U\\subset Z_1% such that the density of %(V,W)% with %(U_1\\cup X_1\\cup U,V_1\\cup V,W_1\\cup W)\\in\\mathcal{A}% is at least %\\delta+\\eta% (or perhaps very slightly less), and the same is true, with the same %U%, if we move %X_1% over to %V_1\\cup V% or %W_1\\cup W%. I’ve got to go in a moment, but it occurs to me that one might be able to get away with less. I’ve tried to choose a 1-dimensional subspace %S_1% such that for many %U% the density is good for _every_ assignment of the wildcards. But perhaps it’s enough just to get the _average_ density good when we set the wildcards, and perhaps that’s easier to prove. Incidentally, here’s a problem that ought to count as a first exercise. To make sense of it, we need to go for a functional version. So let’s suppose that %f:[3]^n\\rightarrow[0,1]% is a function of mean %\\delta% and suppose that there is a dense set system %\\mathcal{U}% such that for every %U\\in\\mathcal{U}% the average of %f(U,V,W)% over all %(V,W)% that make a point with %U% is at least %\\delta+\\eta%. Can we find a combinatorial line on which %f% averages at least %\\delta+\\eta'% (where %\\eta'% is %99\\eta/100,% say)? Of course, we can do it by quoting DHJ(3) but is there an elementary proof? That’s all I’ve got time for for now. 836.1 Just to be clear, to make sense of that last problem, one should either use equal-slices measure or ask for %\\mathcal{U}% to be dense in the probability measure where each element of a set is chosen independently with probability %1/3%."},{"username":"gowers","timestamp":"2009-02-28T22:48:00.000Z","contents":"836.1 Just to be clear, to make sense of that last problem, one should either use equal-slices measure or ask for %\\mathcal{U}% to be dense in the probability measure where each element of a set is chosen independently with probability %1/3%."},{"username":"terence-taoterence-taoterence-taoterence-tao","timestamp":"2009-03-01T00:51:00.000Z","contents":"Density increment Randall, I think the problem we’re seeing in the combinatorial world (that the pullback of a %[2]^n%-combinatorial subspace is not a %[3]^n%-combinatorial subspace) is reflected in the ergodic world that the %\\tau \\rho^{-1}% IP-system does not commute with the %\\sigma \\rho^{-1}% IP-system, and so DHJ(2) for the %\\tau \\rho^{-1}%-invariant factor does not seem to quite give us what we want (applying %\\sigma \\rho^{-1}(w)% to a bunch of %\\tau \\rho^{-1}%-fibers does not give another bunch of %\\tau \\rho^{-1}%-fibers). Ahh, but there’s some sort of “asymptotic commutativity” if one separates the 01 interchange and 02 interchange operations sufficiently. Let me think about this… 837.2 OK, I think I can make Randall’s argument work. In order to exploit asymptotic commutativity, I first need to weaken the notion of a 1-set to a “local 1-set”, which is a 1-set on large slices of %[3]^n%; more precisely, A is a local 1-set if there exists a small set %I \\subset [n]% of “bad” coordinates such that whenever one fixes those coordinates (thus reducing the n-dimensional cube to an %n-|I|%-dimensional cube), the slice of A is a 1-set. (Equivalently, A is insensitive to 0-2 interchanges on coordinates outside of I, but could be extremely sensitive to such changes within I). Every global 1-set is a local 1-set, but not conversely. Suppose a set B has a density increment with a global 1-set A of non-trivial size. Then by a greedy algorithm, we can find a local 1-set A’ of non-trivial size on which B has a “near-maximal density increment” with B in the sense that any other local 1-set A” (with perhaps slightly more bad coordinates than A’, and not too much smaller than A’) does not have a significantly higher B-density than A’ did. (There will be a lot of parameter juggling to sort out to quantify everything here; I will ignore this issue here.) OK, let’s look at the local 1-set A’, which has a small number of bad coordinates I’. Let m be a medium sized number. Pick m random small wildcard sets %I_1, \\ldots, I_m% (which with high probability will be disjoint from each other, and from I’). If we then pick a word w at random from %[3]^n%, then by DHJ(2.5) and the local 1-set nature of A’, with positive probability, the combinatorial space %V(w)% of %2^m% elements formed by taking w and overwriting each of %I_1, \\ldots, I_m% with 0s, 1s, or 2s, will lie in A’. Call the set of w that do this _good_. The key point is that the set A” of good w is itself a local 1-set outside of the bad coordinates %I'' := I' \\cup I_1 \\cup \\ldots \\cup I_m%. And so by hypothesis, B enjoys essentially the same density increment on A” that it did on A’. But on the other hand, A” is the union of (parallel) m-dimensional combinatorial subspaces, and so we get an density increment on one of these spaces. small correction: V(m) should of course have %3^m% elements, not %2^m%. (DHJ(2.5) places %2^m% of the elements of %V(m)% inside A’, and the local 1-set nature of A’ then automatically extends this membership of A’ to the rest of %V(m)%.)"},{"username":"terence-tao","timestamp":"2009-03-01T00:52:00.000Z","contents":"Ahh, but there’s some sort of “asymptotic commutativity” if one separates the 01 interchange and 02 interchange operations sufficiently. Let me think about this…"},{"username":"terence-tao","timestamp":"2009-03-01T01:10:00.000Z","contents":"837.2 OK, I think I can make Randall’s argument work. In order to exploit asymptotic commutativity, I first need to weaken the notion of a 1-set to a “local 1-set”, which is a 1-set on large slices of %[3]^n%; more precisely, A is a local 1-set if there exists a small set %I \\subset [n]% of “bad” coordinates such that whenever one fixes those coordinates (thus reducing the n-dimensional cube to an %n-|I|%-dimensional cube), the slice of A is a 1-set. (Equivalently, A is insensitive to 0-2 interchanges on coordinates outside of I, but could be extremely sensitive to such changes within I). Every global 1-set is a local 1-set, but not conversely. Suppose a set B has a density increment with a global 1-set A of non-trivial size. Then by a greedy algorithm, we can find a local 1-set A’ of non-trivial size on which B has a “near-maximal density increment” with B in the sense that any other local 1-set A” (with perhaps slightly more bad coordinates than A’, and not too much smaller than A’) does not have a significantly higher B-density than A’ did. (There will be a lot of parameter juggling to sort out to quantify everything here; I will ignore this issue here.) OK, let’s look at the local 1-set A’, which has a small number of bad coordinates I’. Let m be a medium sized number. Pick m random small wildcard sets %I_1, \\ldots, I_m% (which with high probability will be disjoint from each other, and from I’). If we then pick a word w at random from %[3]^n%, then by DHJ(2.5) and the local 1-set nature of A’, with positive probability, the combinatorial space %V(w)% of %2^m% elements formed by taking w and overwriting each of %I_1, \\ldots, I_m% with 0s, 1s, or 2s, will lie in A’. Call the set of w that do this _good_. The key point is that the set A” of good w is itself a local 1-set outside of the bad coordinates %I'' := I' \\cup I_1 \\cup \\ldots \\cup I_m%. And so by hypothesis, B enjoys essentially the same density increment on A” that it did on A’. But on the other hand, A” is the union of (parallel) m-dimensional combinatorial subspaces, and so we get an density increment on one of these spaces."},{"username":"terence-tao","timestamp":"2009-03-01T01:18:00.000Z","contents":"small correction: V(m) should of course have %3^m% elements, not %2^m%. (DHJ(2.5) places %2^m% of the elements of %V(m)% inside A’, and the local 1-set nature of A’ then automatically extends this membership of A’ to the rest of %V(m)%.)"},{"username":"ryan-odonnellryan-odonnellterence-tao","timestamp":"2009-03-01T03:48:00.000Z","contents":"Structure Theorems; specifically, question for Terry re #826: Terry, could you clarify on your statement therein, “the energies %\\| S_\\lambda f \\|_2^2%… are basically monotone decreasing in %\\lambda%“? Suppose %f : \\{0,1\\}^n \\to \\mathbb{R}% is of the form %f(x) = h(\\sum_{i=1}^n x_i)%, where %h : \\mathbb{R} \\to \\mathbb{R}% is a nonnegative increasing function. Doesn’t it seem as though %\\| S_\\lambda f \\|_2^2% will be _increasing_ in %\\lambda%? 838.1 The nonparsing formula there is %f : \\{0,1\\}^n \\to \\mathbb{R}%. _Now fixed — Tim_ 838.2 Ryan, I guess one should specify that f should be bounded within [-1,1], and “basically” means “up to o(1) errors, and with %\\lambda% small compared with %\\sqrt{n}%“. In your example h should also be bounded between [-1,1]. In that case, flipping %\\lambda% 0’s to 1’s will only have an effect of increasing f by %O(\\lambda/n) = o(1)% or so on average. The reason for the monotone decrease is because of the absorption formula %S_{\\lambda'} S_{\\lambda} f \\approx S_{\\lambda+\\lambda} f%. Since %S_{\\lambda'}% is basically a contraction for %\\lambda' = o(\\sqrt{n})% (though it does cease to contract for large %\\lambda'%, of course), we see that %\\| S_{\\lambda+\\lambda'} f \\|_{L^2} \\leq \\| S_{\\lambda} f \\|_{L^2} + o(1)%. I guess the “local” picture (small %\\lambda%) is looking a bit different from the “global” picture. Locally, the operation of flipping 0s to 1s is a measure-preserving operation; globally, of course, it isn’t. (Tragedy of the commons!)"},{"username":"ryan-odonnell","timestamp":"2009-03-01T03:49:00.000Z","contents":"838.1 The nonparsing formula there is %f : \\{0,1\\}^n \\to \\mathbb{R}%. _Now fixed — Tim_"},{"username":"terence-tao","timestamp":"2009-03-01T04:03:00.000Z","contents":"838.2 Ryan, I guess one should specify that f should be bounded within [-1,1], and “basically” means “up to o(1) errors, and with %\\lambda% small compared with %\\sqrt{n}%“. In your example h should also be bounded between [-1,1]. In that case, flipping %\\lambda% 0’s to 1’s will only have an effect of increasing f by %O(\\lambda/n) = o(1)% or so on average. The reason for the monotone decrease is because of the absorption formula %S_{\\lambda'} S_{\\lambda} f \\approx S_{\\lambda+\\lambda} f%. Since %S_{\\lambda'}% is basically a contraction for %\\lambda' = o(\\sqrt{n})% (though it does cease to contract for large %\\lambda'%, of course), we see that %\\| S_{\\lambda+\\lambda'} f \\|_{L^2} \\leq \\| S_{\\lambda} f \\|_{L^2} + o(1)%. I guess the “local” picture (small %\\lambda%) is looking a bit different from the “global” picture. Locally, the operation of flipping 0s to 1s is a measure-preserving operation; globally, of course, it isn’t. (Tragedy of the commons!)"},{"username":"ryan-odonnell","timestamp":"2009-03-01T04:04:00.000Z","contents":"Fourier proof of Sperner. I semi-checked to myself that one can prove Sperner by increment-free, purely Fourier arguments. One uses:  \n. the structure theorem (#821).  \n. Mossel’s “Lemma 6.2” to handle the %\\langle f_{\\text{sens}}, f_{\\text{stab}} \\rangle% and %\\langle f_{\\text{sens}}, f_{\\text{sens}} \\rangle% parts.  \n. the triangle inequality argument (#815) to handle the %\\langle f_{\\text{stab}}, f_{\\text{stab}} \\rangle% parts. However, this argument is still quite unsatisfactory to me. For one, it requires %\\delta \\gg 1/\\sqrt{\\log \\log n}%. For another, it requires selecting the parameter called %\\epsilon% in #800 (equivalently, %\\lambda/n% in Terry’s #826) _after_ using the Structure Theorem. And most “wrongly”, this parameter must be %\\Theta_\\delta(1)/n%. In particular, the following theorem is true (I’m 99% sure, at least; proof by “pushing shadows around with Kruskal-Katona”) — however, I don’t think we have _any_ Fourier-based proof at all: Theorem: Let %f : \\{0,1\\}^n \\to \\{0,1\\}% have density %\\delta > 0%. Fix %p = 1/2 - \\epsilon%, %q = 1/2 + \\epsilon%, where %\\epsilon% is, say, %1/n^{.75}%. (Or set %p = 1/2% if you like.) Then %\\mathbb{E}[f(X_{p,q})f(Y_{p,q})] > 0%, where I’m using the notation from [Tim’s wiki entry](http://michaelnielsen.org/polymath1/index.php?title=A_second_Fourier_decomposition_related_to_Sperner%27s_theorem)."},{"username":"terence-tao","timestamp":"2009-03-01T04:06:00.000Z","contents":"Metacomment. One side effect of the threaded system is that we hit 100 comments long before the number assigned to the comments increases by 100; we’re at 838 now but the thread is already longer than most of the other threads. Perhaps we may wish to renew this thread well before 899 (e.g. at 850)?"},{"username":"ryan-odonnellgowers","timestamp":"2009-03-01T04:23:00.000Z","contents":"Sperner. This is very tiny comment. I just wanted to point out that one can prove the “correct” density-Sperner Theorem under the uniform distribution, in the same way Tim proves it under the equal-slices distribution. Assume %\\mathbb{E}[f] = \\delta% under the uniform distribution. Pick a random chain %C% from (0, 0, …, 0) up to (1, 1, …, 1) and then choose %u, v \\sim% Binomial%(n, 1/2)%, independently. Define %x = C(u)%, the %u%th string in the chain, and %y = C(v)%. Then %\\mathbb{E}[f(x)f(y)] = \\mathbb{E}_C[\\mathbb{E}_u[f(C(u))]^2] \\geq \\mathbb{E}_{C,u}[f(C(u))]^2 = \\delta^2%, as %x = C(u)% is uniformly distributed. So we have a distribution on pairs of strings %(x,y)% such that %x% and %y% both have the uniform distribution, and such that they form a nondegenerate “line” with probability at least %\\delta^2 - \\Theta(1/\\sqrt{n})%. PS: If someone wants to express %\\mathbb{E}[f(x)f(y)]% here in terms of Fourier coefficients, I’d be very happy to see it. 840.1 Ryan, I’m not sure how tiny that comment is, and must think about it. Maybe it will turn out that putting the binomial distribution on maximal chains is always more convenient than putting equal-slices measure on the cube. And clearly this same trick works for %{}[3]^n%. This feels to me like a potentially useful further technique to add to our armoury, and when I get the chance I think I’ll add something about it to the wiki. I agree that the Fourier calculation looks more or less compulsory. And something that’s rather nice about this measure is that it combines the uniform distribution on %{}[2]^n% with a very natural non-uniform distribution on combinatorial lines, so it provides a potential answer to a question we were trying to answer way back in the Varnavides thread. Indeed, another exercise we should do is prove a Varnavides-type version of DHJ(3) for this measure on the combinatorial lines (given DHJ(3) itself). Oh no wait a moment — it’s not obvious how to generalize to DHJ(3) because we don’t have an analogue of maximal chains. This is something else to think about."},{"username":"gowers","timestamp":"2009-03-01T16:56:00.000Z","contents":"840.1 Ryan, I’m not sure how tiny that comment is, and must think about it. Maybe it will turn out that putting the binomial distribution on maximal chains is always more convenient than putting equal-slices measure on the cube. And clearly this same trick works for %{}[3]^n%. This feels to me like a potentially useful further technique to add to our armoury, and when I get the chance I think I’ll add something about it to the wiki. I agree that the Fourier calculation looks more or less compulsory. And something that’s rather nice about this measure is that it combines the uniform distribution on %{}[2]^n% with a very natural non-uniform distribution on combinatorial lines, so it provides a potential answer to a question we were trying to answer way back in the Varnavides thread. Indeed, another exercise we should do is prove a Varnavides-type version of DHJ(3) for this measure on the combinatorial lines (given DHJ(3) itself). Oh no wait a moment — it’s not obvious how to generalize to DHJ(3) because we don’t have an analogue of maximal chains. This is something else to think about."},{"username":"gil-kalai","timestamp":"2009-03-01T12:00:00.000Z","contents":"A problem where some of the techniques developed here are potentially useful. Let %f% be a monotone Boolean function. Let %\\psi_k(f)=\\int I_k^p(f)% be the integral of the influence of the k-th variable w.r.t. the probability measure %\\mu_p%. (%\\psi_k(f)% is called the “Shapley value” of %k%.) Let %\\epsilon% be a fixed small real number and let T be the difference between p-q where p is the value for which the probability that %f% is 1 is %1-\\epsilon% and q is the value for which the probability that %f% is 1 is %\\epsilon%. (T is called the length of the “threshold interval for %f%.) It is known that if all %\\psi_k(f) < \\delta% then T is small as well. The known bound is %T < 1/C \\log \\log (1/\\delta)%. The proof relies on connecting the influences for different of %\\mu_p%s. It looks that one %\\log% could be eliminated and that a more careful understanding of the relation between influences and other properties of %f% for different values of %p% may be useful."},{"username":"gowers","timestamp":"2009-03-01T17:00:00.000Z","contents":"Metacomments. 1\\. Terry I agree that with threading we need shorter threads, if you’ll excuse the dual use of the word “thread”. Let’s indeed go for 850\\. 2\\. The other thing is that I’ve been meaning to say for ages that I loved your notion that the wiki could be thought of as an online journal of density Hales-Jewett studies, and the discussions as weekly conferences."},{"username":"gowers","timestamp":"2009-03-01T17:28:00.000Z","contents":"General remark I have a very non-mathematically busy day today, so I’ve got time for just one brief post. First, let me say that I’m quite excited by Terry’s 837.2\\. I haven’t fully digested it yet but I have partially digested it enough to see that it is very definitely of the sort of flavour I was hoping for. It’s frustrating not to be able to spend a few hours today playing around with the argument. And then tomorrow and Tuesday I have two heavy teaching days. So instead, let me throw out a small thought/question. Recall that in the dictionary I gave in 829, long arithmetic progressions go to combinatorial subspaces in %{}[2]^n%. Therefore, if the general plan of modelling an argument on Ajtai-Szemerédi works, it will have to give a new proof of the corners theorem that is similar to their proof but avoids the use of Szemerédi’s theorem. As I’ve already explained, that sounds discouraging at first, but I think it may in fact not be. But I’ve subsequently realized that these thoughts lead inexorably to a suggestion that Jozsef made, way back in [comment 2](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1648) (!), that one should look at a generalization of the corners problem in which you don’t have long APs in the two sets of which you take the Cartesian product. Maybe the time is ripe for having a go at that problem, and in particular seeing whether the Ajtai-Szemerédi approach can be pushed through for it."},{"username":"ryan-odonnell","timestamp":"2009-03-01T22:25:00.000Z","contents":"Measures: One reason I said the comment in #840 is tiny is that I’m finally starting to catch up the fact (well known to the rest of you, I think) that if you don’t mind density increment arguments (and we certainly don’t!) then the underlying probability measure is quite unimportant. Or rather, you can pass freely between any “reasonable” measures, using the arguments sketched [here](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure) on the wiki. In fact, I think “equal-slices” is a good “base measure” to always return to. It has the nice feature that if you have equal-slice density %\\delta%, by density-increment arguments you can assume that you have density very nearly %\\delta% under _almost all_ product measures."},{"username":"ryan-odonnellterence-taoryan-odonnell","timestamp":"2009-03-01T22:29:00.000Z","contents":"Density increment / the problem from #828: What if %\\mathcal{A} = \\mathcal{B} = % a random simple set in %[3]^n% of density %\\delta%, meaning that its %\\mathcal{U}% is a random set of density %\\delta% under the %1/3%-biased distribution? How should we try to find a combinatorial subspace to increment on? (Perhaps we shouldn’t; perhaps instead we should exploit the fact that this %\\mathcal{A}% is extremely 23-insensitive.) Ryan, that’s right; being 20-insensitive, we can invoke DHJ(2.5) and get large subspaces in %{\\mathcal A}={\\mathcal B}%, which of course is a healthy density increment. 844.2\\. Ah, sorry if you said this already in an earlier comment, Terry; I’m still catching up."},{"username":"terence-tao","timestamp":"2009-03-01T22:58:00.000Z","contents":"Ryan, that’s right; being 20-insensitive, we can invoke DHJ(2.5) and get large subspaces in %{\\mathcal A}={\\mathcal B}%, which of course is a healthy density increment."},{"username":"ryan-odonnell","timestamp":"2009-03-02T00:36:00.000Z","contents":"844.2\\. Ah, sorry if you said this already in an earlier comment, Terry; I’m still catching up."},{"username":"anonymous","timestamp":"2009-03-02T00:06:00.000Z","contents":"Density increment in cubes Tim, Re:842\\. In case you are considering corners in cubes, let me mention a few things here. Reading some recent blogs, I thought that you are close to prove a density increment result which would imply the corner result. I write the statement first and then I will argue that it proves a Moser type result.  \nGiven an %IP_n% set (or an n-dimensional Hilbert cube if you wish) with %2^n% elements. Consider the elements as vertices of a graph. The graph is c-dense, i.e. the number of edges is %c2^{2n}%. The graph contains a huge hole, there is an %\\alpha 2^n% empty subgraph. Prove that then it contains a d-dimensional subcube on the vertices spanning at least %(c+\\delta)2^{2d}% edges. %\\delta >0% might depend on c and %\\alpha% but not on n, and d grows with n (arbitrary slowly). Now I will try to post the next part as a new thread."},{"username":"jozsef","timestamp":"2009-03-02T00:11:00.000Z","contents":"845.1 oops, I didn’t give my name for the previous note"},{"username":"jozsef","timestamp":"2009-03-02T00:26:00.000Z","contents":"A Moser type theorem 845.2 … and apparently I don’t know how to generate a thread.  \nStill going backwards, let me state the theorem which would follow from 845: Every dense subset of %[3]^n% contains three elements, formed by taking a string with one or more wildcards %\\ast^1% and %\\ast^2% in it and replacing the first type wildcards by 1,2 and 3, and the second type wildcards by 4,3, and 2, respectively. For example 123412,  \n223312, and 323212 is such triple."},{"username":"jozsef","timestamp":"2009-03-02T00:29:00.000Z","contents":"846.1\\. In the previous statement we were in %[4]^n%, not in %[3]^n%."},{"username":"jozsef","timestamp":"2009-03-02T00:56:00.000Z","contents":"contd. With Ron Graham we proved a colouring version of 846\\. For any %\\log n% colouring of %[4]^n% there is a monochromatic triple like in 846\\. The paper is available at  \n[http://www.math.ucsd.edu/~sbutler/ron/pre_cube.pdf](http://www.math.ucsd.edu/~sbutler/ron/pre_cube.pdf)  \nThe reasonable bound for the colouring version shows that this problem might be easier to work with than with DHJ k=3\\. In the next post I will try to prove that 845 implies 846, that is, an “arithmetic density increment” in a graph with a huge hole implies a Moser type statement.  \n(Sorry about the number of posts, but I don’t want to write long ones as even the shorter ones are full with typos)"},{"username":"jozsef","timestamp":"2009-03-02T02:29:00.000Z","contents":"(part one) Here we show that a density increment argument implies the Moser-type theorem in 846\\. It isn’t new that a density increment would give us what we are looking for, the new feature is that it might be easier to prove density increment in a graph with a huge empty subgraph. For practical reasons we switch to a bipartite settings. Consider a bipartite graph G(A.B), between two copies of the n-dimensional cube, %A=B=\\{0,1\\}^n%. G(A,B) has %2^{n+1}% vertices and, say, %c2^{2n}% edges. There is a natural one to one mapping between the set of edges and a pointset of %[4]^n%. The two vertices of an edge are 0-1 sequences. From the two sequences we get one as follows. The i-th position of the new sequence (0,1,2, or 3) is given by %x + 2^y% where x is the number (0 or 1) in the i-th position of the vertex in A and y is the number in the i-th position of the vertex in B. We say that two edges span the same cube if substituting all 1-s and 2-s by a wildcard, *, we get the same sequence. Observe that if (v1,v2) and (w1,w2), two edges of G(A,B), span the same cube and (v1,w2) is in G(A,B), then we are done, there are three points formed by taking a string with one or more wildcards %\\ast^1% and %\\ast^2% in it and replacing the first type wildcards by 1,2 and 3, and the second type wildcards by 4,3, and 2, respectively. (For example %\\ast^1%23%\\ast^2%12 gives 123412, 223312, and 323212.) Equivalently, it will give a corner on the Cartesian product of an n-dimensional Hilbert cube. I will continue, but I would like to see if the typing is correct so far or not."},{"username":"jozsef","timestamp":"2009-03-02T03:41:00.000Z","contents":"(part two) We said that two edges span the same cube if substituting all 1-s and 2-s by a wildcard we get the same 0,3,* sequence. Similarly we can say that every edge, e, spans a particular subcube; substitute all 1-s and 2-s with wildcards to get a 0,3,* sequence. Any edge, which differs from e in the * positions only, is in the subcube spanned by e. Our moderate goal here is to show that if there are no three edges of the form (v1,v2), (w1,w2), and (v1,w2) in G(A,B) that (v1,v2) and (w1,w2) span the same subcube – in which case we were done – then there is a subcube spanned by many edges containing many other edges (see in post 845). The vertices of the spanning edges provide us the large empty subgraph.  \nWe will follow a simple algorithm. At first consider the set of edges spanning the whole cube. Those are edges without 0 or 3\\. If there are at least %\\alpha n% such edges, then stop. If there are less than %\\alpha n% such edges, then select the densest n-1 dimensional subcube. (Such subcubes are spanned by edges having exactly one 0 and no 3-s and by edges with one 3 and no 0-s.) If the densest subcube is spanned by at least %\\alpha (n-1)% edges then stop, otherwise select the densest n-2 dimensional subcube. If we repeat the algorithm and %\\alpha << c%, then it should terminate in a c-dense d-dimensional subcube spanned by at least %\\alpha 2^d% edges. By the initial assumption there is no edge connecting the end-vertices of the spanning edges. It gives us the huge empty bipartite graph, %K_{\\alpha n/2, \\alpha n/2}%. I didn’t do the actual calculations, but it seems correct to me."},{"username":"gowers","timestamp":"2009-03-02T05:18:00.000Z","contents":"Density increment Terry, I’ve tried to understand your argument properly but there are a few points where I’ve had difficulty working out what you mean. I think they all stem from one main point, which comes in the last paragraph where you say “by hypothesis”. I’m not sure I know what the hypothesis is. You established that it was impossible to get a substantial density _increase_ on a local 1-set, but here you seem to need that you can’t have a substantial density _decrease_. Have I misunderstood something? I hope so. (I’m actually writing this offline in my car so by the time I get to post it perhaps this matter will have been cleared up.)"},{"username":"gowers","timestamp":"2009-03-02T05:19:00.000Z","contents":"Different measures (also written earlier but offline) I want to think some more about Ryan’s observation that the permutations trick can be used with the uniform measure. In particular, I want to understand from a non-permutations point of view what the resulting measure is on pairs %A\\subset B%. So let’s fix two sets %A\\subset B% of sizes %r% and %s%, respectively. The probability that a random permutation gives both %A% and %B% as initial segments is %\\binom ns^{-1}\\binom sr^{-1}% and the probability that we choose those two initial segments (if we binomially choose a random %r% and a random %s% without conditioning on their order) is %2^{-2n}\\binom nr\\binom ns.% So the probability that we choose the sets %A% and %B% is %2^{-2n}\\binom nr\\binom sr^{-1},% which equals %2^{-2n}\\frac{n!(s-r)!}{s!(n-r)!},% which equals  \n%2^{-2n}\\binom ns \\binom {n-r}s^{-1}.% Therefore, the probability of choosing %B% conditional on having chosen %A% is %2^{-n}\\binom ns \\binom {n-r}s^{-1}.% If %A% and %B% were independent, this probability would of course be %2^{-n}%, so the extra weight is … hmm, I’m not really getting anywhere here."},{"username":"gowers","timestamp":"2009-03-02T05:20:00.000Z","contents":"Different measures (again written earlier) How about the Fourier expansion? Suppose we choose %U% and %V% according to this distribution. That is, we choose a random permutation, and then choose initial segments %r% and %s% independently with binomial probabilities on %r% and %s%. What is the expected value of %w_A(U)w_B(V)%? To make the calculation friendlier, I’m going to work out something else that I hope will turn out to be the same. It’s more like Ryan’s p-q approach. To pick a random point I’ll pick a number %m%, binomially distributed with parameters %n% and 1/2, and I’ll then set %p=m/n% and choose a point with the measure %\\mu_p%. Damn, that doesn’t work, because the probability that I end up choosing the empty set is strictly greater than %2^{-n}% (because there’s a chance I’ll pick it if %p% takes a typical value round 1/2, and also a probability %2^{-n}% of taking %p=0%). OK, it’s believable that any average of binomial distributions that ends up with mean %n/2% is guaranteed to be less concentrated than the binomial distribution itself, so probably this idea was doomed. OK I don’t at the moment see a conceptual argument for calculating the Fourier expression. Perhaps it’s a question of a brute-force calculation that one hopes will cancel a lot. But it would be disappointing if the expectation of %w_A(U)w_B(V)% above were not zero when %A\\ne B%."},{"username":"gowers","timestamp":"2009-03-02T05:22:00.000Z","contents":"Metacomment: I’m about to start a new thread, which will start at 851\\. So perhaps further comments on this thread could be limited to a few small replies."},{"username":"terence-taoterence-taogowers","timestamp":"2009-03-02T07:03:00.000Z","contents":"Density increment I’ll start off (perhaps a bit awkwardly) by responding to Tim’s query from 848, which was asking about how a hypothesis that a density increment is near-maximal implies that there are also no significant density _decrements_. This is basically just coming from subtraction. Let A be a set, and suppose that B is a structured set (specifically, a local 1-set of non-trivial size) on which one has a near-maximal density %\\delta_B := |A \\cap B|/|B|%. Then any structured subset B’ of B of non-trivial size also has to have density close to %\\delta_B%. It can’t have a much bigger density, since otherwise we could pass from B to B’ and get a density increment; and it can’t have a much smaller density, since otherwise we could pass from B to %B \\backslash B'% and also get a density increment. There are three parameters that will need to be juggled here to make it all work: the density of A in B, the density of B in %[3]^n%, and the number of bad coordinates in B. The last parameter should not cause a difficulty; as n is so large, we can afford to turn quite a lot of coordinates bad and still retain a lot of usable structure. So it should boil down to making sure that the greedy algorithm that locates B terminates in a bounded amount of time without sending the density of B to zero. The way I think of it is like this: if B contains a small subset B’ on which A has too low of a density, then one can remove B’ from B to improve the density without significantly shrinking the size of B. I don’t think this process can be iterated indefinitely, as the density will eventually shoot up to 1. 851.1 In ergodic theory land, what we are doing here is stating the obvious fact that if a function f has mean %\\delta% and is non-constant, then there is a set B of non-trivial measure on which f is uniformly bounded below by %\\delta+c% for some c>0\\. Here, f is the “conditional expectation of %1_A% to the sigma-algebra generated by local 1-sets”. The problem in the finitary world is that the local 1-sets don’t actually form a sigma-algebra and so f only exists in some “virtual” sense rather than a tangible one, and so one has to do various greedy algorithm contortions to simulate any ergodic theory computation involving f. 851.2 Terry, what worries me in your explanation above is the “of non-trivial size”. When you apply the result, you apply it to the set %A''% of all good words. Now it appears to me that the density of this set goes to zero as m goes to infinity (and we need m to go to infinity in order to carry on with the density-increment argument), and therefore I don’t see how … Ah, I think I do see what you are saying now, and I have in fact used this kind of argument myself sometimes. The point (which you yourself have already said) is that if you remove only a small set then you don’t greatly change the size of B. But this can work only if your structured sets are closed under subtraction. So for instance if you had a maximal density on a subspace, you couldn’t conclude that you had near maximal density on small subspaces of that subspace. But if you remove a local 1-set from a local 1-set you end up with a local 1-set so it’s OK."},{"username":"terence-tao","timestamp":"2009-03-02T07:06:00.000Z","contents":"851.1 In ergodic theory land, what we are doing here is stating the obvious fact that if a function f has mean %\\delta% and is non-constant, then there is a set B of non-trivial measure on which f is uniformly bounded below by %\\delta+c% for some c>0\\. Here, f is the “conditional expectation of %1_A% to the sigma-algebra generated by local 1-sets”. The problem in the finitary world is that the local 1-sets don’t actually form a sigma-algebra and so f only exists in some “virtual” sense rather than a tangible one, and so one has to do various greedy algorithm contortions to simulate any ergodic theory computation involving f."},{"username":"gowers","timestamp":"2009-03-02T14:04:00.000Z","contents":"851.2 Terry, what worries me in your explanation above is the “of non-trivial size”. When you apply the result, you apply it to the set %A''% of all good words. Now it appears to me that the density of this set goes to zero as m goes to infinity (and we need m to go to infinity in order to carry on with the density-increment argument), and therefore I don’t see how … Ah, I think I do see what you are saying now, and I have in fact used this kind of argument myself sometimes. The point (which you yourself have already said) is that if you remove only a small set then you don’t greatly change the size of B. But this can work only if your structured sets are closed under subtraction. So for instance if you had a maximal density on a subspace, you couldn’t conclude that you had near maximal density on small subspaces of that subspace. But if you remove a local 1-set from a local 1-set you end up with a local 1-set so it’s OK."},{"username":"ryan-odonnell","timestamp":"2009-03-02T09:08:00.000Z","contents":"Different measures. Tim, re your #849, I also found that binomial probability distribution on %A \\subseteq B%‘s surprisingly… “awkward” to analyze. I suppose one can just really grind things out and see if, say, the Fourier expansion turns out nicely after all."},{"username":"terence-tao","timestamp":"2009-03-02T11:17:00.000Z","contents":"Metacomment. As I had some idle moments, I decided to spin out a timeline of the story so far for polymath1: [http://michaelnielsen.org/polymath1/index.php?title=Timeline](http://michaelnielsen.org/polymath1/index.php?title=Timeline) It’s remarkably nonlinear, but that’s how maths research goes, I guess. Also: this post needs a “polymath1” tag."},{"username":"terence-tao","timestamp":"2009-03-02T13:05:00.000Z","contents":"Density increment I just realised that the “density increment” trick underlying Randall’s “dense fibres” argument that I translated into a combinatorial argument is not really a density increment, but rather a “mass” increment argument, essentially the same as that used to prove the classical [Hahn decomposition](http://en.wikipedia.org/wiki/Hahn_decomposition_theorem) in measure theory (which, by coincidence, I [taught in my class](http://terrytao.wordpress.com/2009/01/04/245b-notes-1-signed-measures-and-the-radon-nikodym-lebesgue-theorem/) a few weeks ago). Let’s say %A \\subset [3]^n% has density %\\delta%, and let %f = 1_A - \\delta% be the balanced function. Let %{\\mathcal B}% be a family of “structured sets” (such as local 1-sets); this family is secretly supposed to be behaving like a sigma-algebra, but let us try to avoid using that fact. Suppose we have a large set B in %{\\mathcal B}% on which A has a density increment, and more specifically we have %{\\Bbb E}_{x \\in [3]^n} f(x) 1_B(x) \\geq c% for some c>0; note that this hypothesis forces B to be somewhat dense. We now want to restrict B to the fibres of %{\\mathcal B}% on which f is positive. One way to do this is to try to maximise the “mass” %{\\Bbb E}_{x \\in [3]^n} (f(x)-c/2) 1_{B'}(x)% as %B'% ranges over elements of %{\\mathcal B}%; note that we are not dividing by the density of B’, so this is measures a mass increment rather than a density increment. Setting B’=B we know that the mass can be positive. If we find a B’ that maximises the mass, we know that any non-trivial structured subset B” of B’ will have an A-density significantly larger than %\\delta% (e.g. larger than %\\delta+c/4%), since otherwise we could remove this “negative mass” portion B” from B’ and increase the mass of B’ substantially. (Not coincidentally, this is _exactly_ how one proves the Hahn decomposition theorem.)"},{"username":"gowers","timestamp":"2009-03-02T15:09:00.000Z","contents":"Ajtai-Szemerédi First a general remark to add further motivation to the Ajtai-Szemerédi approach. Let’s briefly think about why it has never been extended to prove a result for three-dimensional corners. If you try to do it, you soon see that what you want in place of the applications of Szemerédi’s theorem is applications of the full 2-dimensional Szemerédi theorem, and it seems that the original Ajtai-Szemerédi argument is not powerful enough to do that (not that I’ve thought hard about why). But if we managed to push through to a proof of DHJ(3), it seems not outrageous to hope that we might be able to get a multidimensional version, and that that might be what was needed for DHJ(4), and so on. That’s all a bit pie-in-the-sky at the moment, but that’s all it’s meant to be. Assuming that the Randall/Terry argument is fine (as I do, but for my own benefit I hope to write it up on the wiki soon), where have we actually got to? Here is my assessment, which gets a bit vague in the details. What it definitely gives us is what seems (and that’s a pretty confident “seems”) to be the appropriate analogue of step 2 of the Ajtai-Szemerédi argument [as it is presented on the wiki](http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemerédi%27s_proof_of_the_corners_theorem). That tells us that for almost every %U% we have pretty much the expected density of %(V,W)% such that %(U,V,W)\\in\\mathcal{A}%. Let us call such a %U% good. The obvious analogue of Step 3 is then this: we can find %W% with a positive density of pairs %(U,V)% such that %U% is good and %(U,V,W)\\in\\mathcal{A}.% An unthinking analogue of the fourth step would simply be that amongst these pairs %(U,V)% can be found a combinatorial subspace of dimension tending to infinity. Given the experience with Step 2, I think it pretty unlikely that this will be good enough, but let’s try to understand why not. Fifth step: in the Ajtai-Szemerédi argument they have a long AP of points on the diagonal, which gives rise to a long AP P of vertical lines. They then partition the horizontal lines into long APs with the same common difference as P, using the fact that an AP of vertical lines intersects an AP of horizontal lines with the same common difference in a grid. What could the analogue of this be in the subspace world? Well, an AP of vertical lines translates to taking a combinatorial subspace of Us and then extending each U in all possible ways to a point (U,V,W) in %{}[3]^n%. And when would a structure like that intersect a similar one for Vs in a combinatorial subspace? If we denote by %\\langle Z_0,Z_1,\\dots,Z_m\\rangle% the combinatorial subspace that consists of all finite unions of the %Z_i% that include %Z_0%, and we write %U(Z_0,\\dots,Z_m)% for the set of all %(U,V,W)% with %U\\in\\langle Z_0,Z_1,\\dots,Z_m\\rangle%, then the answer is that %U(Y_0,Y_1,\\dots,Y_m)% intersects %V(Z_0,Z_1,\\dots,Z_m)% in a combinatorial subspace if %Y_0% and %Z_0% are disjoint, and %Y_i=Z_i% for %i=1,2,\\dots,m.% This is almost saying is that the combinatorial subspaces are translates of each other, but there are two additional complications. First, the sets that determine the translates (that is, %Y_0% and %Z_0%) have to be disjoint. Secondly, we cannot partition %{}[2]^n% with translates of a given subspace because all points in all translates will be constant on the wildcard sets, whereas a general point definitely isn’t. So there’s a lot of sorting out to do if an analogue of Step 5 is to be made to work. But what’s encouraging is that these difficulties are rather similar in flavour to the difficulties that we’ve already encountered and dealt with in Step 2."},{"username":"gowersterence-tao","timestamp":"2009-03-02T15:16:00.000Z","contents":"Metacomment. A quick remark about teaching. Terry comments that he has just taught the Hahn decomposition theorem, and implies (I think, reading between the lines) that this has helped him to have certain thoughts more quickly and easily than he would otherwise have had them. So that prompts me to say that I am lecturing probability this term, and noticed when I was thinking about Fourier analysis and equal-slices measure how it made a definite difference to be completely 100% on top of things like expectations of products of independent random variables, conditional probability, etc. etc. The odd thing in that case was that the concepts were quite basic, but nevertheless there was a gain: what was gained was some kind of clarity and fluency that replaced the normal process of muddling through first and tidying up afterwards. Well, when I taught Hahn decomposition a few weeks ago, I thought to myself “Ah, another density/mass increment argument! These arguments are everywhere!”. But other than reinforcing my (already quite strong) belief that energy/density/mass increment arguments are an essential tool in analysis, I don’t think it directly helped me except in the _a posteriori_ sense, when my attempts to explain Randall’s argument more coherently begin to ring a bell and I consciously went back to look at the proof of Hahn decomposition. But perhaps there was indeed some unconscious “priming” due to the fact that I have been spending the last few weeks revisiting the foundations of soft analysis to teach to the UCLA freshman graduate students. (Certainly, in order to convert ergodic theory arguments to finitary ones, one does have to think really hard about deconstructing extremely basic notions in measure theory and functional analysis – for instance, deconstructing the construction of conditional expectation, or the Riesz representation theorem, or the approximation of measurable sets by open sets, or the fact that every bounded monotone sequence converges, are essential first exercises before one can do anything even remotely nontrivial, e.g. finitising the mean ergodic theorem.)"},{"username":"terence-tao","timestamp":"2009-03-03T02:08:00.000Z","contents":"Well, when I taught Hahn decomposition a few weeks ago, I thought to myself “Ah, another density/mass increment argument! These arguments are everywhere!”. But other than reinforcing my (already quite strong) belief that energy/density/mass increment arguments are an essential tool in analysis, I don’t think it directly helped me except in the _a posteriori_ sense, when my attempts to explain Randall’s argument more coherently begin to ring a bell and I consciously went back to look at the proof of Hahn decomposition. But perhaps there was indeed some unconscious “priming” due to the fact that I have been spending the last few weeks revisiting the foundations of soft analysis to teach to the UCLA freshman graduate students. (Certainly, in order to convert ergodic theory arguments to finitary ones, one does have to think really hard about deconstructing extremely basic notions in measure theory and functional analysis – for instance, deconstructing the construction of conditional expectation, or the Riesz representation theorem, or the approximation of measurable sets by open sets, or the fact that every bounded monotone sequence converges, are essential first exercises before one can do anything even remotely nontrivial, e.g. finitising the mean ergodic theorem.)"},{"username":"gowersryan-odonnellrandall","timestamp":"2009-03-02T23:33:00.000Z","contents":"Density increment. I’m trying to write up the Randall/Terry argument on the wiki, but it’s very slow going as I keep getting stuck. And each time I get stuck it takes me ages to decide whether I’ve uncovered a genuine difficulty or simply not got the point. And I’m in that position yet again. Here’s my problem this time. Suppose I’ve passed to a local 1-set A’ where B has maximal density. So we’ve got some coordinates belonging to an index set %I%, and membership of A’ depends just on what happens inside %I% and what the 1-set of a sequence is outside %I%. (I’m using “1-set” in two senses here.) Let’s call this an %(I,1)%-set. Now I’m very happy with the idea that if we pass to a sub-%(I,1)%-set, then we can assume that %B% has near-maximal density there too: if it hasn’t, then we just subtract off the subset. But that’s not what is going on in the argument here. Here we define a %(I\\cup I_1\\cup\\dots\\cup I_m)%-set, and it’s much less obvious to me that we can subtract it off, since if we do, then we are not left with a %(I,1)%-set. Could we use the fact that we have a %(I_1\\cup\\dots\\cup I_m)%-set after we’ve done the subtraction? I don’t see how, because the amount of measure we’ve subtracted is tiny (since the probability that a word w is good goes to zero as m goes to infinity) and the size of the set of bad coordinates has gone up a lot. I suppose what I’m really saying is that I’d be very grateful to see this argument written out in full detail. (Another problem, but I think I can solve this one, is finding a suitable measure on the set of %I_1,\\dots,I_m% to make it true that words are good with positive probability.) 855.2\\. Hmm, perhaps our #855’s are coming to a similar difficulty, to be discussed further. I just wanted to say in this short reply that in your parenthesis at the end seems to match the parenthesis in the second paragraph of what I wrote below. I don’t quite know how to solve this issue, but I do like to remember that the Gunderson-Rodl-Sidorenko paper Jozsef pointed us to shows that whenever you have a subset of %\\{0,1\\}^n% of positive density, it must completely contain an %\\Omega(\\log \\log n)%-dimensional subspace. (This is a quantification of the “multidimensional Sperner” argument on the wiki, I think.) Terry will answer this far more fully at some point but in the meantime I will say this much: once m is fixed, the positive probability that V(w) lies in A’ is bounded below by something like \\delta^{2^m} and the size of the \\alpha_i can be safely bounded above by some finite amount…."},{"username":"ryan-odonnell","timestamp":"2009-03-02T23:48:00.000Z","contents":"855.2\\. Hmm, perhaps our #855’s are coming to a similar difficulty, to be discussed further. I just wanted to say in this short reply that in your parenthesis at the end seems to match the parenthesis in the second paragraph of what I wrote below. I don’t quite know how to solve this issue, but I do like to remember that the Gunderson-Rodl-Sidorenko paper Jozsef pointed us to shows that whenever you have a subset of %\\{0,1\\}^n% of positive density, it must completely contain an %\\Omega(\\log \\log n)%-dimensional subspace. (This is a quantification of the “multidimensional Sperner” argument on the wiki, I think.)"},{"username":"randall","timestamp":"2009-03-03T01:48:00.000Z","contents":"Terry will answer this far more fully at some point but in the meantime I will say this much: once m is fixed, the positive probability that V(w) lies in A’ is bounded below by something like \\delta^{2^m} and the size of the \\alpha_i can be safely bounded above by some finite amount…."},{"username":"ryan-odonnellryan-odonnell","timestamp":"2009-03-02T23:42:00.000Z","contents":"I wonder if I could ask for some clarification on the Randall/Terry argument; specifically, on #837.2. In particular, I’m trying to follow the last paragraph. (Actually, I don’t quite understand the appeal to DHJ(2.5) in the previous paragraph either, but I think I understand my own way around that.) Could we clarify precisely what the definition of %A''% should be? If it’s defined exactly as written in #837.2, then I wouldn’t so much say it’s the union of parallel %m%-dimensional subspaces so much as I would say it’s union of %(|I_1|+\\cdots+|I_m|)%-dimensional subcubes. Which would be fine, but this doesn’t seem to square with the idea that %A'' \\subseteq A'% as discussed in #851. On the other hand, perhaps we shouldn’t take %A''% to be _all_ good %w%, but just those %w% such that the part in %I_1 \\cup \\cdots \\cup I_m% is actually _in_ the large-dimensional subspace %V(m)%. But my worry here is that in general (if the “wildcards sets” are large) that this %A''% will constitute only a negligible (%o_n(1)%) fraction of %A'%, so it should be impossible to conclude that %B% has a density increment on %A''%. 856.2\\. Oops, sorry for the simultaneous post with Tim’s #855; please consider what I wrote #856\\. _[Changed — Tim]_"},{"username":"ryan-odonnell","timestamp":"2009-03-02T23:43:00.000Z","contents":"856.2\\. Oops, sorry for the simultaneous post with Tim’s #855; please consider what I wrote #856\\. _[Changed — Tim]_"},{"username":"gowers","timestamp":"2009-03-03T00:43:00.000Z","contents":"Varnavides version of multidimensional Sperner Ryan, it was amusing to see just how many similarities there were between our near-simultaneous comments. While we’re waiting for a response, here’s roughly how I expected to fill in the details with the appeal to DHJ(2.5). The obvious worry, which you have expressed before in a different context, is that you have to ensure that the wildcard sets can take plenty of different sizes: it’s not enough to go for “easy” combinatorial subspaces where they all have size 1, since %\\mathcal{U}% could be the set of all sets of even size. The thought I had was that one could define the measure in a way that more or less guaranteed that the proof worked. You would first find an M such that every subset A of density %\\delta/2% in %{}[3]^M% contained a subspace of dimension m where you got an element of A whenever you fixed the wildcards to be 1 or 2\\. Then you would choose your random wildcard sets %I_1,\\dots,I_m% as follows. First choose a random “easy” combinatorial subspace of dimension M, with ground set F, and then pick, uniformly at random, m disjoint non-empty subsets of F. I’m not 100% sure that works but it feels as though a double-counting argument should now do the trick."},{"username":"gowers","timestamp":"2009-03-03T01:35:00.000Z","contents":"Density increment I’m going to try to approach this question in small stages, proving little lemmas without being sure they are useful — though I will try to explain why I think they should be. Suppose that one tries to build up a subspace inductively on which %\\mathcal{A}% has a density increment. The problem one faces is this. We can assume that for every %U\\in\\mathcal{U}% there are too many %(V,W)% such that %(U,V,W)\\in\\mathcal{A}%, and we can even find a combinatorial subspace of such %U%, but we do not have a good grip on what happens when we convert that into a combinatorial subspace in %{}[3]^n%. Let’s just look at combinatorial lines. I’d like to find a subset %E\\subset [n]% and a combinatorial line inside %{}[3]^E% such that for all its three points we get too many ways of completing them with 2s and 3s to form an element of %\\mathcal{A}%. The problem is that while I may be able to find several %U\\subset E% with too many completions, at some point I’m going to need %U_1\\subset U_2% such that not only do they both have too many %(V,W)%s, but if I fill %U_2\\setminus U_1% with 2s or 3s then there are _still_ too many %(V,W)%s. Now there might be some hope of that if I could say that for almost every %U\\in\\mathcal{U}% it was the case that for almost all small sets %G%, if I fill %G% with 2s or 3s, then there are still too many %(V,W)%s that complete to a combinatorial line. Why? Because then I could use the fact that there is a positive density of lines (all this depends on the appropriate measures being used) to find a combinatorial line such that %U_2\\setminus U_1% behaved itself. So can we say that about %G%? Well, suppose that for a positive proportion of the %U\\in\\mathcal{U}% there is a positive proportion of small sets %G% that have too few extensions when you fill them with 2s. Then there must be a positive proportion of small sets %G% such that you get too many extensions when you fill them with 2s. And then you can pick a random %G% and get a dense set of %U%s such that you get too many extensions. So if we drop down to %{}[n]\\setminus G% then we have a strengthening of our original hypothesis in that combinatorial subspace. So now suppose we’ve reached the point where we can’t play this game any more. Then my hope is that you get that for almost every %U\\in\\mathcal{U}% (and therefore all once you chuck out the bad ones) it is the case that for almost all %G% you can fill them with 2s or 3s and still get roughly the right number of extensions. But with positive probability %U\\cup G% also belongs to %\\mathcal{U}% (by one of our Varnavides-versions of Sperner) and then perhaps we have something. That’s written quickly and carelessly, so I’m not sure it’s right. But if it is, then maybe one can run more or less the same argument with m-dimensional subspaces instead."},{"username":"terence-taoterence-tao","timestamp":"2009-03-03T01:44:00.000Z","contents":"Dense fibres argument Responding to 855, 856, etc. Firstly, I forgot to define A” in 837.2, as pointed out by Ryan. A” is the union of the V(w) as w ranges over all good words. It has a smallish density that depends on the density of the original 1-set A, the constants in DHJ(2.5), the number m, and the size of the wildcard sets %I_1,\\ldots,I_m%, but – crucially – the density does not depend on the size of the pre-existing bad coordinate set I’. And this is important, because the size of I’ is going to explode (but, importantly, will always be of size O(1) as far as n is concerned). As noted by Tim, if A’ is an (I’,1) set, then %A' \\backslash A''% is not going to be an (I’,1) set – it’s going to be merely an %(I'',1)%-set, where %I'' = I' \\cup I_1 \\cup \\ldots \\cup I_k%. I like to think of the coordinates %I_1,\\ldots,I_k% as being “consumed” or “used up” by the procedure of passing from A’ to %A' \\backslash A''%, in that the operation of interchanging 1 to 2 on these coordinates has been “spent”. But the remaining %n - |I''| \\approx n% interchanging operations remain “unspent”, and that’s good enough for our purposes. (This is the “IP” philosophy from ergodic theory; it’s OK if each operation can only be performed once, so long as one has a huge number of essentially identical copies of that operation to spend.) I don’t think we can just fix I’ in advance and maximise some mass over all (I’,1) sets; one has to do some greedy algorithm of the following sort: 1\\. We start with a 1-set A on which the function %f := 1_B - \\delta% has mass %{\\Bbb E} f 1_A \\geq c%, and a desired dimension m that one wants to get a c/4-density increment on. 2\\. DHJ(2.5) should tell us that any 1-set of density at least c/100 should contain a lot of m-dimensional combinatorial subspaces in which the wildcard sets %I_1,\\ldots,I_m% have size at most r for some %r = O_{m,c}(1)%. Indeed, if one chooses a random such subspace (using an appropriate measure), the probability that the entire subspace lies in the set should be at least %\\varepsilon% for some %\\varepsilon \\gg_{m,c} 1%. 3\\. One should be able to localise DHJ(2.5) to (I,1)-sets rather than 1-sets, so long as |I| is much smaller than n, by the first moment method. 4\\. Initialise %I' = \\emptyset% and A’ = A. 5\\. %A'% is a (I’,1) set with %{\\Bbb E} (f-c/2) 1_{A'} \\geq c/2%. In particular, A’ has density at least c/2. 6\\. Suppose that there exists disjoint %I_1,\\ldots,I_m% in %[n] \\backslash I'% of size at most r, and a %(I'',1)%-set A” of density at least %\\varepsilon% in A’, where %I'' := I' \\cup I_1 \\cup \\ldots \\cup I_m% such that B has density at most %\\delta+c/4% on A”. Then replace I’, A’ by I”, A”; this increases %{\\Bbb E} (f-c/2) 1_{A'}% by at least %c \\varepsilon/4%. Now return to Step 5. 7\\. We can only loop %O_{c,\\varepsilon}(1)% times, and so the net size of I’ at the end of the day is %O_{c,\\varepsilon,m,r}(1)%, and we end up with an (I’,1) set A’ of density at least c/2 such that B has density at least %\\delta+c/4% on every subset A” of the form discussed in Step 6\\. Now invoke DHJ(2.5) to find an A” of this form which is the union of m-dimensional subspaces, and we obtain the desired density increment. [Amazing how much messier the finitary argument is from the ergodic theory argument, where one simply takes the set where the conditional expectation of %1_B% to the sigma-algebra of 02-invariant sets is bigger than %\\delta + c/2% to be A’!] A small correction: the %\\varepsilon% in Step 6 should actually be %3^{-mr} \\varepsilon%. (The set of good w has density %\\varepsilon%, but the set A’, formed from the union of the V(w), could have a slightly smaller density of %3^{-mr} \\varepsilon%. But this is no big deal – as long as it doesn’t depend on n or the size of the wildcard set I’, one is OK.)"},{"username":"terence-tao","timestamp":"2009-03-03T01:51:00.000Z","contents":"A small correction: the %\\varepsilon% in Step 6 should actually be %3^{-mr} \\varepsilon%. (The set of good w has density %\\varepsilon%, but the set A’, formed from the union of the V(w), could have a slightly smaller density of %3^{-mr} \\varepsilon%. But this is no big deal – as long as it doesn’t depend on n or the size of the wildcard set I’, one is OK.)"},{"username":"terence-taoterence-taojozsefterence-taojozsef","timestamp":"2009-03-03T01:58:00.000Z","contents":"Multidimensional Sperner I believe one can get the right sort of multidimensional Sperner just by iterating the right sort of DHJ(2). If A is a subset of %[2]^n% of density %\\delta%, then there should exist an %r=O_\\delta(1)% such that if one picks a random r’ between 1 and r, then picks a random wildcard set I of size r’, then picks a random combinatorial line with that wildcard set, then with probability at least %\\varepsilon = \\varepsilon(\\delta,r) > 0%, the line will lie in A. (In fact one can probably take r to be about %1/\\delta% and %\\varepsilon% to be about %\\delta^2%, using the usual chain proof of Sperner. If one uses equal-slices measure then things may be particularly favourable.) In particular, if we pick r’ and I randomly, then the set %A' \\subset [2]^{n \\backslash I}%, defined as the set of all words %w \\in [2]^{n \\backslash I}% whose associated combinatorial line with wildcard set I lies in A, will have density at least %\\varepsilon/2% with positive probability. In particular there exists r’, I for which this statement is true. If we iterate this m times (replacing %\\delta% by %\\varepsilon/2% at each stage) we conclude that there exist disjoint wildcard sets %I_1,\\ldots,I_m% of size %O_{m,\\delta}(1)% such that the proportion of combinatorial m-spaces with these wildcards that completely lie in A is at least %\\varepsilon_m% for some %\\varepsilon_m = \\varepsilon_m(\\delta) > 0%. This is the multidimensional DHJ(2). The standard derivation of DHJ(2.5) from DHJ(2) should then give the multidimensional DHJ(2.5) needed for my argument. The argument here, of course, is identical to the usual proof of the Szemeredi cube lemma (that dense subsets of [n] contain high-dimensional cubes), by first iterating the fact given that a subset A of [n] of density %\\delta% one can find a positive h such that %A' := (A+h) \\cap A% has density %\\gg \\delta^2%. Terry, I’m not sure that I understand what are you saying here. Would it lead to a Varnavides-type result? Here are the numbers that I see; If a set of subsets of [n] has at least %c_d2^n/n^{1/2^d}% elements, then it contains a d-dimensional subspace. This bound is close to be sharp. Using this we see that a c-dense subset of %2^{[n]}% contains at least %cn^{1-1/2^d}2^n% d-dimensional subspaces, which is not much. Your calculation suggests much more. Dear Jozsef, One has to count the subspaces in an appropriate equal-slices fashion to get the right Varnavides-type result. (This was discussed quite a while back, I think in the 1-199 thread, but no harm in reviving it here.) Let’s start with m=1: if A has density c in %2^{[n]}%, then a random maximal chain %\\emptyset = S_0 \\subset S_1 \\subset \\ldots \\subset S_n = [n]% in %[2]^n% will intersect A in about %c \\sqrt{n}% positions in the middle %O(\\sqrt{n})% of the chain. [The calculation is a little simpler here if one uses equal-slices density for A instead of uniform density, but never mind that.] The pigeonhole principle then tells us that if we randomly choose an r between 1 and O(1/c), and randomly pick i = n/2 + O(\\sqrt{n}), then %S_i, S_{i+r}% will both land in A with probability about c^2. To put it another way, if we pick r randomly between 1 and O(1/c), and then randomly select from all the %2^{n-r} \\binom{n}{r}% combinatorial lines with r wildcards, then the probability that this line will lie in A is %\\gg c^2% (there may be some logarithmic losses of %\\log \\frac{1}{c}% due to the Chernoff inequality etc.) Note that this is **not** the same as choosing a combinatorial line uniformly at random among all %3^n-2^n% such lines, or even amongst all such lines with %O(1/c)% wildcards. This Varnavides-type theorem gives a weighted count of combinatorial lines in the set; a line with r wildcards has a weight proportional to %1 / 2^{n-r} \\binom{n}{r}%. This is the one which I think one should iterate, and not the theorem based on raw (unweighted) cardinality of lines. Thank you Terry!"},{"username":"terence-tao","timestamp":"2009-03-03T02:00:00.000Z","contents":"The argument here, of course, is identical to the usual proof of the Szemeredi cube lemma (that dense subsets of [n] contain high-dimensional cubes), by first iterating the fact given that a subset A of [n] of density %\\delta% one can find a positive h such that %A' := (A+h) \\cap A% has density %\\gg \\delta^2%."},{"username":"jozsef","timestamp":"2009-03-03T03:39:00.000Z","contents":"Terry, I’m not sure that I understand what are you saying here. Would it lead to a Varnavides-type result? Here are the numbers that I see; If a set of subsets of [n] has at least %c_d2^n/n^{1/2^d}% elements, then it contains a d-dimensional subspace. This bound is close to be sharp. Using this we see that a c-dense subset of %2^{[n]}% contains at least %cn^{1-1/2^d}2^n% d-dimensional subspaces, which is not much. Your calculation suggests much more."},{"username":"terence-tao","timestamp":"2009-03-03T04:12:00.000Z","contents":"Dear Jozsef, One has to count the subspaces in an appropriate equal-slices fashion to get the right Varnavides-type result. (This was discussed quite a while back, I think in the 1-199 thread, but no harm in reviving it here.) Let’s start with m=1: if A has density c in %2^{[n]}%, then a random maximal chain %\\emptyset = S_0 \\subset S_1 \\subset \\ldots \\subset S_n = [n]% in %[2]^n% will intersect A in about %c \\sqrt{n}% positions in the middle %O(\\sqrt{n})% of the chain. [The calculation is a little simpler here if one uses equal-slices density for A instead of uniform density, but never mind that.] The pigeonhole principle then tells us that if we randomly choose an r between 1 and O(1/c), and randomly pick i = n/2 + O(\\sqrt{n}), then %S_i, S_{i+r}% will both land in A with probability about c^2. To put it another way, if we pick r randomly between 1 and O(1/c), and then randomly select from all the %2^{n-r} \\binom{n}{r}% combinatorial lines with r wildcards, then the probability that this line will lie in A is %\\gg c^2% (there may be some logarithmic losses of %\\log \\frac{1}{c}% due to the Chernoff inequality etc.) Note that this is **not** the same as choosing a combinatorial line uniformly at random among all %3^n-2^n% such lines, or even amongst all such lines with %O(1/c)% wildcards. This Varnavides-type theorem gives a weighted count of combinatorial lines in the set; a line with r wildcards has a weight proportional to %1 / 2^{n-r} \\binom{n}{r}%. This is the one which I think one should iterate, and not the theorem based on raw (unweighted) cardinality of lines."},{"username":"jozsef","timestamp":"2009-03-03T04:32:00.000Z","contents":"Thank you Terry!"},{"username":"gowersterence-taoterence-tao","timestamp":"2009-03-03T05:06:00.000Z","contents":"Density increment. I’m going to have yet another go at expressing the argument in terms that I understand. Or rather, I’m going to try to prove the result, using what Terry has written as a huge hint as to what kind of argument I can expect to work. However, I do not plan to give anything like full details in this comment: I just want to convince myself of the argument. The set-up once again: I have a 1-set %\\mathcal{A}% (I’d better call it %\\mathcal{A}%, following Terry, to avoid confusion, though earlier I’ve been calling it %\\mathcal{B}% and the dense set that correlates with it %\\mathcal{A}%) defined as the set of all %(U,V,W)% such that %U\\in\\mathcal{U}%; I have a set %\\mathcal{B}% of density %\\delta% such that the density of %\\mathcal{B}% inside %\\mathcal{A}% is at least %\\delta+\\eta%; I want a density increase for %\\mathcal{B}% on a combinatorial subspace with dimension tending to infinity. Step 1\\. For every %U% define %d(U)% to be the probability that if you randomly partition %{}[n]\\setminus U% into two sets %V,W% then the point %(U,V,W)% belongs to %\\mathcal{B}%. Then we are assuming that the average of %d(U)% over all %U\\in\\mathcal{U}% is %\\delta+\\eta.% (One of the details I’m going to be very non-explicit about is what measures I am using.) By an easy averaging argument we may pass to a dense subset of %\\mathcal{U}% such that %d(U)\\geq\\delta+\\eta/2% for _every_ %U% in the dense subset. Let’s replace %\\mathcal{U}% by this subset and %\\eta% by %\\eta/2% and still call them %\\mathcal{U}% and %\\eta%. Step 1.5\\. The obvious thing to do next is pass to an m-dimensional combinatorial subspace (in the %{}[2]^n% sense) that lives inside %\\mathcal{U}%. Let us write it as %Z_0\\cup FU(Z_1,\\dots,Z_m)%, which stands for the set of all sets %Z_0\\cup Z%, where %Z% is a union of some of the %Z_i%. (The F stands, unnecessarily, for “finite”.) But if we do that then we find ourselves wondering why we bothered, since if we treat %Z_0% as a fixed set of 1s and the %Z_i% as wildcard sets and try to fix the remaining coordinates to get a combinatorial subspace of %{}[3]^n% with a density increase, then we simply don’t succeed. It could be that as soon as we fill the %Z_i% with constant strings of 2s or 3s, we end up with sequences that give rise to hardly any extensions that belong to %\\mathcal{B}%. (Just to explain slightly more, our aim is to find a combinatorial subspace such that the average density of possible extensions to a point in %\\mathcal{B}% is too high, so that then we can turn things round and say that a random extension comes from too many points in the combinatorial subspace, which gives us our density increment.) Step 2\\. At this point, we say to ourselves, “Yes, but if that disaster happened all the time, then surely it would have to be compensated for by exploitable density increases elsewhere.” So now we are trying to produce an averaging argument. At this stage, let’s forget that m is supposed to tend to infinity, and treat it as a very large fixed constant. (This is a standard move — if we can always do it for sufficiently large n, then we simply work out what n we needed in terms of m, invert that function, and we’ve got our function of n that tends to infinity. Terry has been using this trick a lot — in fact, easy though it is, it should go on the Tricki.) Given that we are trying to produce an averaging argument, we had better look not just for _one_ combinatorial subspace, but for a dense set of combinatorial subspaces. So now let’s assume we’ve got a Varnavides-type result for multidimensional Sperner that tells us that with positive probability (depending on the density of %\\mathcal{U}% and on m but not on n) if we choose our %Z_0,Z_1,\\dots,Z_m% randomly (according to some carefully chosen probability distribution) then %Z_0\\cup FU(Z_1,\\dots,Z_m)\\subset\\mathcal{U}.% Here I’m thinking of %Z_0% as being a fairly typical %U% and %Z_1,\\dots,Z_m% as being very small sets. Now let’s suppose that however we do that we find that, to our annoyance, there is some way of assigning 1s, 2s and 3s to the sets %Z_1,\\dots,Z_m% and filling %Z_0% with 1s, such that the density of ways of filling the rest of %{}[n]% with 2s and 3s to get an element of %\\mathcal{B}% is a bit less than %\\delta+\\eta% (by some rather small amount that depends on %m%). So each %Z_0% is associated with a positive density (depending on m) of “bad” sequence fragments, formed by taking some small sets %Z_1,\\dots,Z_m% and assigning 1s, 2s and 3s to them. Turning this round we can find a many fragments with a lot of “bad” %Z_0%s. Now if I generate a sequence by picking %Z_0,Z_1,\\dots,Z_m% randomly, filling %Z_0% with 1s the rest with 1s, 2s and 3s and the rest of %{}[n]% with 2s and 3s, then I get almost exactly the same distribution as if I had just chosen a random sequence (or at least, I believe that one can easily ensure this). And I think we can use that to argue that if many fragments give rise to several “bad” %Z_0%s (by which I mean that the density of extensions by 2s and 3s that give rise to elements of %\\mathcal{B}% is too low) then there must be many fragments that give rise to several “good” %Z_0%s. But then by averaging we can fix one of those fragments and pass to a %(|Z_1|+\\dots+|Z_m|)%-codimensional subspace inside which we have a 1-set consisting of sets %U% for which %d(U)% has gone up very very slightly (by an amount that depends on m). Step 3\\. And now we just iterate. If the density goes up by %c=c(m,\\delta,\\eta)% each time, and if our sets %Z_i% have size bounded above by %r%, then we need something like %mr/c% to be distinctly less than %n%. If that is the case, then the iteration will stop at some point where we can find a combinatorial subspace %Z_0\\cup FU(Z_1,\\dots,Z_m)% inside %\\mathcal{U}% with no possible bad assignments of 1s, 2s and 3s to the %Z_i% with %i\\geq 1%. And that means that for every element of that combinatorial subspace we get too many 2,3-extensions that belong to %\\mathcal{B}%. Turning that round, we pick 2s and 3s randomly outside %Z_0\\cup\\dots\\cup Z_m% and get on average a density increase in the subspace. Terry, I’d be interested to know whether you think that’s exactly the same as your argument, or roughly the same, or the same apart from an error that I’ve introduced, or correct and slightly different, or wrong for a reason that’s hard to correct but once you manage to correct it you get precisely your argument, or what. 861.1 Tim, I think this is essentially the same argument, but I like how you use an additional pigeonhole principle to freeze (and then discard) all the “bad” coordinates, thus avoiding the need to distinguish between global and local 1-sets. It’s not as if there was anything I was planning to do with those bad coordinates anyway, so one may as well pigeonhole them away. (Incidentally, if one does some Ramsey-theoretic preprocessing of the set, it may turn out that one has enough “stationarity” that one does not need to pass to subspaces at all… but this is overkill and would in any event produce worse quantitative bounds.) I guess the bounds will in fact end up being reasonably civilised at this stage; the bad set I of coordinates is only increasing linearly with the number of iterations, as you point out, not exponentially or anything. Some minor remarks: – I don’t think the averaging argument in Step 1 is actually needed; one can work with the global density of B in A rather than the local densities d(U) of B associated to U. (If it later turns out that there are lots of Us in A with too low of a B-density, then this will only help us when the time comes to make our density/mass increment.) – In order not to have to keep balancing the density of B in A, against the density of A, it is probably better to stare at the mass %{\\Bbb E} (1_A - \\delta - \\eta/2) 1_B% rather than the density, as this (a) seems to go up by a non-trivial amount at every stage of the iteration, and (b) it automatically provides a lower bound on the density of A, which is needed to stop the amount removed from A at each stage of the iteration from shrinking to zero."},{"username":"terence-tao","timestamp":"2009-03-03T05:35:00.000Z","contents":"861.1 Tim, I think this is essentially the same argument, but I like how you use an additional pigeonhole principle to freeze (and then discard) all the “bad” coordinates, thus avoiding the need to distinguish between global and local 1-sets. It’s not as if there was anything I was planning to do with those bad coordinates anyway, so one may as well pigeonhole them away. (Incidentally, if one does some Ramsey-theoretic preprocessing of the set, it may turn out that one has enough “stationarity” that one does not need to pass to subspaces at all… but this is overkill and would in any event produce worse quantitative bounds.) I guess the bounds will in fact end up being reasonably civilised at this stage; the bad set I of coordinates is only increasing linearly with the number of iterations, as you point out, not exponentially or anything."},{"username":"terence-tao","timestamp":"2009-03-03T05:41:00.000Z","contents":"Some minor remarks: – I don’t think the averaging argument in Step 1 is actually needed; one can work with the global density of B in A rather than the local densities d(U) of B associated to U. (If it later turns out that there are lots of Us in A with too low of a B-density, then this will only help us when the time comes to make our density/mass increment.) – In order not to have to keep balancing the density of B in A, against the density of A, it is probably better to stare at the mass %{\\Bbb E} (1_A - \\delta - \\eta/2) 1_B% rather than the density, as this (a) seems to go up by a non-trivial amount at every stage of the iteration, and (b) it automatically provides a lower bound on the density of A, which is needed to stop the amount removed from A at each stage of the iteration from shrinking to zero."},{"username":"gowers","timestamp":"2009-03-03T06:28:00.000Z","contents":"Density increment. I want to throw out another thought, which is that there is something we would be silly not to try to do, though it may turn out not to be feasible. Actually, it may be one of those cases where Randall and Terry will be able to give an informed guess whether it has any chance of working by looking at things from an ergodic perspective. It looks as though we now know that correlation with a 1-set implies a density increase, but what about correlation with a 12-set? What is a 12-set? It is what on the wiki I called a special set of complexity 1: you take set systems %\\mathcal{U}% and %\\mathcal{V}% and define %\\mathcal{A}% to be the set of all x with 1-set in %\\mathcal{U}% and 2-set in %\\mathcal{V}%. Equivalently, it is the set of (U,V,W) with %U\\in\\mathcal{U}% and %V\\in\\mathcal{V}%. (Thus, a 1-set is a 12-set where %\\mathcal{V}=[3]^n.%) If the argument on the wiki that says that line-free sets correlate with 12-sets is correct, and if correlation with a 12-set implies correlation with a subspace, then the whole problem is solved, so the motivation for this question is rather obvious. A few preliminary thoughts. First, I think it is easy to prove (by imitating the DHJ(2.5) argument) that a dense 12-set %\\mathcal{A}% contains multidimensional subspaces. You just choose some random copy of %{}[2]^M% inside %{}[3]^n% (where %M% is small enough for a random point in this random copy to be approximately uniformly distributed). In that copy, %\\mathcal{A}% will on average be dense, and therefore contain a multidimensional subspace in the %{}[2]^n% sense. And then, since %\\mathcal{A}% is a 12-set, we can allow the wildcards to be 3 and we still live inside %\\mathcal{A}%. (I think this argument has probably appeared several times already in our discussions — I may even be one of the people to have given it.) That proof should give a Varnavides-type statement too. So what would the next stage be? Let’s choose a random combinatorial subspace as follows. First we choose random large sets %U_0% and %V_0%. Then we choose random small wildcard sets %Z_1,\\dots,Z_m.% And then we … er … hope that the density of %\\mathcal{B}% inside the resulting subspace (where we assign values 1 to %U_0%, 2 to %V_0%, anything we like to the %Z_i%, and 3 to the rest) is at least %\\delta+\\eta/2.% If that always fails to be the case, then we argue as follows. With some small but positive probability p, the random combinatorial subspace is a subset of %\\mathcal{A}%. That means that for some random assignment of 1s, 2s and 3s to the %Z_i% we get too few pairs %(U_0,V_0)% belonging to %\\mathcal{B}% out of the ones that belong to %\\mathcal{A}%. (By the way, I should confess that I’m losing the thread here and don’t really know whether what I’m writing is correct. But I’ll just plough on.) So that should be balanced by some other random assignment that gives too many pairs. And then we can get a subspace with a small density increase on a 12-set. I must go to bed in the knowledge that that argument could be anywhere along the spectrum that goes from complete nonsense at one end to a proof of DHJ(3) at the other. (For now I’ll leave unspecified what measure I am placing on that spectrum.) Perhaps someone will be able to tell me by the time I get up tomorrow."},{"username":"gowersgowers","timestamp":"2009-03-03T19:57:00.000Z","contents":"Density increment Here’s a slightly more precise, but still far from checked, version of what I said in 862. 1\\. This is a way that one could choose a random point %(U,V,W)\\in[3]^n%. You choose a random disjoint pair %(U_0,V_0)% and some random small wildcard sets %Z_1,\\dots,Z_m%. You assign values to the wildcards, fill %U_0% with 1s, %V_0% with 2s and put 3s everywhere else. That gives you your point, and it’s more or less uniformly distributed. 2\\. Now let’s condition on %Z_1,\\dots,Z_m% and the assignment of their values. (I’ll call this a random sequence fragment.) If the probability that %(U,V,W)\\in\\mathcal{B}% given the random sequence fragment is ever more than %\\delta+\\eta% (plus a tiny tiny amount) then we’ve got a density increase on a 12-set in a subspace, and can iterate. 3\\. Therefore, this conditional probability cannot be more than %\\delta+\\eta% or we’re done. 4\\. But there is a positive probability that %U_0\\cup FU(Z_1,\\dots,Z_m)\\subset\\mathcal{U}% and %V_0\\cup FU(Z_1,\\dots,Z_m)\\subset\\mathcal{V}.% Moreover, the density of %\\mathcal{B}% inside the subspace %\\langle U_0,V_0;Z_1,\\dots,Z_m\\rangle% (I hope it’s easy to guess what that means) is at most %\\delta+\\eta/2% or we’re done. So, turning things round, there is in fact a positive probability that the conditional probability in 3 is somewhat less than %\\delta+\\eta.% 5\\. Therefore, by 1 (I think) there is a positive probability that it is somewhat more than %\\delta+\\eta% and we’re done. Again, that sort of feels as though it could be correct, but it also feels as though it could collapse when I try to write it out properly. 863.2 I see now that Step 2 is not very clear because I am not mentioning %\\mathcal{U}% and %\\mathcal{V}%. If in Step 1 we discard the point and try again unless %U\\in\\mathcal{U}% and %V\\in\\mathcal{V}% then we end up with a roughly uniformly distributed point in %\\mathcal{A}%. (That is, the restriction of the distribution to %\\mathcal{A}% is roughly uniform.) Then in Step 2 I want to fix %Z_1,\\dots,Z_m%, and the probability I’m talking about is the probability of being in %\\mathcal{B}% given that you are in %\\mathcal{A}%. It is this that cannot go above %\\delta+\\eta% unless it is possible to get a density increase and an iteration. If nobody sees an obvious flaw in this approach by tomorrow morning (my time) then I suppose I’ll try to work out some details properly. I’m slightly suspicious of Steps 4 and 5\\. One tiny remark for clarification: this is not supposed to be the Ajtai-Szemerédi approach: it’s a different argument that would combine the density increase on a 12-set (as given on the wiki) with a Randall/Terry style argument to get from that to a density increase on a subspace. My dream for the project as a whole is that we’ll end up with lots of different proofs …"},{"username":"gowers","timestamp":"2009-03-04T07:13:00.000Z","contents":"863.2 I see now that Step 2 is not very clear because I am not mentioning %\\mathcal{U}% and %\\mathcal{V}%. If in Step 1 we discard the point and try again unless %U\\in\\mathcal{U}% and %V\\in\\mathcal{V}% then we end up with a roughly uniformly distributed point in %\\mathcal{A}%. (That is, the restriction of the distribution to %\\mathcal{A}% is roughly uniform.) Then in Step 2 I want to fix %Z_1,\\dots,Z_m%, and the probability I’m talking about is the probability of being in %\\mathcal{B}% given that you are in %\\mathcal{A}%. It is this that cannot go above %\\delta+\\eta% unless it is possible to get a density increase and an iteration. If nobody sees an obvious flaw in this approach by tomorrow morning (my time) then I suppose I’ll try to work out some details properly. I’m slightly suspicious of Steps 4 and 5\\. One tiny remark for clarification: this is not supposed to be the Ajtai-Szemerédi approach: it’s a different argument that would combine the density increase on a 12-set (as given on the wiki) with a Randall/Terry style argument to get from that to a density increase on a subspace. My dream for the project as a whole is that we’ll end up with lots of different proofs …"},{"username":"randallterence-taorandallrandall","timestamp":"2009-03-04T01:45:00.000Z","contents":"Speculative reply to 862. From the ergodic perspective, it looks like correlation with a dense 12-set implying a mass increment is still on the order of double recurrence. However, the picture I seem to be getting is that maybe it’s worth pursuing anyway. Here is the idea. Say we assume some fancy double recurrence result (FDR) that is not, on the face of things, as general as DHJ (3). Like for example, what are called IP Szemeredi or IP Roth on the Wiki (comments 2 and 469, respectively). And say we can use this to show that correlation with a dense 12-set implies a density increment on a subspace. Then, assuming that really was all that was needed in the first place, we will have reduced DHJ to FDR, which might not be close to finishing, but might at least be progress. The reason I think this might be feasible is, in the ergodic world the projection P onto the asymptotically 02 invariant sets commutes with the projection Q onto the asymptotically 01 invariant sets. So, PQ ought to project to L^2 of some larger sigma-algebra. Correlating with something measurable with respect to this sigma-algebra gets you big fibers, you just need a way to bring these fibers back to themselves along a subspace. That’s where FDR comes in. I think you can probably work in a product space with genuine corners now. The idea is you want to show that A intersect T_{01}A intersect T_{02}A is non-empty (I suppress alpha in the notation, which tells you which coordinates to flip…also you want only to flip things that are zero; strictly speaking the notation I am using probably doesn’t mean anything taken literally, it’s only meant to be suggestive.) Well, on the horizontal coordinate, 1s and 2s are interchangeable, so T_{01}=T_{02} (more or less). On the vertical coordinate, 0s and 1s are interchangeable, so T_{01}=Id (more or less). So basically, you have T_{01} acting on each coordinate, and you need the putative corners type result (FDR) to bring sets in the Cartesian product back to themselves. Dear Randall, I think in fact that double recurrence for 12-sets collapses to single recurrence by the argument sketched out in 862\\. It’s easiest to explain for the corners problem: to find corners (x,y), (x+r,y), (x,y+r) in a Cartesian product %A \\times B%, it suffices to just find the latter two points (x+r,y), (x,y+r) of the corner inside this product, as this will automatically place the third guy inside the corner as well. So the corners theorem in this case follows trivially from the pigeonhole principle; and for similar reasons, DHJ(3) for 12-sets follows from DHJ(2). In ergodic language: if %f = f_1 f_2%, where %f_1% is S-invariant and %f_2% is T-invariant, then the double recurrence integral %\\int f S^n f T^n f% collapses to the single recurrence integral %\\int S^n (f_1^2 f_2) T^n(f_2^2 f_1)%. In the DHJ world, if %f = f_1 f_2%, %f_1% is %\\rho \\sigma^{-1}%-invariant, and %f_2% is %\\tau \\sigma^{-1}%-invariant, then %\\int \\sigma(f) \\rho(f) \\tau(f) \\approx \\int \\rho(f_1^2 f_2) \\tau(f_1 f_2^2)% at least for words w that are sufficiently “large”. 864.2 Clarification. The set A I am trying to bring back is not a 12-set. It is a set that is measurable with respect to the sigma-algebra of 12-sets. (Notice that, unlike i-sets, ij-sets do not form an algebra.) In general, the aim of my last post was to outline a method for obtaining a density increment on a subspace for a set that correlates with a 12-set. If I understand Tim correctly, that may be the last piece. What I hope is possible is to fill in that gap with a double recurrence theorem that is, on the face of things, weaker (and hopefully easier to prove directly) than DHJ (3). 864.3 Obviously I meant “measurable with respect to the sigma algebra generated by the 12-sets”; although what I actually typed is an amusing oxymoron."},{"username":"terence-tao","timestamp":"2009-03-04T05:28:00.000Z","contents":"Dear Randall, I think in fact that double recurrence for 12-sets collapses to single recurrence by the argument sketched out in 862\\. It’s easiest to explain for the corners problem: to find corners (x,y), (x+r,y), (x,y+r) in a Cartesian product %A \\times B%, it suffices to just find the latter two points (x+r,y), (x,y+r) of the corner inside this product, as this will automatically place the third guy inside the corner as well. So the corners theorem in this case follows trivially from the pigeonhole principle; and for similar reasons, DHJ(3) for 12-sets follows from DHJ(2). In ergodic language: if %f = f_1 f_2%, where %f_1% is S-invariant and %f_2% is T-invariant, then the double recurrence integral %\\int f S^n f T^n f% collapses to the single recurrence integral %\\int S^n (f_1^2 f_2) T^n(f_2^2 f_1)%. In the DHJ world, if %f = f_1 f_2%, %f_1% is %\\rho \\sigma^{-1}%-invariant, and %f_2% is %\\tau \\sigma^{-1}%-invariant, then %\\int \\sigma(f) \\rho(f) \\tau(f) \\approx \\int \\rho(f_1^2 f_2) \\tau(f_1 f_2^2)% at least for words w that are sufficiently “large”."},{"username":"randall","timestamp":"2009-03-04T08:34:00.000Z","contents":"864.2 Clarification. The set A I am trying to bring back is not a 12-set. It is a set that is measurable with respect to the sigma-algebra of 12-sets. (Notice that, unlike i-sets, ij-sets do not form an algebra.) In general, the aim of my last post was to outline a method for obtaining a density increment on a subspace for a set that correlates with a 12-set. If I understand Tim correctly, that may be the last piece. What I hope is possible is to fill in that gap with a double recurrence theorem that is, on the face of things, weaker (and hopefully easier to prove directly) than DHJ (3)."},{"username":"randall","timestamp":"2009-03-04T08:36:00.000Z","contents":"864.3 Obviously I meant “measurable with respect to the sigma algebra generated by the 12-sets”; although what I actually typed is an amusing oxymoron."},{"username":"ryan-odonnellterence-taoterence-taorandall","timestamp":"2009-03-04T06:41:00.000Z","contents":"Structure Theorem/FK-McCutcheon approach. I know we have been making progress lately on Tim’s Ajtai-Szemeredi approach, but I’ve been thinking also about Terry’s approach, started in #818, of finitising Randall’s proof of the Furstenberg-Katznelson argument. Call me craven, but there’s something nice about also working an angle that “in principle” is known to succeed. Perhaps we might even end up with a glorious amalgam of the two approaches. The crux of Terry’s [wiki version](http://michaelnielsen.org/polymath1/index.php?title=Furstenberg-Katznelson_argument) seems to be a proposed structure theorem which splits any function %f% into a “01-almost periodic relative to 12-low influence” part and a “01-uniform relative to 12-low influence” part. It’s slightly tricky for me to keep even the definitions in mind, and I had a moment of despair contemplating actually executing such a structure theorem. Hence I fantasise that some kind of simple structure theorem (a la #821 and #826) might also do the job. Let me throw this fantasy out there and see if it can survive for a bit. Put a graph on %[3]^n% where vertices are connected by an edge if they have Hamming distance 1\\. Define an operator %L^{01}% on functions %f : [3]^n \\to \\mathbb{R}% by %L^{01}f(x) = f(x) - \\mathbb{E}[f(x')]%, where %x'% is formed from %x% by choosing a random coordinate and flipping it between 0 and 1 (if it is not 2). Similarly define the operator %L^{12}%. For %t \\geq 0%, define also the operator %H^{01}_t = e^{-tL^{01}}%, and similarly %H^{12}_t%. Note for intuition that if we had defined %H^{01}_t% for functions on %[2]^n%, we would have gotten the operator %T_{e^{-t/n}}% (as used in #822). These operators form a semigroup. So my hope is to have the structure theorem be %f = H^{12}_{t} H^{01}_{t'} f + (f - H^{12}_{t} H^{01}_{t'} f)% for some carefully chosen small quantities %t%, %t'%. Let me give an extraordinarily non-rigorous argument for why this might work out. First, is %H^{12}_{t} H^{01}_{t'} f% “01-almost periodic relative to 12-low influence”? Well, in fact perhaps it’s even 01-low influence. The idea here is that if %t% and %t'% are both very small, then hopefully %H^{12}_t% and %H^{01}_{t'}% approximately commute. Then %H^{12}_{t} H^{01}_{t'} f \\approx H^{01}_{t'} H^{12}_{t} f%, and the latter function has 01-low influence because it has an %H^{01}_{t'}% out front. And what about %g := (f - H^{12}_{t} H^{01}_{t'} f)%? Is it “01-uniform relative to 12-low influence”? At the grossest level, I take this to mean, is it true that %\\langle H^{01}_{s} g, h \\rangle% is small for any 12-low influence %h%? (Note that %H^{01}_{s}% should be self-adjoint so this should be the same as asking if %\\langle g, H^{01}_{s} h \\rangle% is small for 12-low influence %h%.) By definition, this correlation is %\\langle H^{01}_{s} f, h \\rangle - \\langle H^{01}_{s} H^{12}_{t} H^{01}_{t'} f, h \\rangle%. Again, in the second term here we hope to commute the %H^{12}% operator to the front, use self-adjointness to put it on the other side, and get %\\langle H^{01}_{s} f, h \\rangle - \\langle H^{01}_{s+t'} f, H^{12}_{t} h \\rangle%. Now I again fantasise that perhaps by picking %s% and/or %t% carefully (at a “plateau in the energy spectrum”) we can say that %H^{01}_s f \\approx H^{01}_{s+t'} f%. Then we’d get %\\langle H^{01}_{s} f, h \\rangle - \\langle H^{01}_{s} f, H^{12}_{t} h \\rangle% which equals %\\langle H^{01}_s f, h - H^{12}_t h \\rangle%. But %h% is 12-low influence so %h \\approx H^{12}_t h% and hence this is indeed small. — I’m normally much soberer than this post would suggest. Dear Ryan, I’m hopeful on this long-term strategy too, but from experience with finitising the ergodic proof of Szemeredi’s theorem, I’m pretty sure that the proof is going to be yuckier than the Ajtai-Szemeredi-flavoured approach. 01-uniformity of a function f relative to 12-low influence is a bit stronger than what you’re saying. It’s the assertion that %{\\Bbb E} g(\\ell(0)) f(\\ell(1)) h(\\ell)% (1) is small for all bounded g and all 12-low influence h, where %\\ell% ranges over lines. The point here is that h depends not only on the fixed coordinates of h, but also on the location of the wildcards. The statement you’re saying is roughly equivalent to saying that (1) is small in the case when h doesn’t depend on the location of the wildcards. Perhaps I can motivate things a bit better by considering the analogous notion when counting three-term progressions. Here, the analogue of “12-low influence” is simply “constant”, and a function %f: {\\Bbb Z}/N{\\Bbb Z} \\to [-1,1]% is “uniform” (the analogue of “01-uniform relative to 12-low influence”) if one has %{\\Bbb E}_{n,r} g(n) f(n+r) h(r)% small for all bounded g and all bounded h which are independent of n. Unfortunately, the structure theorems that show up in ergodic theory, once one leaves the “1-step” world and moves to the “higher-step” or “relative” world, are a bit messy, especially in the finitary world. In principle the machinery from my paper will be relevant here, but I’m reluctant to unpack it all now; I’m hoping that the Ajtai-Szemeredi thread may lead to some new insights or simplifications first. (Also, Tim Austin may soon be coming out with some new work that could also assist with this; more on this once it is more firm.) I forgot to say that the key point in the length three progressions example was that while h did not depend on the n parameter (which roughly corresponds to the “fixed positions” of the line), it still depended on the r parameter (which correspond to the “wildcards” of the line). Because of this, I don’t see an easy structure theorem here… the cheapest way is to take Fourier transforms and extract out the large Fourier modes, but this can’t really be done just by heat operators. 865.3 Ryan, it might help to consider the functions %f(U,V,W)=e(\\sum_V -y_i +\\sum_W y_i)%, %g(U,V,W)=e(\\sum_V y_i)%, %h(U,V,W)=e(\\sum_W -y_i)%. Then %f(w(0)) g(w(1)) h(w(2)) = 1% for all variable words %w%, so your structure theory had better not be counting %g% as “01-uniform relative to 12-low-influence” or some similar category. On the other hand, %g% looks like it may be orthogonal to 12-low-influence."},{"username":"terence-tao","timestamp":"2009-03-04T09:44:00.000Z","contents":"Dear Ryan, I’m hopeful on this long-term strategy too, but from experience with finitising the ergodic proof of Szemeredi’s theorem, I’m pretty sure that the proof is going to be yuckier than the Ajtai-Szemeredi-flavoured approach. 01-uniformity of a function f relative to 12-low influence is a bit stronger than what you’re saying. It’s the assertion that %{\\Bbb E} g(\\ell(0)) f(\\ell(1)) h(\\ell)% (1) is small for all bounded g and all 12-low influence h, where %\\ell% ranges over lines. The point here is that h depends not only on the fixed coordinates of h, but also on the location of the wildcards. The statement you’re saying is roughly equivalent to saying that (1) is small in the case when h doesn’t depend on the location of the wildcards. Perhaps I can motivate things a bit better by considering the analogous notion when counting three-term progressions. Here, the analogue of “12-low influence” is simply “constant”, and a function %f: {\\Bbb Z}/N{\\Bbb Z} \\to [-1,1]% is “uniform” (the analogue of “01-uniform relative to 12-low influence”) if one has %{\\Bbb E}_{n,r} g(n) f(n+r) h(r)% small for all bounded g and all bounded h which are independent of n. Unfortunately, the structure theorems that show up in ergodic theory, once one leaves the “1-step” world and moves to the “higher-step” or “relative” world, are a bit messy, especially in the finitary world. In principle the machinery from my paper will be relevant here, but I’m reluctant to unpack it all now; I’m hoping that the Ajtai-Szemeredi thread may lead to some new insights or simplifications first. (Also, Tim Austin may soon be coming out with some new work that could also assist with this; more on this once it is more firm.)"},{"username":"terence-tao","timestamp":"2009-03-04T09:46:00.000Z","contents":"I forgot to say that the key point in the length three progressions example was that while h did not depend on the n parameter (which roughly corresponds to the “fixed positions” of the line), it still depended on the r parameter (which correspond to the “wildcards” of the line). Because of this, I don’t see an easy structure theorem here… the cheapest way is to take Fourier transforms and extract out the large Fourier modes, but this can’t really be done just by heat operators."},{"username":"randall","timestamp":"2009-03-04T10:50:00.000Z","contents":"865.3 Ryan, it might help to consider the functions %f(U,V,W)=e(\\sum_V -y_i +\\sum_W y_i)%, %g(U,V,W)=e(\\sum_V y_i)%, %h(U,V,W)=e(\\sum_W -y_i)%. Then %f(w(0)) g(w(1)) h(w(2)) = 1% for all variable words %w%, so your structure theory had better not be counting %g% as “01-uniform relative to 12-low-influence” or some similar category. On the other hand, %g% looks like it may be orthogonal to 12-low-influence."},{"username":"terence-taogowersrandall","timestamp":"2009-03-04T10:11:00.000Z","contents":"12-sets Randall raises a good point about 12-sets not being closed under union, though they are still closed under intersection. To borrow a notation I floated a while back, one should make a distinction between “basic 12-sets” (the intersection of a 1-set and a 2-set, which to continue the topological analogy might be thought of as “sub-basic 12-sets”), and “general 12-sets”, which are unions (a bounded number of) of basic 12-sets. In the iterative process that is carving out the 12-set A’ on which to locate a density increment, we are removing basic 12-sets from A’ at each iteration, and so the complexity of the 12-set A’ (i.e. the number of basic 12-sets needed to union together to make A’) increases with the iteration. This is going to cause a problem. Some sort of regularity lemma might be needed here (the ergodic analogue of this would be the Lebesgue differentiation theorem: every general 12-set is very dense inside a basic 12-set). But this might not be enough. The other alternative is to take advantage of the freedom in the finitary world to restrict to smaller subspaces in one’s hunt for a density increment; this is a trump card that the ergodic world doesn’t really have with current technology. There is a chance that by continually restricting, one may only have to work with basic 12-sets and not general 12-sets. I’ll think about it… 866.1 This connects in an amusing way with a terminological difficulty I had, and that you picked up, on the wiki. A basic 12-set is like a rank-1 tensor, and at one point I tried to use the word “rank” instead of “complexity” for that reason. But I was inconsistent about it. If we went on to DHJ(4) things would get more complicated still. For example, there’s an important distinction between a 12-set (an intersection of a 1-set and a 2-set) and a {1,2}-set (a set A such that whether or not (U,V,W,X) belongs to A depends only on the pair (U,V)). Then a {1,2}-function of rank at most k would be a sum of at most k basic {1,2}-functions, etc. etc. 866.2 Terry, having read Tim’s very exciting posts more carefully, I see what you meant now. Yes, I raised the point of the 12-sets not forming an algebra, but apparently I was too caught up in the ergodic paradigm to ignore it, as I should have. In the ergodic setting one develops a habit of thinking not about how one function correlates with another, but rather how a function projects to a subspace. Hence my counterproductive impulse to immediately go searching for an algebra on which to project."},{"username":"gowers","timestamp":"2009-03-04T16:07:00.000Z","contents":"866.1 This connects in an amusing way with a terminological difficulty I had, and that you picked up, on the wiki. A basic 12-set is like a rank-1 tensor, and at one point I tried to use the word “rank” instead of “complexity” for that reason. But I was inconsistent about it. If we went on to DHJ(4) things would get more complicated still. For example, there’s an important distinction between a 12-set (an intersection of a 1-set and a 2-set) and a {1,2}-set (a set A such that whether or not (U,V,W,X) belongs to A depends only on the pair (U,V)). Then a {1,2}-function of rank at most k would be a sum of at most k basic {1,2}-functions, etc. etc."},{"username":"randall","timestamp":"2009-03-05T00:39:00.000Z","contents":"866.2 Terry, having read Tim’s very exciting posts more carefully, I see what you meant now. Yes, I raised the point of the 12-sets not forming an algebra, but apparently I was too caught up in the ergodic paradigm to ignore it, as I should have. In the ergodic setting one develops a habit of thinking not about how one function correlates with another, but rather how a function projects to a subspace. Hence my counterproductive impulse to immediately go searching for an algebra on which to project."},{"username":"gowers","timestamp":"2009-03-04T16:57:00.000Z","contents":"Density increment I’m going to give a sort of commentary on 863 (which I reproduce here) as a first step towards either making it more precise or discovering a problem with it. _1\\. This is a way that one could choose a random point %(U,V,W)\\in[3]^n%. You choose a random disjoint pair %(U_0,V_0)% and some random small wildcard sets %Z_1,\\dots,Z_m%. You assign values to the wildcards, fill %U_0% with 1s, %V_0% with 2s and put 3s everywhere else. That gives you your point, and it’s more or less uniformly distributed._ I don’t foresee any problems with this step. It would use the uniform measure on %{}[3]^n% and exploit the fact that slices near %(n/3,n/3,n/3)% all have roughly the same size. The size of the wildcard sets would be some large constant that depended on the density of %\\mathcal{B}%. _2\\. Now let’s condition on %Z_1,\\dots,Z_m% and the assignment of their values. (I’ll call this a random sequence fragment.) If the probability that %(U,V,W)\\in\\mathcal{B}% given the random sequence fragment is ever more than %\\delta+\\eta% (plus a tiny tiny amount) then we’ve got a density increase on a 12-set in a subspace, and can iterate._ Here is a more precise formulation of what I mean, taking into account comment 863.1\\. Let me establish my terminology. A _sequence fragment_ is a set %Z\\subset[n]% and a function %\\sigma% from %Z% to %\\{1,2,3\\}.% An _extension_ of a sequence fragment %(Z,\\sigma)% is a sequence %x\\in[3]^n% such that the restriction of %x% to %Z% is %\\sigma.% If %\\mathcal{A}% is any subset of %{}[3]^n%, I shall write %\\mathcal{A}(Z,\\sigma)% for the set of all extensions of %(Z,\\sigma)% that belong to %\\mathcal{A}%, which can be naturally identified with a subset of %{}[3]^{[n]\\setminus Z}.% The precise statement of Step 2 is now this. If we can ever find a sequence fragment %(Z,\\sigma)% such that %|\\mathcal{B}(Z,\\sigma)|/|\\mathcal{A}(Z,\\sigma)|% is a bit bigger than %\\delta+\\eta,% then we can restrict the coordinates in %Z% to %\\sigma% and iterate. Or at least, we can do that as long as %\\mathcal{A}(Z,\\sigma)% isn’t too small. One thing that makes me slightly anxious is the fact that the densities of the sets %\\mathcal{A}(Z,\\sigma)% may vary quite widely. But it occurs to me here that we may be able to do some kind of regularization by passing to a sequence fragment that maximizes this density and observing that the relative density of %\\mathcal{B}(Z,\\sigma)% must also be preserved or we would have found a density increment somewhere along the line. I’m pretty convinced this can be done if it’s needed, so I think for now I won’t worry about this problem too much (which makes me much more confident about the double counting arguments later on). I have to go so will continue this later."},{"username":"gowersjon","timestamp":"2009-03-04T18:51:00.000Z","contents":"Density increment. _4\\. But there is a positive probability that %U_0\\cup FU(Z_1,\\dots,Z_m)\\subset\\mathcal{U}% and %V_0\\cup FU(Z_1,\\dots,Z_m)\\subset\\mathcal{V}.% Moreover, the density of %\\mathcal{B}% inside the subspace %\\langle U_0,V_0;Z_1,\\dots,Z_m\\rangle% (I hope it’s easy to guess what that means) is at most %\\delta+\\eta/2% or we’re done. So, turning things round, there is in fact a positive probability that the conditional probability in 3 is somewhat less than %\\delta+\\eta.%_ This is a bit easier to say in the new language. If %Z_1,\\dots,Z_m% are disjoint sets, let’s write %\\sum_i\\eta_iZ_i% for the sequence fragment that takes the value %\\eta_i% on %Z_i% (so %Z=Z_1\\cup\\dots\\cup Z_m%). And if %x\\in[3]^{[n]\\setminus Z}% then let’s write %x+\\sum_i\\eta_iZ_i% for the extension of this fragment that takes value %x_i% at each %i\\in[n]\\setminus Z%. For any given %Z_1,\\dots,Z_m% let %\\mathcal{A}(Z_1,\\dots,Z_m)% be the intersection of all the sets %\\mathcal{A}(\\sum_i\\eta_iZ_i)%. That is, %\\mathcal{A}(Z_1,\\dots,Z_m)% consists of all %x\\in[3]^{[n]\\setminus Z}% such that %x+\\sum_i\\eta_iZ_i\\in\\mathcal{A}% for every %\\eta_1,\\dots,\\eta_m\\in[3]%. It is easy to check (and I’ve done so — I promise) that %\\mathcal{A}(Z_1,\\dots,Z_m)% is a 12-set. In fact, I think I’ll need to give a quick proof of that later on. At this point we use the multidimensional Sperner type result to say that if we choose %(Z_1,\\dots,Z_m)% at random then the expected density of %\\mathcal{A}(Z_1,\\dots,Z_m)% is positive (meaning bounded below by a positive constant that depends only on %\\delta, \\eta, m% and the sizes of the sets %Z_i).% This is saying that if %\\mathcal{A}% is a dense 12-set, then a positive density of all possible m-dimensional combinatorial spaces (with very small wildcard sets) is contained in %\\mathcal{A}%. This bit is not thoroughly checked, but I don’t think there’s much doubt that it can be with a bit of effort. Now let’s suppose that %x\\in\\mathcal{A}(Z_1,\\dots,Z_m).% Let %U_0% be the 1-set of %x% and let %V_0% be the 2-set of %x%. Then we must have that the union of %U_0% and any subcollection of the %Z_i% belongs to %\\mathcal{U}%, and similarly for %V_0% and %\\mathcal{V}%. And the converse is true too, since %\\mathcal{A}% is a 12-set. Let %\\mathcal{U}(Z_1,\\dots,Z_m)% be the set of all %U_0% such that %U_0\\cup FU(Z_1,\\dots,Z_m)\\subset\\mathcal{U}% (to use my notation above), and similarly for %\\mathcal{V}(Z_1,\\dots,Z_m)%. Then %\\mathcal{A}(Z_1,\\dots,Z_m)% is (naturally identified with) the set of all %x\\in[3]^{[n]\\setminus Z}% with 1-set in %\\mathcal{U}(Z_1,\\dots,Z_m)% and 2-set in %\\mathcal{V}(Z_1,\\dots,Z_m).% So it really is a 12-set in %{}[3]^{[n]\\setminus Z}.% Now let’s fix a choice of %Z_1,\\dots,Z_m% such that %\\mathcal{A}(Z_1,\\dots,Z_m)% has positive density. And let’s write %\\mathcal{A}'% for %\\mathcal{A}(Z_1,\\dots,Z_m)%, and similarly for %\\mathcal{U}'% and %\\mathcal{V}'%. For every %x\\in\\mathcal{A}',% the subspace of all points %x+\\sum\\eta_iZ_i% contains at most %(\\delta+\\eta/2)3^m% points in %\\mathcal{B}% or we have our density increase on a subspace. Therefore, if we choose a random %(\\eta_1,\\dots,\\eta_m)%, the proportion of %x\\in\\mathcal{A}'% such that %x+\\sum\\eta_iZ_i\\in\\mathcal{B}% is on average at most %\\delta+\\eta/2%. So choose %(\\eta_1,\\dots,\\eta_m)% such that this proportion is at most %\\delta+\\eta/2%. Now let’s suppose that %\\mathcal{B}% has near maximal density %\\delta+\\eta% in %\\mathcal{A}(\\sum\\eta_iZ_i)%, which I will denote by %(\\mathcal{U},\\mathcal{V})(\\sum\\eta_iZ_i)%. This set naturally partitions into four parts, according to whether the 1-set/2-set of x belongs/does not belong to %\\mathcal{U}'%/%\\mathcal{V}'%. The density of %\\mathcal{B}% in the %\\mathcal{U}'%—%\\mathcal{V}'% part is too small, so elsewhere it must be too big. That gives us a density increase for %\\mathcal{B}% on a 12-set and we can iterate. All that can go wrong is if %\\mathcal{B}% does not have near maximal density in %\\mathcal{A}(\\sum\\eta_iZ_i)%. But that would have to happen with positive probability, which means that somewhere we would get a density increase. Metacomment. At the moment, as is clear, I haven’t quite got to the point of plunging in and writing things up formally, but the formality is increasing and I’m still not getting any sense that it’s on the point of collapse. This raises questions about how we should proceed if we get to the writing-up stage. The wiki has been great for that, but I think the inconvenience of it would start to bite quite hard if one were actually writing things up in complete detail. (Unfortunately, Luca hasn’t yet written a LaTeX2wiki converter …) What I’d ideally like is to start writing things in LaTeX but in such a way that others can edit it. I’m not sure if that is technically feasible, however. Another possibility would be to write a skeleton version on the wiki, with statements of the main lemmas and things like that, and then work on the proofs in LaTeX. Or I could just go ahead and do what Ryan did and post a link to a pdf, which anybody could comment on, and which I’d happily send to anyone if they wanted to add to it or make changes (though we would then have to know at all times who was in charge of the latest version). Or we could do something like that but split it into a number of subfiles, one for each section. 870\\. Metacomment Perhaps using the wiki for LaTeX may not be that cumbersome, the wiki has the great advantage of being a common repository already, plus it has the tools to compare changes between versions. The idea would be to use the wiki simply as a storing device for the latest common LaTeX version, not a place to edit or view the paper. Each time one user A would like to read and/or change something in the LaTeX (say one section at a time as you suggest), A would simply copy-paste the latest wiki version of that section into a blank LaTeX template file on A’s computer and continue editing and monitoring the pdf locally. When done A would simply: (1) copy-paste that new LaTeX section onto the current wiki version; (2) check whether some changes to that section by some other user B have been made during A’s local editing. If not, then no problem: A’s version is now the common current one and appears as such on the wiki. On the other hand if A sees that a new version by B had appeared in between, then A would quickly edit the wiki of that section and add as first line “merging in progress” (a signal preventing disciplined other users to make further changes). Then A and B would need to dicuss their respective version in the wiki discussion page or a blog thread, reach a satisfactory common one, add it to the wiki, and finally remove the “merging in progress” tag. Since only a few people are working on the project this situation should be fairly rare and localized, so that independent parts of the common overall LaTeX file would progress quickly. Archived versions of the whole pdf file, say one per day, might be storable on a separate blog thread for example."},{"username":"jon","timestamp":"2009-03-05T20:00:00.000Z","contents":"Metacomment Perhaps using the wiki for LaTeX may not be that cumbersome, the wiki has the great advantage of being a common repository already, plus it has the tools to compare changes between versions. The idea would be to use the wiki simply as a storing device for the latest common LaTeX version, not a place to edit or view the paper. Each time one user A would like to read and/or change something in the LaTeX (say one section at a time as you suggest), A would simply copy-paste the latest wiki version of that section into a blank LaTeX template file on A’s computer and continue editing and monitoring the pdf locally. When done A would simply: (1) copy-paste that new LaTeX section onto the current wiki version; (2) check whether some changes to that section by some other user B have been made during A’s local editing. If not, then no problem: A’s version is now the common current one and appears as such on the wiki. On the other hand if A sees that a new version by B had appeared in between, then A would quickly edit the wiki of that section and add as first line “merging in progress” (a signal preventing disciplined other users to make further changes). Then A and B would need to dicuss their respective version in the wiki discussion page or a blog thread, reach a satisfactory common one, add it to the wiki, and finally remove the “merging in progress” tag. Since only a few people are working on the project this situation should be fairly rare and localized, so that independent parts of the common overall LaTeX file would progress quickly. Archived versions of the whole pdf file, say one per day, might be storable on a separate blog thread for example."},{"username":"gil-kalai","timestamp":"2009-03-04T21:12:00.000Z","contents":"Here is a variant on Sperner that I wonder what is the situation for it, and if our discussion is relevant. Supposet that for every k dimensional discrete cube you have a specific “forbidden” pair of maximal distance elements. E.g. for k=5 you can exclude the pair {(10100),(010111)} How large can be a subset of {0,1}^n so that for every k whenevr you fix n-k coordinates you do not have the forbidden pair for the remaining k coordinates. (The case that for every k you forbid {(000…),(11,..,1)} is Sperner. we can try even to forbid pairs which depend on the identity of the n-k coordinates or on the contant of the n-k coordinates (but not both as a random large density subset shows)."},{"username":"gowers","timestamp":"2009-03-06T01:28:00.000Z","contents":"Progress report. No time to do anything more today. This is to say that I’ve written a [skeleton proof on the wiki](http://michaelnielsen.org/polymath1/index.php?title=An_outline_of_a_density-increment_argument) and will try to flesh it out in the near future."},{"username":"ryan-odonnellryan-odonnellgil-kalai","timestamp":"2009-03-07T02:39:00.000Z","contents":"Passing between measures. I noted that Tim’s skeleton proof on the wiki at one point sketches why, if you have density %\\delta% in the equal-slices measure, you can pass to a largish combinatorial subspace on which you have density %\\geq \\delta - o(1)% in the uniform measure. I wrote up this proof carefully [on the wiki](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures). I tried to make it crisp; hopefully that didn’t introduce major mistakes. Nothing unexpected happened; the result is that you can pass to a subspace of dimension at least %\\epsilon n% while losing only %O(\\epsilon \\sqrt{n}) + \\exp(-\\Omega(\\epsilon n))% additively in the density. If someone wants to amuse themselves, they can try to evaluate this quantity that arises: %(1/k^2) \\int_{p_1 + \\cdots + p_k = 1} \\sqrt{\\sum_{i=1}^k 1/p_i - k^2}%. It’s not hard to check that this is %O(\\sqrt{k})% (I think), but is it in fact %O(1)%? 871.1\\. In the amusement, it should be %1/k% out front, not %1/k^2%. 871.2 2 questions: How general is the passing between measure phenomenon goes. When there is a group action say for the cup set problem, whenever you prove a bound for any measure the same bound apply to the uniform measure by averaging. What is the situation here? It looks ok for moving from equal slice to uniform (when you pass the a larger subspace) but how general measures you can start with. I am probably trying to get away with not reading some detailed proofs and missed some crucial postings, but let me still ask: on the very conceptual and general level, what is the main ingredient that allows fourier proofs to work after all the initial examples of various sets with irregular number of lines and no large fourier coefficients?"},{"username":"ryan-odonnell","timestamp":"2009-03-07T02:42:00.000Z","contents":"871.1\\. In the amusement, it should be %1/k% out front, not %1/k^2%."},{"username":"gil-kalai","timestamp":"2009-03-07T03:50:00.000Z","contents":"871.2 2 questions: How general is the passing between measure phenomenon goes. When there is a group action say for the cup set problem, whenever you prove a bound for any measure the same bound apply to the uniform measure by averaging. What is the situation here? It looks ok for moving from equal slice to uniform (when you pass the a larger subspace) but how general measures you can start with. I am probably trying to get away with not reading some detailed proofs and missed some crucial postings, but let me still ask: on the very conceptual and general level, what is the main ingredient that allows fourier proofs to work after all the initial examples of various sets with irregular number of lines and no large fourier coefficients?"},{"username":"gowersgil","timestamp":"2009-03-07T04:07:00.000Z","contents":"Fourier etc. Gil, I’m not quite certain to which Fourier proofs you are referring. But if you mean how can a density-increment strategy have a chance of working if it is not the case that the wrong number of lines implies a large Fourier coefficient, then the answer is (i) that one can keep localizing to subspaces and (ii) that one can directly prove correlation with a 12-set (on a suitable subspace) rather than via some kind of expansion. If you are referring to Fourier approaches to Sperner, the answer (depending on which approach you are talking about — both are written up on the wiki) is either that some non-Fourier ingredients are included, or that equal-slices measure is used so that some of the troublesome examples no longer work. Basically though, if I had to choose a one-word answer to your question, it would be “localization”. 872.1 (just trying to catch up) I suspect this “localization” means that at the end the emerging argument has a strong (probably central) ingredient of Szemeredi-like-regularity-lemma; so we have a distinction between Roth-like-density-increment and Szemeredi-like-density-increment."},{"username":"gil","timestamp":"2009-03-19T02:23:00.000Z","contents":"872.1 (just trying to catch up) I suspect this “localization” means that at the end the emerging argument has a strong (probably central) ingredient of Szemeredi-like-regularity-lemma; so we have a distinction between Roth-like-density-increment and Szemeredi-like-density-increment."},{"username":"gowersgowers","timestamp":"2009-03-07T04:35:00.000Z","contents":"Probable collapse. I was beginning to think that the argument [sketched out on the wiki](http://michaelnielsen.org/polymath1/index.php?title=An_outline_of_a_density-increment_argument&oldid=658) had the ring of truth about it. But as so often happens the effort of writing it up has thrown up a problem that seems serious. I’ve only just realized it, so I’m not sure that it can’t be got round by some sort of trick, or more elaborate argument, but at the moment it feels to me as though a new idea is still needed. Here, in brief, is the difficulty, which I noticed when trying to write up Step 5 in full detail. The basic idea, which I got from Terry’s 837.2, was to have _two_ iterations going on, one in which one tries to get a density increase on a subspace, and the other in which one tries to get a density increase on a 12-set. The problem, which escaped me before, is that the density increase on the 12-set depends on the density of that 12-set. (It comes from the Varnavides density of multidimensional subspaces that you can find in the 12-set.) So there is no guarantee that this inner iteration will terminate. And unfortunately, at least as the argument goes at the moment, the density of the 12-set may drop quite a bit when one does the inner iteration. I’ll probably think a bit more about whether something can be done about this, but I think progress is more likely to come either from returning to the Ajtai-Szemerédi template or from a new approach to getting from increased density on a 12-set to increased density on a subspace. Incidentally, one might wonder whether Terry’s argument has the same problem, but it doesn’t, and the reason it doesn’t is instructive. He makes use of the principle that if you have a function f defined on a set X and can find a small subset Y where f is less dense than average, then you can remove that subset from X. If Y is small, then you will get only a small density increase on the complement of Y, but you will have removed only a small set from X so things are OK. Now contrast that with a situation where you have a function %f% defined on a set %X_1\\times X_2% and it is less dense on a subset %Y_1\\times Y_2%. By averaging, we get a density increase on one of %X_1\\times(X_2\\setminus Y_2),% %(X_1\\setminus Y_1)\\times Y_2% or %(X_1\\setminus Y_1)\\times (X_2\\setminus Y_2)%. However, if %Y_1% and %Y_2% both have density %\\epsilon% and the density increase is in the last of these sets, then we get a density increase that’s proportional to %\\epsilon^2% but have to drop to a subset of size around %(1-2\\epsilon)% times the size of the original set. And this is too expensive if you do it over and over again. Basically the difference is that %(1-1/n)^n% converges to a positive limit while %(1-1/n)^{n^2}% converges to zero. 873.1 Actually, the situation is slightly worse: I now see that the problem that I discussed in the last paragraph above is precisely the problem that Terry was talking about in 866 and Randall in 864.2\\. A question that occurred to me just now was whether one might be able to deal with low-complexity members of the sigma-algebra of 12-sets. But I now see that that still creates problems, since then we don’t get a multidimensional Sperner-type theorem that just depends on the density of the set (or rather, finding such a result doesn’t seem any easier than DHJ(3) itself)."},{"username":"gowers","timestamp":"2009-03-07T05:08:00.000Z","contents":"873.1 Actually, the situation is slightly worse: I now see that the problem that I discussed in the last paragraph above is precisely the problem that Terry was talking about in 866 and Randall in 864.2\\. A question that occurred to me just now was whether one might be able to deal with low-complexity members of the sigma-algebra of 12-sets. But I now see that that still creates problems, since then we don’t get a multidimensional Sperner-type theorem that just depends on the density of the set (or rather, finding such a result doesn’t seem any easier than DHJ(3) itself)."},{"username":"terence-taogowers","timestamp":"2009-03-07T06:32:00.000Z","contents":"Unfortunately I won’t have much time to devote to this project for the next few days, but one possible way around the problem may be to factor the density increment into two pieces. Suppose that f has a density increment on %X_1 \\times X_2%, then roughly speaking this means that %f 1_{X_1}% has a density increment on %X_2%. One could try to hold %X_1% fixed and start passing to subspaces to increment the density of %f 1_{X_1}% until it reaches some saturation point, and then f would have some sort of “relative density increment” on %X_1%, and then one could pass to subspaces again to clean up %X_1%. This is terribly vague and there are a large number of issues, including the fact that 1-sets and 2-sets are not independent (so the analogy with Cartesian products is slightly misleading), though perhaps some preliminary regularity lemma type arguments might deal with that problem. 874.1 Terry, I don’t think I understand your suggestion even vaguely. For instance, if in the %\\mathbb{Z}_n^2% world we choose random sets %V,X_1,X_2\\subset\\mathbb{Z}_n% and take all points %(x,y)\\in X_1\\times X_2% such that %x+y\\in V%, then we have a density increase on %X_1\\times X_2%, but it doesn’t seem to be possible to do much by looking at each variable separately."},{"username":"gowers","timestamp":"2009-03-07T16:18:00.000Z","contents":"874.1 Terry, I don’t think I understand your suggestion even vaguely. For instance, if in the %\\mathbb{Z}_n^2% world we choose random sets %V,X_1,X_2\\subset\\mathbb{Z}_n% and take all points %(x,y)\\in X_1\\times X_2% such that %x+y\\in V%, then we have a density increase on %X_1\\times X_2%, but it doesn’t seem to be possible to do much by looking at each variable separately."},{"username":"gowers","timestamp":"2009-03-07T16:05:00.000Z","contents":"875. Now that this has happened, I want to suggest another question, which I had been slightly suppressing. It was always a bit of a worry that we were trying to get from a density increase on a 12-set to a density increase on a subspace when we did not know an analogous argument in %{}[n]^2.% I had persuaded myself that this might be OK because we were passing to combinatorial subspaces instead of long APs, but in retrospect that was not a very convincing argument. So my question is this. Suppose we allow ourselves to use Szemerédi’s theorem as a black box and we have a subset %A\\subset[n]^2% of density %\\delta% that correlates with a dense Cartesian product %X\\times Y%. Can we get a density increase on a grid (defined to be a product %P\\times Q%, where %P% and %Q% are APs with the same common difference and length tending to infinity)? If we use arguments that would also prove a functional version, then it’s fairly easy to prove that what we are really trying to do here is prove that the characteristic function of %X\\times Y% can be approximated by a positive linear combination of characteristic functions of grids. Roughly, the argument goes like this. If you can’t approximate it (in %L_1%) then the Hahn-Banach theorem gives you a bounded function %f% that has average at most zero on all grids but average 1 on %X\\times Y%. Taking %g% to be %\\delta+\\eta f% for some small %\\eta% we get a function with a density increase on %X\\times Y% but no density increases on grids. Conversely, if the approximation is possible, then an easy averaging argument gets you from a density increase on %X\\times Y% to a density increase in one of the grids. In one dimension, the analogous question is easy to answer. If you have a dense subset %X\\subset[n]% then you can apply Szemerédi’s theorem to find an arithmetic progression %P\\subset X%. If you remove %P% from %X% you still have a dense set, so can repeat. Keep going until you’re left with just a small proportion of %P% and you have then approximated %P% by a union of arithmetic progressions. The problem in two dimensions is that while it is not too hard to find a single grid inside %X\\times Y% (by averaging find %s% such that %X\\cap(Y+s)% is dense and apply Szemerédi’s theorem), when you remove this grid, you no longer have a Cartesian product. And perhaps it is also worth mentioning that the trivial argument does not work: if you approximately partition both %X% and %Y% into APs and take their Cartesian products, you won’t get grids, because the common differences in the two directions will not necessarily be the same. It is for this reason that I think our best hope is to go back to Ajtai-Szemerédi, because they avoided this problem."},{"username":"gowers","timestamp":"2009-03-07T22:24:00.000Z","contents":"Ajtai-Szemerédi My natural instinct is to go off and have a hard think about how to adapt the Ajtai-Szemerédi proof, but I want to try to do things the polymath way, which may well mean a few comments that are vague, or go nowhere, or where I can’t quite explain properly what I’m trying to do. However, here goes. What we appear to be able to do is choose %W% such that there are many pairs %(U,V)% with %(U,V,W)\\in\\mathcal{A}%, and with the additional property that for almost all of the %U% involved in those pairs we have almost the expected density of %(V',W')% such that %(U,V',W')\\in\\mathcal{A}%. Having fixed such a %W%, we can also find a combinatorial subspace in %{}[2]^n% consisting entirely of %U% with both properties. What we would like to do at that point is say that there are many combinatorial subspaces in %{}[3]^n% that project down to this combinatorial subspace in %{}[2]^n%. Temporarily forgetting that last paragraph, let’s rephrase the Ajtai-Szemerédi argument as follows. By the usual averaging argument, we may assume that almost all grids have almost the right density. But if we choose a random grid, then with positive probability we find that it has a positive proportion of horizontal lines that are forbidden, because all the vertical lines point to points in a dense diagonal, and several horizontal lines point to points in the same dense diagonal. In other words, with positive probability, we get a density increment on a 1-set rather than a 12-set. I have to go for a little while, but I’m going to think along these lines for a bit."},{"username":"randallgowersrandall","timestamp":"2009-03-07T23:28:00.000Z","contents":"More speculation I had a look on the wiki at the Ajtai-Szem proof page (I am somewhat embarrassed to admit I had never seen this proof before), and I have a few initial thoughts that may or may not lead in a positive direction. First, I don’t see how multidimensional DHJ (2) can be the analog for Szemeredi’s theorem, given the disparity in depth between the results. In particular, it seems to me that whatever the analog for Szem is going to be, it has got to be a multiple recurrence theorem. I can’t think of what that would be in the DHJ setup, though. Now, as it happens I have long been pushing for a reduction of the problem to something else anyway, so thinking along these lines, here is a general proof strategy. Step 1\\. Reduce DHJ (3) to IP Roth using the “density increment on a subspace for a set correlated with a 12-set” strategy I tried to outline in a speculative fashion in 864\\. (This step is indeed speculative; I have no idea whether or how it can work.) Step 2\\. Reduce IP Roth to “IP* Szemeredi” via the Ajtai-Szemeredi argument. (I have not thought about this at all. It might be really easy, though, if the analogs are the right ones, given how easy the Ajtai-Szem argument is.) Step 3\\. Come up with a combinatorial proof of IP* Szemeredi. (No real idea how this might go….) IP* Szemeredi: Let %\\delta>0, k\\in {\\bf N}%. There is an %n% such that if %A\\subset [n]^{[n]}% with %|A|>\\delta n^n% then %A% contains a configuration of the form %\\{f+i\\cdot 1_\\alpha: 0\\leq i for some %\\alpha\\subset [n]% and some %f:[n]\\rightarrow [n]%. 877.1 Randall, that first point of yours is one I’ve mentioned a couple of times, but I haven’t really properly justified my view that it shouldn’t be a problem. So I’m going to think about it now. I may well end up deciding that it is a problem after all. 877.2 Well there is certainly no reason not to uses subspaces in that way; in fact it’s part of the Furstenberg-Katznelson argument (cf. first three lines of p. 7 of my notes). Something different seems to be going on in the Ajtai-Szemeredi argument, though, where Szemeredi’s theorem seems to be doing virtually all of the work."},{"username":"gowers","timestamp":"2009-03-07T23:46:00.000Z","contents":"877.1 Randall, that first point of yours is one I’ve mentioned a couple of times, but I haven’t really properly justified my view that it shouldn’t be a problem. So I’m going to think about it now. I may well end up deciding that it is a problem after all."},{"username":"randall","timestamp":"2009-03-08T07:44:00.000Z","contents":"877.2 Well there is certainly no reason not to uses subspaces in that way; in fact it’s part of the Furstenberg-Katznelson argument (cf. first three lines of p. 7 of my notes). Something different seems to be going on in the Ajtai-Szemeredi argument, though, where Szemeredi’s theorem seems to be doing virtually all of the work."},{"username":"gowers","timestamp":"2009-03-07T23:42:00.000Z","contents":"Ajtai-Szemerédi Hmm, what I said above is actually _not_ the Ajtai-Szemerédi argument because they don’t choose a random grid like that. Instead, they choose a single set of vertical lines and a random grid that runs across in the other direction. So let’s think more like that. I’ll start by ignoring the requirement that the vertical lines have to have roughly the right density. Moving over to the %{}[3]^n% world, I find my “dense diagonal” — that is, a %W% such that many %(U,V,W)% belong to %\\mathcal{A}%. Now I want to look for a combinatorial subspace with a good property of some kind. What should that property be? Let %\\mathcal{U}% be the set of all %U% such that %(U,[n]\\setminus(U\\cup W),W)\\in\\mathcal{A}% and let %\\mathcal{V}% be the set of all %V% such that %([n]\\setminus(V\\cup W),V,W)\\in\\mathcal{A}.% Then a good property would be if all the points in the combinatorial subspace had their 1-sets in %\\mathcal{U}%. Then we would know that no point with its 2-set in %\\mathcal{V}% could belong to %\\mathcal{A}%, or something along those lines, which would give us correlation with a 2-set. That would give us a density increase as long as we also knew that the density of %\\mathcal{A}% in the combinatorial subspace was almost maximal. I’m finding this boringly hard to do on screen. I’ll allow myself a little bit of offline time to try to clarify what I’m saying."},{"username":"gowers","timestamp":"2009-03-08T02:44:00.000Z","contents":"Correlation with 1-sets Nothing conclusive to report, so instead I want to revisit the question of showing that correlation with a 1-set implies a density increase on a subspace. In particular, I want to get a feel for whether the double iteration is necessary. It occurs to me that a Hahn-Banach argument ought to prove that the question is equivalent to showing that the characteristic function of a 1-set can be approximated in %L_1% by a positive linear combination of characteristic functions of subspaces. At some point I’ll check that, and maybe even put it on the wiki, but for now I’ll assume it. So how should we write an arbitrary dense 1-set as a positive combination of subspaces? I think I see a way. No time to be fully detailed, but I’m aiming for something that’s equivalent to what we did above. The first step would be to choose small wildcard sets %(Z_1,Z_2,\\dots,Z_m)% such that a positive proportion (depending on m) of the combinatorial subspaces with those wildcard sets are subsets of the given 1-set. All these combinatorial subspaces are disjoint, so we can safely remove them. Let %Z=Z_1\\cup\\dots\\cup Z_m% and partition the rest of %{}[3]^n% according to how points restrict to %Z%. For every sequence %z\\in[3]^Z%, let %\\mathcal{U}(z)% be the set of all sequences %y\\in[3]^{[n]\\setminus Z}% such that %(y,z)% belongs to the 1-set %\\mathcal{U}%. Also, let %\\mathcal{U}'% be the set of all %y\\in[3]^{[n]\\setminus Z}% such that %(y,z)\\in\\mathcal{U}% for every %z% that is constant on all the %Z_i%. The precise partition we shall take is this. For each %z% that is _not_ constant on all the %Z_i% we take the set %\\mathcal{U}(z)%. And for each %z% that _is_ constant on all %Z_i% we take the set %\\mathcal{U}(z)\\setminus\\mathcal{U}'(z)%. This has given us a 1-set inside each combinatorial subspace obtained by fixing the coordinates in %Z%. And the average density of those 1-sets is down by a small factor (depending on m) from the density of the original 1-set. So we can iterate the procedure. I think that’s a nice clean way of presenting the Randall/Terry result about 1-sets. Of course, it’s still using something similar to the double iteration."},{"username":"jozsefgowersjozsef","timestamp":"2009-03-08T04:55:00.000Z","contents":"Shelah’s v.s. DHJ It seems to be a serious difficulty to follow the double iteration in a density incremental argument. It might be helpful to check what would be a Shelah-like density proof look like for k=3\\. The first step is actually the same; prove that for any 2-colouring of %3^{[n]}% there are (many) “flip-flop” subspaces. A d-dimensional subspace of %3^{[n]}% can be represented by d classes of wildcards, the elements from same class always have the same characters. Two elements in the subspace are neighbours if they differ only in one wildcard class where one is 2 and the other is 1\\. The subspace is flip-flop if any pair of neighbours have the same colour. The second step would be to show that there is a monochromatic line in %2^{[d]}%. In our case it would mean to show that there is a flip-flop d-subspace with at least %c_d3^d% elements where we allow %c_d% to go to 0 slowly. There are two advantages; first that we allow %c_d% to go to 0, second that the number of flip-flop subspaces is independent of the original density as it follows from the two colouring of %3^{[n]}%.  \nThis looks quite promising to me, but let me check first what did I write here…  \nThe first part of Shelah’s proof shows that 880.1 Jozsef, can you explain where the 2-colourings come in? The points are red or blue depending if they are in our dense set or not. I will write more details soon."},{"username":"gowers","timestamp":"2009-03-08T05:54:00.000Z","contents":"880.1 Jozsef, can you explain where the 2-colourings come in?"},{"username":"jozsef","timestamp":"2009-03-08T06:19:00.000Z","contents":"The points are red or blue depending if they are in our dense set or not. I will write more details soon."},{"username":"jozsefjozsefjozsefjozsefjozsef","timestamp":"2009-03-08T06:39:00.000Z","contents":"880.2 For DHJ, we say that a subspace is flip-flop if there are no neighbours that one is in our set and the other isn’t. (One might think that we can’t gain anything from this if most of the pairs are not from the set, however we will never make any statistics on the number of neighbour pairs inside or outside of our set in a subspace.) To prove that there are many flip-flop subspaces we can follow the original colouring proof; Colour every element of our dense subset by red and the points in the complement by blue. I will try to find a link to the proof or I will write it up myself. Then, the Varnavides type argument gives many flip-flop subspaces. The number of d-dimensional flip-flop subspaces is independent of the density of our pointset, but it certainly depends on the dimension d. The second observation is that every flip-flop subspace is sparse or there is a line. I think I should write up this part. 880.2 For the proof of the existence of flip-flop subspaces I have find two books on Google, “Ramsey Theory” by Ronald L. Graham, Bruce L. Rothschild, Joel H. Spencer, and Jukna’s “Extremal Combinatorics”. There is a nice paper A. Nilli, “Shelah’s proof of the Hales–Jewett theorem” , Mathematics of Ramsey theory (Algorithms Combin.) , 5 , Springer (1990) pp. 150–151, but I was unable to find it online. I think that the original name was “fliptop” for a colouring of a subspace where top neighbours received the same colour, but the top isn’t special, the bottom pair would work as well, so I’ve changed it to flip-flop as it’s more appropriate (and funny) All proofs I know for the existence of flip-flop subspaces are recursive. (see the references above) For a d-dimensional flip-flop subspace one needs the recursion %r_{1+1}=r^2_i3^{r_i}% with %r_1=3%. n should be at least %r_1+r_2+...+r_d% to guarantee a d-dimensional flip-flop subspace in %3^{[n]}%. This was also the type of proof I knew for the Sperner subspace theorem, but checking Tim’s write up in the Wiki, I realized that his proof is somewhat different. Well, Tim’s proof in the Wiki isn’t significantly different from the “traditional” proof but it’s elegantly written. Note that the recursive proofs give very uneven subspaces; the sizes of wildecards are increasing recursively as well. It isn’t a problem when one considers HJ where every point has its colour, however this property makes it difficult the use of such subspaces for density problems. After the second reading of Tim’s proof I see now that one can choose p, q, and %\\delta% that one can get “balanced” subspaces."},{"username":"jozsef","timestamp":"2009-03-08T06:58:00.000Z","contents":"880.2 For the proof of the existence of flip-flop subspaces I have find two books on Google, “Ramsey Theory” by Ronald L. Graham, Bruce L. Rothschild, Joel H. Spencer, and Jukna’s “Extremal Combinatorics”. There is a nice paper A. Nilli, “Shelah’s proof of the Hales–Jewett theorem” , Mathematics of Ramsey theory (Algorithms Combin.) , 5 , Springer (1990) pp. 150–151, but I was unable to find it online. I think that the original name was “fliptop” for a colouring of a subspace where top neighbours received the same colour, but the top isn’t special, the bottom pair would work as well, so I’ve changed it to flip-flop as it’s more appropriate (and funny)"},{"username":"jozsef","timestamp":"2009-03-08T08:54:00.000Z","contents":"All proofs I know for the existence of flip-flop subspaces are recursive. (see the references above) For a d-dimensional flip-flop subspace one needs the recursion %r_{1+1}=r^2_i3^{r_i}% with %r_1=3%. n should be at least %r_1+r_2+...+r_d% to guarantee a d-dimensional flip-flop subspace in %3^{[n]}%. This was also the type of proof I knew for the Sperner subspace theorem, but checking Tim’s write up in the Wiki, I realized that his proof is somewhat different."},{"username":"jozsef","timestamp":"2009-03-08T09:46:00.000Z","contents":"Well, Tim’s proof in the Wiki isn’t significantly different from the “traditional” proof but it’s elegantly written. Note that the recursive proofs give very uneven subspaces; the sizes of wildecards are increasing recursively as well. It isn’t a problem when one considers HJ where every point has its colour, however this property makes it difficult the use of such subspaces for density problems."},{"username":"jozsef","timestamp":"2009-03-08T11:41:00.000Z","contents":"After the second reading of Tim’s proof I see now that one can choose p, q, and %\\delta% that one can get “balanced” subspaces."},{"username":"gowersterence-taogowers","timestamp":"2009-03-08T06:51:00.000Z","contents":"Correlation with 12-sets I’ve got to go to bed, but an idea has occurred to me. Maybe I’ll see by the morning that it’s nonsense. But a 12-set is just an intersection of a 1-set with a 2-set. So maybe one can use 879 to partition (almost all of) the 1-set into subspaces, and then use 879 again to partition the intersection of the 2-set with each of those subspaces into further subspaces, thereby ending up with a partition of the 12-set into subspaces. For Cartesian products in grids it would work like this. Given %X\\times Y%, you first partition %X\\times[n]% into grids, which is easy. And then inside each of those grids you partition the intersection of that grid with %{}[n]\\times Y% into further grids. If that second argument is correct then (i) I don’t know why I didn’t spot it before and (ii) it suggests that the first one has a good chance of being correct. And if the first one is correct, it seems to do DHJ(3). Off to bed while this still feels good … 881.2 This looks like it would work in the %{}[n]^2% world, and give an answer to your 875 (and would also formalise my 874, for that matter). The one thing to bear in mind is that Szemeredi allows one to take the spacing of the long arithmetic progressions in X or in Y to be of size O(1) rather than O(n), by working locally. (Meanwhile, the length of the progressions is something like %{}\\log \\log \\log \\log \\log \\log \\log n%.) That way, you don’t lose too much when taking the GCD of two different spacings. Of course, in the Hales-Jewett world, we don’t have GCD, but the trick of rendering a few coordinates “bad” and working with local 1-sets, etc. rather than global ones may help. (We may eventually have to also break out the Ramsey theory to make the local statistics match the global statistics; this is related to Furstenberg-Katznelson’s “strong stationarity” which, after talking to Tim Austin a bit, I suspect we may have to exploit to finish off this problem.) 881.3 Terry, I think I don’t need to worry about GCDs of spacings. In the corners world I just partition a 1-set into grids, and then the restriction of the 2-set to each grid is still a 2-set, so I partition its restriction to each grid into further grids. What’s more, this can be seen as a sort of dualized version of what Ajtai and Szemerédi do themselves. I’ve woken up still feeling very good about this, and plan to get wikifying straight away."},{"username":"terence-tao","timestamp":"2009-03-08T07:51:00.000Z","contents":"881.2 This looks like it would work in the %{}[n]^2% world, and give an answer to your 875 (and would also formalise my 874, for that matter). The one thing to bear in mind is that Szemeredi allows one to take the spacing of the long arithmetic progressions in X or in Y to be of size O(1) rather than O(n), by working locally. (Meanwhile, the length of the progressions is something like %{}\\log \\log \\log \\log \\log \\log \\log n%.) That way, you don’t lose too much when taking the GCD of two different spacings. Of course, in the Hales-Jewett world, we don’t have GCD, but the trick of rendering a few coordinates “bad” and working with local 1-sets, etc. rather than global ones may help. (We may eventually have to also break out the Ramsey theory to make the local statistics match the global statistics; this is related to Furstenberg-Katznelson’s “strong stationarity” which, after talking to Tim Austin a bit, I suspect we may have to exploit to finish off this problem.)"},{"username":"gowers","timestamp":"2009-03-08T12:27:00.000Z","contents":"881.3 Terry, I think I don’t need to worry about GCDs of spacings. In the corners world I just partition a 1-set into grids, and then the restriction of the 2-set to each grid is still a 2-set, so I partition its restriction to each grid into further grids. What’s more, this can be seen as a sort of dualized version of what Ajtai and Szemerédi do themselves. I’ve woken up still feeling very good about this, and plan to get wikifying straight away."},{"username":"gowersrandall","timestamp":"2009-03-08T15:10:00.000Z","contents":"Progress report I am in the middle of wikifying the latest DHJ(3) attempt. This time I would actually be prepared to put money on the argument working (unlike last time, when there were too many slightly complicated bits that I felt I didn’t fully understand). So far, I’ve written up a [new proof of the corners theorem](http://michaelnielsen.org/polymath1/index.php?title=Modification_of_the_Ajtai-Szemerédi_argument) to serve as a template for the new DHJ(3) argument. The new proof of the corners theorem is not totally new: it is more like a reorganization of the ideas that go into the Ajtai-Szemerédi argument. Nevertheless, it simplifies things in a way that is crucial for the DHJ(3) proof. 882.1 I have looked at your new proof of corners and it really does make less mysterious what Szemeredi is doing. When I initially read the original proof of Ajtai/Szemeredi yesterday, it struck me that Szemeredi’s theorem was being used not once but twice…what was confusing was that it was used once on the diagonal, then again on one of the coordiates. The use on the diagonal gave an impression that DHJ (2) would go proxy for it in the DHJ (3) case. This struck me (everyone else too, I gather) as odd, given the disparity in depth, etc…. Something had to explain the fact that one wasn’t doing something about “compactness relative to the diagonal” or some such, and Szemeredi was the only culprit on offer. Your proof uses Szemeredi’s theorem twice also, once on each of the coordinates; indeed, it now appears that what Szemeredi’s theorem is actually doing in that proof is going proxy for a relative compactness over the diagonal notion. And, in the DHJ (3) case, what you have in mind to fill in here is the idea of partitioning a dense 1 set into dense subspaces, if I understand correctly. (So it’s that which corresponds to the use of Szemeredi after all, not DHJ (2)). And this isn’t all that surprising anymore, given that the proof of that seemed to involve (at least at the very superficial level I understand it at) a look at the decomposition over the diagonal. Aesthetically, all of this seems dead on, so I will not take your bet (and will indeed be quite depressed if something else is amiss)."},{"username":"randall","timestamp":"2009-03-08T20:38:00.000Z","contents":"882.1 I have looked at your new proof of corners and it really does make less mysterious what Szemeredi is doing. When I initially read the original proof of Ajtai/Szemeredi yesterday, it struck me that Szemeredi’s theorem was being used not once but twice…what was confusing was that it was used once on the diagonal, then again on one of the coordiates. The use on the diagonal gave an impression that DHJ (2) would go proxy for it in the DHJ (3) case. This struck me (everyone else too, I gather) as odd, given the disparity in depth, etc…. Something had to explain the fact that one wasn’t doing something about “compactness relative to the diagonal” or some such, and Szemeredi was the only culprit on offer. Your proof uses Szemeredi’s theorem twice also, once on each of the coordinates; indeed, it now appears that what Szemeredi’s theorem is actually doing in that proof is going proxy for a relative compactness over the diagonal notion. And, in the DHJ (3) case, what you have in mind to fill in here is the idea of partitioning a dense 1 set into dense subspaces, if I understand correctly. (So it’s that which corresponds to the use of Szemeredi after all, not DHJ (2)). And this isn’t all that surprising anymore, given that the proof of that seemed to involve (at least at the very superficial level I understand it at) a look at the decomposition over the diagonal. Aesthetically, all of this seems dead on, so I will not take your bet (and will indeed be quite depressed if something else is amiss)."},{"username":"jozsefgowers","timestamp":"2009-03-09T00:45:00.000Z","contents":"Strong Sperner It is very possible that by now Tim is just polishing the write up of a combinatorial DHJ, but still let me go back to the unevenness of multidimensional Sperner or flip-flop subspaces which one can get by recursive arguments. It would be better to have a control on the arithmetic structure of such subspaces. In his Wiki article Tim describes a strong version of multidimensional Sperner. Unfortunately the argument there uses DHJ what we don’t want to use. On the other hand we might get a similar result by using multidimensional Szemeredi. Given a dense subset of %2^{[n]}% denoted by A. Take a random permutation of [n]. An element of A is “d-nice” if it consists of d intervals, each has length %n/2d\\pm \\sqrt{n}%, and each interval begins at position %id% for some %0\\leq i\\leq n/d%. (Suppose that d divides n) Any interval like this can be represented as a point in a d-dimensional %[\\sqrt{n}]^d% cube. If it’s dense then multidimensional Szemeredi gives us a strong Sperner. Metacomment: I wouldn’t say I’d got to the polishing stage exactly, but if you want to see what’s going on it’s [here](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument). At the moment, I simply don’t see anywhere where it can go wrong, but I’ve had that feeling about wrong proofs in the past, so I won’t feel entirely happy until I’ve got a bit further. However, the main point of this comment is to say that I strongly support your idea of looking at flip-flop subspaces. What I would really like is for the polymath collaboration to produce a polyproof. If the Ajtai-Szemerédi approach works, then that’s just the start: I’d like a triangle-removal approach, a Shkredov approach, and an ergodic approach, and if a Shelah-influenced approach is potentially feasible too then I’m very interested."},{"username":"gowers","timestamp":"2009-03-09T01:06:00.000Z","contents":"Metacomment: I wouldn’t say I’d got to the polishing stage exactly, but if you want to see what’s going on it’s [here](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument). At the moment, I simply don’t see anywhere where it can go wrong, but I’ve had that feeling about wrong proofs in the past, so I won’t feel entirely happy until I’ve got a bit further. However, the main point of this comment is to say that I strongly support your idea of looking at flip-flop subspaces. What I would really like is for the polymath collaboration to produce a polyproof. If the Ajtai-Szemerédi approach works, then that’s just the start: I’d like a triangle-removal approach, a Shkredov approach, and an ergodic approach, and if a Shelah-influenced approach is potentially feasible too then I’m very interested."},{"username":"ryan-odonnelljozsefryan-odonnellgowersryan-odonnell","timestamp":"2009-03-09T01:05:00.000Z","contents":"Wikification. Here is a short writeup of the [multidimensional Sperner stuff](http://www.cs.cmu.edu/~odonnell/multidim-sperner.pdf) roughly following Terry.860, as used in Tim.879\\. I will wikify it soon. In fact, this was more or less already on the wiki, in Tim’s [latest additions](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument). Here are the parameters: Let %A \\subseteq [2]^n% have density %\\delta% and let %Y_1, \\dots, Y_d% be arbitrary disjoint subsets of %[n]% of cardinality %r \\geq (1/\\delta)^{O(2^d)}%. Choose a random nondegenerate %d%-dimensional subspace as follows. For each %i%, choose a random nondegenerate combinatorial line in %[2]^{Y_i}%, uniformly from the %3^r - 1% possibilities. (Actually, you can choose the line from virtually any reasonable distribution.) Form the final subspace by taking the Cartesian product of these lines, and then filling in the coordinates outside the %Y_i%‘s uniformly at random. Then this entire subspace is contained in %A% with probability at least %\\exp(-\\exp(O(1) \\ln(1/\\delta) 2^d))%. The %O(1)% can more or less be %2%. 884.1\\. It is a nice write up! There is a gap between the upper and lower bounds in Gunderson-Rodl-Sidorenko. As I remember, the density is between %n^{-1/2^d}% and %n^{-d/2^d}%. Ryan, do you think that you can close the gap? 884.2\\. Done; it’s [here](http://michaelnielsen.org/polymath1/index.php?title=Sperner%27s_theorem). I’d change Tim’s writeup to point to it, but he seems to be editing it currently <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> 884.3 I’ve added a link. (I’ve kept what I wrote too because I quite like having different styles of explanation on the wiki, even of the same result.) 884.4\\. Hi Joszef — not sure about closing the gap… Actually, as written it doesn’t quite even match [GDR]: it requires density approximately %n^{-1/2^{d+1}}% rather than their %n^{-1/2^d}%."},{"username":"jozsef","timestamp":"2009-03-09T01:23:00.000Z","contents":"884.1\\. It is a nice write up! There is a gap between the upper and lower bounds in Gunderson-Rodl-Sidorenko. As I remember, the density is between %n^{-1/2^d}% and %n^{-d/2^d}%. Ryan, do you think that you can close the gap?"},{"username":"ryan-odonnell","timestamp":"2009-03-09T01:38:00.000Z","contents":"884.2\\. Done; it’s [here](http://michaelnielsen.org/polymath1/index.php?title=Sperner%27s_theorem). I’d change Tim’s writeup to point to it, but he seems to be editing it currently <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span>"},{"username":"gowers","timestamp":"2009-03-09T02:04:00.000Z","contents":"884.3 I’ve added a link. (I’ve kept what I wrote too because I quite like having different styles of explanation on the wiki, even of the same result.)"},{"username":"ryan-odonnell","timestamp":"2009-03-09T02:06:00.000Z","contents":"884.4\\. Hi Joszef — not sure about closing the gap… Actually, as written it doesn’t quite even match [GDR]: it requires density approximately %n^{-1/2^{d+1}}% rather than their %n^{-1/2^d}%."},{"username":"gowersryan-odonnellgowers","timestamp":"2009-03-09T04:54:00.000Z","contents":"DHJ(3) I’m now pretty confident that the modified-Ajtai-Szemerédi-based approach to DHJ(3) is in the bag. I have a complete informal write-up [on this page of the wiki](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument), though some of the ingredients (such as getting a density increment on a 12-set in a subspace) are on other pages. A certain amount of work will be needed to get it into an acceptable form for a journal article, but not, I hope, as much as all that. If everyone else shares my belief that it works, then I’d be more interested in pressing on and doing DHJ(k), or at least thinking about its feasibility, than in rushing to write it up with all the numbers put in. Also, it seems to me that the statement about line-free sets correlating locally with 12-sets should be provable using localization rather than equal-slices measure, and that would bring it into line with the rest of the proof. So that’s something else I think we should try to do before writing anything up properly. In my next comment I’m going to speculate a little about DHJ(4). 885.2\\. Hi Tim. I agree it’s looking pretty solid. Thanks for all the wikification! I plan to check it over tonight. 885.3 That’s great — I’ve got to go to bed pretty soon, but I’ll look forward to checking in the morning to see whether it still looks solid to you by then. Thanks for your wikification too!"},{"username":"ryan-odonnell","timestamp":"2009-03-09T05:02:00.000Z","contents":"885.2\\. Hi Tim. I agree it’s looking pretty solid. Thanks for all the wikification! I plan to check it over tonight."},{"username":"gowers","timestamp":"2009-03-09T05:36:00.000Z","contents":"885.3 That’s great — I’ve got to go to bed pretty soon, but I’ll look forward to checking in the morning to see whether it still looks solid to you by then. Thanks for your wikification too!"},{"username":"gowersgowersrandall","timestamp":"2009-03-09T05:32:00.000Z","contents":"DHJ(4) How might the argument generalize to DHJ(4)? Probably a good way to think about this is to try to deduce the 3D corners result from the full 2D Szemerédi theorem. This wouldn’t be much use as a proof of the corners theorem because nobody knows a proof of the full 2D Szemerédi theorem that does not also give 3D corners. (Probably one could falsify that last sentence in silly ways, but I think it’s basically true.) However, there is reason to hope that (i) the multidimensional DHJ(3) theorem can be obtained from the one-dimensional theorem by some kind of trickery and (ii) it can be used as an ingredient for proving DHJ(4) in the way that multidimensional Sperner was used for proving DHJ(3). Eventually, of course, I want to come back to (i) and (ii) but for now I’ll stick to the easier world of 3D corners. For 2D corners, the first step is to find a dense diagonal. The nice thing about a dense diagonal is that it gives rise to lots of forbidden points: indeed, if (x,y’) and (x’,y) both belong to the same diagonal, with %x, then %(x,y)% is not allowed in the set. Even better, the set of all forbidden points has a nice Cartesian-product structure. (In fact, it’s the points of a Cartesian product that lie below the diagonal, but that will contain a large Cartesian product.) The analogue of a diagonal for 3D corners is a plane of constant %x+y+z%. How can a dense diagonal forbid other points? Answer: if you have a suitably aligned equilateral triangle %(x+d,y,z),(x,y+d,z),(x,y,z+d)% then you forbid the point %(x,y,z)%. Now the 2D corners theorem tells us that a dense diagonal plane contains many such equilateral triangles, so we end up forbidding a good lot of points. What is rather less clear is what kind of structure that set of points has. In fact, it’s so unclear that I think I’d better stop this comment because I do not immediately have anything useful to say about it. Actually, perhaps I do. What would be very nice would be to get a density increase on a dense (12,23,13)-set. By that I mean a dense set B of the form %\\{(x,y,z):(x,y)\\in R,(y,z)\\in S,(x,z)\\in T\\}.% That would be nice because it is the natural analogue of a 12-set (natural, that is, for anyone who has thought about hypergraph regularity and that kind of thing). If the world is a friendly place, it will turn out that the set of points that form the bottom vertex of a 3D corner with the other three vertices in the dense diagonal plane is a dense (12,23,13)-set. Is it? Yes of course it is! It consists of all points (x,y,z) such that three conditions hold. The first is that if you go in the z direction until you hit the diagonal plane, you hit it at a point in A. But that condition depends on (x,y) only. The other two conditions depend on (y,z) and (x,z). OK, this is looking good. So now we’ve got our dense (12,23,13)-set that’s disjoint from A. By averaging we find one with which A correlates. So now all we have to do is partition a dense (12)-set (where this does not mean the same as a 12-set, but rather it means a set that depends just on (x,y)) into large 3D grids. And that we can easily do using 2D-Szemerédi! If the (12)-set is all (x,y,z) such that %(x,y)\\in B,% then by 2D Szemerédi we can partition almost all of B into large grids with fairly small width. For each such grid G we can then partition %G\\times[n]% into large 3D grids, and we’re done. The rest of the argument is almost exactly as before. Obviously, this technique is going to work to show that Szemerédi in d dimensions implies corners in d+1 dimensions. So it looks very promising for DHJ(k). The first target, it seems to me, is to get a multidimensional version of DHJ(3). Somehow the whole thing feels as though it is not going to be too hard … 886.1 No time to write it now, but I see how to deduce multidimensional DHJ(3) from DHJ(3). And indeed it is not hard. 886.2 General k I had a look at the FK proof for general k and found some very interesting parallels between their proof and what Tim has been doing in the past 48 hours. (And what he proposes above.) Quite striking, really, right down to the trick of cutting things up in one dimension first, then the other. (I must have forgotten this trick, as I didn’t consider using it for k=3.) At any rate, the general outline suggested for k=4 (and beyond) looks terribly sound."},{"username":"gowers","timestamp":"2009-03-09T06:04:00.000Z","contents":"886.1 No time to write it now, but I see how to deduce multidimensional DHJ(3) from DHJ(3). And indeed it is not hard."},{"username":"randall","timestamp":"2009-03-09T10:17:00.000Z","contents":"886.2 General k I had a look at the FK proof for general k and found some very interesting parallels between their proof and what Tim has been doing in the past 48 hours. (And what he proposes above.) Quite striking, really, right down to the trick of cutting things up in one dimension first, then the other. (I must have forgotten this trick, as I didn’t consider using it for k=3.) At any rate, the general outline suggested for k=4 (and beyond) looks terribly sound."},{"username":"ryan-odonnell","timestamp":"2009-03-09T09:42:00.000Z","contents":"Wikification. I started going through the proof Tim has sketched from the beginning, trying to fill in a few small details. I thought briefly about removing the use of equal-slices density in the [first part of the argument](http://michaelnielsen.org/polymath1/index.php?title=Line-free_sets_correlate_locally_with_complexity-1_sets), wherein it is shown that line-free sets correlate with 12-sets. It wasn’t immediately clear to me how to do this. Therefore I decided to leave it alone, and work out the “technicality” of passing from relative density under equal-slices to relative density under uniform, discussed in the last paragraph of [the proof sketch](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument). Specifically, this requires the details in the “more details” section of the [currently abortive original density-increment plan](http://michaelnielsen.org/polymath1/index.php?title=An_outline_of_a_density-increment_argument). Therefore, I worked for a bit to clean these up. As usual, no surprises; everything is fine. Indeed, one can do it passing to subspaces of %\\Omega(n)% dimension. I added the last 1% to Tim’s sketch and put it in the [passing between measures](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures) wiki article. The only minor innovation is noting that you can write equal-slices _exactly_ as a mixture of uniform-distributions-on-subspaces."},{"username":"ryan-odonnell","timestamp":"2009-03-09T11:21:00.000Z","contents":"More wikification: equal-slices. I added the following observation (which I assume was clear to most everyone already) to the wiki entry on [equal-slices measure](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure), which helped me understand the “Hang on” part of the proof that [line-free sets correlate with 12-sets](http://michaelnielsen.org/polymath1/index.php?title=Line-free_sets_correlate_locally_with_complexity-1_sets). Another equivalent way to draw from the equal-slices distribution is as follows. Start with a string of %n% “dots” %\\bullet%. Next, place a “bar” %\\mid% randomly in one of the %n+1% “slots” between (and to the left and right of) the dots. Next, place a second bar randomly in one of the %n+2% slots formed by the string of %n% dots and one bar. (At this point we have determined the “slice”.) Next, fill in all the dots to the left of the leftmost bar with %1%‘s; fill in all the dots between the two bars with %3%‘s (not %2%‘s!); and, fill in all the dots to the right of the rightmost bar with %2%‘s. Delete the bars. Finally, randomly permute the resulting string of %1%‘s, %2%‘s, and %3%‘s. With this viewpoint, it may be easier to understand the joint distribution of the 1-set and the 2-set of a string drawn from equal-slices. Specifically, it is one that is useful for proving density-Sperner’s theorem. **Fact:** Let %z% be a string drawn from the equal-slices distribution, in the manner described above. Let %x \\in [2]^n% be the string that would have been formed had we filled in all the dots to the left of the _first_ bar with %1%‘s and all the dots to its right with %2%‘s. Similarly, let %y \\in [2]^n% be the string that would have been formed had we filled in all the dots to the left of the _second_ bar with %1%‘s and all the dots to its right with %2%‘s. Then the following should be easy to verify: (i) %x% and %y% are both distributed according to the equal-slices distribution on %[2]^n% (but not independently); (ii) %x, y, z% form a combinatorial line in %[3]^n%; in particular, %x% and %y% are “comparable” in %[2]^n%, i.e., either %x \\leq y% or %x \\geq y%; (iii) %\\Pr[\\text{line is degenerate}] = \\Pr[x = y] = 2/(n+2)%. From these facts we can derive the density version of Sperner’s Theorem: **Theorem:** Suppose %A \\subseteq [2]^n% has equal-slices density %\\delta%. Then according to the above distribution on %(x,y) \\in [2]^n \\times [2]^n%, we get a nondegenerate combinatorial line in %A% with probability at least %\\delta^2 - \\frac{2}{n+2}%."},{"username":"ryan-odonnell","timestamp":"2009-03-09T12:30:00.000Z","contents":"Last wikification of the night. Okay, using the above mentality I was able to rewrite in my own words Tim’s proof that line-free sets correlate with 12-sets. I added these words [to the wiki](http://michaelnielsen.org/polymath1/index.php?title=Line-free_sets_correlate_locally_with_complexity-1_sets), modulo the passing from uniform density to equal-slices density (which is still in that article and also partially [here](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures)). It’s pretty late at night for me, so I hope I got it right."},{"username":"gowersgowersjozsef","timestamp":"2009-03-09T14:12:00.000Z","contents":"Multidimensional DHJ(3) My main teaching days are Mondays and Tuesdays this term, and today and tomorrow are the last two such days of term. So I’ll be fairly busy, but I hope I’ll still have a bit of time for blogging and wikification. Here I want, as a pre-wikification exercise, to sketch a proof that DHJ(k) implies multidimensional DHJ(k). I’ve woken up with the feeling that DHJ(k) is going to go through almost as easily as DHJ(3). If that is the case, it will be unexpected for two reasons. First, it will give a proof of Szemerédi’s theorem that has a strong claim to be the simplest known. (The only rival I can think of is a particularly clean approach via infinitary hypergraphs, due to Elek and Szegedy, but I may be wrong.) Secondly, it would be the first proof of Szemerédi’s theorem for which “the general case is the case k=3”. By that I mean that in all other proofs you have to go at least as far as k=4 before it’s obvious how to generalize, and in some you have to go to k=5\\. (Perhaps a true understanding of the problem would require a proof that generalizes straightforwardly from the k=2 case …) Back to multidimensional DHJ(k). Here’s what I think works. Let %\\mathcal{A}% be a density-%\\delta% subset of %{}[k]^n% and let %M% be large enough so that every subset of %{}[k]^M% of density at least %\\theta% contains a combinatorial line. Now split %{}[k]^n% up into %{}[k]^M\\times[k]^{n-M}.% For a proportion at least %\\delta/2% of the points y in %{}[k]^{n-M}% the set of %x\\in[k]^M% such that %(x,y)\\in\\mathcal{A}% has density at least %\\delta/2%. Therefore, by DHJ(k) (with %\\theta=\\delta/2%) we have a combinatorial line. Since there are fewer than %(k+1)^M% to choose from, by the pigeonhole principle we can find a combinatorial line %L\\in[k]^M% and a set %\\mathcal{A}_1% of density %\\delta/2(k+1)^M% in %{}[k]^{n-M}% such that %(x,y)\\in\\mathcal{A}% whenever %x\\in L% and %y\\in\\mathcal{A}_1.% And now by induction we can find an %(m-1)%-dimensional subspace in %\\mathcal{A}_1% and we’re done. This gives a truly horrible bound, and should mean that if DHJ(k) goes through as I expect (and Randall also expects, I’m glad to see from 886.2), the bound that comes out at the end will probably be of Ackermann type, so it will be comparable to the bounds that come out of the hypergraph approach. (A small challenge that I know some people out there would enjoy is to try to see how this approach to Szemerédi fits in with the general philosophy that all the different proofs are at some deep level manifestations of closely related ideas. There are distinct echoes of hypergraphs in this proof, and yet it is far easier than hypergraph regularity and counting — what is going on? Possibly that we are “cheating” by continually passing to subspaces, but why can’t we do that with hypergraphs? Or can we? Perhaps there’s a way of passing to subgraphs without throwing away too many degenerate simplices. Hmm … I quite like that but no time to pursue it just at the moment.) Multidimensional DHJ(3) is [now wikified](http://michaelnielsen.org/polymath1/index.php?title=DHJ%28k%29_implies_multidimensional_DHJ%28k%29). I did write up the strong multidimensional Sperner proof which uses multidim. Szemeredi on the wiki, however it might be totally uninteresting if DHJ is easier than hypergraph removal."},{"username":"gowers","timestamp":"2009-03-09T14:28:00.000Z","contents":"Multidimensional DHJ(3) is [now wikified](http://michaelnielsen.org/polymath1/index.php?title=DHJ%28k%29_implies_multidimensional_DHJ%28k%29)."},{"username":"jozsef","timestamp":"2009-03-09T21:09:00.000Z","contents":"I did write up the strong multidimensional Sperner proof which uses multidim. Szemeredi on the wiki, however it might be totally uninteresting if DHJ is easier than hypergraph removal."},{"username":"ryan-odonnell","timestamp":"2009-03-09T20:17:00.000Z","contents":"Corrected/expanded a bit the “passing between measures” article on the wiki."},{"username":"ryan-odonnell","timestamp":"2009-03-09T21:00:00.000Z","contents":"Wikification. I finished the 1% fleshing out required in the article proving that [line-free sets correlate with 12-sets](http://michaelnielsen.org/polymath1/index.php?title=Line-free_sets_correlate_locally_with_complexity-1_sets), including all the passing back and forth between uniform and equal-slices measures. I think the only bit remaining undone here is instantiating all the parameters."},{"username":"jozsefjozsefjozsefterence-tao","timestamp":"2009-03-09T21:27:00.000Z","contents":"wiki Meta comment: Something happened with the wiki. It seems that it has been hacked. Be careful with the links there. Just seems to be spammers changing the page. I reversed it. It wasn’t enough, the page is wrong again. I’m not sure what to do. I changed it back again, but I don’t think it will stay like this for long. I blocked the offending IP. If that fails, the next step would be to protect the main page by limiting edits to signed in users, I guess."},{"username":"jozsef","timestamp":"2009-03-09T21:42:00.000Z","contents":"Just seems to be spammers changing the page. I reversed it."},{"username":"jozsef","timestamp":"2009-03-09T21:46:00.000Z","contents":"It wasn’t enough, the page is wrong again. I’m not sure what to do. I changed it back again, but I don’t think it will stay like this for long."},{"username":"terence-tao","timestamp":"2009-03-09T21:58:00.000Z","contents":"I blocked the offending IP. If that fails, the next step would be to protect the main page by limiting edits to signed in users, I guess."},{"username":"terence-taogowers","timestamp":"2009-03-09T23:06:00.000Z","contents":"893. I’ve been busy, so I haven’t been able to stop by much recently, but things look pretty good at this point. I agree that the DHJ(3) Ajtai-Szemeredi sketch looks pretty solid. (An amusing side note: when I had Ajtai-Szemeredi described to me, I thought that they were already doing what we were doing now, i.e. getting correlation with an unstructured Cartesian product and then partitioning that product into grids. So I was a little confused when Tim was insisting that what we were doing was not quite Ajtai-Szemeredi… but now I see the subtle difference between the two approaches.) It looks like Tim Austin has come up with an alternate proof that is also based on correlation with 12-sets, etc. but is based on triangle-removal type strategies rather than density-increment strategies. It also requires a preliminary use of Graham-Rothschild to regularise a large number of statistics so as to make them stable under freezing of coordinates, and so is likely to give poorer bounds. But it is closer in spirit to the original intent of Polymath1\\. I believe Tim will come on here himself to report on this soon (he’s working in an ergodic theory setting), and I will focus on trying to finitise it. (It’s likely to be cleaner than the finitisation of Furstenberg-Katznelson, because one does not have to deal with relative almost periodicity.) 893.1 Re your amusing aside, I have had a very similar experience, which can sort of be deduced from my initial blog comment on the Ajtai-Szemerédi proof. (It can be found at the end of [the article on the wiki](http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemerédi%27s_proof_of_the_corners_theorem#An_old_blog_comment_that_used_to_be_the_main_article).) At that stage I only half remembered their proof, and likewise assumed that they must have _started_ with the dense diagonal, got a global Cartesian product disjoint from A, and deduced a density increment on a grid."},{"username":"gowers","timestamp":"2009-03-10T02:39:00.000Z","contents":"893.1 Re your amusing aside, I have had a very similar experience, which can sort of be deduced from my initial blog comment on the Ajtai-Szemerédi proof. (It can be found at the end of [the article on the wiki](http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemerédi%27s_proof_of_the_corners_theorem#An_old_blog_comment_that_used_to_be_the_main_article).) At that stage I only half remembered their proof, and likewise assumed that they must have _started_ with the dense diagonal, got a global Cartesian product disjoint from A, and deduced a density increment on a grid."},{"username":"tim-austingowers","timestamp":"2009-03-10T01:24:00.000Z","contents":"I seem to have come to this at a handy moment, following Terry’s post Having been following progress here (albeit only in fits and starts) and talking with Terry about the ideas that have come out, it struck me late last week that in the infinitary world of stochastic processes that Furstenberg and Katznelson move to, the approach raised here for obtaining obstructions to uniformity that are built from ij-sets can actually be coupled to a lot of machinery that’s already known from other things to give a new infinitary proof of their multiple recurrence result, without anything else being required. In particular, it uses an infinitary analog of `energy increment’ to improve the structure of a stochastic process, and then an appeal to an `infinitary hypergraph removal lemma’ originally motived by some work of Terry on infinite random hypergraphs, both which I recently used to play a similar game around the multidimensional Szemeredi Theorem (arXiv 0808.2267, in case it’s of interest). In fact, it turned out that this could be written up completely in just a couple of days by judiciously cutting, pasting and re-notating writeups of other things, so this is now done and on the arXiv: once it becomes publicly visible it’ll be at 0903.1633\\. I feel I should possibly offer my assurances that I wouldn’t have rushed from a moment of realization to completing a preprint if it really hadn’t been so very quick and mechanical from that point on, without requiring any input of new ideas from me. For what it’s worth, I’ve thought only briefly about finitizing this approach and Terry has already said most of what I could say. As it stands it will require a preliminary heavy appeal to Graham-Rothschild (Carlson-Simpson, in the infinitary world) and then proceeding in close analogy with hypergraph removal strategies. So it is rather removed from the density-increment approach that I think is now mainly being pursued here, and would look set to give much worse bounds unless some other new idea can remove the reliance on Graham-Rothschild. 894.1 Tim, this is great news and very much in the spirit of polymath leading to multiple proofs and all-round improved understanding."},{"username":"gowers","timestamp":"2009-03-10T02:29:00.000Z","contents":"894.1 Tim, this is great news and very much in the spirit of polymath leading to multiple proofs and all-round improved understanding."},{"username":"randallterence-taogowers","timestamp":"2009-03-10T01:31:00.000Z","contents":"Book proof of Szemeredi? Or “This is a theorem Harry Furstenberg stole from Szemeredi…we’re stealin’ it back.” Regarding what Tim (Gowers) said about an easy proof of Szemeredi materializing, as well as what Terry said about avoiding relative almost periodicity (which seems to be exactly what makes this proof easy as well), it seems natural whether one of us should think about writing up carefully a “book proof of Szemeredi’s theorem”. For starters, it seems to me that this might entail proving Jozsef’s comment no. 2 from a multi-dim Sperner type of hypothesis, then pushing an induction to k-dimensional corners ala Tim’s 886\\. I am hoping it could be easier in the details than DHJk; that should be clear by the end of the first step, though. It may in fact be that, paradoxically, the book proof of Szemeredi may ultimately pass through DHJ. For instance, observe that the original Ajtai-Szemeredi proof of corners had to pass through Szemeredi, whereas by lifting up from [n] to cubes, we can substitute (multi-dimensional) DHJ(2) in place of Szemeredi. The hypergraph regularity/removal proof of (multidimensional) Szemeredi is not too bad, actually, despite its reputation. The ergodic version of it, which Tim Austin wrote up in the arXiv a few months back, is perhaps a touch simpler than the Furstenberg-Katznelson proof based on repeated extensions by relatively almost periodic functions, being instead based on extending the entire system up to a more “pleasant” system enjoying a number of useful relative independence properties. I’ve been thinking about this too, and it may not be as paradoxical as all that — just a bit unexpected at first. For instance, it is generally accepted that the van der Waerden theorem is “really” the Hales-Jewett theorem (at least if you prove it combinatorially), and that starting with a subset of [n] distracts from what is actually going on. And something like that seems to have been the case here too: for a while the fact that Ajtai and Szemerédi used Szemerédi’s theorem was a distraction, in that it made it seem as though their approach reached a dead end at the corners theorem when in fact the structure they should have been using was a cube. So Jozsef’s comment 2 was spot on. Randall, there may be room for disagreement over your slogan, but I can’t help liking it …"},{"username":"terence-tao","timestamp":"2009-03-10T02:34:00.000Z","contents":"It may in fact be that, paradoxically, the book proof of Szemeredi may ultimately pass through DHJ. For instance, observe that the original Ajtai-Szemeredi proof of corners had to pass through Szemeredi, whereas by lifting up from [n] to cubes, we can substitute (multi-dimensional) DHJ(2) in place of Szemeredi. The hypergraph regularity/removal proof of (multidimensional) Szemeredi is not too bad, actually, despite its reputation. The ergodic version of it, which Tim Austin wrote up in the arXiv a few months back, is perhaps a touch simpler than the Furstenberg-Katznelson proof based on repeated extensions by relatively almost periodic functions, being instead based on extending the entire system up to a more “pleasant” system enjoying a number of useful relative independence properties."},{"username":"gowers","timestamp":"2009-03-10T02:46:00.000Z","contents":"I’ve been thinking about this too, and it may not be as paradoxical as all that — just a bit unexpected at first. For instance, it is generally accepted that the van der Waerden theorem is “really” the Hales-Jewett theorem (at least if you prove it combinatorially), and that starting with a subset of [n] distracts from what is actually going on. And something like that seems to have been the case here too: for a while the fact that Ajtai and Szemerédi used Szemerédi’s theorem was a distraction, in that it made it seem as though their approach reached a dead end at the corners theorem when in fact the structure they should have been using was a cube. So Jozsef’s comment 2 was spot on. Randall, there may be room for disagreement over your slogan, but I can’t help liking it …"},{"username":"gowers","timestamp":"2009-03-10T02:50:00.000Z","contents":"Metacomment: I’m doing my best not to write newly numbered comments here, since this thread is about to run out (at 899) and then we’ll reach an important milestone — comment number 1000\\. I feel that we deserve a decent-length post before that one. (My main activity at the moment is over at the wiki. I’m currently working on generalizing the local correlation with 12-sets from %{}[3]^n% to %{}[k]^n.)%"},{"username":"terence-taogowers","timestamp":"2009-03-10T03:09:00.000Z","contents":"Austin’s proof I will probably focus the (limited) time I have available for this thread on trying to explicate Austin’s proof in finitary language. I know you guys don’t actually have access to it yet, but let me try to informally describe some of the details. Take all this _cum grano solis_; I have not yet fully digested the argument and some of the details may be slightly or perhaps even massively incorrect. Let me take [3] = {0,1,2} (rather than {1,2,3}) for sake of arbitrarily fixing the conventions (we’ve been a bit inconsistent on this). Let’s define a trilinear form %\\Lambda( f, g, h)% on functions %f, g, h: [3]^n \\to {\\Bbb R}% by the formula %\\Lambda(f,g,h) = {\\Bbb E}_{\\ell} f(\\ell(0)) g(\\ell(1)) h(\\ell(2))% where %\\ell: [3] \\to [3]^n% varies along combinatorial lines with respect to some measure that I will intentionally leave vague. DHJ(3) is equivalent to the following “triangle-removalish” type statement: **DHJ(3)’**: Let %f, g, h: [3]^n \\to [0,1]% be such that %\\Lambda(f, g, h) = o(1)%. Then %{\\Bbb E}_{x \\in [3]^n} f(x) g(x) h(x) = o(1)%. Roughly speaking, Austin’s strategy is to “regularise” the situation so that 12-sets, 01-sets are “relatively independent” over a common algebra of 1-sets, and similarly for the 12-sets and 02-sets, etc., with the 0-sets, 1-sets, 2-sets themselves being relatively independent over %\\emptyset%-sets (which, for us, I think means “unions of large subspaces”, and which can be ignored by passing to a large subspace). I don’t understand this part well yet, but it is analogous to the Szemeredi regularity lemma. There is also a preliminary reduction to “strong stationarity” which means, roughly, that the statistics of various 01-sets, etc. (e.g. the density of an overlap between a relevant 01-set and a relevant 02-set) doesn’t change of we freeze a bounded number of coordinates. This reduction is obtained via Graham-Rothschild and is going to be hideously expensive as regards quantitative bounds, but never mind that for now. 1\\. Once one has this regularisation, this makes DHJ(3)’ is easy when f is an (indicator of a) (basic) 12-set, g is a basic 02-set, and h is a basic 01-set, much as it is easy to find corners connecting three sets A, B, C in %[n]^2% when those sets are Cartesian products in the right fashion. Indeed things seem to collapse to DHJ(2.5) in this case. (This is analogous to the triangle removal lemma for three bipartite graphs when each of the three graphs is a complete bipartite graphs between one cell in each vertex set.) 2\\. Next, this implies DHJ(3)’ when f,g,h are non-basic 12-sets, 02-sets, and 01-sets, i.e. finite unions of basic sets (or more precisely finite linear combinations of indicators of basic sets), but where the lower bound depends on the complexity of the partition into basic sets. (This is analogous to the triangle removal lemma for unions of complete bipartite graphs between cells.) 3\\. Next, this implies DHJ(3)’ when f (and similarly g, h) are “Borel 12-sets” (to continue the topological analogy much as Borel sets can be approximated by open sets), which means that given any %\\varepsilon > 0%, one can approximate f to within %\\varepsilon% by a non-basic 12-set of bounded complexity. This is because pigeonhole ensures that there are a lot of non-basic 12-sets which are 99% occupied by f, and one will be able to get this case from applying Case 2 to these “rich” sets, and using some relative independence properties. (This is analogous to triangle removal for unions of 99%-complete bipartite graphs between cells.) 4\\. Next, we obtain DHJ(3)’ when f (and similarly g,h) is the sum of a Borel 12-set and something which is highly orthogonal to all basic 12-sets, including the very small basic 12-sets coming from the cells of an approximation to the Borel 12-set. This is basically because the guy which is highly orthogonal to all basic 12-sets is so uniform as to have essentially no contribution to %\\Lambda(f,g,h)%. (This is analogous to triangle removal for a triplet of bipartite graphs which have been regularised.) 5\\. Finally, we have a regularity lemma that tells us that arbitrary f,g,h decompose in this fashion (possibly after localising to a large subspace). This is a “soft” energy increment argument, analogous to that in the regularity lemma. One has to keep freezing coordinates while performing this increment argument, so it is important that one has the strong stationarity property first before one sets up the regularity argument. Maybe I’ll try to write a more coherent version of the above on the wiki at some point. Another wordpress bug for your collection: it doesn’t like \\varemptyset (so I changed yours to \\emptyset)."},{"username":"gowers","timestamp":"2009-03-10T03:44:00.000Z","contents":"Another wordpress bug for your collection: it doesn’t like \\varemptyset (so I changed yours to \\emptyset)."},{"username":"gowersgowersgowers","timestamp":"2009-03-10T03:58:00.000Z","contents":"Progress report I’ve reached the first point where it isn’t almost trivial to generalize the argument we have for DHJ(3) to an argument for DHJ(k). I think it’s going to be a sort of middling difficulty — that is, you can’t do it in five minutes but you know you’ll get there in at most a few hours. (Actually, I think it may be easier than that, but I haven’t yet tried.) Over on the wiki, I have written up [part of an argument](http://michaelnielsen.org/polymath1/index.php?title=Line_free_sets_correlate_locally_with_dense_sets_of_complexity_k-2#Proof_of_the_main_result.2C_with_a_gap_still_left_to_fill) that generalizes to %{}[k]^n% the fact that a dense line-free set (in equal-slices measure) correlates with a dense 12-set. The problem in the %{}[k]^n% case is to prove that the lower-complexity set you get is dense. The proof for %{}[3]^n% uses a quantitative Sperner theorem (namely that you get a positive proportion of all possible combinatorial lines if you start with a dense set — all with respect to equal-slices measure). The proof for %{}[k]^n% needs a similar statement about combinatorial lines in %{}[k-1]^n.% It should be obtainable from DHJ(k-1) by a suitable averaging argument, but it’s not one I can do in my head. 897.1 I’ve got a vague outline of an argument, but have to go to bed. Here’s how it goes. We’re given a dense (in equal-slices measure) subset %\\mathcal{B}\\subset[k-1]^n.% I claim that it contains a dense set of combinatorial lines (where that means that if you randomly permute %{}[n]% and then randomly divide it up into %k% subintervals, filling the first %k-1% with 1 to k-1 and treating the final one as a wildcard set, and then with positive probability that line lies in your set). I think that follows by an averaging argument where you choose a large subspace randomly by first picking the sizes of the wildcard sets and then choosing the sets themselves, and then you apply DHJ(k-1) and average up. Finally, I think that if you pick an equal-slices random point x in %{}[k]^n% and consider all the points in %{}[k-1]^n% you get by replacing the js of x by 1s, 2s, …, (k-1)s, then the distribution on the resulting combinatorial lines is not radically different from the distribution described above. In particular, I think the probability that all those points lie in the dense subset of %{}[k-1]^n% is positive. 897.2 Just realized that “not radically different from” might be better changed to “identical to”."},{"username":"gowers","timestamp":"2009-03-10T05:31:00.000Z","contents":"897.1 I’ve got a vague outline of an argument, but have to go to bed. Here’s how it goes. We’re given a dense (in equal-slices measure) subset %\\mathcal{B}\\subset[k-1]^n.% I claim that it contains a dense set of combinatorial lines (where that means that if you randomly permute %{}[n]% and then randomly divide it up into %k% subintervals, filling the first %k-1% with 1 to k-1 and treating the final one as a wildcard set, and then with positive probability that line lies in your set). I think that follows by an averaging argument where you choose a large subspace randomly by first picking the sizes of the wildcard sets and then choosing the sets themselves, and then you apply DHJ(k-1) and average up. Finally, I think that if you pick an equal-slices random point x in %{}[k]^n% and consider all the points in %{}[k-1]^n% you get by replacing the js of x by 1s, 2s, …, (k-1)s, then the distribution on the resulting combinatorial lines is not radically different from the distribution described above. In particular, I think the probability that all those points lie in the dense subset of %{}[k-1]^n% is positive."},{"username":"gowers","timestamp":"2009-03-10T05:37:00.000Z","contents":"897.2 Just realized that “not radically different from” might be better changed to “identical to”."},{"username":"gowers","timestamp":"2009-03-10T04:25:00.000Z","contents":"Metacomment: I’m going to think offline for a bit about the problem in 897\\. Before I clock off for the night I’ll say whether I think I’ve sorted it out, in case anyone else wants to think about it. (It feels as though it might be Ryan territory.) And I’ve now put up a new post in case the comment count reaches 899\\. _[Am clocking off now — see 897.1 above.]_"},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"I have posted this to the old thread but in case I should  \nhave put it in the new thread I am posting it here again. We have already shown that if  \nThe center slice is 42 or 43 and it has  \n17 or more points then there at most 124  \nCase 2  \nThe center slice is 42 and c is less that 17  \nThen we must have c is 12  \nAnd we must have on of the 8  \n(6, 24, 12, 0, 0) sets.  \nWithout less of generality  \nWe can assume it to be %\\Gamma _{220}+\\Gamma _{202}+\\Gamma _{022}+\\Gamma _{112}+\\Gamma _{211}%  \nIn particular the middles slice  \nContains the points  \n2122 21212 21221  \n23322 23232 23223 and the *1*** and *3*** slices  \nhave a d value of at least three and so have at most  \n41 points now if the center slice is 42 we have 124 points  \nand we are done so it must be 43 but then it has value 18 or more  \nand and by case 1 we have 124 points or less so we are done."},{"username":"terence-tao","timestamp":"0006-03-20T04:00:00.000Z","contents":"Moser(3) I think we are now quite close to classifying all the 125 point Moser sets (if any) and thus computing %c'_5% precisely. From Kristal’s lemma and the averaging argument from my 793, 795 comments, we now know that (after rotating a 125 point Moser set) that the 1****, 3**** slices have (a,b,c,d,e) distribution from the following four choices: * (4,16,23,0,0) [score: 62 1/3]  \n* (3,16,24,0,0) [score: 63]  \n* (4,15,24,0,0) [score: 62 3/4]  \n* (2,16,24,0,0) [score: 62] with total score at least 125\\. The maximal or near-maximal “c” values of these slices force the “c” value of the middle slice 2**** to be at most 1\\. Now let A, B, C, D, E, F be the number of points in the Moser set with no 2s, one 2, …, five 2s respectively. We already know that E=F=0; the above discussion shows that D is at most 1, and also A is at most 4+4=8\\. Meanwhile, a double counting argument shows that 2B+C is at most 160 (details on the wiki). Since %125 = A+B+C+D+E+F \\leq A + \\frac{1}{2} 160 + \\frac{1}{2} C + D% we thus have %C \\geq 90 - 2A - 2D%. (1) But this looks quite hard to satisfy; C can at most be 80! Furthermore, if D is 1 (e.g. if 11222 is in the set), then C can be at most 77, because of the three lines (e.g. 11×22, 112×2, 1122x) going through the D-point that have their other vertices as C-points. Also, as pointed out by Michael in 784, if C is 80, then A is at most 4\\. So it’s looking quite hard to actually satisfy the equation (1). Perhaps just a little more case analysis will finish the problem off for good… (and then we might look at how one can reduce the reliance on computer proofs, and perhaps even hope for a completely human proof)."},{"username":"terence-tao","timestamp":"0006-03-20T04:00:00.000Z","contents":"Moser(3) Actually, I think I can show that D=1\\. Suppose not (e.g. if 11222 was in the set); then there would be two choices of coordinates in which one of the side slices would have d=1 (e.g. 1**** and *1****). But the table at [http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuqNcxJ171Bbw&hl=en](http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuqNcxJ171Bbw&hl=en) shows that such slices have a score of at most 59 7/12, so that the total score from those two coordinates is at most 63 + 59 7/12 = 122 7/12\\. On the other hand, the other three slices have a net score of at most 63+63 = 126\\. This averages out to at most 124.633… < 125, a contradiction. So D=0."},{"username":"jason-dyer","timestamp":"0007-03-20T04:00:00.000Z","contents":"%\\overline{c}^\\mu_6 \\neq 17% (The solutions I-VI are defined as in my last proof.) Here, “upper triangle” means the first four rows of the triangular lattice (with 060 at top) and “lower trapezoid” means the bottom three rows. Suppose 11 removals leave a triangle-free set. First, suppose that 5 removals come from the upper triangle and 6 come from the lower trapezoid. Suppose the trapezoid 600-420-321-303 used solution IV. There are three disjoint triangles 402-222-204, 213-123-114, and 105-015-006\\. The remainder of the points in the lower trapezoid (420, 321, 510, 501, 402, 312, 024) must be left open. 024 being open forces either 114 or 015 to be removed. Suppose 114 is removed. Then 213 is open, and with 312 open that forces 222 to be removed. Then 204 is open, and with 024 that forces 006 to be removed. So the bottom trapezoid is a removal configuration of 600-411-303-222-114-006, and the rest of the points in the bottom trapezoid are open. All 10 points in the upper triangle form equilateral triangles with bottom trapezoid points, hence 10 removals in the upper triangle would be needed, so 114 being removed doesn’t work. Suppose 015 is removed. Then 006-024 forces 204 to be removed. Regardless of where the removal in 123-213-114, the points 420, 321, 222, 024, 510, 312, 501, 402, 105, and 006 must be open. This forces upper triangle removals at 330, 231, 042, 060, 051, 132, which is more than the 5 allowed, so 015 being removed doesn’t work, so the trapezoid 600-420-321-303 doesn’t use solution IV. Suppose the trapezoid 600-420-321-303 uses solution VI. The trapezoid 303-123-024-006 can’t be IV (already eliminated by symmetry) or VI’ (leaves the triangle 402-222-204). Suppose the trapezoid 303-123-024-006 is solution VI. The removals from the lower trapezoid are then 420, 501, 312, 123, 204, and 015, leaving the remaining points in the lower trapezoid open. The remaining open points is forces 10 upper triangle removals, so the trapezoid 600-420-321-303 doesn’t use solution VI. Therefore the trapezoid 303-123-024-006 is solution V. The removals from the lower trapezoid are then 420, 510, 312, 204, 114, and 105\\. The remaining points in the lower trapezoid are open, and force 9 upper triangle removals, hence the trapezoid 303-123-024-006 can’t be V, and the solution for 600-420-321-303 can’t be VI. The solution VI’ for the trapezoid 600-420-321-303 can be eliminated by the same logic by symmetry. Therefore it is impossible for 5 removals come from the upper triangle and 6 come from the lower trapezoid.  \nTherefore 4 removals come from the upper triangle and 7 come from the lower trapezoid. At this point note the triangle 141-411-141 must have one point removed, so let it be 141 and note that any logic that follows is also true for a removal of 411 and 141 by symmetry. This implies the upper triangle must have either solution I or II. Suppose it has solution II. Note there are five disjoint triangles 600-510-501, 411-321-312, 402-222-204, 213-123-114, and 105-015-006. Suppose 420 and 024 are removed. Then, noting 303 must be open, 606 must be removed, leaving 510 open. 510-240 forces 213 to be removed, and 510-150 force 114 to be removed. 213 are 114 are in the same disjoint triangle. Hence both 420 and 024 both can’t be removed. So at least either 420 or 024 is open. Let it be 420, noting by symmetry identical logic will apply if 024 is removed. Then 321, 222, and 123 are removed based on 420 and the open spaces in the upper triangle. This leaves four disjoint triangles 600-501-510, 402-303-312, 213-033-015, 204-114-105\\. So 411 and 420 are open, forcing the removal of 510\\. This leaves 501 open, and 501-411 forces the removal of 402\\. 600-303, and 330 are then open, forming an equilateral triangle. Therefore 420 isn’t open, therefore the upper triangle can’t have solution II. Therefore the upper triangle has solution I. Suppose 222 is open. 222 with open points in the upper triangle force 420, 321, 123, and 024 to be removed. This leaves four disjoint triangles 411-501-402, 213-303-204, 015-105-006, and 132-312-114\\. This would force 8 removals in the lower trapezoid, so 222 must be closed. Therefore 222 is removed. There are six disjoint triangles 150-420-123, 051-321-024, 231-501-204, 132-402-105, 510-150-114, and 312-042-015\\. So 600, 411, 393, 114, and 006 are open. 600-240 open forces 204 to be removed and 600-150 open forces 105 to be removed. This forces 501 and 402 to be open, but 411 is open, so there is the equilateral triangle 501-411-402. Therefore the solution of the upper triangle is not II, and we have a contradiction. So %\\overline{c}^\\mu_6 \\neq 17%."},{"username":"kareem-carr","timestamp":"0009-03-20T04:00:00.000Z","contents":"Terry had previously mentioned that a GA (Genetic Algorithm) based solution might be something to try so I implemented one. The best results were: [3]^5, 150  \n[3]^6, 450  \n[3]^7, 1302  \n[3]^8, 3780  \n[3]^9, 11340 The GA fairly easily found the first three. It starts getting challenging around 8 and 9\\. I have run the program several times on each example and the results are replicable. I am having trouble with [3]^10\\. The best result so far is 32272\\. I feel some confidence in the results for 7,8 and 9 as I’ve attained them several times (the smaller ones more often than the large ones due to computational restraints). At this point, it might be reasonable to conjecture that the procedure used to predict 1302, 3780 and 11340 will continue to be valid for larger n. I will put the solutions on my blog."},{"username":"terence-tao","timestamp":"2010-03-20T04:00:00.000Z","contents":"Genetic algorithms Kareem, this is very interesting! One is tempted to see how the algorithm would cope with the other three quantities %c'_n, c^\\mu_n, \\overline{c}^\\mu_n% we are studying (and in particular to see what it does with %c'_5%). One difficulty with the Moser problem is that there are substantially more lines (the cube %[3]^n% has %4^n-3^n% combinatorial lines, but %(5^n-3^n)/2% geometric lines). It would also be great to have some details on implementation and various statistics (e.g. population size, number of iterations needed to reach the solution, etc.)."},{"username":"jason-dyer","timestamp":"0007-03-20T04:00:00.000Z","contents":"Metacomment. Someone (Marc again?) posted lower bound improvements to %\\overline{c}^\\mu_{8}% and %\\overline{c}^\\mu_{9}%. I have modified the spreadsheet to match. Since the editor didn’t log in to the wiki, all we have is the IP address. This brings up the odd notion that someone could contribute to the project yet be entirely anonymous."},{"username":"michael-peake","timestamp":"0008-03-20T04:00:00.000Z","contents":"Scores for %c'_6% It might be possible to adapt Terry’s score to limit the possible values of %c'_6%. The following argument assumes f=g=0, which is likely to be correct. Consider the sixty corners 11****, 13**** and so on.  \nGive the %c'_4% arrangements the score %a/15+b/10+c/6+d/3+e%. The total of the scores equals the number of points in the solution. Of the 41-point, 42-point and 43-point solutions, the highest score is 5.833  \nIf that were the highest possible score, %c'_6% would be at most 350."},{"username":"kareem-carr","timestamp":"0008-03-20T04:00:00.000Z","contents":"Genetic Algorithm Details ** Lookup Table ** I created a lookup table for each c_n on which I ran the genetic algorithm. This speeds up checking if a chromosome is line-free tremendously, making this tractable. (Any ideas about efficient ways to do this will have a very large effect on the speed of the algorithm). ** Encoding Solutions ** Chromosome structure: I code the solutions as a list of ones and zeros of length 3^n. The elements are ordered in this way 1…11, 1…12, 1…13, 1…21 and so on. A 0 in position 1 means element 1…11 is not in the set, otherwise it is. ** Making a new generation ** Selection: Any solutions below the mean of the population are completely ignored. The rest are selected with a probablity related to their score. Crossover operator: I pick a random number, m between 2 and (3^n) – 1 and I make a new chromosome with the first m elements of a one chromosome and the last (3^n) – m elements of another chromosome. Mutation operator: I pick a few points at random and flip them. I check any new chromosomes to make sure they are valid. I do this by going through the chromosome elements in a random order (visiting each one) and removing points that conflict with other points. ** Population size ** I use elitism. (I keep the best 5 or sometimes 10 between generations.) Population size: 60 I make more children than I can use (somewhere between 60-80) making the algorithm somewhat Malthusian. I take the best of the children and use them to replace the rest of the population. ** Non-standard addition ** An extremely effective trick has been to use a greedy algoritm on the list of all solutions to: 1\\. pick a small but fixed number of points in each chromosome and flip them if they improve the score. 2\\. run the greedy algorithm on the whole chromosome, going through all positions in a random order, and flipping any that make an improvement. I vary the number of times I use 1 versus 2 in order to control the computational costs of this step. I have found that without this step the algorithm is dramatically less good. ** Adaptive Elements ** The mutation rate is adaptive. I keep statistics on whether there are any repetitions in the chromosomes I generated and if there are, I increase the mutation rate until they disappear. Thus the mutation rate is just high enough to make sure a maximum number of novel solutions are being explored. If I have increased the rate twice in a row then I double the increment size and if I decrease the rate twice in the row then I half the increment size. (This allows quick changes in the rate if necessary.) I also keep statistics on the probability that crossover and the probability that mutaton improve the score. I randomly choose one or the other in proportion to their effectiveness in the last generation. ** Cataloging solutions ** I set a criteria for when I think the genetic algorithm is stuck (200 generations with no change in the best performer). If there is no improvement then I restart the algorithm and I store the best solution. ** Final comments ** For n less than 8, this works very quickly. For n=5, a quick look at a few examples shows 150 is attained within 40 generations. This gets faster after the program has attained the solution once because a better mutation rate than my default guess is found."},{"username":"kareem-carr","timestamp":"0008-03-20T04:00:00.000Z","contents":"906. In a previous thread, I saw that 12 solutions were found for c_5\\. I was also able to find these solutions. In addition, I found 12 solutions for c_7\\. I only found a single solution for c_8 and c_9\\. Could this mean that if n is a prime larger than 3, there are 12 solutions? I am not sure if this is already known but the 12 solutions for c_5 can be constructed from 9 smaller units: 6 of length 5 and 3 of length 70\\. Each solution contains four disjoint units: two of length 70 and two of length 5. The 12 solutions for c_7 can be similarly constructed from 9 smaller units: 6 of length 21 and 3 of length 630\\. Each solution again contains four disjoint units: two of length 630 and two of length 21. I will put the units used to make the solutions on my website along with the solutions as soon as soon as I have enough time to organize the data."},{"username":"terence-tao","timestamp":"0008-03-20T04:00:00.000Z","contents":"Scores for %c'_6% Michael: this is a nice observation! The proposed upper bound of 350 is close enough to the current lower bound of 344 that one can hope that %c'_6% may actually be feasible to compute exactly. (It would be interesting to see how the genetic algorithm performs on this problem.) There may be three ways to establish rigorously that 5.833 is the highest %c'_6%-score of all four-dimensional Moser sets. Firstly, I would assume that Klas’s integer programming methods (which were able to find the maximal value of %a+b+c+d+e%, namely 43) would also be able to find the maximal value of this score in a comparable amount of time (and even classify extremisers, etc.). Secondly, it may be that for 40-point sets and below one can use the inequalities that one already has on a,b,c,d,e (some are collected at the wiki page) together with the inequality %a+b+c+d+e \\leq 40%. A third way is to iterate the score strategy. One can presumably dispose of the e=1 case (we know already, for instance, that such sets have at most 39 points). Once e is gone, we can bound the score of a 4D Moser set by an average of a different type of score for 3D Moser sets. Now, the set of possibilities of statistics (a,b,c,d) of a 3D Moser set should be computable more or less exactly. [Actually, come to think of it, this is a worthwhile project to do anyway, as it may help us a lot in obtaining a human proof of Klas’s very valuable computer-generated facts about 4D Moser statistics that we have been relying heavily on in our 5D analysis. I guess a warmup is to first completely understand the (a,b,c) statistics of a 2D Moser set.]"},{"username":"kareem-carr","timestamp":"0009-03-20T04:00:00.000Z","contents":"907. I have noticed that each group of 70 seems to have two possible groups of 5 which are associated with it. Thus to construct a solution, we pick two groups of 70 (of the three possible) and then for each selected group of 70, we pick one of the two possible associated groups of 5\\. (Again, if this is repetition, I apologize.) I think the same principle applies to the c_7 solutions."},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Moser statistics I’ve started on the wiki a proposal to control the statistics (a,b,c,…) of Moser sets in low dimensions. Some definitions: * A statistic (a,b,c,…) is _attainable_ if there is a Moser set with that statistic. For instance, (2,4,0) is attainable due to the Moser set {11, 12, 21, 23, 32, 33}. * An attainable statistic is _Pareto-optimal_ if it is not pointwise dominated by another attainable statistic. For instance (2,4,0) is Pareto-optimal, but (2,3,0) is not. * An attainable statistic is _extremal_ if it is not the convex combination of other attainable statistics; this is stronger than Pareto-optimal. For instance (3,2,0) is Pareto-optimal but not extremal (it’s a convex combination of (2,4,0) and (4,0,0). If one wants to maximise a linear score (or more generally, a convex score) over all statistics, it suffices to check it on extremal statistics. So it looks like a worthwhile task to classify extremal statistics for small n. So far I have * n=0: The extremal statistics are (1).  \n* n=1: The extremal statistics are (2,0), (1,1).  \n* n=2: The extremal statistics are (4,0,0), (2,4,0), (2,2,1). There is also an additional Pareto-optimal statistic (3,2,0). It looks like n=3 should be completely computable, and then (perhaps with the assistance of the known statistics for 41+ points) n=4 should be also."},{"username":"jason-dyer","timestamp":"0009-03-20T04:00:00.000Z","contents":"Alternate cube problem type Back in the [other thread in the replies to O’Donnell .832](http://gowers.wordpress.com/2009/02/23/brief-review-of-polymath1/#comment-2470) I mentioned using alternate wildcard types to produce variations of Moser’s. In patricular consider one with two wildcards * and #, such that * gives 1, 2, 3 and # gives 2, 3, 1\\. (In other words, rather than the permutation between the wildcards being a 2-cycle, it’s a 3-cycle.) The leads to lines like for example **# which gives 112, 223, and 331. I was curious how this looked so I tried a picture for [3]^3: [Picture of 3 cycle permutation of wildcards](http://numberwarrior.files.wordpress.com/2009/03/3cycle.png) I did not add in the standard combinatorial lines from having only a single wild card. How many removals are needed so there are no more lines? Is this a known problem, or a new one? And if it’s a new problem, what would it be called? “3-cycle cube problem” doesn’t make me entirely happy."},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Alternate cube problem Dear Jason, I haven’t seen this problem before, but if you allow six wildcards that encode all 3! permutations of 123, then this is basically the capset problem (i.e. computing %c''_n%). Randall McCutcheon and Seva Lev independently proposed what we now call the “Strong Roth theorem” on the wiki, which corresponds to using the three wildcards 123, 231, 312 (i.e. cyclic permutations of 123). This problem, like the capset problem has the advantage of being translation-invariant with respect to the addition structure of %({\\Bbb Z}/3{\\Bbb Z})^n%. I’ll put something to this effect on the wiki."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"If a Moser set has 124 points or less and its center slice is of side 41  \nIt must have 11 points or less with two twos in its center slice. Suppose it has 12 or more then all 12 points have values  \nnot equal to 2 divided among 4 coordinates there are 24 such values  \nSo one coordinate must have six of these values. Now if four of these are are equal to one or four equal to three  \nwe have one side slice equal to 40 (since a 41 point slice can have  \nat most d equal to three) and combined with  \nthe fact we have shown the center slice must be 41 or lower and the third slice must be 43 or less we have the total points sum to 124  \nor less and we are done so they must be divided 3 3 and then each side  \nslice must be 41 or less and that combined with the center slice being 41 or less we 124 points or less and we are done."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"If the center slice of a Moser set is size 41 or more the set must have  \n124 or less points. Because we have shown c of the center slice must be less than 12  \nwe are left only with case where there are c = 6, note that these points have three 2’s  \noverall in the configuration . We will first show that there must  \nbe three or more points with three twos outside the center slice, Now since c = 6 the 6 points have  \n12 coordinates with value 1 or 3 divided among 4 coordinates  \none coordinate must have at least three of these coordinates now if we slice  \nat this coordinate we will three coordinates which will not be in the  \ncenter slice then if there are less than three coordinates with three twos  \nouts we will end up with c for the center slice being less than 6  \nand hence the center slice will have to have 40 points or less and the three points  \nwith two threes that moved outside the center will if they lie  \nin one slice lower its value to 41 hence lower the total value to or less  \nif they are split among two slices those slices will have value 42 or less  \nand combined with 40 or less of the center will give 124 or less. So we have must 9 points then we must have one cut with side slices 43 or more and since d=0 for the side slices we have all 9 points with three twos  \nIn the center slice then we have 18 values not equal to 2 divided among  \n4 coordinates one must have 5 values we cut along that coordinate  \nand if one of the side slices has 4 or more points then that slice will have 40 or  \nless points and that combined with the 41 points of the center slice  \nand the fact the remaining slice must have 43 or less points gives 124  \nand we are done. So we must have 3 such points in one outside slice  \nand two in another. That gives at most 41 points in the center slice  \nat most 42 points in one side slice and at most 41 points in the other slice  \nwhich gives less than 124 points and we are done."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"If a Moser set contains a point with three twos it has 124  \npoints or less.  \nWe have there is cut which gives one slice with  \n43 points on each outside slice thus the point with three twos  \nmust lie in the center slice of this cut we slice at a value where this point  \ndoes not equal two then in the new set of slices the center slice  \nmust have 40 points or less and the point with three twos must  \nlie on one of the outside slices if it has value 41 the we must have  \na total of 124 points and we are done if it has value 42 it must have  \ntwo points with three twos and one coordinate at which these points  \nhave values 3 and 1 we slice at this value and get that the center slice  \nmust have 40 points or less and since the two side slices have at least  \none point with three two’s each they must have 42 points or less and  \nthe total is 124 or less and we are done."},{"username":"michael-peake","timestamp":"2010-03-20T04:00:00.000Z","contents":"Pareto set of %{}[3]^3% There seem to be 22 Pareto-optimal statistics: (3,6,3,1),(4,4,3,1),(4,6,2,1),(2,6,6,0),(3,6,5,0),(4,4,5,0),(3,7,4,0),(4,6,4,0), (3,9,3,0),(4,7,3,0),(5,4,3,0),(4,9,2,0),(5,6,2,0),(6,3,2,0),(3,10,1,0),(5,7,1,0), (6,4,1,0),(4,12,0,0),(5,9,0,0),(6,6,0,0),(7,3,0,0),(8,0,0,0) If I use the same argument as in 904, %c'_5% is only restricted to 132 or below, which is not very effective. Even excluding points with three 2s, as Kristal has achieved, this argument only restricts %c'_5% to 128 or below."},{"username":"kareem-carr","timestamp":"0007-03-20T04:00:00.000Z","contents":"Hi everyone, Sorry about the delay. Judging from the traffic there has been a lot of interest. There is now a zip file on my website with the solutions that I have managed to compute so far. I tried to make it as complete as I could. [http://twofoldgaze.wordpress.com/2009/03/06/hales-jewett/](http://twofoldgaze.wordpress.com/2009/03/06/hales-jewett/)"},{"username":"kareem-carr","timestamp":"0007-03-20T04:00:00.000Z","contents":"Just one warning, there is an error in the long form of the disjoint sets which generate the solutions for [3]^7\\. The elements listed are of length 5 and not the expected 7 (a consequence of not setting a parameter n=7 somewhere after I had finished doing the 12 solutions of n=5.) I will get it fixed."},{"username":"terence-tao","timestamp":"0007-03-20T04:00:00.000Z","contents":"917. I’m a bit busy today, so only some quick comments here: Kristal: I haven’t had time to verify your results yet, but they look very interesting! If you have time you might want to merge them with the lemmas on the wiki page. Michael: Nice! Your list shows, by the way, that the previous computation that a 3D Moser set with 222 has at most 13 points is sharp. (This is one of the reasons it’s nice to have a list of extremal triples). Presumably if one reduces from Pareto-optimal triples to extremal triples then one should have a much smaller (and hence more tractable) set of triples to work with. Once one has those, one should be able to describe the sharp linear inequalities between a,b,c,d in 3D. For instance, in 2D, where the extremals are (4,0,0), (2,4,0), (2,2,1), there is only one non-trivial sharp linear inequality %2a+b + 2c \\leq 8% between the a,b,c, together of course with the trivial inequalities %a \\geq 0%, %0 \\leq b \\leq 4%, %0 \\leq c \\leq 1%. Once we have the sharp linear inequalities in 3D, the averaging trick should then give us a bunch of useful linear inequalities in 4D. These are probably not optimal in themselves (in particular, they probably won’t be able to recover the sharp bound of 43, which still does not have a short human proof) but I’m hoping in conjunction with the data on 41, 42, and 43 point sets, we can do things like verify your conjecture that 4D Moser sets have a 5D score of at most 5.833. Kareem: thanks for the data and the details of the GA!"},{"username":"jason-dyer","timestamp":"0009-03-20T04:00:00.000Z","contents":"%\\overline{c}^\\mu_7 \\leq 22% Finishing %\\overline{c}^\\mu_6% will take some time, so here’s a quick chunk off of %\\overline{c}^\\mu_7%. I’m using the same eight extremal solutions to %\\overline{c}^\\mu_3% as previous proofs: Solution I: remove 300, 020, 111, 003  \nSolution II: remove 030, 111, 201, 102  \nSolution III (and 2 rotations): remove 030, 021, 210, 102  \nSolution III’ (and 2 rotations): remove 030, 120, 012, 201 Suppose the 8x8x8 lattice can be triangle-free with only 13 removals. Slice the lattice into region A (070-340-043) region B (430-700-403) and region C (034-304-007). Each region must have at least 4 points removed. Note there is an additional disjoint triangle 232-322-223 that also must have a point removed. Therefore the points 331, 133, and 313 are open. 331-313 open means 511 must be removed, 331-133 open means 151 must be removed, and 133-313 open means 115 must be removed. Based on the three removals, the solutions for regions A, B, and C must be either I or II. All possible combinations for the solutions leave several triangles open (for example 160-520-124). So we have a contradiction, and %\\overline{c}^\\mu_7 \\leq 22%."},{"username":"jason-dyer","timestamp":"2010-03-20T04:00:00.000Z","contents":"…and I completely blanked on the rotations of II (fixing 151 helps nullify the III cases, but allows II to be rotated), which means I need to go back and fix my previous proofs. The proof of 918 is still fine, although a complete listing of cases might be in order."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"For n=6 a Moser set must contain 373 or less points. We have from 912 that if n=5 a 125 point set must have its center slice  \n40 or less. Then slice arbitrarily call the coordinate i if the Moser set is 374  \nor more two of the slices must have 125 points and those slices must  \nhave a center slice of 40 then if we slice again call that coordinate j so we have 9  \nsets of 81 points with each set representing one choice of two coordinates i, j  \nby the above For the two values of i with 125 points the center slices must be forty or less  \nwhich means the center slice of the set with respect to j has at most  \n40 + 40 + 43 points and by looking at the j slices we get 125 + 125 + 123 = 373 points or less."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"If a Moser set has 125 points then the number of points with  \nexactly two 2’s is either 78 or 79\\. It must be less than 80  \nby lemma three in the section of the wiki dealing with Moser  \nsets for n=5. If it is 77 or less then the three or more missing points must have  \n6 or more values equal to 2 divided among 5 coordinates  \none coordinate must have two such values. Then in  \nthat coordinate b must be equal to 30 or less. Now  \nif this center slice is 38 or less then we have 38 + 43 + 43 is less than 125 and we are done. So it must be 39 or more. We have  \n4a + b is less than or equal to 64 from the section of the  \nwiki on Moser sets devoted to n = 4\\. so we have a + b  \nis 39 or more and 4a + b is 64 or less we can subtract  \nand get 3a is less than 25 so a is less than 9 but that combined  \nwith b is thirty or less gives a total set size of 38 or less which results in a contradiction as noted above. So the number of points with exactly 2 2’s in a Moser set with n=5  \nWith 125 or more points is greater than 77 and not equal to 80.  \nSo it must be 78 or 79."},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"I am going to have to redo 912 I made a mistake in which I assumed their  \nhad to be two side slices in one cut with 43 points or more.  \nHowever it can be easily fixed. Here is the corrected version: If the center slice of a Moser set is size 41 or more the set must have  \n124 or less points. Because we have shown c of the center slice must be less than 12  \nwe are left only with case where there are c = 6, note that these points have three 2’s  \noverall in the configuration . We will first show that there must  \nbe three or more points with three twos outside the center slice, Now since c = 6 the 6 points have  \n12 coordinates with value 1 or 3 divided among 4 coordinates  \none coordinate must have at least three of these coordinates now if we slice  \nat this coordinate we will three coordinates which will not be in the  \ncenter slice then if there are less than three coordinates with three twos  \nouts we will end up with c for the center slice being less than 6  \nand hence the center slice will have to have 40 points or less and the three points  \nwith two threes that moved outside the center will if they lie  \nin one slice lower its value to 41 hence lower the total value to or less  \nif they are split among two slices those slices will have value 42 or less  \nand combined with 40 or less of the center will give 124 or less.  \nSo we have must 9 points then we must have one cut with side slices in one of the four categories  \n1\\. (2,16,24,0,0) [42 points, score: 62]  \n2\\. (4,16,23,0,0) [43 points, score: 62 1/3]  \n3\\. (4,15,24,0,0) [43 points, score: 62 3/4]  \n4\\. (3,16,24,0,0) [43 points, score: 63]  \nSee the wiki on Moser sets, the section n = 5 lemma four and following discussion  \nfor the details of this. This implies d=0 for the side slices of this cut we have all 9 points with three twos  \nin the center slice then we have 18 values not equal to 2 divided among  \n4 coordinates one must have 5 values we cut along that coordinate  \nand if one of the side slices has 4 or more points then that slice will have 40 or  \nless points and that combined with the 41 points of the center slice  \nand the fact the remaining slice must have 43 or less points gives 124  \nand we are done. So we must have 3 such points in one outside slice  \nand two in another. That gives at most 41 points in the center slice  \nat most 42 points in one side slice and at most 41 points in the other slice  \nwhich gives less than 124 points and we are done."},{"username":"klas-markstrom","timestamp":"0002-03-20T04:00:00.000Z","contents":"Fujimura’s problem  \nEarlier today coded up Fujimura’s problem for integer programming. Here are the results n=3, maximum 6 points, 10 solutions  \nn=4, maximum 9 points, 1 solution  \nn=5, maximum 12 points, 1 solution  \nn=6, maximum 15 points, 4 solutions  \nn=7, maximum 18 points, 85 solutions  \nn=8, maximum 22 points, 72 solutions  \nn=9, maximum 26 points, 183 solutions  \nn=10, maximum 31 points, 6 solutions  \nn=11, maximum 35 points, 576 solutions  \nn=12, maximum 40 points, 876 solutions Here are the solutions for n=3  \n[http://abel.math.umu.se/~klasm/solutions-n=3-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=3-k=3-FUJ) Here are the solutions for n=4  \n[http://abel.math.umu.se/~klasm/solutions-n=3-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=3-k=3-FUJ)"},{"username":"klas-markstrom","timestamp":"0002-03-20T04:00:00.000Z","contents":"924.  \nHere are the solutions for n=5  \n[http://abel.math.umu.se/~klasm/solutions-n=5-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=5-k=3-FUJ) Here are the solutions for n=6  \n[http://abel.math.umu.se/~klasm/solutions-n=6-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=6-k=3-FUJ)"},{"username":"klas-markstrom","timestamp":"0002-03-20T04:00:00.000Z","contents":"925  \nHere are the solutions for n=7  \n[http://abel.math.umu.se/~klasm/solutions-n=7-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=7-k=3-FUJ) Here are the solutions for n=8  \n[http://abel.math.umu.se/~klasm/solutions-n=8-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=8-k=3-FUJ)"},{"username":"klas-markstrom","timestamp":"0002-03-20T04:00:00.000Z","contents":"926  \nHere are the solutions for n=9  \n[http://abel.math.umu.se/~klasm/solutions-n=9-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=9-k=3-FUJ) Here are the solutions for n=10  \n[http://abel.math.umu.se/~klasm/solutions-n=10-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=10-k=3-FUJ)"},{"username":"klas-markstrom","timestamp":"0002-03-20T04:00:00.000Z","contents":"927  \nHere are the solutions for n=11  \n[http://abel.math.umu.se/~klasm/solutions-n=11-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=11-k=3-FUJ) Here are the solutions for n=12  \n[http://abel.math.umu.se/~klasm/solutions-n=12-k=3-FUJ](http://abel.math.umu.se/~klasm/solutions-n=12-k=3-FUJ)"},{"username":"kareem-carr","timestamp":"0003-03-20T04:00:00.000Z","contents":"928. Hi everyone, I have been computing the first few c’_prime numbers and I got an extremal that was too large for n=3\\. Can someone help me find the geometric line in this solution? {112, 113, 121, 123, 131, 132, 211, 213, 231, 233, 311, 312, 321, 323, 332, 333} Thank you. (I apologize if this is a repeated comment, something seems like it went wrong with my submission.)"},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"Oops, I think that was my fault; I wrote %c'_3 = 14% on the blog post when it should be %c'_3=16%. Corrected now…"},{"username":"guest","timestamp":"0006-03-20T04:00:00.000Z","contents":"930. Kareem, Have you tried a version of your genetic algorithm *without* crossover? In other words, a randomized version of hill-climbing based on your mutation operator? (run many times with different starting points.) I’ve heard several people who have worked with GAs report that sometimes switching to randomized hill-climbing, iterated with different initial conditions, gives results more efficiently. (if nothing else, this is easy to test–just comment out the crossover step!)"},{"username":"michael-peake","timestamp":"0004-03-20T04:00:00.000Z","contents":"Details of Moser(3), 3-dim Following on from 914\\. and 917., the extremal statistics are (3,6,3,1),(4,4,3,1),(4,6,2,1),(2,6,6,0),(4,4,5,0),(4,6,4,0),(4,12,0,0),(8,0,0,0) Their sharp linear bounds are :  \n2a+b+2c+4d <= 22  \n3a+2b+3c+6d <= 36  \n7a+2b+4c+8d <= 56  \n6a+2b+3c+6d <= 48"},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"If a Moser set has 125 points or more then it must have at least one cut where the center slice has 39 points or less. If not all points must have center slice exactly equal to 40, Now one set of outside slices must have scores that sum to 125 or more then by the discussion following lemma 4 of the Wiki on Moser sets in the section devoted to n=5 and the slices must be chosen from the following  \n• (2,16,24,0,0) [42 points, score: 62]  \n• (4,16,23,0,0) [43 points, score: 62 1/3]  \n• (4,15,24,0,0) [43 points, score: 62 3/4]  \n• (3,16,24,0,0) [43 points, score: 63] Now since the slice has 40 points the slice must  \nContain the slice with 42 points and one of the slices  \nWith 43 points however since the sum is 125 or more  \nIt must contain the bottom slice thus the following is forced  \nAs statistics of slices (2,16,24,0,0) [42 points, score: 62]  \n(3,16,24,0,0) [43 points, score: 63] Now the sum of these two slices is 125  \nSince the sum of all 5 sets of slices is 625  \nBy lemma 4 of the section devoted to n=5  \nSo we can subtract and get the sum of the  \nFour remaining slices is 500 but then we can repeat the  \nProcess getting another cut with the same set of statistics of slices  \nAnd the three remaining sets of slices equal to 375 we can repeat  \nThe process until we are done and get all 5 slices have the  \nFollowing statistics  \n(2,16,24,0,0) [42 points, score: 62]  \n(3,16,24,0,0) [43 points, score: 63] The presence in the above of the 24 in the third position  \nmeans that the configuration must have all 80 points  \nwith exactly two twos but by lemma 3 of the Wiki  \nin the section devoted to n=5 we have that implies  \nthat the configuration has 124 points or less and we are done."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"If a configuration has 125 or more points  \nthen it must have 6, 7 or 8 points with  \nno twos. There must be a cut of the configuration whose outside slices sum to  \n125 or more.  \nIf it has 40 points it must sum to exactly 125  \nand since we have the sum of all 5 sets of outside slices  \nis 125 we can subtract and get the remaining sets of slices  \nhas sum 500 we keep repeating this step until a cut with  \ncenter slice 39 or less is chosen this must happen because  \nwe will eventually run out of cuts with center slice 40.  \nWe have shown that there is at least one cut  \nwith center slice 39 or less. Then we will eventually get  \na cut whose side slices sum to 125 or more whose  \ncenter slice is less than 40 then since we have the side slices  \nMust be 1. (2,16,24,0,0) [42 points, score: 62]  \n2. (4,16,23,0,0) [43 points, score: 62 1/3]  \n3. (4,15,24,0,0) [43 points, score: 62 3/4]  \n4. (3,16,24,0,0) [43 points, score: 63]  \nThe center slice must be of size 39 and the choice of the outside slice  \nwith 42 points is not possible then since the values of the remaining  \npossible slices have a=3 or 4 we must have 6, 7 or 8 points with  \nno coordinates equal to 2 in the configuration and we are done."},{"username":"kristal-cantwell","timestamp":"0002-03-20T04:00:00.000Z","contents":"If a Moser set with exactly 125 points has exactly 78 points  \nwith two 2’s it must have either 7 or 8 points  \nwith no twos. We know that it must have 6, 7 or 8 points with  \nno twos, if it has 6 then it must have 125 minus 78 minus 6  \nequals 41 points with exactly one two. Then one of the center slices must contain 9 or more  \npoints with exactly one two in the configuration. But that leads to  \nA contradiction since the slice is equal to 39 or 40  \nTake the case where it is equal to 39 We have 4a + b is less than or equal to 64 from the section of the wiki on Moser sets devoted to n = 4\\. so we have a + b is 39 and 4a + b is 64 or less we can subtract and get 3a is less than 25 so a is less than 9 and we have a contradiction so the center slice must have 40 points  \nThen again we have 4a + b is less than or equal to 64 from the section of the wiki on Moser sets devoted to n = 4\\. so we have a + b is 40 and 4a + b is 64 or less we can subtract and get 3a is less than 24 so a is less than 8 but that gives a contradiction so we are done."},{"username":"terence-tao","timestamp":"0003-03-20T04:00:00.000Z","contents":"Moser(3) I reorganised the Moser wiki page a bit to incorporate the recent progress (and also simplified some of the arguments). It now seems that we have the distribution (A,B,C,D,E,F) of 125-point Moser sets in 5D pinned down to just three possibilities: * (6,40,79,0,0,0)  \n* (7,40,78,0,0,0)  \n* (8,39,78,0,0,0) It seems that we are reaching the limit of what we can do by analysing slices, and we will have to start understanding sphere packing better…"},{"username":"kareem-carr","timestamp":"0004-03-20T04:00:00.000Z","contents":"guest: “Have you tried a version of your genetic algorithm *without* crossover?” Thanks for the suggestion. This is only my second genetic algorithm project so I appreciate the input. I tried it without crossover and crossover does seem to help. In the beginning, I tried crossover on the chomosomes with regular crossover points at the one third and two thirds mark. That did seem to be disasterous, leading to very early convergence. Do you happen to know why it was suggested that crossover not be used? I measure the probability that a particular operator can improve solutions so bad operators are not over used. However, if the issue is early convergence then I have no ideas on defending against that other than automated restarting of the algorithm."},{"username":"jason-dyer","timestamp":"0008-03-20T04:00:00.000Z","contents":"GA improvement _I created a lookup table for each c_n on which I ran the genetic algorithm. This speeds up checking if a chromosome is line-free tremendously, making this tractable. (Any ideas about efficient ways to do this will have a very large effect on the speed of the algorithm)._ Could you go into detail about how you do this now?"},{"username":"kareem-carr","timestamp":"0009-03-20T04:00:00.000Z","contents":"Jason Dyer: For each element, I figured out the two other elements that are needed to make a line. For instance, 123 means that both 122 and 121 can’t be in the set. For each element, I have all possible pairs that would conflict if they were both elements of the set. If I do a simple transformation of 1->0,2->1 and 3->2 then I get numbers in base three which can then of course be representable as numbers in base 10\\. Thus I can represent each pair that I need to avoid as two numbers. (This is good as integers can be stored easily in a big table.) So that’s how I store them, I number everything from 0 to 3^n-1 and for each element I have a list of pairs that I have to check for. I store all this in an array where the nth row contains each pair in order. The solutions are represented as binary numbers of length 3^n where if the mth element of [3]^n is a member of the set then the mth binary digit is set to 1 otherwise it is set to zero. To check if a solution valid, for each position p, I check row p of my lookup table. I check pair by pair looking at the binary digits corresponding to each pair and making sure they aren’t both set to 1. For example, my look up table for c_2 is the following: 1 2 3 6 4 8  \n0 2 4 7 -1 -1  \n0 1 5 8 -1 -1  \n0 6 4 5 -1 -1  \n0 8 1 7 3 5  \n2 8 3 4 -1 -1  \n0 3 7 8 -1 -1  \n1 4 6 8 -1 -1  \n0 4 2 5 6 7 The -1’s mark the end of the list so my program knows when the list of pairs has ended."},{"username":"michael-peake","timestamp":"0005-03-20T04:00:00.000Z","contents":"%c'^6 \\le 365% Use the classification of 3-dim solutions. The %{}[3]^6% cube has 160 corner cubes, eg 111***,  \n240 edge cubes, eg. 112*** and 120 face cubes, 122*** Give the corner cubes a score %s_1 = 0.05a+0.026b + 0.05c + 0.1d%  \nGive the edge cubes a score %s_2 = 0.074b+0.0415c + 0.0714d+0.1429e%  \nGive the face cubes a score %s_3 = 0.0755c + 0.0286d + 0.0357e + 0.1f% From the list of extremal solutions, %s_1 \\le 0.556, s_2 \\le 0.831, s_3 \\le 0.645% The total score is %a+b+c+d+e+f = 160s_1+240s_2 + 120s_3 < 365.8%"},{"username":"kareem-carr","timestamp":"0008-03-20T04:00:00.000Z","contents":"Possible lower bounds for c_6 and c_7 I just wanted to give a bit of an update concerning c’_n. My best solutions are: c’_5: 124 (399 unique solutions – 3 million generations)  \nc’_6: 353 (26 unique solutions – 3 million generations)  \nc’_7: 978 (1 solution – 5.4 million generations) I will put these on my website when I can so that the solutions can be doublechecked. At this point, I can say this is a much harder problem than c_n. I have included the amount of computation that I have thrown at each problem in order to indicate the diminishing returns with increasing n. I think it’s entirely possible that there are larger solutions. (Sorry I keep forgetting to put the numbers, Terry.)"},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"%c'_6% Kareem, it seems that your 6D and 7D solutions are better than those we already had, it will be interesting to analyse their structure. (It occurred to me that one might be able to “seed” a GA by throwing in some decent solutions that we already have and seeing whether evolution will improve it… so perhaps it’s still worth compiling a pool of reasonably good solutions here even if they are non-optimal.) Incidentally, your 353-point example example shows that the conjecture in 904 is incorrect; either the score a/15+b/10+c/6+d/3+e for 4D sets can exceed 5.8333, or the 353-point example has a nonzero value of f or g. I guess what is going on here is that large values of d or e, which don’t show up in the 41+ point examples, can occur for smaller examples and result in a score larger than that for the big examples."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"If a Moser set for n=5 has 125 points  \nIt must have the following statistics:  \n• (7,40,78,0,0,0) or  \n• (8,39,78,0,0,0) Assume it has statistics (6,40,79,0,0,0),  \nthe only remaining case  \nNow look at the 6 points with all coordinates not  \nequal to 2\\. Any set of 5 of these has one pair of  \npoints that differ in exactly two coordinates  \nSee the proof of lemma 3 in the Moser wiki  \nWe start with 5 of these points and get at least one  \npair that differs in exactly two coordinates if there is more than  \none we choose a pair  \nThen we remove one of the points the pair and add the  \nadditional point to replace then we have two pairs. Then we note that each of these pairs of points  \nhas the potential of forming a line with the  \npoint p that has two twos in the coordinates i and j that the pair  \nof points differ and where the points of the pair have  \nthe same coordinate p has that coordinate now  \neach such point p has the possibility of blocking two  \nsuch lines names those pairs nameless the pair  \nof points that has 11 and 33 in the coordinates where  \np has two twos and the same coordinates as p elsewhere  \nand the points that have 13 and 31 in the points where  \np has two 2’s and the same coordinates elsewhere Now in the case of this Moser set with  \nStatistics (6,40,79,0,0,0)  \nWe have two such pairs and since one of 80  \nPoints with two twos is missing to avoid  \nA line the missing point must block both lines for the two  \nPairs as above. Now we have the from the above  \nfour points which agree  \nOn three coordinates and take on all  \npossible values on the remaining two coordinates.  \nThey have the potential of forming lines  \nwith those points with one two, namely those  \npoints which agree which agree with the four  \npoints on the the three coordinates mentioned  \nabove and have one two and one arbitrary  \nvalue on the remaining two coordinates  \nthere are four such points. The point p  \nmentoned earlier which agrees with the above  \npoints in the three coordinates mentioned earlier  \nand has 2’s in the remaining coordinate  \nforms two lines with these points.  \nThese lines are empty since p or any  \nof the other four points are in the Moser  \nset they form a a line with the points  \nin the two pairs. The two lines were counted in the  \nInequality 2B + C is less than or equal to  \n160 with weight one half. which is lemma three in the Wiki section n=5  \nwe can remove these two lines and count over the remaining lines  \nsince they are empty 2B +C will not be effected but the 160  \nwill be reduced to 158 so then we will have  \n2B +C is less than or equal to 158  \nbut 2B+C is 159 in the case with statistics  \nStatistics (6,40,79,0,0,0)  \nSo only the remaining two statistics are possible and we are done."},{"username":"terence-tao","timestamp":"0003-03-20T04:00:00.000Z","contents":"An alternate proof of %c'_4 \\leq 43% Here is a relatively short, mostly human proof that %c'_4 \\leq 43% using the extremals from the 3D theory. When e=1 (i.e. the 4D set contains 2222) then we have at most 41 points (in fact at most 39) by counting antipodal points, so assume e=0. Define the score of a 3D slice to be a/4+b/3+c/2+d. Observe from double counting that the size of a 4D set is the sum of the scores of its eight side slices. But by looking at the extremals (see the spreadsheet [http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuKZ2DyzO9EOg&hl=en#](http://spreadsheets.google.com/ccc?key=p5T0SktZY9DuKZ2DyzO9EOg&hl=en#) ) we see that the largest score is 44/8, attained at only one point, namely when (a,b,c,d) = (2,6,6,0). So the only way one can have a 44-point set is if all side slices are (2,6,6,0), or equivalently if the whole set has statistics (a,b,c,d,e) = (4,16,24,0,0). But then we have all the points with two 2s, which means that the four “a” points cannot be separated by Hamming distance 2\\. We conclude that we must have an antipodal pair among the “a” points with an odd number of 1s, and an antipodal pair among the “a” points with an even number of 1s. By the symmetries of the cube, we may take the a-set to then be 1111, 3333, 1113, 3331\\. But then the “b” set must exclude both 1112 and 3332, and so can have at most three points in the eight-point set xyz2 (with x,y,z=1,3) rather than four (to get four points one must alternate in a checkerboard pattern). Adding this to the at most four points of the form xy2z, x2yz, 2xyz we see that b is at most 16, a contradiction. This argument also heuristically explains why the 43-point Moser sets have statistics close to (4,16,24,0,0)."},{"username":"terence-tao","timestamp":"0005-03-20T04:00:00.000Z","contents":"%c'_6 \\leq 364% Michael’s method in 939 can be pushed a little bit. Given a Moser set in %[3]^n%, let %\\alpha_i% denote the fraction of the points with i 2s in the cube that lie in the set; in terms of the (a,b,c,d,e) notation, we have %\\alpha_0 = a/2^n, \\alpha_1 = b/(n 2^{n-1})%, etc. The double counting argument applied to side slices of %[3]^{n+1}% tells us that any linear inequality between the %\\alpha_i% that holds at dimension n, is also true at dimension n+1\\. Applying to middle slices instead, we see that any linear inequality between the %\\alpha_i% that holds at dimension n, is also true at dimension n+1 after replacing each %\\alpha_i% with %\\alpha_{i+1}%. For instance, when n=3 a typical inequality is %2a+b+2c+4d \\leq 22%, which in normalized notation is %8\\alpha_0+6\\alpha_1+6\\alpha_2+2\\alpha_3 \\leq 11%. This inequality then holds for all higher n. For %n \\geq 4% we also get the shifted inequality %8\\alpha_1+6\\alpha_2+6\\alpha_3+2\\alpha_4 \\leq 11%, for %n \\geq 5% we get %8\\alpha_2+6\\alpha_3+6\\alpha_4+2\\alpha_5 \\leq 11%, etc. Throwing all the inequalities descended from n=3 into a big list and using Maple’s linear programming routines, I can get an upper bound of 44 in four dimensions, 126 in five dimensions, and 364 in six dimensions. Here is the 6D code: > with(simplex); > X := [8*a+6*b+6*c+2*d<=11, 4*a+4*b+3*c+d<=6, 7*a+3*b+3*c+d <= 7, 4*b+2*c+d<=4, 4*a+6*c+2*d<=7, 6*a+3*c+d<=5, 2*a+d<=2, 2*b+d <= 2, 2*c+d <= 2, d <= 1, 8*b+6*c+6*d+2*e<=11, 4*b+4*c+3*d+e<=6, 7*b+3*c+3*d+e <= 7, 4*c+2*d+e<=4, 4*b+6*d+2*e<=7, 6*b+3*d+e<=5, 2*b+e<=2, 2*c+e <= 2, 2*d+e <= 2, e <= 1, 8*c+6*d+6*e+2*f<=11, 4*c+4*d+3*e+f<=6, 7*c+3*d+3*e+f <= 7, 4*d+2*e+f<=4, 4*c+6*e+2*f<=7, 6*c+3*e+f<=5, 2*c+f<=2, 2*d+f <= 2, 2*e+f <= 2, f <= 1, 8*d+6*e+6*f+2*g<=11, 4*d+4*e+3*f+g<=6, 7*d+3*e+3*f+g <= 7, 4*e+2*f+g<=4, 4*d+6*f+2*g<=7, 6*d+3*f+g<=5, 2*d+g<=2, 2*e+g <= 2, 2*f+g <= 2, g Z := 64*a+192*b+240*c+160*d+60*e+12*f+g; > Z := 64*a+192*b+240*c+160*d+60*e+12*f+g; > maximize(Z, X, NONNEGATIVE); > evalf(subs(%,Z)); 364.6984127 Incidentally, the statistics (a,b,c,d,e,f,g) of the maximiser are [31.49206349, 94.98412698, 121.9047619, 83.80952381, 25.07936508, 7.428571429, 0.] or in normalized form %(\\alpha_0,\\ldots,\\alpha_6)% [0.4920634921, 0.4947089947, 0.5079365079, 0.5238095238, 0.4179894180, 0.6190476190, 0.] thus one is filling up about half of each Behrend sphere (except for the origin 222222), which seems to be different behaviour from lower dimensions. [Perhaps this is because 6D is the first place where we really have to deal with sets of density less than 1/2.] Presumably one could use the 4D and 5D data to generate more inequalities to dump into the pot, which should shave this upper bound downwards a bit. (But I did try tossing in the fact that 4D sets have at most 43 points, and (optimistically) 5D sets have at most 124 points, but got no improvement in the 6D bound this way.)"},{"username":"gilang","timestamp":"2011-03-20T04:00:00.000Z","contents":"Prof. Tao, I am very interested with your blog although i am not a talented in mathematic. But now i will not ask you all about the complexity or what you have posted. My question just simple and for long time with big patient i waiting for propose this quastion for one of the great mathematician. What you feel or crossed in your mind that you are one of the outstanding mathematician ? Can you imagine that when you at 7 years old you just a little boy study mathematic and now you take the control on mathematic, i mean You haved extended Mathematic standpoint not as a student of mathematic anymore? Maybe my question too laud/praise for you and i know you are a humble mathematician as I know. But this is my trully question from my mind with big curiosity to you? I will be glad when you respond my question"},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"Moser set n=5. If a Moser set for n=5 has 125 points  \nIt must have the following statistics:  \n• (8,39,78,0,0,0) Assume it does not then the only remaining case  \nis that it must have these statistics:  \n• 7,40,78,0,0,0) Now look at the 7 points with all coordinates not  \nequal to 2 Any set of 5 of these has one pair of  \npoints that differ in exactly two coordinates  \nsee the proof of lemma 3 in the Moser wiki.  \nWe start with 5 of these points and get at least one  \npair that differs in exactly two coordinates if there is more than  \none we choose a pair  \nthen we remove one of the points the pair and add the  \nadditional point to replace then we have two pairs. We  \nkeep doing this until we have three pairs. If a line is to be avoided there must be a point with  \ntwo 2’s not in the in Moser set which has 2’s  \nwhere the points of the pair differ and for the  \npoints where the pair agree it also agrees, if this  \ndoes not happen we will have a line. Now we have the from the above  \nwe have four points which agree  \non three coordinates and take on all  \npossible values on the remaining two  \ncoordinates. They have the potential of forming lines  \nwith those points with one two, namely those  \npoints which agree which agree with the four  \npoints on the the three coordinates mentioned  \nabove and have one two and one arbitrary  \nvalue on the remaining two coordinates  \nthere are four such points call the a, b,c,d. The point p  \nmentoned earlier which agrees with the above  \npoints in the three coordinates mentioned earlier  \nand has 2’s in the remaining coordinate  \nforms two lines with these points.  \nthese lines are empty since p or any  \nof the other four points are in the Moser  \nset they form a a line with the points  \nin the two pairs. The two lines were counted in the  \nInequality 2B + C is less than or equal to  \n160 with weight one half. which is lemma three  \nwe can remove these two lines and count over the remaining lines  \nsince they are empty 2B +C will not be effected but the 160  \nwill be reduced to 158 so then we will have  \n2B +C is less than or equal to 158  \nbut 2B+C is 158 in the case with statistics  \nStatistics (7,40,78,0,0,0) Now this gives 2B+C for the remaining lines is less  \nThan or equal to 158 but 2B+C in this case = 158  \nWhich means that every line must have exactly two points  \nWe will use this fact in the proof that follows by starting  \nWith empty points taking lines that aren’t the set that  \nHave the empty set and hence have to have the remaining  \nPoint in the line we will use these points to force a line  \nIn the set and thus get a contradiction. For the empty set we start with the points a, b, c, d  \nMentioned earlier have one two we are going  \nTo force the existence of two sets of four points  \nSimilar to a,b,c,d  \nThey are identical to points of a, b, c,d except  \nIn one coordinate. Let us assume that the points  \nOf p have the following form 111xy where  \nThe fixed coordinates are assumed to be 1  \nAnd in the first three places and x and y are  \nThe moving coordinates. Then a b c d  \nAre of the form 11121, 11123, 11112, 11132.  \nThe two new sets of similar to a b c d  \nAre 13121, 13123, 13112, 13132 and  \n11321, 11323, 11312, 11332 we get that  \nthey must be in the Moser set by the above  \nand looking at lines 1×121, 1×123, 1×112, 1×132  \nand 11×21, 11×23, 11×12, 11×32  \nwhere x denotes a coordinate that may take  \nany of three values which are  \nnot in the lines removed and hence by the above  \nmust have 2 points and hence give us the 8 new  \npoints mentioned above in the Moser set.  \nNow we note that the points  \n13121, 13123, 13112, 13132  \ntogether with 13122 forms a monochromatic line  \nin fact two monochromatic lines so the 13122  \nmust not be in the set since we already have  \n11122 is not in the set all other points with  \ntwo twos must be in the set now  \nlook at the points 11321, 11323, 11312, 11332  \nwhich must be in the set  \ntogether with 11322 which must be in the set they  \nform a monochromatic line in the set so we get  \na contradiction. Similarly we can form 8 such points  \nfrom the points p and a, b, c and d above  \nno matter what the value of the the fixed coordinates  \nand no matter the position of the fixed coordinates  \nand force a monochromatic line in the same way  \nand the get a contradiction so the only case remaining  \nis: that having statistics (8,39,78,0,0,0) and we are done."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"Note on 945 I should not have put the word monochromatic before line in several  \ncases in the second to last paragraph."},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"Rewrite of 942 If a Moser set for n=5 has 125 points  \nIt must have the following statistics:  \n• (7,40,78,0,0,0) or  \n• (8,39,78,0,0,0) Assume it has statistics (6,40,79,0,0,0),  \nthe only remaining case.  \nNow look at the 6 points with all coordinates not  \nequal to 2 in the set. Any set of 5 of these has one pair of  \npoints that differ in exactly two coordinates  \nSee the proof of lemma 3 in the Moser wiki.  \nWe start with 5 of these points and get at least one  \npair that differs in exactly two coordinates if there is more than  \none we choose a pair  \nThen we remove one of the points the pair and add the  \nadditional point to replace then we have two pairs. Then we note that each of these pairs of points  \nhas the potential of forming a line with the  \npoint p that has two twos in the coordinates i and j that the pair of points differ and where the points of the pair have  \nthe same coordinate p has that coordinate. So p must be absent from the Moser set. Now  \neach such point p has the possibility of blocking two  \nsuch lines. the pair of points that has 11 and 33 in the coordinates where  \np has two twos and the same coordinates as p elsewhere  \nand the points that have 13 and 31 in the points where  \np has two 2’s and the same coordinates elsewhere. Now in the case of this Moser set with  \nstatistics (6,40,79,0,0,0)  \nWe have two such pairs and since only one of 80  \npoints with two twos is missing to avoid  \na line the missing point must block both lines for the two  \npairs as above. Now we have from the above  \nfour points which agree  \non three coordinates called fixed coordinates and take on all  \npossible values on the remaining two coordinates.  \nThey have the potential of forming lines  \nwith those points with one two, namely those  \npoints which agree with the four  \npoints on the three fixed coordinates mentioned  \nabove and have one two and one arbitrary  \nvalue on the remaining two coordinates.  \nThere are four such points.  \nThey must be absent from the Moser set  \nto avoid formin a line.  \nThe point p  \nmentioned earlier which agrees with the above  \npoints in the three fixed coordinates  \nand has 2’s in the remaining coordinate  \nforms two lines with these points.  \nThese two lines are empty since p or any  \nof the other four points are not in the Moser  \nset . The two lines were counted in the  \ninequality 2B + C is less than or equal to  \n160 with weight one half. which is lemma three in the Wiki section n=5  \nwe can remove these two lines and count over the remaining lines  \nsince they are empty 2B +C will not be effected but the 160  \nwill be reduced to 158 so then we will have  \n2B +C is less than or equal to 158  \nbut 2B+C is 159 in the case with statistics  \n(6,40,79,0,0,0). So only the remaining two statistics are possible and we are done."},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"Rewrite of 945. If a Moser set for n=5 has 125 points  \nIt must have the following statistics:  \n• (8,39,78,0,0,0) Assume it does not then the only remaining case  \nis that it must have these statistics:  \n• 7,40,78,0,0,0) Now look at the 7 points in the set with all coordinates not  \nequal to 2 Any set of 5 of these has one pair of  \npoints that differ in exactly two coordinates  \nsee the proof of lemma 3 in the Moser wiki.  \nWe start with 5 of these points and get at least one  \npair that differs in exactly two coordinates if there is more than  \none we choose a pair  \nthen we remove one of the points the pair and add the  \nadditional point to replace then we have two pairs. We  \nkeep doing this until we have three pairs.  \nIf a line is to be avoided there must be a point with  \ntwo 2’s not in the in Moser set which has 2’s  \nwhere the points of the pair differ and for the  \npoints where the pair agree it also agrees, if this  \ndoes not happen we will have a line. Now from the above  \nwe have four points which agree  \non three coordinates called fixed coordinates and take on all  \npossible values on the remaining two  \ncoordinates. They have the potential of forming lines  \nwith those points with one two, namely those  \npoints which agree with the four  \npoints on the three fixed coordinates mentioned  \nabove and have one two and one arbitrary  \nvalue on the remaining two coordinates  \nthere are four such points call them a,b,c,d. The point p  \nmentioned earlier which agrees with the above  \npoints in the three coordinates mentioned earlier  \nand has 2’s in the remaining coordinate  \nforms two lines with these points.  \nthese lines are empty since if p or any  \nof the other four points are in the Moser  \nset they form a line with the points  \nin the two pairs. The two lines were counted in the  \nInequality 2B + C is less than or equal to  \n160 with weight one half. which is lemma three  \nwe can remove these two lines and count over the remaining lines  \nsince they are empty 2B +C will not be effected but the 160  \nwill be reduced to 158\\. So then we will have  \n2B +C is less than or equal to 158  \nbut 2B+C is 158 in the case with statistics  \n(7,40,78,0,0,0) Now this gives 2B+C for the remaining lines is less  \nThan or equal to 158 but 2B+C in this case = 158.  \nWhich means that every one of the remaining lines must have exactly two points. We will use this fact in the proof that follows by starting  \nwith the points a, b, c, d which as noted above are not in the Moser set. Taking lines from the remaining lines that  \ncontain a, b, c or d and hence have to have the remaining  \npoint in the line. We will use these points to force a line  \nIn the set and thus get a contradiction. We start with the points a, b, c, d  \nmentioned earlier. We are going  \nto force the existence of two sets of four points  \nsimilar to a,b,c,d  \nThey are identical to points of a, b, c, d except  \nin one coordinate. Let us assume that the points  \nOf p have the following form 111xy where  \nThe fixed coordinates are assumed to be 1  \nAnd in the first three places and x and y are  \nThe moving coordinates. Then a b c d  \nAre of the form 11121, 11123, 11112, 11132.  \nThe two new sets of similar to a b c d  \nAre 13121, 13123, 13112, 13132 and  \n11321, 11323, 11312, 11332 we get that  \nthey must be in the Moser set by the above  \nand looking at lines 1×121, 1×123, 1×112, 1×132  \nand 11×21, 11×23, 11×12, 11×32  \nwhere x denotes a coordinate that may take  \nany of three values which are  \nnot in the lines removed and hence by the above  \nmust have 2 points and hence give us the 8 new  \npoints mentioned above are in the Moser set.  \nNow we note that the points  \n13121, 13123, 13112, 13132  \nwhich by the above must be in the Moser set  \ntogether with 13122 forms a monochromatic line  \nin fact two monochromatic lines so the 13122  \nmust not be in the Moser set. Since we already have  \n11122 is not in the set all other points with  \ntwo twos must be in the set now  \nlook at the points 11321, 11323, 11312, 11332  \nwhich must be in the set  \ntogether with 11322 which must be in the Moser set by the above they  \nform a monochromatic line in the Moser set so we get  \na contradiction. Similarly we can form 8 such points  \nfrom the points p and a, b, c and d above  \nno matter what the value of the the fixed coordinates  \nand no matter the position of the fixed coordinates  \nand force a monochromatic line in the same way  \nand the get a contradiction so the only case remaining  \nis: that having statistics (8,39,78,0,0,0) and we are done."},{"username":"kristal-cantwell","timestamp":"0003-03-20T04:00:00.000Z","contents":"A Moser set for n=5 has 124 points A Moser set for n=5 has 124 points.  \nIf it doesn’t it must have 125 points since we have shown that a contradiction results if there are more than 126 points. Since it has 125 points  \nit must have the following statistics:  \n• (8,39,78,0,0,0) Now look at the 8 points in the Moser set with all coordinates not  \nequal to 2 Any set of 5 of these has one pair of  \npoints that differ in exactly two coordinates  \nsee the proof of lemma 3 in the Moser wiki.  \nWe start with 5 of these points and get at least one  \npair that differs in exactly two coordinates if there is more than  \none we choose a pair  \nthen we remove one of the points the pair and add the  \nadditional point to replace then we have two pairs. We  \nkeep doing this until we have four pairs. There must be points with exactly 2 coordinates equal to two for each of the coordinates where the pairs differ with coordinates. We have four pairs and only two possible points absent from the Moser set as we see by looking at the statistics. Now each of these points can block at most two of the lines formed by a pair. Since those coordinates constant in a pair must be constant in the point as well so only pairs which agree with the point on these constant coordinates can be blocked and there are only two such pairs for each set of three constant coordinates. They are the following points those which agree on the constant coordinates and have all 1’s or all 3’s on the remaining coordinates or the remaining two points which agree on the constant coordinates. Since we have four pairs and only two points with exactly two 2’s missing from the Moser set they must block the four lines formed by all four pairs. Each point blocks two lines. Call these two points %p_1% and %p_2%.  \nNow from the above  \nwe have two sets of four points. Each set agrees  \non three coordinates called fixed coordinates and takes on all  \npossible values on the remaining two  \ncoordinates. Each set has the potential of forming lines  \nwith those points with one two, namely those  \npoints which agree with the four  \npoints on the three fixed coordinates mentioned  \nabove and have one two and one arbitrary  \nvalue on the remaining two coordinates  \nthere are four such points for each set. The points %p_1% and %p_2% mentioned earlier  \neach form two lines with one set of four points mentioned above. So we have four such lines. These lines are empty since if %p_1% or %p_2% or any  \nof the either set of four points are in the Moser  \nset they form a line with the points  \nin the four pairs. The four lines were counted in the  \nInequality 2B + C is less than or equal to  \n160 with weight one half. which is lemma three  \nwe can remove these four lines and count over the remaining lines  \nsince they are empty 2B +C will not be effected but the 160  \nwill be reduced to 156\\. So then we will have  \n2B +C is less than or equal to 156  \nbut 2B+C is 156 in the case with statistics  \n(8,39,78,0,0,0)  \nNow this gives 2B+C for the remaining lines is less  \nThan or equal to 156 but 2B+C in this case = 156.  \nWhich means that every one of the remaining lines must have exactly two points.  \nWe will use this fact in the proof that follows by starting  \nwith one set of four points with exactly one two which are associated with %p_1%. Which as noted above are not in the Moser set. Taking lines from the remaining lines that  \ncontain these and hence have to have the remaining  \npoint in the line. We will use these points to force a line  \nn the set and thus get a contradiction. We will have to do this for two sets of four points because one of the sets formed might be the other set of four points associated with the other two pairs. However this can be dealt with because there are enough degrees of freedom to pick two and in fact three such sets of four points.  \nLet us assume that the points  \nOf two pairs forming one of the sets noted above have the following form 111xy where  \nThe fixed coordinates are assumed to be 1  \nAnd in the first three places and x and y are  \nThe moving coordinates. Then a b c d  \nAre of the form 11121, 11123, 11112, 11132.  \nThe two new sets are  \n13121, 13123, 13112, 13132 and  \n11321, 11323, 11312, 11332 we get that  \nthey must be in the Moser set by the above  \nand looking at lines 1×121, 1×123, 1×112, 1×132  \nand 11×21, 11×23, 11×12, 11×32  \nwhere x denotes a coordinate that may take  \nany of three values which are  \nnot in the lines removed and hence by the above  \nmust have 2 points and hence give us the 8 new  \npoints mentioned above are in the Moser set unless the exception is as noted above that one of the two groups of 4 is the group of four points associated with the other set formed by the points of the other two pairs. In any case we will have one set of four points out of the two sets of four points that must be in the Moser set.  \nNow we note that these points without loss of generality can 13121, 13123, 13112, 13132\\. The proof is essentially the same for both cases.  \nThese points by the above must be in the Moser set.  \ntogether with 13122 forms a line in fact two lines. Now if 13122 is not in the Moser set it must be one of %p_1% or %p_2% however by construction it cannot be %p_1% and if it were %p_2% then the set of four points 13121, 13123, 13112, 13132 would be the second set formed by the second set of two pairs associated with $platex p_2$ but we have chosen these points so that does not occur so it is in the Moser set.  \nSo we have a line in the Moser set so we have a contradiction. Similarly we can form 8 such points  \nfrom the four pairs above  \nno matter what the value of the fixed coordinates  \nand no matter the position of the fixed coordinates  \nand force a monochromatic line in the same way  \nand the get a contradiction so the only case remaining  \nis: that having statistics (8,39,78,0,0,0) and we are done."},{"username":"terence-tao","timestamp":"0006-03-20T04:00:00.000Z","contents":"Moser(3) Kristal, this is great! I haven’t yet had time to go through the arguments and wikify them but I will get round to it at some point. The next step will be to try to reduce the dependence on computer data. I think the classification of 3D extremals will help out a lot here. There are some very specific facts we need from the 4D data (e.g. no 41+ points sets with d >= 4, etc.) and it may well be that the inequalities coming from the 3D theory are enough for this. This should be checkable with Maple, and then _a posteriori_ we can then write down a human proof. Incidentally, I have submitted %c''_n% to the OEIS at [http://www.research.att.com/~njas/sequences/A157795](http://www.research.att.com/~njas/sequences/A157795)"},{"username":"terence-tao","timestamp":"0007-03-20T04:00:00.000Z","contents":"Moser(3) I’ve put Kristal’s arguments on the wiki. It would be good for others to go through the proof and keep an eye out for any corrections or simplifications. The points where the n=4 data is relied on should be inspected particularly carefully, to see if alternate arguments are available."},{"username":"kareem-carr","timestamp":"0008-03-20T04:00:00.000Z","contents":"c’_7  \nThe 978 point solution:  \n[http://twofoldgaze.wordpress.com/2009/03/10/978-element-solution/](http://twofoldgaze.wordpress.com/2009/03/10/978-element-solution/) c’_6  \n[http://twofoldgaze.wordpress.com/2009/03/10/353-element-solution/](http://twofoldgaze.wordpress.com/2009/03/10/353-element-solution/)"},{"username":"kareem-carr","timestamp":"0008-03-20T04:00:00.000Z","contents":"953. My program is still running for the next few hours so I can’t access it yet but it seems I just found a 985 solution for c’_7."},{"username":"kareem","timestamp":"2010-03-20T04:00:00.000Z","contents":"(It’s hard to keep these posting numbers straight when sleep deprived.) My largest solution for c_7 is 988\\. One more caveat, the algorithm I used to display the long form solutions has a bug in it. I will correct it soon."},{"username":"kareem","timestamp":"2010-03-20T04:00:00.000Z","contents":"955. The term c_7 in my last comment should be c’_7."},{"username":"kareem-carr","timestamp":"2010-03-20T04:00:00.000Z","contents":"956. The good news is that I corrected the bug and the solutions on my blog are correct now. I updated my 978 solution to a 988 solution. There is some bad news unfortunately. The long form solutions that I provided for c_n are probably all wrong in the same way. So stick with the short form solutions for now. For a quick fix, if things look wrong just shift the element to the one just below it so 113 -> 112 and so on. I will fix it as soon as I can."},{"username":"kareem-carr","timestamp":"2010-03-20T04:00:00.000Z","contents":"957. Terry: I thought your suggestion of solution seeding was a great idea so I implemented it using a stock of better than 972 solutions. Then I spent some time visualizing the solutions and I realize they have patterns. So I checked to see what parts of the chromosomes are conserved between solutions and that seems to be an effective form of solution seeding also. [http://twofoldgaze.files.wordpress.com/2009/03/visualization1.pdf](http://twofoldgaze.files.wordpress.com/2009/03/visualization1.pdf) [http://twofoldgaze.files.wordpress.com/2009/03/visualization2.pdf](http://twofoldgaze.files.wordpress.com/2009/03/visualization2.pdf) guest: I thought a little more about your comment and decided to give it a more rigorous evaluation. I spent some time with a notepad and a pencil carefully noting the effectiveness of crossover. Paradoxically, without mutation crossover with a stocastic sort of greedy algorithm seemed to improve the solutions 5% of the time which was approximately the same as not doing crosover. However, when I implement crossover with mutation (with an underlying greedy algorithm to ensure no missed opportunities), 20% of the time crossover improves a solution versus 15% for mutation. Something about mutation seems to be boosting the effectiveness of crossover. (When I say it improves a solution, I mean a random solution in the population is improved, not necessarily the best solution.)"},{"username":"michael-peake","timestamp":"0005-03-20T04:00:00.000Z","contents":"Moser(3), dim-5 There is one step I am not sure of. The proof of Lemma 7 relies on slices with non-zero d having scores of 59 7/12 or below. But there are two examples in the list, with d=3 and d=2, with higher scores than that."},{"username":"terence-tao","timestamp":"0007-03-20T04:00:00.000Z","contents":"5D Moser(3) Thanks Michael. Fortunately one can rule out the d>1 cases by observing from Lemma 7 that there is a way to cut the cube so that the side slices have at most one C-point missing (i.e. one side has c=24 and the other has c at least 23) and no D-points, and hence the middle slice has at most one D-point. I’ve updated the wiki appropriately. There is also an alternate approach from Cantwell.912, Cantwell.913\\. Both approaches rely to some extent on the 4D statistics. It would be good to have an alternate proof of this fact which relied less on these statistics (e.g. relying more on the 3D inequalities, which are human-verifiable)."},{"username":"michael-peake","timestamp":"0004-03-20T04:00:00.000Z","contents":"5D Moser(3) Congratulations to Kristal on doing three quarters of the proof, and for completing the proof."},{"username":"terence-tao","timestamp":"0005-03-20T04:00:00.000Z","contents":"Yes, congratulations! It looks like this project is achieving many of its intended objectives; by a coincidence of timing, the same is true of the other half of the project at Gowers’ blog. There is now a retrospective discussion there at [http://gowers.wordpress.com/2009/03/10/polymath1-and-open-collaborative-mathematics](http://gowers.wordpress.com/2009/03/10/polymath1-and-open-collaborative-mathematics) ; I think some feedback from our side of the project would be valuable. There are a few loose ends to tie up, I think: the proof of %c'_6=124% could be simplified, with less of a dependence on computer data; and it would be interesting to see how the genetic algorithm performs for some other numbers, whether there are any cheap ways to improve its performance, and what an analysis of its solutions can tell us about what %c'_7% (say) might look like. There’s also the hyperoptimistic conjecture to look at; we have some computer data now but our human-level understanding of the problem is fairly primitive still. But I think we are in general moving to the “endgame” and should begin thinking about issues such as how to write things up (I plan to comment on this over at Tim’s blog in a bit)."},{"username":"link-starbureiy","timestamp":"2010-03-20T04:00:00.000Z","contents":"Congratulations to all of you on a job well done over the past weeks!"},{"username":"klas-markstrom","timestamp":"0001-03-20T04:00:00.000Z","contents":"HOC  \nI have done some more work on the weighted problem in the HOC and I found the results interesting. I have not been able to prove that I have optimal values for n greater than 5, but for n=6 there is a unique solution of weight 15, which is the size of the optimal solution for Fujimura’s problem. Likewise there is a unique solution of weight 18 for n=7\\. The solutions are here  \n[http://abel.math.umu.se/~klasm/solutions-n=6-k=3-HOC](http://abel.math.umu.se/~klasm/solutions-n=6-k=3-HOC)  \n[http://abel.math.umu.se/~klasm/solutions-n=7-k=3-HOC](http://abel.math.umu.se/~klasm/solutions-n=7-k=3-HOC)  \nIn order to find these solutions I have specified that the program should look for all solutions of this exact weight. For n=7 the program find a solution and then prove that it is the only one in 4 seconds. For n=8 the integer program suddenly gets a lot harder to solve and the program has not yet found a solution. The tricky thing is that for this problem it is much harder to reduce the upper bound for the integer program ( I have multiplied all weight in the program by n! to get integer weights).  \nFor n=6 I expect to soon have reduced the upper bound on the optimal solutions to strictly less than 16, again having to use a linux cluster. However, since we don’t know that the optimum for this problem is an integer the upper bound does not imply that the unique solution of weight 15 is the optimum, and reducing the bound to 15 seems too hard for my current program. There is an interesting contrast here. For Fujimiura’s problem the number of optimal solutions seems to grow, on the average, with n, but if the solutions for n=6,7 for the weighted problem are optimal then this problem has a unique optimal solution for n=4,5,6,7"},{"username":"klas-markstrom","timestamp":"0001-03-20T04:00:00.000Z","contents":"And congratulations to Kristal for proving an upper bound which was too hard for my program!"},{"username":"christian-elsholtz","timestamp":"0002-03-20T04:00:00.000Z","contents":"Moser Congratulations to Kristal!! Here is a comment on the symmetry of the Moser sets.  \nThe Moser sets in dimension 5, with 124 points:  \nThe statistics of all 243 points is: (32,80,80,40,10,1)  \nThe statistics of the known examples is: (4,40,80,0,0) For each of the 40 and 80 points the antipodal point is included.  \nFor the 4 points with no 2 there are example with no antipodal pairs, and there is  \nan example where 2 of the 4 points are antipodal, the other 2 are not. To the best of my knowledge no symmetric solution (with regard to the  \ncentre) is known to exist.  \nIf the 124 solutions should all come from the same 40+80 points,  \nthen there are no 4 symmetric points. Those examples in dimension 6 (344 points) and 7 (960 points), that were  \nimplicitly in Chvatal’s paper, Canadian Math Bulletin, Vol 15, 1972, 19-21.  \n“Remarks on a problem of Moser”, based on (elementary) bounds from coding  \ntheory, contained some full Behrend spheres,  \nand some rather empty ones, so were in a similar spirit to the one above. The better examples by Kareem Carr  \n353 points, have distribution  \n(22, 66, 165, 100, 0, 0 0)  \nout of  \n(64, 192, 240, 160, 60, 12, 1) for all 729 points, so have no almost full sphere. The study of antipodal points of the 353 example on Kareems webpage gives  \n65 pairs of antipodal points  \n45 of these with two 2’s, and 20 with three 2’s. For the 988 example in dimension 7 on Kareem’s webpage with distribution:  \n(36, 65, 336, 551, 0, 0, 0, 0)  \ncompared to (for all 2187 points)  \n(128, 448, 672, 560, 280, 84, 14, 1)  \nlet us make the following observations:  \n1) The sphere with three 2’s is quite full.  \n2) there are 272 antipodal pairs,  \n271 of these with three 2’s, and one pair with one 2.  \n(Of course there must be many antipodal pairs, if a sphere is almost  \nfull). Perhaps this gives a heuristical clue how good examples in some further  \nmedium sized dimensions look like.  \nOne almost full sphere, with (dimension/2) 2’s and in the other  \npartial spheres:  \nif point x occurs, then the antipodal does not occur. Remarkably, the behaviour in dimension (2 to 5), 6 and now 7 are so  \ndifferent… In any case, perhaps this helps finding good “seeds” for the algorithm?"},{"username":"kareem-carr","timestamp":"0007-03-20T04:00:00.000Z","contents":"Christian Elsholtz: If you will give me a few days to make them available, I have 25 other solutions of length 353 that might be interesting. Perhaps they have different properties."},{"username":"terence-tao","timestamp":"0007-03-20T04:00:00.000Z","contents":"Dimension transition in Moser(3) Two comments… it seems that the best solutions in 6D and 7D we have so far are avoiding all but the first four Behrend spheres (the strings with 0, 1, 2, or 3 twos). To speed up the GA, perhaps we could simply _assume_ that all other spheres are empty and remove those chromosomes from the genotype. (I believe this will cut down the number of lines by a significant factor – quite a few lines pass through the inner spheres). The second comment is that I think one reason for the transition is that in 5D and lower, the optimal solutions have density >1/2, and in 6D and higher, the optimal solutions have density <1/2\\. The thing is that the linear inequalities we have on sphere densities cannot rule out the possibility that every sphere has density 1/2 or less. Consider for instance the 3D case, in which the full set [3]^3 has statistics (8,12,6,1), so a mythical “half-density” Moser set would have statistics (4,6,3,0.5). Of course this is not possible, however the statistics (4,6,4,0) and (4,6,2,1) are possible, and so from the perspective of linear inequalities, nothing is preventing half-densities everywhere. I don’t know whether the same is true in 4D: half of (16,32,24,8,1) is (8,16,12,4,0.5), and I don’t know whether this is attainable as a convex combination of Moser 4D statistics. But this phenomenon may help explain the tendency for the higher D statistics to fill out about half of every sphere. (This also matches with the linear programming I did in 944.)"},{"username":"jason-dyer","timestamp":"0009-03-20T04:00:00.000Z","contents":"HOC I checked the solution in Markström.961 of n=6, which is the same as the equal slice solution 510, 420, 330, 240, 150, 501, 402, 303, 204, 105, 015, 024, 033, 042, 051\\. (It’s all combinations of 1 & 2, 2 & 3, and 1 & 3.) This is the same solution as the general bound for Fujimura of that size."},{"username":"jason-dyer","timestamp":"0009-03-20T04:00:00.000Z","contents":"HOC A quick scan indicates the same thing applies to n=7. Klas, look for a solution of 21 on n=8 and see if you get one solution just like the general bound for Fujimura. If so I am guessing the HOC is false."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"Thank you to everyone for the congratulations! My congratulations to everyone here and also those who worked on the other half of this project. I will be working on proving there are no sets with 41 or more points with d greater than or equal to 4."},{"username":"kareem-carr","timestamp":"0001-03-20T04:00:00.000Z","contents":"Terry: “Two comments… it seems that the best solutions in 6D and 7D we have so far are avoiding all but the first four Behrend spheres (the strings with 0, 1, 2, or 3 twos). ” It turns out I had already been doing something like this when I tried to preserve the conserved regions between solutions. I had been looking at the conserved regions set to zero and the ones set to one both together and singly. I noticed that the conserved zero region (points consistently left out) turned out to be of length 379 (which I didn’t check by hand) but is the same size as the set of points with strings with four or more 2’s. In the same spirit, I had a region of conserved one’s which turns out to contain 411 points. Perhaps someone else will see the pattern. {13, 31, 37, 39, 41, 43, 49, 67, 85, 91, 93, 95, 97, 103, 111, 117, 119, 123, 125, 127, 133, 139, 145, 147, 149, 151, 157, 175, 193, 199, 201, 203, 211, 229, 247, 253, 255, 257, 259, 273, 275, 277, 281, 285, 287, 289, 291, 295, 301, 307, 311, 313, 319, 325, 327, 331, 333, 335, 339, 341, 343, 345, 347, 349, 353, 369, 371, 375, 379, 383, 385, 387, 389, 393, 395, 397, 399, 401, 409, 415, 417, 419, 421, 433, 437, 443, 447, 449, 451, 453, 455, 457, 469, 471, 473, 475, 481, 499, 517, 523, 525, 527, 535, 553, 571, 577, 579, 581, 583, 599, 601, 603, 605, 609, 611, 613, 615, 617, 619, 631, 635, 643, 661, 679, 685, 687, 689, 691, 697, 733, 739, 745, 751, 757, 761, 773, 775, 781, 787, 793, 797, 799, 805, 813, 815, 819, 825, 829, 833, 837, 839, 845, 855, 861, 865, 867, 869, 871, 873, 879, 881, 889, 903, 905, 907, 913, 921, 923, 925, 927, 933, 937, 939, 941, 949, 955, 957, 959, 961, 967, 981, 983, 987, 989, 991, 993, 995, 999, 1001, 1007, 1017, 1019, 1023, 1031, 1035, 1037, 1047, 1049, 1055, 1059, 1061, 1071, 1077, 1109, 1115, 1125, 1127, 1133, 1135, 1145, 1149, 1151, 1155, 1157, 1159, 1181, 1185, 1189, 1191, 1193, 1195, 1197, 1199, 1203, 1207, 1211, 1225, 1227, 1229, 1231, 1237, 1243, 1247, 1249, 1251, 1253, 1261, 1263, 1265, 1281, 1297, 1299, 1301, 1305, 1307, 1315, 1317, 1321, 1325, 1341, 1347, 1349, 1351, 1353, 1355, 1357, 1365, 1367, 1369, 1371, 1373, 1375, 1381, 1387, 1389, 1399, 1405, 1407, 1409, 1411, 1415, 1419, 1421, 1423, 1425, 1427, 1435, 1441, 1443, 1447, 1453, 1471, 1489, 1495, 1497, 1525, 1543, 1549, 1551, 1553, 1555, 1561, 1567, 1569, 1571, 1573, 1575, 1577, 1585, 1587, 1589, 1597, 1603, 1605, 1607, 1633, 1651, 1657, 1659, 1661, 1663, 1669, 1687, 1705, 1711, 1713, 1715, 1729, 1733, 1735, 1737, 1739, 1745, 1747, 1749, 1751, 1753, 1759, 1765, 1767, 1769, 1777, 1783, 1785, 1787, 1791, 1797, 1801, 1803, 1805, 1807, 1809, 1811, 1817, 1827, 1829, 1833, 1835, 1837, 1841, 1843, 1845, 1847, 1853, 1855, 1857, 1859, 1861, 1875, 1877, 1879, 1885, 1891, 1893, 1895, 1897, 1899, 1901, 1905, 1911, 1915, 1921, 1927, 1931, 1933, 1939, 1981, 1983, 1985, 1987, 1993, 2011, 2029, 2035, 2037, 2039, 2047, 2053, 2055, 2057, 2059, 2061, 2063, 2067, 2069, 2071, 2073, 2075, 2077, 2083, 2091, 2093, 2095, 2101, 2137, 2143, 2145, 2147, 2149, 2155, 2173}"},{"username":"christian-elsholtz","timestamp":"0002-03-20T04:00:00.000Z","contents":"the 411 points above all have 3 two’s.  \nOtherwise coordinate stastistics alone will not help, since it contains for example the points  \n{2, 2, 2, 1, 1, 1, 3}, {2, 2, 2, 1, 1, 3, 1}, {2, 2, 2, 1, 1, 3, 3}, {2, 2, 2, 1, 3, 1, 1},  \n{2, 2, 2, 1, 3, 3, 1}, {2, 2, 2, 3, 1, 1, 3}, {2, 2, 2, 3, 1, 3, 3}, {2, 2, 2, 3, 3, 1, 1}, {2, 2, 2, 3, 3, 1, 3},  \n{2, 2, 2, 3, 3, 3, 3} etc.  \nthat is some but not all permutations of 2221113.  \nIt also pairs of points with Hamming distance 1."},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"Integer programming I discovered an integer programming package (Optimization) on Maple 12, and gave it a whirl, see [http://michaelnielsen.org/polymath1/index.php?title=Maple_calculations](http://michaelnielsen.org/polymath1/index.php?title=Maple_calculations) for details. I reconfirmed the fact that by throwing in all the linear inequalities we already know about, we can get %c'_6 \\leq 364%. The extremal statistics are (31,94,126,83,23,7,0). (*) [In comparison, Kareem’s 353-point solution has statistics (22, 66, 165, 100, 0, 0 0).] If we can find some reason why the statistics (*) are impossible, this should eventually be convertible to a linear inequality which I can then add to the integer program to improve the bound. Hopefully we can iterate this computer+human procedure to get the bounds to be closer to each other. If I assume g=1 (i.e. the point 222222 is in the set), then the upper bound improves substantially to 355, with extremal (24,96,120,80,30,4,1). If instead I assume e=f=g=0 as in Kareem’s example, the upper bound similarly improves to 356, with extremal (24,72,180,80,0,0,0). The 7D computations were disappointing; the best upper bound I could get was 1092, i.e. 3 times the current 6D bound."},{"username":"terence-tao","timestamp":"0005-03-20T04:00:00.000Z","contents":"Linear programming There are some more inequalities available. For instance in 4D, we have %a+b+c+d+e \\leq 43%, but when e=1, we have the better estimate %a+b+c+d+e \\leq 39% (which I also verified by linear programming). Thus we in fact have %a+b+c+d+5e \\leq 43%. Similarly, the %c'_5=124% result gives %a+b+c+d+e+f \\leq 124% in 5D, but when f=1 the linear inequalities improve this to 119, thus %a+b+c+d+e+6f \\leq 124%. I inserted this into the integer program. Sadly, this did not improve the upper bound %c'_6 \\leq 364%, but it did give a prettier extremiser, namely (32,96,120,80,30,6,0), i.e. every Behrend sphere except for the last one is filled with density exactly 1/2\\. Also, this argument managed to improve the bound for %c'_7% from 1092 to 1086. The wiki page is adjusted to reflect these improvements, of course."},{"username":"terence-tao","timestamp":"0005-03-20T04:00:00.000Z","contents":"Linear programming I found a 4D inequality which was inconsistent with the above 1/2-density scenario, and it gave some good results (basically it is useful for eliminating scenarios in which d, e, etc. are large). It exploits the fact that the 41+ 4D solutions have small d and e. Indeed, the data shows that e=0 and a+b+c+d+e+d/2 <= 43 for such solutions. Meanwhile, linear programming shows that for 40- point solutions, one has a+b+c+d+e+d/2 <= 43, and furthermore a+b+c+d+e+d/2 <= 40.5 if e=0\\. We conclude that a+b+c+d+e+d/2+5e/2 <= 43\\. Inserting this into the integer programming machine gives significant improvements, namely %c'_6 \\leq 360% and %c'_7 \\leq 1075%. To proceed further, one needs to find a reason why the %c'_6% extremiser statistics, which happen to be (29,87,134,97,8,5,0), are not attainable. It may also be useful to figure out why the e=f=g=0 extremiser statistics (24, 72, 180, 80, 0, 0, 0) are not attainable; the densities here are quite clean, namely (3/8, 3/8, 3/4, 1/2, 0, 0, 0). Wiki page updated as before."},{"username":"kareem-carr","timestamp":"0005-03-20T04:00:00.000Z","contents":"Christian Elsholtz: Interesting points. The one thing I would add is that the 411 are probably not perfect as they are based on a guess of the better solutions found so far. Although, I realize I should have taken them more seriously as I was very surprised to see that the 379 zero points conserved matched so well with the Behrend spheres."},{"username":"klas-markstrom","timestamp":"2012-03-20T04:00:00.000Z","contents":"HOC Jason, I ran the program for n=8 and solutions of weight 21\\. The program quickly found a solution and in a few minuets proved that it is the unique solutions of that weight.  \nThe solution is here  \n[http://abel.math.umu.se/~klasm/solutions-n=8-k=3-HOC-21](http://abel.math.umu.se/~klasm/solutions-n=8-k=3-HOC-21)"},{"username":"jason-dyer","timestamp":"0005-03-20T04:00:00.000Z","contents":"A semi-optimist conjecture Thanks Klas! Now I can say… For %n > 2%, %c^\\mu_n = 3(n-1)% This implies at least the [optimist conjecture](http://gowers.wordpress.com/2009/02/23/brief-review-of-polymath1/#comment-2405) (if not the original version of the HOC, which I guess was an inequality?) which implies DHJ (3)."},{"username":"terence-tao","timestamp":"0008-03-20T04:00:00.000Z","contents":"Linear programming I made a data entry error causing the bounds announced earlier to be off a little bit. Currently I can show %c'_6 \\leq 361% and %c'_7 \\leq 1078%, with the 361 bound improving just to 360 if one assumes e=f=g=0, and to 355 if one assumes g=1\\. The 361 extremiser is (28,80,160,80,10,3,0). Jason, I am not sure I understand what you are getting at here. Every solution to Fujimura gives a solution to weighted DHJ, so %c^\\mu_n% always has to be at least as large as %\\overline{c}^\\mu_n%; in particular, %c^\\mu_8% has to be at least %\\overline{c}^\\mu_8=22%. Actually, this now confuses me quite a bit. Back in 923, 925, Klas found 72 22-point triangle-free sets in n=8\\. This will imply the existence of many 21-point triangle-free sets also, simply by removing one point from these examples. One can convert this to having many line-free sets with weight 21 in 8D, but the integer program tells us that there is only a unique solution… so something is strange somewhere."},{"username":"jason-dyer","timestamp":"2010-03-20T04:00:00.000Z","contents":"HOC Oh yes, even though the weights of the individual points represented by the edges of the lattice are larger, each complete slice has a weight that adds up to 1\\. This *is* really odd."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"Moser set n=4 Here I am proving a partial result. Eventually I want to get the following:  \nIf a Moser set for n=4 has 4 or more points with three 2’s it has less than 41 points. If a Moser set for n=4 has six or more points with three 2’s it must have less than 41 points. We note that there cannot be a point with four 2’s as then we would have 39 points. We have 5 points with exactly three 2’s and one coordinate not equal to 2\\. That gives 5 values not equal to 2 and four coordinates one coordinate must have the coordinate value not equal to 2 from 2 of the five points. One value must be three the other one. We slice at this point and get two cubes with the center point filled which by the n=3 section of the wiki on Moser sets must have 13 points or less. Since there are six or more points with three 2’s the center slice must have the remaining four or more. Now if we have 41 or more points it must have a center slice equal to 15 points or more. However by the Pareto-optimal statistics in the section n=3 of the wiki we see that a cube with c greater than three can have value at most 14\\. So there at most 13+14+13=40 points and we are done."},{"username":"klas-markstrom","timestamp":"2011-03-20T04:00:00.000Z","contents":"HOC  \nThis got me really worried too so I have been checking things and I think I have found the problem. In order to get an integer problem with integer coefficients I multiplied all weights with n!. For small n this is perfectly fine but for the larger n this will actually cause the bound used to specify a solution of a given weight to be too large for the datatype the linear programing solver uses for coefficients. So I created formally correct input files for the solver, but the weights are to large for the solver I have been using. This means that the claimed uniqueness in the weighted problem for n greater than 5 is meaningless. For the other problems all weights and coefficients are 0/1 so they do not suffer from this."},{"username":"jason-dyer","timestamp":"0006-03-20T04:00:00.000Z","contents":"Making c^mu_n tractable This is starting to get frustrating to work with in that we have no good way to approach c^mu_n by hand. Is there at least a conjecture we can make here that if true which might help us get a handle on it?"},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"%c^\\mu_n% Well, we may have to go back to the very small values of n. For instance, I can work out %c^\\mu_2 \\leq 4% by hand, but already %c^\\mu_3 \\leq 6% is rather challenging. My guess is that we should be looking at the slice densities %\\alpha_{a,b,c} := |A \\cap \\Gamma_{a,b,c}|/|\\Gamma_{a,b,c}|% of the line-free set A, and finding linear relations between them, perhaps to throw into an integer program to maximise %c^\\mu_n%, which is the maximal sum of all the %\\alpha_{a,b,c}%. (This of course sounds a lot like what Klas is doing, but this will be an integer program on %\\binom{(n+1)(n+2)}{2}% variables rather than %3^n% variables, and so has a chance of being scaled up to reasonable values of n, such as 6 or 7.) We actually did a little bit of this way back in the 200 thread (see e.g. Jakobsen.210, Jakobsen.216, Tao.219, Peake.220). We know that given any equilateral triangle, the sum of the %\\alpha_{a,b,c}% in that triangle adds up to at most 2\\. This should already give some bound; after finding that bound, we can see how to improve it."},{"username":"christian-elsholtz","timestamp":"0007-03-20T04:00:00.000Z","contents":"Linear Programming. In 770 Terry described that improvements on bounds of type  \n4D: %a+b+c+d+5e \\leq 43 % etc help improving bounds in dimension 7. I expect, that for In dimension 4, with e=1 a much better result holds.  \nIn 778 KS Chua mentioned that the best example of this type he found had  \n36 points, (and his programme found a 124 point example in 5D). Also, those programmes that classified the 43 extremisers could hopefully  \nbe adapted to verify the maximum number of points in 4D. If 36 is the maximum, this gives  \n%a+b+c+d+8e \\leq 43 % Similar, for 5D I would expect a considerable gain over the currently used  \n%a+b+c+d+e+6f \\leq 124 %."},{"username":"terence-tao","timestamp":"0008-03-20T04:00:00.000Z","contents":"Linear programming Christian: yes, that’s a great idea. Inserting the improvements to %a+b+c+d+e \\leq 43% that we already had were already instrumental in killing off the large e (or large f,g) counterexamples in 6D, 7D, and improved their bounds significantly (from 164 to 161 in 3D, and from 1092 to 1078 in 3D). Further such improved estimates would undoubtedly do better, and perhaps even eradicate e, f, g entirely in 6D (note that Kareem’s examples have e=f=g=0). Another inequality that could stand improving is the inequality %a+b+c+d+e + d/2 + 5e/2 \\leq 43% in 4D. For e=0 this comes by inspection of the 41+ point data (in particular, the fact that d is so small in this regime) plus use of the existing 3D inequalities for 40- point data (somewhat analogous to Kristal’s 976). For e=1 I am relying purely on the linear programming from the 3D inequalities. Getting a better upper bound for a+b+c+d+e+d/2 in the e=1 case (currently it is 40.5) would help. I quite like this approach as it wastes as little as possible: any “excess” estimate one gets in, say, 4D, can be recycled into improvements in 5D and higher. This may well be the way to go for the HOC. For n=2, say, we should be able to classify the Pareto-optimal and extremal statistics for the slice densities %\\alpha_{a,b,c}%, giving linear inequalities. These inequalities then imply analogues in 3D, which we could then integer program and apply further combinatorial analysis to beget further linear inequalities to use in 4D, and so on and so forth."},{"username":"michael-peake","timestamp":"2010-03-20T04:00:00.000Z","contents":"Moser(3) Diagonal cubes in 6D The 6D cube contains 120 3D cubes of the form xxyyzz. The two xs take the values (11,22,33) or (13,22,31), and the same with the two ys and two zs. Each point in the 3D cube contains 0,2,4 or 6 twos, so this introduces a set of new inequalities connecting a,c,e and g. These inequalities are easiest to express in the density notation %\\alpha_i%. One of the inequalities for the simple 3D cube is %8\\alpha_0 + 6\\alpha_1 + 6\\alpha_2 + 2\\alpha_3 \\le 11%. The corresponding inequality for these diagonal cubes is %8\\alpha_0 + 6\\alpha_2 + 6\\alpha_4 + 2\\alpha_6 \\le 11%. I don’t know whether these inequalities provide new information. Certainly the solution %\\alpha_i = 1/2%, which satisfies all the old 3D inequalities, satisfies the new ones too."},{"username":"terence-tao","timestamp":"2010-03-20T04:00:00.000Z","contents":"6D Moser(3) diagonal cubes Thanks Michael! Actually I had put those cubes in the linear program already (it’s the term op(subs([b=c,c=e,d=g],X3)) in the definition of X6) but I had forgotten to tell people about this. But yes, it does help. Without it, the bound on %c'_6% stays where it as at 361, but the bound on %c'_6% under the additional assumption g=1 worsens from 355 to 358\\. (It makes sense that this new inequality is really of use when g is non-zero.) This cuts down the inequality a+b+c+d+e+f+7g <= 361 to a+b+c+d+e+f+4g <= 361, which ends up worsening the %c'_7% bound from 1078 to 1079. There are also cubes in which each wildcard occurs a different number of times (e.g. 1xyy23) but I was unable to figure out how to use those “skew” cubes to generate more inequalities."},{"username":"kevin-obryant","timestamp":"2012-03-20T04:00:00.000Z","contents":"I don’t see anywhere anybody taking up question II.A: what exactly is the bound that comes out of the Behrend/Elkin construction? Elkin’s bound is %r_3(n) > C n (\\log n)^{1/4} 2^{-\\sqrt{8 \\log_2 n}}%. He stated it just for n of a particular form, but it works for all n. The second ingredient is that %\\binom{n}{a,b,c}%, this size of %\\Gamma_{a,b,c}% is at least %C 3^n/n% provided that all of a, b, c, are between %n/3- \\sqrt n% and %n/3+\\sqrt n%. The final ingredient is that if %x_i% (with %1\\leq i \\leq 3%) are a line and %x_i% is in %\\Gamma_{a_i,b_i,c_i}%, then %c_i-a_i% are a (possibly constant) arithmetic progression of integers. Now to put these together. Let R be a largest set of integers without 3-term APs that is contained in %(-\\sqrt{n}/2,\\sqrt{n}/2)%. The set %A = \\bigcup_{c-n/3 \\in R} \\Gamma_{n/3,b,c}% has no lines, and has size at least %\\sum_{c-n/3\\in R} \\binom{n}{n/3,b,c}%. For each element r of R, we can take %c=r+n/3% and %b=n/3-r%, in which case %\\binom{n}{n/3,b,c} > C 3^n /n%. The size of A is then at least %C r_3(\\sqrt{n}) \\cdot C 3^n/n%, which is at least %C (\\log n)^{1/4} \\exp_3(n- \\alpha \\sqrt{\\log_3 n})%, and %\\alpha=2\\sqrt{\\log_3(2)}%. Here, %\\exp_3(x)% is shorthand for %3^x%. As I write this, it doesn’t feel very optimal to set %a=n/3%, instead of letting it vary at least a little. In particular, I am unpleased by the asymmetry."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"c^{\\mu}_3=6 If we have all  \nThree points of the form aaa removed  \nThen the remaining points have value 7 and  \nWe have covered all lines that have any set of moving coordinates  \nAnd all constant points equal to one value this leaves  \nThe lines xab a,b not equal. Each point of the set  \nabc covers three of these lines the entire set covers each of these lines  \nthere is no duplication the only alternative is to remove a point  \nabc and cover the lines with points of the form aab which have  \na higher weight and only cover two lines each this would lower the weight  \nso the maximum weight occurs when all of abc is omitted along with  \nthe three points aaa and that weight is 6\\. If we have only two points removed of the form xxx then  \nThe weight is at most 8 say the point not removed is 222  \nThen we must cover the lines xx2 and x22 we have three six such  \nLines and all the xx2 must be covered one at a time by either 112  \nOr 332 the x22 must be covered one at a time by 322 or 122  \nThese points must be removed and the that lowers the weight  \nTo 8 – 3*(2/6) – 3*(2/6) = 6 again we have c^{\\mu} must be less than 6 If we have one point removed say 111 then we must cover all lines of  \nThe form xx2 xx3 and x22 and x33 the best we can do is to cover  \nTwo of these at once as no point can cover x22 and x33 or xx2 and xx3  \nOnly points of the form xxy can cover these lines and as we have 12 lines  \nCovering 2 at a times we must have 6 such lines which will have weight 3/6  \nSo we will have removed one point with weight one and 6 more  \nWith weight 1/2 giving remaining weight 6 or less for the remaining points. Finally we have no points of the form xxx removed but then we will  \nhave a line of the form xxx so we must have the earlier three cases and the weight of the points not removed can be at most 6."},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"Second try at c^{\\mu}_3=6 I don’t think the previous proof works I made some changes. c^{\\mu}_3=6  \nIf we have all  \nThree points of the form xxx removed  \nThen the remaining points have value 7 and  \nWe have covered all lines any set of moving coordinates  \nAnd all constant points equal to one value this leaves  \nThe lines xab a,b not equal. Each point of the set  \nabc covers three of these lines the entire set covers each of these lines  \nthere is no duplication the only alternative is to remove a point  \nabc and cover the lines with points of the form aab which have  \na higher weight and only cover one line each this would lower the weight  \nso the maximum weight occurs when all of abc is omitted along with  \nthe three points xxx and the weight is 6 If we have only two points removed of the form xxx then  \nThe weight is at most 8 say the point not removed is 222  \nThen we must cover the lines xx2 and x22 we have three six such  \nLines and all the xx2 must be covered one at a time by either 112  \nOr 332 the x22 must be covered one at a time by 322 or 122  \nThese points must be removed and the that lowers the weight  \nTo 8 – 3*(2/6) – 3*(2/6) = 6 again we have c^{\\mu} must be less than 6 If we have one point removed say 111 then we must cover all lines of  \nThe form xx2 xx3 and x22 and x33 the best we can do is to cover these lines  \nTwo of at once as no point can cover x22 and x33 or xx2 and xx3  \nAnd we have all points of the form aab when a = 2 and b =3  \nOr b=2 and a=3  \nOnly points of the form xxy can cover these lines and as we have 12 lines  \nCover 2 at a times we must have 6 such lines which will have weight 2/6  \nSo we will have removed one point with weight one and six more  \nWith weight 1/3 giving remaining weight 7 and we will have  \nLines of the form x11 and xx1 to cover as well as x12 and x13  \nThe lines of the form x11 will have to be covered by 3 lines with  \nWeight 1/3 however we are not done as we can split a point of the  \nForm 223 into 113 and 221 then we will cover the lines 22x and xx3  \nWith two lines instead of one but we will cover one line of the  \nForm 11x however we due this we will either have to split  \n3 pairs and we will have weight 6 since 7-1=6 or split two  \nand use one point of the form 11x for the remaining line of the  \nform 11x or again the total is 6 or split one and use 2 and again  \nthe remaining points total 6. Finally we have no points of the form xxx removed but then we will  \nhave a line of the form xxx"},{"username":"michael-peake","timestamp":"0001-03-20T04:00:00.000Z","contents":"4D Moser(3) Diagonal cube There are 12 3D cubes that fit diagonally in 4D, with coordinates xxyz, where xx = (11,22,33) or (13,22,31), and y and z both range over (1,2,3). Their points have (a,b,c,d,e) = (8,8,6,4,1)  \nSo each a-point is in 6, each b-point in 3, c-pt in 3, d-pt in 6, e-pt in 12 Of the solutions without a coordinate line, there were 37 Pareto-optimal solutions and the following ten extremal solutions. (34321),(42321),(44221),(44311),(24440), (42340),(44240),(44600),(48400),(80000). These satisfy %2a+b+2c+2d+4e \\le 24, 4a+b+2c+2c+4e \\le 32% and %a \\le 8, b\\le 8, c \\le 6, d \\le 4, e\\le 1% The inequality for the 4D set is 4A + B + 2C + 4D + 16E \\le 96  \n8A + B + 2C + 4D + 16E \\le 128"},{"username":"jason-dyer","timestamp":"0002-03-20T04:00:00.000Z","contents":"c^{\\mu}_3=6 (rephrased) Just to make sure I understand I am rephrasing Kristal’s proof. Consider the line 111, 222, 333\\. Either one is removed, two are removed, or three are removed. (It’s impossible to have zero removed — we are left with the line.) Suppose all three are removed. Then we are left with 18 disjoint lines represented by a single wildcard. This can be done by removing all points on the slice \\Gamma_{1,1,1}; each removal takes out 3 of the disjoint lines and we are left a weight of 6\\. Removing from the hexagon of slices 120-012-012-102-201-210 removes only 2 disjoint lines at a time with a higher weight, so the removal we’ve already found is optimal. Suppose two are removed, leaving (suppose, noting symmetry) 222\\. Then we are left with disjoint lines **2, 2**, *2* (corresponding to the slices \\Gamma_{2,1,0} and \\Gamma_{0,1,2} and *22, 2*2, 22* (corresponding to the slices \\Gamma_{1,2,0} and \\Gamma{0,2,1}). There must be one removal for each of the six disjoint lines, removing a weight of 2 and leaving a weight of 6. While I was typing Kristal changed her proof of one removed, so I’ll finish the rest later. :)"},{"username":"jason-dyer","timestamp":"0002-03-20T04:00:00.000Z","contents":"c^{\\mu}_3=6 question Looking at your new one point proof: Kristal, are you sure we assume the slices %\\Gamma{0,2,1}% and %\\Gamma{0,1,2}% have to be removed? (It’s what you call “points of the form aab when a = 2 and b =3  \nOr b=2 and a=3”.) Even though it’s non optimal to pick two from one of the other slices it doesn’t put us quite over the weight limit (and shouldn’t be too hard to prove that it will) but since it’s not an immediate contradiction we can’t just assume we pick the locally optimal scenario."},{"username":"michael-peake","timestamp":"0002-03-20T04:00:00.000Z","contents":"5D Moser(3) containing diagonal cubes xxyyz Following on from 986\\. A 5D Moser cube contains 60 3D cubes diagonally, in the form xxyyz. Each contains points (a,b,c,d,e,f) = (8,4,8,4,2,1). The sets without geometric lines have 42 Pareto maxima and the following 13 extrema:  \n(324211),(413211)(422211)(424111)(424201)(224420)(413320), (424220)(444020)(422410)(424400)(448000)(800000)  \ngiving the following ten inequalities  \n%4\\alpha + \\beta+2\\gamma +2\\delta + \\epsilon + \\phi \\le 5.5, 2\\epsilon + \\phi \\le 2, 4\\alpha+4\\gamma+2\\delta+\\epsilon+\\phi \\le 6, 4\\alpha+2\\beta+2\\gamma+2\\delta+\\epsilon+\\phi \\le 6, 7\\alpha+\\beta+2\\gamma+2\\delta+\\epsilon+\\phi\\le 7, 4\\alpha+2\\epsilon+\\phi\\le 4, 8\\alpha+4\\gamma+2\\delta+\\epsilon+\\phi \\le 8, 8\\alpha+2\\beta+2\\gamma+2\\delta+\\epsilon+\\phi \\le 8, 4\\alpha+2\\beta+2\\delta+2\\epsilon+\\phi \\le 6, 8\\alpha+2\\beta+2\\delta+2\\epsilon+\\phi \\le 8%"},{"username":"terence-tao","timestamp":"0003-03-20T04:00:00.000Z","contents":"Diagonal Moser cubes Michael, this is great! I of course plunked the inequalities into the linear program at [http://michaelnielsen.org/polymath1/index.php?title=Maple_calculations](http://michaelnielsen.org/polymath1/index.php?title=Maple_calculations) to see what improves. The 4D bound for a+b+c+d+e+d/2 for e=1 improved from 41.5 to 40, giving a new inequality a+b+c+d+e+d/2 + 3e <= 43\\. The 5D bound a+b+c+d+e+f <= 119 for f=1 improved to 117, giving a new inequality a+b+c+d+e+f+7f <= 124\\. The %c'_6% bound unfortunately did not budge at 361, but the extremal changed (e and f became even smaller), suggesting that at least some of the 361-point cases were eliminated. When g=1, the bound dropped from 355 to 352, which is below Kareem’s example and thus demonstrates for the first time that g=0 for the %c'_6% extremiser. In 7D, the bound for %c'_7% improved from 1078 to 1073, and when h=1 there is a further improvement from 1071 to 1065\\. So the new inequalities are having an impact…"},{"username":"michael-peake","timestamp":"0005-03-20T04:00:00.000Z","contents":"Diagonal Moser cubes Inequalities arising from xxxyz diagonals:  \nLetters a to f are densities  \n8a+4b+2c+2d+4e+2f <=11  \n4a+2b+1c+2d+2e+1f <= 6  \n0a+0b+2c+0d+0e+1f <= 2  \n4a+4b+1c+0d+2e+1f <= 6  \n7a+2b+1c+1d+2e+1f <= 7  \n8a+2b+1c+2d+2e+1f <= 8  \n4a+0b+2c+0d+0e+1f <= 4  \n4a+0b+2c+2d+2e+1f <= 6  \n8a+0b+2c+2d+2e+1f <= 8  \n8a+4b+1c+0d+2e+1f <= 8 Inequalities arising from xxxxyz diagonals  \na,b,c,e,f,g are densities. These cubes do not intersect the d slice  \n8a+4b+2c+2e+4f+2g <= 11  \n0a+0b+2c+0e+0f+1g <= 2  \n4a+2b+1c+2e+2f+1g <= 6  \n7a+2b+1c+1e+2f+1g <= 7  \n4a+0b+2c+0e+0f+1g <= 4  \n4a+0b+2c+2e+2f+1g <= 6  \n8a+0b+2c+2e+2f+1g <= 8  \n8a+2b+1c+2e+2f+1g <= 8  \n4a+4b+1c+0e+2f+1g <= 6  \n8a+4b+1c+0e+2f+1g <= 8 Inequalities from xxxyyz diagonals  \nLetters a to g are densities  \n4a+2b+0c+3d+1e+1f+1g <= 6  \n4a+0b+2c+3d+1e+1f+1g <= 6  \n8a+2b+0c+3d+1e+1f+1g <= 8  \n8a+0b+2c+3d+1e+1f+1g <= 8  \n0a+4b+0c+0d+2e+0f+1g <= 4  \n0a+0b+4c+0d+0e+2f+1g <= 4  \n8a+4b+0c+0d+2e+0f+1g <= 1  \n8a+0b+4c+0d+0e+2f+1g <= 1"},{"username":"terence-tao","timestamp":"0008-03-20T04:00:00.000Z","contents":"992. Michael, there seems to be something wrong with your third set of inequalities (xxxyyz), for instance they are inconsistent with the densities a=1, b=c=d=e=f=g=0, which is of course feasible. Perhaps the weighting is a bit off?"},{"username":"kevin-obryant","timestamp":"2010-03-20T04:00:00.000Z","contents":"Question II.A revisited Post 984 is a bit off, as I used %n% in a place I should have had %\\sqrt{n}%. Here’s an improved construction, with corrected analysis of its size. It comes down to this: %c_n > C 3^{n - 4\\sqrt{\\log 2}\\sqrt{\\log n}+\\frac 12 \\log \\log n}% for some absolute constant %C%, and where all logarithms are base-3. For convenience, let %n% be a multiple of 3\\. Elkin’s bound gives %r_3(\\sqrt{n}) > C \\sqrt{n} (\\log n)^{1/4} \\exp_2(-2 \\sqrt{\\log_2 n})%, and let %R% be a subset of %(-3\\sqrt{n}/2,3\\sqrt{n}/2)% without 3-term APs and with size %r_3(\\sqrt{n})%, and with all elements being integer multiples of 3 (again as a matter of convenience). For each %r,s\\in R%, let %a = (n-r-s)/3%. The set %A% is the union of all %\\Gamma_{a,a+r,a+s}%. Since all of %a, a+r,a+s% are between %n/3-2\\sqrt{n}% and %n/3+2\\sqrt{n}%, the size of %\\Gamma_{a,a+r,a+s}% is at least %C 3^n / n%. Since there are %r_3(\\sqrt{n})^2% choices for %r% and %s%, we have a set with size at least  \n%C (\\sqrt{n} (\\log n)^{1/4} \\exp_2(-2 \\sqrt{\\log_2 n}))^2 3^n / n.%  \nThis simplifies to %C \\sqrt{\\log n} \\exp_3(n-\\alpha \\sqrt{\\log_3(n)})%, where %\\alpha=4 \\sqrt{\\log_3(2)}%, assuming my algebra holds this time. Now suppose that %x_i\\in \\Gamma_{a_i,a_i+r_i,a_i+s_i}% is a combinatorial line in the set %A%. Then %(a_i+s_i)-(a_i)=s_i% is a 3-term AP contained in %R%, so the %s_i% are all the same. Similarly, all of the %r_i% are the same, and therefore all of the %a_i% are the same, too. But this implies that the %x_i% sequence is constant, which means the line is degenerate."},{"username":"michael-peake","timestamp":"2011-03-20T04:00:00.000Z","contents":"Moser’s diagonal cubes – correction Hi Terry. Thanks for checking that. The last two inequalities for xxxyyz should have 8 on the RHS.  \n8a+4b+0c+0d+2e+0f+1g <= 8  \n8a+0b+4c+0d+0e+2f+1g <= 8  \nThe extremals for xxxyyz are  \n(4113111),(3223111),(2224220),(4113220),(4224200), (4223011),(4226000),(4222220),(4422020),(4224020), (4222111),(4223101),(4242200),(4444000),(8000000) I checked the other inequalities, and I believe they are correct.  \nThe density inequalities for xxyz, which I didn’t give above, are  \n4a+2b+3c+2d+e <= 6  \n8a+2b+3c+2d+e <= 8"},{"username":"seva","timestamp":"0006-03-20T04:00:00.000Z","contents":"While all the major players are (supposedly) still out there, I wonder whether anything intelligent can be said on the smallest possible size of a Kakeya set in %{\\mathbb F}_3^r%. Let’s denote it %k_r%; thus, %k_r% is the smallest integer for which a set %K\\subset {\\mathbb F}_3^r% exists such that %|K|=k_r% and for every %d\\in {\\mathbb F}_3^r\\setminus\\{0\\}%, the set %K% has a line in the direction %d%. (A line in the direction %d% is, of course, a three-element set of the form %\\{a-d,a,a+d\\}%.) Clearly, we have %k_1=3%, and it is easy to see that %k_2=7%. Using a computer, I also found %k_3=13% and %k_4\\le 27%. I suspect that, indeed, %k_4=27% holds (meaning that in %{\\mathbb F}_3^4% one cannot get away with just %26% elements), and I am very curious to know whether %k_5=53%: notice the pattern in %3,7,13,27,53,\\ldots % As to the general estimates, we have %k_r(k_r-1)\\ge 3(3^r-1)% and, on the other hand, %k_r\\le 2^{r+1}-1: % the former since for each %d\\in {\\mathbb F}_3^r\\setminus\\{0\\}% there are at least three ordered pairs of elements of a Kakeya set with difference %d%, the latter due to the fact that the set of all vectors in %{\\mathbb F}_3^r% such that at least one of the numbers %1% and %2% is missing among their coordinates is a Kakeya set. (I actually can improve the lower bound to something like %k_r\\gg 3^{0.51r}%.) Also, we have the trivial inequalities %k_r\\le k_{r+1}\\le 3k_r; % can the upper bound be strengthened to %k_{r+1}\\le 2k_r+1%?"},{"username":"terence-tao","timestamp":"0008-03-20T04:00:00.000Z","contents":"Thread moving As is now our tradition, I’m declaring this thread full and am moving to the 1100-1199 thread at [https://terrytao.wordpress.com/2009/03/14/dhj3-1100-1199-density-hales-jewett-type-numbers/](https://terrytao.wordpress.com/2009/03/14/dhj3-1100-1199-density-hales-jewett-type-numbers/) I’ll try to respond to several of the interesting new comments on this thread at the other thread."},{"username":"jason-dyer","timestamp":"2009-03-10T05:32:00.000Z","contents":"Metacomment. Appropriate timing of note:  \nCantwell.949: A Moser set for n=5 has 124 points"},{"username":"terence-taogowers","timestamp":"2009-03-10T07:37:00.000Z","contents":"DHJ(k) A small comment: from experience with hypergraph regularity, I would imagine that DHJ(k) will not follow exactly from DHJ(k-1), but from something more like DHJ(k-1,k), and so the induction hypothesis may need to have an additional parameter than just k. (For instance, the simplex removal lemma for 3-uniform hypergraphs requires, at base, something resembling the %K_4%-removal lemma for graphs rather than the triangle removal lemma for graphs. While the %K_4%-removal and triangle removal lemmas have nearly identical proofs, I don’t think one can easily deduce the former from the latter if the latter is a black box.) 1000.1 An %\\epsilon^2% comment: you mean of course DHJ(k-2,k). I think the jury is still out on whether we need to prove the full DHJ(k-2,k) before doing DHJ(k) or whether passing to subspaces the whole time makes it unnecessary. I’m hoping for the latter, but am ready to switch if we have to. An %\\epsilon^3% comment: you are now our official [powers-of-ten champion](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1803) …"},{"username":"gowers","timestamp":"2009-03-10T14:37:00.000Z","contents":"1000.1 An %\\epsilon^2% comment: you mean of course DHJ(k-2,k). I think the jury is still out on whether we need to prove the full DHJ(k-2,k) before doing DHJ(k) or whether passing to subspaces the whole time makes it unnecessary. I’m hoping for the latter, but am ready to switch if we have to. An %\\epsilon^3% comment: you are now our official [powers-of-ten champion](https://gowers.wordpress.com/2009/02/01/a-combinatorial-approach-to-density-hales-jewett/#comment-1803) …"},{"username":"anonymousryan-odonnellgowersgowersryan-odonnell","timestamp":"2009-03-10T11:20:00.000Z","contents":"Line and equal-slice distributions in %[k]^n%. It seems to me that the right generalization of the “[another useful equivalent definition](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure)” of equal-slices on %[k]^n% is as follows: Pick %z \\in [k]^n% from the equal-slices distribution. Form %x^i \\in [k-1]^n% by changing all the %k%‘s in %z% into %i%‘s, for %i = 1...k-1%. Finally, let %y^1, ... y^{k-1}% be a random permutation of the %x^i%‘s. Certainly the set %\\{y^1, ..., y^{k-1}, z\\}% is a combinatorial line, which is degenerate only if %z% had no %k%‘s. I _believe_ that each %y^i% is individually distributed as equal-slices on %[k-1]^n%, but I can’t quite seem to prove it to myself at this late hour. Anyone want to prove or disprove? (If it’s true, there must be some slick way to see it, like “break up a necklace of %n% beads into %k% pieces, then pretend you did it in one the %k!% different orders, then…” etc.) 1001.1\\. That was me writing; forgot to log in. 1001.2 I’m pretty sure it’s false. In fact, isn’t it obviously false, since the expected number of %i%s in %y^i% is twice the expected number of any j for %j\\ne i%? However, I think what can be proved is that the distribution on each individual %y^i% is not importantly different from the equal-slices distribution, in that the collection of dense sets is the same in both cases. This is analogous to what happens in the corners problem, where the number of corners containing a point is different from point to point (even if we start off with a triangular grid) but this doesn’t matter because the discrepancy between the uniform measure of a set and the point-in-random-corners measure of a set depends only on the measures and not on %n%. 1001.3 However, I think I may be misunderstanding what you have written, since I don’t quite see why you bother to permute the %x^i%s (given that they were already distributed in a way that is permutation-invariant — or at least that’s how it appears to me from what you say). 1001.4\\. Oops, yes, I think what I meant to say was “Pick a permutation %\\pi% on %[k-1]% and then form %y^i = \\pi(x^i)%, where by %\\pi(x^i)% we mean permute the %j%-sets of %x^i% according to the permutation %\\pi%.” I will write a new comment on this topic later today."},{"username":"ryan-odonnell","timestamp":"2009-03-10T11:21:00.000Z","contents":"1001.1\\. That was me writing; forgot to log in."},{"username":"gowers","timestamp":"2009-03-10T13:12:00.000Z","contents":"1001.2 I’m pretty sure it’s false. In fact, isn’t it obviously false, since the expected number of %i%s in %y^i% is twice the expected number of any j for %j\\ne i%? However, I think what can be proved is that the distribution on each individual %y^i% is not importantly different from the equal-slices distribution, in that the collection of dense sets is the same in both cases. This is analogous to what happens in the corners problem, where the number of corners containing a point is different from point to point (even if we start off with a triangular grid) but this doesn’t matter because the discrepancy between the uniform measure of a set and the point-in-random-corners measure of a set depends only on the measures and not on %n%."},{"username":"gowers","timestamp":"2009-03-10T14:41:00.000Z","contents":"1001.3 However, I think I may be misunderstanding what you have written, since I don’t quite see why you bother to permute the %x^i%s (given that they were already distributed in a way that is permutation-invariant — or at least that’s how it appears to me from what you say)."},{"username":"ryan-odonnell","timestamp":"2009-03-10T20:41:00.000Z","contents":"1001.4\\. Oops, yes, I think what I meant to say was “Pick a permutation %\\pi% on %[k-1]% and then form %y^i = \\pi(x^i)%, where by %\\pi(x^i)% we mean permute the %j%-sets of %x^i% according to the permutation %\\pi%.” I will write a new comment on this topic later today."},{"username":"randallgowersrandallrandallrandallrandall","timestamp":"2009-03-10T11:49:00.000Z","contents":"895 again. Okay, maybe the book proof of Szemeredi goes through DHJ, but I hope not…I was trying before to get someone to bite on trying something potentially simpler than DHJ (3) that would still prove the corners theorem via the Ajtai-Szem method. (And in particular didn’t use Szemeredi’s theorem as a lemma.) Of course, being the token ergodic theorist, you can probably guess that I would like someone to bite so I don’t have to try to check myself, because I am woefully slow and error prone when it comes to such things. So let me try again…I’ll just think out loud and if none of this makes any sense, everyone can feel free to ignore. What if I take as my basic object pairs %(\\alpha, \\beta)% of %[n]%. A diagonal is a set %D_\\gamma= \\{ (\\alpha,\\beta) : \\alpha+\\beta =\\gamma\\}%, where %+% is multi-set union and %\\gamma% is some fixed multi-set (of multiplicity no greater than 2 in each element). A corner is a triple of the form %\\{(\\alpha,\\beta), (\\alpha+\\eta,\\beta), (\\alpha,\\beta+\\eta)\\}%, where %\\eta% can be a set plus an anti-set (an antiset is like a set where every element has a multiplicity of -1). Okay, let %A% be a set of pairs of density at least %\\delta%, i.e. %|A|\\geq \\delta 4^n%. Now, for some rather small, or at least not-so-big %m%, most of the whole space is covered by diagonals whose “multiplicity 2-or-zero” (for the defining multiset %\\gamma%) set of indices has size no bigger than %m%. Diagonals of this sort have at least %2^{n-m}% members. Take one of these diagonals %D_\\gamma% on which %A% is dense, say of relative density at least %\\delta /2%. If %A% is corner-free, then any pair of the form %(\\alpha, \\beta)%, where %(\\gamma -\\beta, \\beta)\\in A% and %(\\alpha, \\gamma-\\alpha) \\in A% is forbidden. That should be something like %4^{n-m-1} \\delta^2 % forbidden pairs, and these pairs lie in a Cartesian product. This should mean that %A% has a density increment on some other Cartesian product, call it %X\\times Y%. The next step is presumably to use the multidimensional Sperner’s theorem to carve almost all of %B=X\\times Y% up into subgrids of some large but fixed size %r%. (Here a subgrid is a collection %(\\zeta\\cup FU \\{\\beta_1,\\ldots ,\\beta_r\\}) \\times(\\alpha\\cup FU \\{\\beta_1,\\ldots ,\\beta_r\\}) %.) I wasn’t really anticipating how hard this would be but maybe you can use the iteration method we use to carve 1-spaces up. Now that I am thinking about this finally, maybe one can do it all in one go. Namely, you can always find %\\beta_1,\\ldots ,\\beta_r% with combined size at most %M% such that the probability that %(\\zeta\\cup FU \\{\\beta_1,\\ldots ,\\beta_r\\}) \\subset X% and %(\\alpha\\cup FU \\{\\beta_1,\\ldots ,\\beta_r\\}) \\subset Y% is above some constant %c%. So for those pairs %(\\zeta, \\alpha)% where you get the containment you were shooting for, remove those grids from %B% and keep them as part of your partition; partition the other pairs into %4^T% classes according to the value taken on the subgrid associated with that pair. Each of these classes is the intersection of %B% with a subgrid and we iterate…just like in the proof of DHJ. And, I guess that’s it. Assuming all of the above is correct, you get an increment on a subgrid. It’s almost 3 here and too late to check so I will apologize in advance if something is amiss. 1002.1 Randall, is your problem the same as Jozsef’s in comment 2? In one of the very early comments I tried to translate that one into set-theoretic language and ended up with something pretty similar. This is mainly to say that I agree that it’s worth seeing whether Szemerédi can be done without going the whole way to DHJ. For now I’m going to concentrate my efforts on DHJ(k) but will follow what you have to say with interest. I suppose there are two issues: can DHJ be avoided, and is the resulting proof substantially simpler? 1002.2 Yes, it was intended to be the same (not sure whether Jozsef intended to allow inverted corners, but I am here). Oh, yes…I see your comment 7\\. That looks to be exactly the interpretation I am employing. Well, perhaps Terry will wake up and say whether the alleged proof I gave is really a proof or can be made into a proof. (And whether it’s simpler than what we have already.) Since he composed and typed the breakthrough post (837?) in eighteen minutes, perhaps he’ll be able to do this despite being very busy with other things. On a more general philosophical note, Tim, to we ergodic theorists (who have trouble with subscripts appearing on the numbers capital N appearing in proofs), the dynamical proof of van der Waerden (not that of Furstenberg and Weiss but the easy one, due to Blszczyk, Plewik and Turek) is much more pleasant than any proof I have seen of HJ. (Actually, I have long wanted to see a proof, even an unpleasant one, of HJ that made efficacious use of non-metrizable topological dynamics.) 1002.3 Obviously I meant “efficacious use of *metrizable* topological dynamics”. There exist proofs using non-metrizable topological dynamics (e.g. Furstenberg and Katznelson’s proof). It may not be possible, or it may require some non-commutative stuff like in the FK proof of DHJ. There is a Bergelson-Leibman proof that tried to use recurrence for continuous maps of compact metric spaces, but it was realized after the paper appeared that neither continuity of the transformations nor completeness of the space was actually ever used. 1002.4 No induction…. I think the idea I had to prove Szemeredi via Ajtai-Szem type argument using Jozsef’s formulation from comment 2 is dead in the water at k=3.5\\. The problem I don’t see a way around is that there don’t seem to be enough corners at k=3 to forbid enough points at k=4\\. Obviously not a problem for DHJ since the number of lines at k=3 is 4^k which is equal to the number of points at k=4\\. Oh well…if the above is correct (nobody checked?) it may be the simplest proof of the corner’s theorem, at least. Hmm…I finally had a look at the above myself, and it seems to be complete garbage for (at least) two reasons…. Perhaps now I am convinced that the DHJ proof is the right one for Szemeredi after all. (Maybe.)"},{"username":"gowers","timestamp":"2009-03-10T13:18:00.000Z","contents":"1002.1 Randall, is your problem the same as Jozsef’s in comment 2? In one of the very early comments I tried to translate that one into set-theoretic language and ended up with something pretty similar. This is mainly to say that I agree that it’s worth seeing whether Szemerédi can be done without going the whole way to DHJ. For now I’m going to concentrate my efforts on DHJ(k) but will follow what you have to say with interest. I suppose there are two issues: can DHJ be avoided, and is the resulting proof substantially simpler?"},{"username":"randall","timestamp":"2009-03-10T19:39:00.000Z","contents":"1002.2 Yes, it was intended to be the same (not sure whether Jozsef intended to allow inverted corners, but I am here). Oh, yes…I see your comment 7\\. That looks to be exactly the interpretation I am employing. Well, perhaps Terry will wake up and say whether the alleged proof I gave is really a proof or can be made into a proof. (And whether it’s simpler than what we have already.) Since he composed and typed the breakthrough post (837?) in eighteen minutes, perhaps he’ll be able to do this despite being very busy with other things. On a more general philosophical note, Tim, to we ergodic theorists (who have trouble with subscripts appearing on the numbers capital N appearing in proofs), the dynamical proof of van der Waerden (not that of Furstenberg and Weiss but the easy one, due to Blszczyk, Plewik and Turek) is much more pleasant than any proof I have seen of HJ. (Actually, I have long wanted to see a proof, even an unpleasant one, of HJ that made efficacious use of non-metrizable topological dynamics.)"},{"username":"randall","timestamp":"2009-03-10T22:58:00.000Z","contents":"1002.3 Obviously I meant “efficacious use of *metrizable* topological dynamics”. There exist proofs using non-metrizable topological dynamics (e.g. Furstenberg and Katznelson’s proof). It may not be possible, or it may require some non-commutative stuff like in the FK proof of DHJ. There is a Bergelson-Leibman proof that tried to use recurrence for continuous maps of compact metric spaces, but it was realized after the paper appeared that neither continuity of the transformations nor completeness of the space was actually ever used."},{"username":"randall","timestamp":"2009-03-11T23:34:00.000Z","contents":"1002.4 No induction…. I think the idea I had to prove Szemeredi via Ajtai-Szem type argument using Jozsef’s formulation from comment 2 is dead in the water at k=3.5\\. The problem I don’t see a way around is that there don’t seem to be enough corners at k=3 to forbid enough points at k=4\\. Obviously not a problem for DHJ since the number of lines at k=3 is 4^k which is equal to the number of points at k=4\\. Oh well…if the above is correct (nobody checked?) it may be the simplest proof of the corner’s theorem, at least."},{"username":"randall","timestamp":"2009-03-26T00:57:00.000Z","contents":"Hmm…I finally had a look at the above myself, and it seems to be complete garbage for (at least) two reasons…. Perhaps now I am convinced that the DHJ proof is the right one for Szemeredi after all. (Maybe.)"},{"username":"gowers","timestamp":"2009-03-10T14:49:00.000Z","contents":"Metacomment: because of more spam problems on the wiki, you now have to be registered on it and logged in if you want to edit the main page."},{"username":"terence-taogowers","timestamp":"2009-03-10T23:10:00.000Z","contents":"Metacomment. This might be a good time to set up a “meta” thread to deal with issues not directly related to finishing off the proof of DHJ(3)/DHJ(k) [1000-1049] or of computing DHJ-type numbers in small dimension [900-999]. For instance, this seems like a good time to face issues concerning writing up the results, how to perform proper attribution, etc. (At a more frivolous level, there was a suggestion on the wiki to come up with a logo for that wiki; I think that might be a fun topic for the meta thread. More generally, discussion of the strengths and weaknesses of the wiki, or the threaded format, etc. would be apropos at this time, I think.) OK what I’ll do is get on and write a post that I’ve been intending to write, in which I discuss general meta-issues that have arisen out of how things have gone. I think there’s a lot to talk about, so comments on that should be a natural place for discussing these issues amongst others."},{"username":"gowers","timestamp":"2009-03-10T23:45:00.000Z","contents":"OK what I’ll do is get on and write a post that I’ve been intending to write, in which I discuss general meta-issues that have arisen out of how things have gone. I think there’s a lot to talk about, so comments on that should be a natural place for discussing these issues amongst others."},{"username":"terence-taoterence-taoterence-taogowers","timestamp":"2009-03-10T23:43:00.000Z","contents":"Austin’s proof I’ve managed to digest a key ingredient in Austin’s proof (Lemma 6.1 of [http://arxiv.org/abs/0903.1633](http://arxiv.org/abs/0903.1633) ) which may be of interest here. Roughly speaking, it says that 1-sets and 2-sets (say) are always “locally independent” of each other. A more precise statement is the following. Let %f: [3]^n \\to [-1,1]% be 02-insensitive (e.g. it is the indicator function of a 1-set), and let %g: [3]^n \\to [-1,1]% be 01-insensitive (e.g. it is the indicator of a 2-set). Suppose also that %{\\Bbb E}_{x \\in [3]^n} f(x) g(x)% is large. Then f has large average on large-dimensional spaces, or (equivalently) correlates with a 012-low influence function (a function which is almost invariant under any interchanges between 0, 1, and 2). Proof. Let x be a random string %[3]^n%, let %A \\subset [n]% be a random medium-sized set of indices, and let %\\pi_{A, 02 \\to 0}(x) = \\pi_{A, 2 \\to 0}(x)% be the string in %[3]^n% formed by replacing all occurrences of 0 or 2 in A with 0\\. If A is not too large, then %\\pi_{A, 02 \\to 0}(x)% is still roughly uniformly distributed in %[3]^n%, and so %{\\Bbb E}_{x \\in [3]^n} f( \\pi_{A,02 \\to 0}(x) ) g( \\pi_{A, 02 \\to 0}(x) )% is large. But as f is 02-insensitive, %f(\\pi_{A,02 \\to 0}(x)) = f(x)%. As g is 01-insensitive, %g(\\pi_{A,02 \\to 0}(x)) = g(\\pi_{A,02 \\to 1}(x)) = g(\\pi_{A,012 \\to 1}(x))%, where %\\pi_{A,012 \\to 1}(x)% is the string formed by sending every digit of x in A to 1\\. Thus %{\\Bbb E}_{x \\in [3]^n} f( x ) g( \\pi_{A, 012 \\to 1}(x) )% is large. Freezing A, we thus conclude that f has large mean on many |A|-dimensional subspaces; alternatively, we see that f correlates with the 012-low influence function %G(x) := {\\Bbb E}_A g( \\pi_{A, 012 \\to 1}(x) )% (if we choose A in a suitably Poisson manner). A corollary of this is that if one has a 1-set A and a 2-set B which are “strongly stationary” in the sense that their average local density on medium-sized subspaces is close to their global density, then A and B are roughly independent, thus the density of the 12-set %A \\cap B% is roughly the density of A multiplied by the density of B. As one might imagine, this fact is extremely useful for doing any sort of “counting lemma” involving 12-sets. To be continued… A small correction: where I said that f and g could be indicators of 1-sets and 2-sets respectively, it might be better to think of them as the _balanced functions_ of a 1-set and 2-set, i.e. the indicator minus the density. 1003.2 Incidentally, this framework also gives a short proof of “lack of lines implies correlation with a local 12-set”. With the notation as before, observe that %x, \\pi_{A,01 \\to 1}(x), \\pi_{A,02 \\to 2}(x)% will form a combinatorial line with high probability if A is not too small. Thus if %{\\mathcal A}% is a set with no lines and density %\\delta% then %{\\Bbb E} (1_{\\mathcal A}-\\delta)(x) 1_{\\mathcal A}( \\pi_{A,01 \\to 1}(x)) 1_{\\mathcal A}( \\pi_{A,02 \\to 2}(x))% is large. But if we freeze A and the coordinates outside of A (thus passing to an |A|-dimensional subspace), then %1_{\\mathcal A}( \\pi_{A,01 \\to 1}(x))%, %1_{\\mathcal A}( \\pi_{A,02 \\to 2}(x))% are indicators of a 2-set and a 1-set respectively, and the claim follows. 1003.3 Terry, I’m trying to digest 1003.2\\. It seems to me that the displayed line is true only if there are many x such that %\\pi_{A,01\\rightarrow 1}(x)% and %\\pi_{A,02\\rightarrow 2}(x)% both belong to %\\mathcal{A}.% In the previous proof this was done with the help of Sperner and that seemed necessary. I’m not sure where the Sperner step is appearing in what you’ve written."},{"username":"terence-tao","timestamp":"2009-03-10T23:44:00.000Z","contents":"A small correction: where I said that f and g could be indicators of 1-sets and 2-sets respectively, it might be better to think of them as the _balanced functions_ of a 1-set and 2-set, i.e. the indicator minus the density."},{"username":"terence-tao","timestamp":"2009-03-10T23:55:00.000Z","contents":"1003.2 Incidentally, this framework also gives a short proof of “lack of lines implies correlation with a local 12-set”. With the notation as before, observe that %x, \\pi_{A,01 \\to 1}(x), \\pi_{A,02 \\to 2}(x)% will form a combinatorial line with high probability if A is not too small. Thus if %{\\mathcal A}% is a set with no lines and density %\\delta% then %{\\Bbb E} (1_{\\mathcal A}-\\delta)(x) 1_{\\mathcal A}( \\pi_{A,01 \\to 1}(x)) 1_{\\mathcal A}( \\pi_{A,02 \\to 2}(x))% is large. But if we freeze A and the coordinates outside of A (thus passing to an |A|-dimensional subspace), then %1_{\\mathcal A}( \\pi_{A,01 \\to 1}(x))%, %1_{\\mathcal A}( \\pi_{A,02 \\to 2}(x))% are indicators of a 2-set and a 1-set respectively, and the claim follows."},{"username":"gowers","timestamp":"2009-03-11T03:50:00.000Z","contents":"1003.3 Terry, I’m trying to digest 1003.2\\. It seems to me that the displayed line is true only if there are many x such that %\\pi_{A,01\\rightarrow 1}(x)% and %\\pi_{A,02\\rightarrow 2}(x)% both belong to %\\mathcal{A}.% In the previous proof this was done with the help of Sperner and that seemed necessary. I’m not sure where the Sperner step is appearing in what you’ve written."},{"username":"ryan-odonnellryan-odonnellgowersryan-odonnellgowers","timestamp":"2009-03-11T01:54:00.000Z","contents":"Random lines/equal slices for DHJ(k+1). I think what I wrote in 1001 was correct. Let me restate it here for clarity: Let %z \\in [k+1]^n% be drawn from equal-slices. Form the string %v^j \\in [k]^n% by changing all %(k+1)%‘s in %z% to %j%‘s, for %j = 1 \\dots k%. Finally, pick a random permutation %\\pi% on %[k]% and define strings %x^i = v^{\\pi(i)}%, for %i = 1 \\dots k%. Then each string %x^i% is distributed as equal-slices on %[k]^n%. Further, since %(v^1, \\dots, v^k, z)% is a combinatorial line (“in order”), we also have that %\\{x^1, \\dots, x^k, z\\}% is a combinatorial line (possibly “out of order”). The proof is [here](http://www.cs.cmu.edu/~odonnell/equal-slices.pdf); I’ll wikify it soon. 1004.1\\. Wikified [here](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_distribution_for_DHJ(k)). Hopefully this will help with the [generalization of line-free sets correlating with 12-sets](http://michaelnielsen.org/polymath1/index.php?title=Line_free_sets_correlate_locally_with_dense_sets_of_complexity_k-2#Proof_of_the_main_result.2C_with_a_gap_still_left_to_fill). 1004.2 Ryan, I’m still struggling with the statement of what you are proving. Suppose k=2\\. Then I pick %z\\in[3]^n% from equal slices and form the strings %u% and %v% by turning the 3s of %z% into 1s and 2s, respectively. Then I randomly decide whether to interchange %u% to %v%. Now let’s look at the event that the number of 1s in %u% is between %0.49n% and %0.51n.% If I choose %u% according to equal-slices that is %0.02%. But if I choose it by starting with z, then either I start with about that number of 1s and change the 3s to 2s, or I start with about that number of 2s and change the 3s to 1s. The probability seems to me to be smaller because %z% is being asked to have rather a lot of 1s or rather a lot of 2s. Since you’ve got a wikified proof, what I’m asking here is for an explanation of where I’m going wrong. 1004.3\\. Hmm, I’m not quite sure what I can say to help. In your example, when you pick %u% from equal-slices on %[2]^n% “in the normal fashion”, then as you say the event %A% = “number of %1%‘s in %u% is in %[.49n, .51n]%” occurs with probability exactly %.02%. When you pick it in the funny way, through, %z%, what is the probability? As you say, half the time %u% is formed by changing %z%‘s %3%‘s to %2%‘s. In this case, %A% occurs iff %z% had between %.49n% and %.51 n% %1%‘s. The other half of the time, %u% is formed by changing %z%‘s %3%‘s to %1%‘s. In this case, %A% occurs iff %z% had between %.49n% and %.51 n% %1%‘s-and-%3%‘s-together, iff %z% had between %.49 n% and %.51 n% %2%‘s. Clearly the probability %z% has between %.49n% and %.51n% %1%‘s is the same as the probability %z% has between %.49n% and %.51n% %2%‘s. So we are reduced to asking why this probability is indeed %.02%. Well, I’m not sure what to say, except that it is. It comes from the fact that %\\int_{.49}^{.51} 2(1-p) dp = .02%, where %2(1-p)% is the density of the first component in a uniform draw from the %3%-simplex. Note that this integral is a bit of a “coincidence”, because it is **not** true that, e.g., %\\int_{0}^{.49} 2(1-p) dp = .49%. The reason this all works is a bit easier to understand in the case of %k = 2%; see [this explanation](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure#Another_useful_equivalent_definition) in the older wiki article on equal-slices. 1004.4 Thanks — I think I’m starting to get it now."},{"username":"ryan-odonnell","timestamp":"2009-03-11T02:12:00.000Z","contents":"1004.1\\. Wikified [here](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_distribution_for_DHJ(k)). Hopefully this will help with the [generalization of line-free sets correlating with 12-sets](http://michaelnielsen.org/polymath1/index.php?title=Line_free_sets_correlate_locally_with_dense_sets_of_complexity_k-2#Proof_of_the_main_result.2C_with_a_gap_still_left_to_fill)."},{"username":"gowers","timestamp":"2009-03-11T02:56:00.000Z","contents":"1004.2 Ryan, I’m still struggling with the statement of what you are proving. Suppose k=2\\. Then I pick %z\\in[3]^n% from equal slices and form the strings %u% and %v% by turning the 3s of %z% into 1s and 2s, respectively. Then I randomly decide whether to interchange %u% to %v%. Now let’s look at the event that the number of 1s in %u% is between %0.49n% and %0.51n.% If I choose %u% according to equal-slices that is %0.02%. But if I choose it by starting with z, then either I start with about that number of 1s and change the 3s to 2s, or I start with about that number of 2s and change the 3s to 1s. The probability seems to me to be smaller because %z% is being asked to have rather a lot of 1s or rather a lot of 2s. Since you’ve got a wikified proof, what I’m asking here is for an explanation of where I’m going wrong."},{"username":"ryan-odonnell","timestamp":"2009-03-11T03:53:00.000Z","contents":"1004.3\\. Hmm, I’m not quite sure what I can say to help. In your example, when you pick %u% from equal-slices on %[2]^n% “in the normal fashion”, then as you say the event %A% = “number of %1%‘s in %u% is in %[.49n, .51n]%” occurs with probability exactly %.02%. When you pick it in the funny way, through, %z%, what is the probability? As you say, half the time %u% is formed by changing %z%‘s %3%‘s to %2%‘s. In this case, %A% occurs iff %z% had between %.49n% and %.51 n% %1%‘s. The other half of the time, %u% is formed by changing %z%‘s %3%‘s to %1%‘s. In this case, %A% occurs iff %z% had between %.49n% and %.51 n% %1%‘s-and-%3%‘s-together, iff %z% had between %.49 n% and %.51 n% %2%‘s. Clearly the probability %z% has between %.49n% and %.51n% %1%‘s is the same as the probability %z% has between %.49n% and %.51n% %2%‘s. So we are reduced to asking why this probability is indeed %.02%. Well, I’m not sure what to say, except that it is. It comes from the fact that %\\int_{.49}^{.51} 2(1-p) dp = .02%, where %2(1-p)% is the density of the first component in a uniform draw from the %3%-simplex. Note that this integral is a bit of a “coincidence”, because it is **not** true that, e.g., %\\int_{0}^{.49} 2(1-p) dp = .49%. The reason this all works is a bit easier to understand in the case of %k = 2%; see [this explanation](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure#Another_useful_equivalent_definition) in the older wiki article on equal-slices."},{"username":"gowers","timestamp":"2009-03-11T04:16:00.000Z","contents":"1004.4 Thanks — I think I’m starting to get it now."},{"username":"terence-taogowers","timestamp":"2009-03-11T01:55:00.000Z","contents":"Quantitative bounds? Is our understanding of the density-increment proof of DHJ(3) good enough that we can venture a probable quantitative bound for n in terms of %1/\\delta%? Naively I am expecting tower-exponential behaviour (with the height of the tower being either polynomial or exponential in %1/\\delta%). 1005.1 I’m pretty sure it’s a tower. The rough reason is that when we drop to a subspace we obtain that subspace by applying multidimensional Sperner, which I think restricts its dimension to the log of the previous dimension. But I think the density increase we can get that way is good, so I think the bound should be a tower of height polynomial in %1/\\delta% and therefore not worse than what you get out of the Ajtai-Szemerédi proof of the corners theorem. Also, if we’ve got the stomach for it, we could think about trying to Shkredovize the argument and beat Shelah’s bound for the colouring DHJ(3). That would be a serious undertaking though, as it would require us to understand the global obstructions rather than just the local ones."},{"username":"gowers","timestamp":"2009-03-11T03:08:00.000Z","contents":"1005.1 I’m pretty sure it’s a tower. The rough reason is that when we drop to a subspace we obtain that subspace by applying multidimensional Sperner, which I think restricts its dimension to the log of the previous dimension. But I think the density increase we can get that way is good, so I think the bound should be a tower of height polynomial in %1/\\delta% and therefore not worse than what you get out of the Ajtai-Szemerédi proof of the corners theorem. Also, if we’ve got the stomach for it, we could think about trying to Shkredovize the argument and beat Shelah’s bound for the colouring DHJ(3). That would be a serious undertaking though, as it would require us to understand the global obstructions rather than just the local ones."},{"username":"ryan-odonnellgowersryan-odonnell","timestamp":"2009-03-11T02:27:00.000Z","contents":"Equal-slices. I wonder: can we mostly work in the equal-slices measure for the whole proof (of DHJ(3), say)? What are the advantages of the uniform distribution? 1006.1 My take on this is that equal-slices measure is very good when you want to mix random points and random lines, but the uniform measure is better when you want to restrict to subspaces. Roughly, the reason for the latter is that if you start with equal-slices and restrict a few coordinates, then the distribution on the resulting subspace becomes much more like a uniform one, or a suitably weighted uniform one. For example, if you’re told that a point’s restriction to the first m coordinates has roughly equal numbers of 1s, 2s and 3s, then with very high probability it belongs to a slice with roughly equal numbers of 1s, 2s and 3s, so the distribution in that (n-m)-dimensional subspace is roughly uniform. It’s possible that we could argue that we can restrict to subspaces and get an equal-slices density increase, but I think we would still have to go via uniform, so in the end I don’t see a compelling argument for doing that. 1006.2\\. That’s a good point. I guess I was idly hoping that the fact that equal-slices is [exactly equal to a (fairly natural) mixture of uniform distributions on subspaces](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures#Passing_from_equal-slices_to_uniform) might allow us a clever way to stay entirely in the equal-slices world."},{"username":"gowers","timestamp":"2009-03-11T03:12:00.000Z","contents":"1006.1 My take on this is that equal-slices measure is very good when you want to mix random points and random lines, but the uniform measure is better when you want to restrict to subspaces. Roughly, the reason for the latter is that if you start with equal-slices and restrict a few coordinates, then the distribution on the resulting subspace becomes much more like a uniform one, or a suitably weighted uniform one. For example, if you’re told that a point’s restriction to the first m coordinates has roughly equal numbers of 1s, 2s and 3s, then with very high probability it belongs to a slice with roughly equal numbers of 1s, 2s and 3s, so the distribution in that (n-m)-dimensional subspace is roughly uniform. It’s possible that we could argue that we can restrict to subspaces and get an equal-slices density increase, but I think we would still have to go via uniform, so in the end I don’t see a compelling argument for doing that."},{"username":"ryan-odonnell","timestamp":"2009-03-11T04:08:00.000Z","contents":"1006.2\\. That’s a good point. I guess I was idly hoping that the fact that equal-slices is [exactly equal to a (fairly natural) mixture of uniform distributions on subspaces](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures#Passing_from_equal-slices_to_uniform) might allow us a clever way to stay entirely in the equal-slices world."},{"username":"gowers","timestamp":"2009-03-11T04:55:00.000Z","contents":"DHJ(k) I think I’ve now proved (though with details a bit hazy at a couple of points) the analogue for k of the fact that line-free sets in %{}[3]^n% correlate locally with 12-sets. The [proposed argument](http://michaelnielsen.org/polymath1/index.php?title=Line_free_sets_correlate_locally_with_dense_sets_of_complexity_k-2#DHJ.28k.29_implies_equal-slices_Varnavides-DHJ.28k.29) can probably be tidied up in a few places. I haven’t yet thought about whether any of the rest of DHJ(k) is going to present problems, but it feels as though we’re closing in on it pretty rapidly. That’s my lot for today though."},{"username":"terence-tao","timestamp":"2009-03-11T05:20:00.000Z","contents":"Re: 1003.3 Yes, you’re right, one also needs Sperner. Without it, what one gets is that if %{\\mathcal A}% is uniformly distributed with respect to 12-sets, then %{\\Bbb E} 1_{\\mathcal A} 1_{\\mathcal A}( \\pi_{A,01 \\to 1}(x)) 1_{\\mathcal A}( \\pi_{A,02 \\to 2}(x)) \\approx {\\Bbb E} \\delta 1_{\\mathcal A}( \\pi_{A,01 \\to 1}(x)) 1_{\\mathcal A}( \\pi_{A,02 \\to 2}(x))% and then one has to use DHJ(2) to say that the RHS is large (which one can do, after letting A vary in a sufficiently Poisson-like manner). I would imagine that the same thing works in higher k, perhaps this is already implicit in 1007."},{"username":"ryan-odonnell","timestamp":"2009-03-11T09:38:00.000Z","contents":"Equal-slices distribution. I think the following is an easier explanation of what I wrote. Claim: Pick %z \\in [k]^n% according to equal-slices. Then pick %J \\in [k-1]% uniformly and form %x% by changing all the %k%‘s in %z% to %J%‘s. Then %x% is distributed according to equal-slices on %[k-1]^n%. Note: it’s helpful to think of %k%‘s as %\\ast%‘s. Proof sketch via coupling: Draw a circle of %n% dots. Place bar(1) uniformly at random in the %n+1% slots. Then place bar(2) uniformly at random in the resulting %n+2% slots. Etc., placing %k% bars. Pick %J \\in [k-1]% uniformly. Look at the arc of dots following bar(k) clockwise, up until the next bar. Fill in these dots with %k%‘s. Look also at the arc of dots preceding bar(k) counterclockwise, up until the next bar. Fill in these dots with %J%‘s. Next, for each of the remaining %k-2% arcs of dots, fill it all with the same digit — using up the remaining digits %[k-1] \\setminus \\{J\\}% in some arbitrary way. Finally, delete the bars, cut the circle to make it a segment, and then randomly permute the whole sequence of digits. We claim the resulting sequence is distributed as equal-slices on %[3]^n%, and therefore we can take it as %z%. Now forming %x% from %z% is the same as doing the above procedure but using %J%‘s where we were using %k%‘s before. But really, happens under this new procedure? You place %k-1% bars as if you were doing equal-slices on %[k-1]^n%; then you throw in a phantom extra bar but only use it insofar as deciding to call the arc it lands in the %J%-arc; then you define the other %k-2% arcs according to the digits in %[k-1] \\setminus \\{J\\}%, etc. Now granted, the arc chosen to be the %J%-arc is not uniformly distributed among arcs — it’s biased towards the longer arcs. But who cares, since %J% is uniformly random on %[k-1]%?"},{"username":"ryan-odonnellgowers","timestamp":"2009-03-11T10:01:00.000Z","contents":"“Varnavides”-DHJ(k). (Sorry for the scare quotes, but it strikes me as funny somehow to name this concept after a person.) Tim, I can’t quite follow what you wrote at the end of your proof sketch that line-free sets in %[k]^n% correlate with lower-complexity sets. I was wondering if you could clarify. In particular, as you say it seems we need to show that DHJ(k) actually implies Varnavides-DHJ(k) under some appropriate measures. As you suggest, it seems that equal-slices is what to hope for here (it works pretty well for %k = 2% at least!). In other words, we may hope that if %A \\subseteq [k]^n% has equal-slices measure %\\geq \\delta% then if you choose an “equal-slices-combinatorial-line” (i.e., draw from equal-slices on %([k] \\cup \\{\\ast\\})^n%) then there is some %c(\\delta) > 0% chance that all %k% points are in %A%. Looking at that statement now I worry slightly that it is a bit optimistic. Well, let’s hope it’s true, and let me ask, Tim, about [what you wrote](http://michaelnielsen.org/polymath1/index.php?title=Line_free_sets_correlate_locally_with_dense_sets_of_complexity_k-2#DHJ.28k.29_implies_equal-slices_Varnavides-DHJ.28k.29). I guess I don’t quite understand what’s going on with the parameter %M%. On one hand, I assume it’s to be “small” (although still perhaps %\\omega_{n \\to \\infty}(1)%), because otherwise bounding the number of lines in %[k]^M% by %(k+1)^M% looks worrisome. On the other hand, this makes me worry a bit about bullet-point 3: if %\\nu% is supported on %M%-dimensional subspaces then somehow I don’t expect that a set of lines %\\mathcal{L}% being large under %\\nu% should mean it’s large under a more “global” measure on lines such as equal-slices. Quite likely I’m mistaking your argument or missing something — would you be able to clarify? Thanks! 1010.1 Ryan, the argument is so sloppily written (little more than a declaration that my gut feeling is that it works) that it is good that you apply a little pressure of this kind. I think I have answers to your specific queries, but that’s not the same as a proof. But maybe it will tip you over to believing that it works. First of all, my parameter M is not supposed to tend to infinity. As I said in the write-up, it’s meant to be the smallest integer such that we have DHJ(k) with density %\\zeta%, where %\\zeta% depends only on the equal-slices density %\\theta% of the set inside which we’re trying to find lots of combinatorial lines. So what about your second worry? The point here is that the M-dimensional subspaces I’m averaging over are absolutely _not_ local. Instead, they’re supposed to be far more tailored to equal-slices. What I’m not doing is fixing almost all coordinates and taking M small wildcard sets, or something like that. Instead, I’m randomly choosing the sizes of the M wildcard sets (which I allow to add up to 1, so there are in fact no fixed coordinates at all) and then uniformly at random choosing a line from the resulting subspace. That’s why I’m hoping that the resulting measure on lines is sufficiently global in character."},{"username":"gowers","timestamp":"2009-03-11T17:16:00.000Z","contents":"1010.1 Ryan, the argument is so sloppily written (little more than a declaration that my gut feeling is that it works) that it is good that you apply a little pressure of this kind. I think I have answers to your specific queries, but that’s not the same as a proof. But maybe it will tip you over to believing that it works. First of all, my parameter M is not supposed to tend to infinity. As I said in the write-up, it’s meant to be the smallest integer such that we have DHJ(k) with density %\\zeta%, where %\\zeta% depends only on the equal-slices density %\\theta% of the set inside which we’re trying to find lots of combinatorial lines. So what about your second worry? The point here is that the M-dimensional subspaces I’m averaging over are absolutely _not_ local. Instead, they’re supposed to be far more tailored to equal-slices. What I’m not doing is fixing almost all coordinates and taking M small wildcard sets, or something like that. Instead, I’m randomly choosing the sizes of the M wildcard sets (which I allow to add up to 1, so there are in fact no fixed coordinates at all) and then uniformly at random choosing a line from the resulting subspace. That’s why I’m hoping that the resulting measure on lines is sufficiently global in character."},{"username":"jozsefjozsefjozsef","timestamp":"2009-03-11T12:00:00.000Z","contents":"A Shelah-type argument I’m quite optimistic that a Shelah type argument might work for the general DHJ(k) problem. It’s late evening and I had a hard day, so I’m not sure how far can I go with the argument tonight. Let me give some basic definitions first. In %k^{[n]}% two points are neighbours if they have same coordinates everywhere but in one position where one is k-2 and the other is k-1\\. For a subset %S\\subset k^{[n]}% the space %k^{[n]}% is fliptop if a point p is in S iff any neighbour of p is also in S. (I will wikify it if the argument turns out to be useful) Our goal is to find a large fliptop subspace which is dense. By induction there will be a k-1 combinatorial line which gives a full combinatorial line because of the fliptop property of the subspace. I will follow Shelah’s strategy with some extra conditions to make it (hopefully) work for the density version.  \nSuppose that %S\\subset k^{[n]}% is c-dense and we already know the subspace version of DHJ for k-1\\. Partition [n] into m consecutive intervals of lengths %l_1, l_2, \\ldots, l_m.% We will repeat two steps to get a subspace eventually. The first one should keep the density high and the second will provide the fliptop property.  \nStep one: For any point in %k^{[l_m]}% there is a certain number of points in S with that tail. Select the points from %k^{[l_m]}% which are tails of at least %(c-\\epsilon_1)k^{n-l_m}% elements of S. The set of such points is denoted by H1\\. Applying the induction hypothesis we know that %k^{[l_m]}% will contain a d1-dimensional subspace where every element avoiding (running) coordinate k-1 are from H1 (so it has many extensions that are in S).  \nStep two: Consider the elements of the d1-dimensional subspace where every coordinate is k-1 or k-2\\. We say that two such points are equivalent if they are the tails of the same subset of S. There are no more than %k^{n-l_m}% such equivalence classes. Therefore if %d1 > k^{n-l_m}% then we have a Sperner pair, two points where the set of k-1 coordinates of a point contains the other point’s set of k-1 coordinates. It defines a combinatorial line in the subspace and we will work with this line in the following steps.  \nWe will repeat steps one and two in %l_{m-1}%. I will continue, but I would like to read over first what I wrote. 1011\\. contd. Let us denote the k points of the combinatorial line selected previously by %P^1_0, P^1_1,\\ldots ,P^1_{k-1}% (indexed by the running coordinates.)  \nStep one: select set H2 from %k^{[l_{m-1}]}%.  \nSelect a point if with any %P_i% it is the tail of at least %(c-\\epsilon_2)k^{n-l_m-l_{m-1}}% elements of S where %0\\leq i\\leq k-2%. There are two cases; either there are many such points or there is a tail of at least %(c+\\delta)k^{n-l_m-l_{m-1}}% elements of S. (More careful calculations are needed, but let me sketch the argument first.) If there are enough points, the there is a d2-dimensional subspace in %k^{[l_{m-1}]}% where every point without coordinate k-1 is from H2.  \nStep two: Consider the elements of the d2-dimensional subspace where every coordinate is k-1 or k-2\\. We say that two such points, a and b. are equivalent if for any element %t\\in k^{[n-l_m-l_{m-1}]}%, %t,a,P_i% is in S iff %t,b,P_i% is in S. There are no more than %k^{n-l_m-l_{m_1}}+1% such equivalence classes. If %d2 > k^{n-l_m-l_{m_1}}+1% then there is a Sperner pair, two points where the set of k-1 coordinates contains the other point’s set of k-1 coordinates. It defines the second combinatorial line in the subspace and we will work with the two lines in the following steps. Let us denote the k points of the new combinatorial line by %P^2_0, P^2_1,\\ldots ,P^2_{k-1}% There are a few typos in the argument. I should fix them eventually, but let me mention her one which might be quite misleading; “There are no more than %k^{n-l_m}% such equivalence classes.” actually, there are no more than %2^{k^{n-l_m}}% such equivalence classes. Similarly, in the second iteration There are no more than %2^{k^{n-l_m-l_{m-1}+1}}% such equivalence classes."},{"username":"jozsef","timestamp":"2009-03-11T12:56:00.000Z","contents":"contd. Let us denote the k points of the combinatorial line selected previously by %P^1_0, P^1_1,\\ldots ,P^1_{k-1}% (indexed by the running coordinates.)  \nStep one: select set H2 from %k^{[l_{m-1}]}%.  \nSelect a point if with any %P_i% it is the tail of at least %(c-\\epsilon_2)k^{n-l_m-l_{m-1}}% elements of S where %0\\leq i\\leq k-2%. There are two cases; either there are many such points or there is a tail of at least %(c+\\delta)k^{n-l_m-l_{m-1}}% elements of S. (More careful calculations are needed, but let me sketch the argument first.) If there are enough points, the there is a d2-dimensional subspace in %k^{[l_{m-1}]}% where every point without coordinate k-1 is from H2.  \nStep two: Consider the elements of the d2-dimensional subspace where every coordinate is k-1 or k-2\\. We say that two such points, a and b. are equivalent if for any element %t\\in k^{[n-l_m-l_{m-1}]}%, %t,a,P_i% is in S iff %t,b,P_i% is in S. There are no more than %k^{n-l_m-l_{m_1}}+1% such equivalence classes. If %d2 > k^{n-l_m-l_{m_1}}+1% then there is a Sperner pair, two points where the set of k-1 coordinates contains the other point’s set of k-1 coordinates. It defines the second combinatorial line in the subspace and we will work with the two lines in the following steps. Let us denote the k points of the new combinatorial line by %P^2_0, P^2_1,\\ldots ,P^2_{k-1}%"},{"username":"jozsef","timestamp":"2009-03-11T22:18:00.000Z","contents":"There are a few typos in the argument. I should fix them eventually, but let me mention her one which might be quite misleading; “There are no more than %k^{n-l_m}% such equivalence classes.” actually, there are no more than %2^{k^{n-l_m}}% such equivalence classes. Similarly, in the second iteration There are no more than %2^{k^{n-l_m-l_{m-1}+1}}% such equivalence classes."},{"username":"jozsefgowers","timestamp":"2009-03-11T13:00:00.000Z","contents":"I’ll continue tomorrow morning, but I hope that what I wrote makes sense and maybe someone will tell me if this plan is feasible or not. 1012.1 I’ve just read through it and nothing jumps out at me as wrong, or obviously not going to work. I suppose my one worry is a metaworry, which is that many people must have tried to produce a density version of Shelah’s argument, and there doesn’t seem to be any sign of an obstacle in the above sketch. On the other hand, a balancing metareassurance is that you have a record of finding dazzlingly simple arguments. I haven’t yet thought about the part where you write “more careful calculations are needed”. Perhaps another reason to be hopeful is that one way of explaining what Shelah did is to say that he changed an inductive argument from deducing HJ(k) from HJ(2) with the repeated help of HJ(k-1) to deducing HJ(k) from HJ(k-1) with the repeated help of HJ(2), and there is no obvious reason to think that that cannot be done for density as well. Anyhow, if it works … amazing!"},{"username":"gowers","timestamp":"2009-03-11T13:20:00.000Z","contents":"1012.1 I’ve just read through it and nothing jumps out at me as wrong, or obviously not going to work. I suppose my one worry is a metaworry, which is that many people must have tried to produce a density version of Shelah’s argument, and there doesn’t seem to be any sign of an obstacle in the above sketch. On the other hand, a balancing metareassurance is that you have a record of finding dazzlingly simple arguments. I haven’t yet thought about the part where you write “more careful calculations are needed”. Perhaps another reason to be hopeful is that one way of explaining what Shelah did is to say that he changed an inductive argument from deducing HJ(k) from HJ(2) with the repeated help of HJ(k-1) to deducing HJ(k) from HJ(k-1) with the repeated help of HJ(2), and there is no obvious reason to think that that cannot be done for density as well. Anyhow, if it works … amazing!"},{"username":"gowers","timestamp":"2009-03-11T16:54:00.000Z","contents":"Shelah. I took the car to be serviced today, which resulted in a long walk to work, during which I pondered Jozsef’s argument. I can’t see a problem, but neither have I thought it through fully. So I want to try to assess its feasibility by seeing if I can sketch it. (Of course, Jozsef has already sketched it, but there’s no harm in having more than one sketch, and if I manage to sketch it without looking any further at what he wrote, then it might make us feel better in the way that people feel better about computer-assisted proofs if more than one person does the computer part and different programs are used.) What is remarkable about Jozsef’s proposal if it works is that it is so similar to what Shelah did: it would make it very mysterious that the proof had not been discovered earlier (which would add another layer of mystery to the already remarkable fact that Shelah’s proof was itself not discovered earlier). The idea is this. Suppose that we are trying to prove DHJ(k) with density %\\delta%. We begin by choosing some parameters. First, we choose %m% so large that DHJ(k-1) is true with density %\\delta/2%. That is, we assume as an inductive hypothesis that every subset of %{}[k-1]^m% of density at least %\\delta/2% contains a combinatorial line. Next, we choose a very rapidly increasing sequence of integers (though in the context of some proofs of this kind the rate of increase is not too bad — this proof should result in a primitive recursive bound for DHJ(k), one level beyond a tower, just like Shelah’s) %r_1<. (Jozsef called them %l_1< but on my screen an %l% is just a vertical line so I’ve changed the notation.) We shall repeatedly apply the following averaging argument: if %a_1,\\dots,a_M% are integers between 0 and 1, and the average of the %a_i% is %\\delta%, then at least %\\epsilon N% of the %a_i% are at least %{}\\delta-\\epsilon.% The proof is trivial (by contradiction). Now let %n=r_1+\\dots+r_m% and think of %{}[k]^n% as %{}[k]^{r_1}\\times\\dots\\times[k]^{r_m}.% Let %\\mathcal{A}% be a subset of %{}[k]^n% of density %\\delta.% For each %y\\in[k]^{r_m},% let %\\mathcal{A}_y=\\{x\\in[k]^{n-r_m}:(x,y)\\in\\mathcal{A}\\}.% Then the average density of %\\mathcal{A}_y% over %y\\in[k]^{r_m}% is %\\delta%, so the density of %y% such that %\\mathcal{A}_y% has density at least %3\\delta/4% is at least %\\delta/4%. Let %\\mathcal{B}_m% be the set of all %y\\in[k]^{r_m}% such that %\\mathcal{A}_y% has density at least %3\\delta/4%. By the pigeonhole principle, we can find a subset %\\mathcal{C}_m% of %\\mathcal{B}_m% of density at least %\\gamma=\\delta/4k^{n-m_r}% such that %\\mathcal{A}_y% is the same for every %y\\in\\mathcal{C}_m%. Let us define a ”binary subspace” to be a set where you fix some of the coordinates and allow the remaining ones to take the values k-1 and k. If t is small enough, and if you choose a random binary subspace of %{}[k]^{r_m}% by randomly choosing t coordinates to be the variable ones and randomly fixing the others, and if you then choose a random point in one of these random binary subspaces, the result will be very close to the uniform distribution on %{}[k]^{r_m}%. Therefore, by an easy averaging argument we can find a binary subspace inside which the density of %\\mathcal{C}_m% is at least %\\gamma/2.% (Note that for this to work we need %r_m% to be large enough in terms of %\\gamma% and %t%.) And then by Sperner’s theorem we can find two points in the binary subspace that form a combinatorial line (in the %{}[2]^n% sense but with %k-1% and %k% replacing 1 and 2, or 0 and 1 if you prefer). We can extend this binary combinatorial line to a full combinatorial line in %{}[k]^{m_r}%, by allowing the wildcards to take the values %1,2,\\dots,k-2% as well. The result is a … OK, this is the first point at which I’ve got stuck, though I think Jozsef may have dealt with the difficulty I am facing. Time to end this comment and start a new one."},{"username":"gowersjozsefgowersjozsefgowers","timestamp":"2009-03-11T17:06:00.000Z","contents":"Shelah The problem with what I was doing just above is this. I’ve just defined a line %L=(y_1,\\dots,y_k)% in %{}[k]^{m_r}%, and I know that the sets %\\mathcal{A}_{y_{k-1}}% and %\\mathcal{A}_{y_k}% are the same. However, I don’t know anything about %\\mathcal{A}_{y_i}% for %i\\leq k-2,% and it would be a serious problem if e.g. they turned out to be empty. However, I’ve ignored Jozsef’s step 1, which is perhaps his main new idea. He begins by using DHJ(k-1) to pass to a subspace where %\\mathcal{A}_y% has density close to %\\delta% for _all_ sequences %y,% provided only that they do not have any (variable) coordinate equal to %k%. (This can be done by an averaging argument similar to the one I used above to get dense binary subspaces — this time one wants dense (k-1)-ary subspaces.) But now I’ve reached the point where I don’t understand Jozsef’s argument. How do I know that it is not the case that %\\mathcal{A}_y% is empty whenever one of the variable coordinates of %y% is equal to k? I don’t see it at the moment, so I think I’d better wait for Jozsef to wake up. 1014.1 Dear Tim, I don’t think that we need that %\\mathcal{A}_y% is dense if one of the variable coordinates is equal to k. This is actually a crucial observation, that’s why we need the fliptop property. At the last step, when only %r_1% remains, one word of %r_1% spans a dense subset of the %\\{1,2,..,k-1\\}^{m-1}% part of the subspace. By induction there is a line, and by the fliptop property there is an extension to k. Note that the fliptop property was defined by considering running coordinates k as well, however it wasn’t sensitive for density in any ways. 1014.2 I’m still mystified. In Step 1, let’s choose a subspace such that for every sequence %y% that avoids running coordinate %k%, there is a set of density almost %\\delta% of %x% such that %(x,y)\\in\\mathcal{A}%. Now we go to Step 2\\. Nothing we’ve done so far stops %\\mathcal{A}_y% being empty for every %y% that _does_ involve %k%. So we choose our Sperner pair and have two points %y% and %y'% (where %y'% is obtained from %y% by changing some of the %(k-1)%s to %k%s). How can that help? Indeed, it seems that %\\mathcal{A}_y% could be empty for every point in the combinatorial line you select. Tim – I have to go to the University now, where I will rethink your question again, but I think that the answer is that it is not possible that you switch a running coordinate to k from k-1 and the density drops because the two points are neighbours. I will get back to you soon. Jozsef, that gives me time to be more precise about my question. See comment 1016 below."},{"username":"jozsef","timestamp":"2009-03-11T21:47:00.000Z","contents":"1014.1 Dear Tim, I don’t think that we need that %\\mathcal{A}_y% is dense if one of the variable coordinates is equal to k. This is actually a crucial observation, that’s why we need the fliptop property. At the last step, when only %r_1% remains, one word of %r_1% spans a dense subset of the %\\{1,2,..,k-1\\}^{m-1}% part of the subspace. By induction there is a line, and by the fliptop property there is an extension to k. Note that the fliptop property was defined by considering running coordinates k as well, however it wasn’t sensitive for density in any ways."},{"username":"gowers","timestamp":"2009-03-11T23:15:00.000Z","contents":"1014.2 I’m still mystified. In Step 1, let’s choose a subspace such that for every sequence %y% that avoids running coordinate %k%, there is a set of density almost %\\delta% of %x% such that %(x,y)\\in\\mathcal{A}%. Now we go to Step 2\\. Nothing we’ve done so far stops %\\mathcal{A}_y% being empty for every %y% that _does_ involve %k%. So we choose our Sperner pair and have two points %y% and %y'% (where %y'% is obtained from %y% by changing some of the %(k-1)%s to %k%s). How can that help? Indeed, it seems that %\\mathcal{A}_y% could be empty for every point in the combinatorial line you select."},{"username":"jozsef","timestamp":"2009-03-11T23:59:00.000Z","contents":"Tim – I have to go to the University now, where I will rethink your question again, but I think that the answer is that it is not possible that you switch a running coordinate to k from k-1 and the density drops because the two points are neighbours. I will get back to you soon."},{"username":"gowers","timestamp":"2009-03-12T00:02:00.000Z","contents":"Jozsef, that gives me time to be more precise about my question. See comment 1016 below."},{"username":"gil-kalaigowers","timestamp":"2009-03-11T23:39:00.000Z","contents":"A bit off-topic “If this were a conventional way of producing mathematics, then it would be premature to make such an announcement — one would wait until the proof was completely written up with every single i dotted and every t crossed — ” hmmm, maybe we should regard it premature also in this new mode. What do the rules say? “but this is blog maths and we’re free to make up conventions as we go along. So I hereby state that I am basically sure that the problem is solved” Congratulations! I must admit that even if a full k=3 proof will take 200 more comments and several more weeks the success of the project exceeded my expectations. “(though not in the way originally envisaged).” Actually, this is a lovely aspect and a sign of success of the open collaboration idea, isn’t it? 1015.1 Gil, I feel slightly awkward about it, and won’t feel entirely happy until it’s written up in the conventional way. But if one regards a write-up as having a tree structure, then it’s all there on the wiki down to the small sticks, and only a few leaves are missing. Somehow there don’t seem to be any points where one says “I don’t quite see it but I’m sure it works.” It’s more like “I do see it but it’s slightly tedious to write out in full.” Despite that, I wouldn’t have written what I wrote if it had not been for the fact that other participants seem to share my confidence. But for now I will keep the “(probably)”."},{"username":"gowers","timestamp":"2009-03-11T23:49:00.000Z","contents":"1015.1 Gil, I feel slightly awkward about it, and won’t feel entirely happy until it’s written up in the conventional way. But if one regards a write-up as having a tree structure, then it’s all there on the wiki down to the small sticks, and only a few leaves are missing. Somehow there don’t seem to be any points where one says “I don’t quite see it but I’m sure it works.” It’s more like “I do see it but it’s slightly tedious to write out in full.” Despite that, I wouldn’t have written what I wrote if it had not been for the fact that other participants seem to share my confidence. But for now I will keep the “(probably)”."},{"username":"gowersjozsefjozsefgowersjozsefgowers","timestamp":"2009-03-12T00:12:00.000Z","contents":"Shelah Jozsef, I’m still trying to follow your argument in 1011\\. Let me try to be completely precise about where it seems to me to go wrong. Of course, it could be me who is going wrong. Consider the following example. I take a random subset S of %{}[k]^{r_m}% and then take the set %\\mathcal{A}% of all sequences in %{}[k]^{r_1+\\dots+r_m}% such that the final part lies in S. This set has density 1/2\\. Now I apply your step 1, and it happens that the %d_1%-dimensional subspace I choose has the following property: every point with all running coordinates less than k belong to S, and all other points belong to the complement of S. (With high probability such a subspace exists, and no steps have been taken to avoid it.) Then the pigeohole argument gives you that all the points with running coordinates less than k can be joined to anything you like in %{}[k]^{r_1+\\dots+r_{m-1}}.% However, if you take any line given by the application of Sperner, then both its top two points must involve a k and give the empty set. Therefore, the whole line gives the empty set (meaning that if your mth part belongs to the line then you don’t belong to %\\mathcal{A}%). So at the second stage of the iteration I don’t see how you can do Step 1 (because of the “with any %{}P_i%” requirement). Tim, what you wrote is perfectly right with the exeption that in step one I never consider running coordinates larger than k-1\\. (with my old notation “larger than k-2”) So, in your example fliptop means that the neighbours given by the last (m-th) k, k-1 pair are simultaneously not in S. I’m still not sure that the argument works, but I don’t see any serious problem yet. I have office hours now, so I will beging to write up the argument and if I won’t have too many students to show up then I will post the sketch in an hour or so. Just to try to be more explicit still, let’s imagine that %k=3% and %r_m=6%. Suppose that all sequences of six 0s and 1s are the tail of every preceding sequence, and that the two points %001122% and %001222% are the tails of precisely the same set of sequences, so you choose as your line the line %001*22%. It seems to me that there is no reason for any point in that line to be the tail of any previous sequence. A separate question: is the aim to construct in each %{}[k]^{r_i}% a line %L_i% in such a way that the product of the lines is fliptop (that is, insensitive to changing 1s to 2s when k=3), and %\\mathcal{A}% is dense in that product? My difficulty is that I don’t see the relevance of a statement about %\\{0,1\\}^{r_m}% when the line %L_m% might have 2 as one of its fixed coordinates. I see now. One should be more careful with the selection of the fliptop lines. I still think that it is duable, but it seems that we have to consider more subspaces. I don’t have much time, but I will think about it. I should stress here that I definitely think this is worth attempting. Even if it turns out not to work 100% straightforwardly, the following question exists and is clearly an interesting one. You have a set %\\mathcal{A}\\subset[k]^n% of density %\\delta%. How large a subspace %S% can you find such that %\\mathcal{A}% has density at least %\\delta/2% in the subspace, and membership of %\\mathcal{A}% in the subspace does not change if you flip a %k% to a %k-1%? I do not see any philosophical reason for that being significantly harder than Sperner plus induction, though such a reason might emerge I suppose."},{"username":"jozsef","timestamp":"2009-03-12T00:52:00.000Z","contents":"Tim, what you wrote is perfectly right with the exeption that in step one I never consider running coordinates larger than k-1\\. (with my old notation “larger than k-2”) So, in your example fliptop means that the neighbours given by the last (m-th) k, k-1 pair are simultaneously not in S. I’m still not sure that the argument works, but I don’t see any serious problem yet."},{"username":"jozsef","timestamp":"2009-03-12T02:40:00.000Z","contents":"I have office hours now, so I will beging to write up the argument and if I won’t have too many students to show up then I will post the sketch in an hour or so."},{"username":"gowers","timestamp":"2009-03-12T02:49:00.000Z","contents":"Just to try to be more explicit still, let’s imagine that %k=3% and %r_m=6%. Suppose that all sequences of six 0s and 1s are the tail of every preceding sequence, and that the two points %001122% and %001222% are the tails of precisely the same set of sequences, so you choose as your line the line %001*22%. It seems to me that there is no reason for any point in that line to be the tail of any previous sequence. A separate question: is the aim to construct in each %{}[k]^{r_i}% a line %L_i% in such a way that the product of the lines is fliptop (that is, insensitive to changing 1s to 2s when k=3), and %\\mathcal{A}% is dense in that product? My difficulty is that I don’t see the relevance of a statement about %\\{0,1\\}^{r_m}% when the line %L_m% might have 2 as one of its fixed coordinates."},{"username":"jozsef","timestamp":"2009-03-12T03:42:00.000Z","contents":"I see now. One should be more careful with the selection of the fliptop lines. I still think that it is duable, but it seems that we have to consider more subspaces. I don’t have much time, but I will think about it."},{"username":"gowers","timestamp":"2009-03-12T05:36:00.000Z","contents":"I should stress here that I definitely think this is worth attempting. Even if it turns out not to work 100% straightforwardly, the following question exists and is clearly an interesting one. You have a set %\\mathcal{A}\\subset[k]^n% of density %\\delta%. How large a subspace %S% can you find such that %\\mathcal{A}% has density at least %\\delta/2% in the subspace, and membership of %\\mathcal{A}% in the subspace does not change if you flip a %k% to a %k-1%? I do not see any philosophical reason for that being significantly harder than Sperner plus induction, though such a reason might emerge I suppose."},{"username":"ryan-odonnellgowers","timestamp":"2009-03-12T02:43:00.000Z","contents":"Varnavides-DHJ(k). In 1010.1, Tim wrote _“What I’m not doing is fixing almost all coordinates and taking M small wildcard sets, or something like that. Instead, I’m randomly choosing the sizes of the M wildcard sets (which I allow to add up to 1, so there are in fact no fixed coordinates at all) and then uniformly at random choosing a line from the resulting subspace.”_ Tim, I think I get it now; as usual I was misinterpreting the word “subspace” and was thinking you were doing what you said you weren’t doing <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> I agree now that the sketch looks relatively solid. Let me see if I can fill in some “leaves”, as you say. Great — I’m off for a couple of hours and won’t be able to do much more today, if anything."},{"username":"gowers","timestamp":"2009-03-12T02:50:00.000Z","contents":"Great — I’m off for a couple of hours and won’t be able to do much more today, if anything."},{"username":"ryan-odonnellryan-odonnellryan-odonnell","timestamp":"2009-03-12T03:40:00.000Z","contents":"Is Equal-Slices of Equal-Slices equal to Equal-Slices? Suppose we want to try to prove Varnavides-DHJ(3); as in #1010, let’s try to show that if %A \\subseteq [3]^n% has equal-slices density %\\delta > 0%, and we choose a random combinatorial line by drawing its template from equal-slices on %([3] \\cup \\{\\ast\\})^n%, then with probability at least $\\eps(\\delta) > 0$ the whole line is in %A% (assuming %n% is sufficiently large). Tim’s strategy for this involves, among other things, picking a combinatorial subspace of dimension %M% with _no_ free coordinates, according to some distribution %\\nu%; then drawing a point from that subspace (thought of as %[k]^M%) from some other distribution, %\\sigma%. It would be particularly helpful if the overall distribution is equal-slices on %[3]^n%. Also helpful would be if $lambda \\nu$ is a relatively “natural” distribution; also %\\sigma% should have full support. (It’s also not essential that %\\nu% generates subspaces with _exactly_ %M% dimensions, but never mind that for now.) Here is perhaps the simplest possible proposal: Let %\\nu% be equal-slices on %[M]^n%, and let %\\sigma% be equal-slices on %[k]^M%. The question is: might combining these two actually give equal-slices on %[3]^n% _precisely_?! I will submit some evidence in a reply to this post. 1018.1\\. Evidence. Well, my first piece of “evidence” is that in these simple probability matters, sometimes things just turn out nicely <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> Here is slightly more rigorous evidence… (I would try to simply give a proof, but I haven’t the time to work on it this evening, so I thought I’d quickly throw it out there.) For my poor brain, let me fix %M = 100% and fix %k = 3%, %[k] = \\{R, G, B\\}% (red, green, blue). We can think of the “outer” equal slices as choosing %p_1 + \\cdots + p_{100} = 1% uniformly, then drawing from the resulting product distribution on %\\{\\ast_1, \\dots, \\ast_{100}\\}^n%. We can think of the “inner” equal slices as choosing %q_R + q_G + q_B = 1% uniformly, then changing each %\\ast_i% to %R% with probability %q_R%, %G% with probability %G%, and %B% with probability %q_B%. If you think about it a minute, you see that the final is string is drawn from a _randomly chosen product distribution_ %s_R +s_G + s_B = 1% on %\\{R,G,B\\}^n%. That’s good! If we could just show that that %(s_R, s_G, s_B)% is indeed distributed as “uniform on the %3%-simplex” we’d be done. So how _is_ it chosen? Well, we think of %(q_R, q_G, q_R)% as being chosen first. Then, think of %p_1, \\dots, p_{100}% as being independent Exponential%(1)%‘s. (Of course, this does not give %100% numbers adding to %1%, but we use these numbers as proportions, rather than probabilities.) Then %(s_R, s_G, s_B)% gets generated by putting each %p_i% into the “R” bucket with probability %q_R%, into the “G” bucket with probability %q_G%, and into the “B” bucket with probability %q_B%. Finally, you add up all the Exponentials in each bucket, and %(s_R, s_G, s_B)% is proportional to these totals. So what is the distribution on the three bucket totals? It shouldn’t be too hard to really work it out, but let me write heuristically here. Once %(q_R, q_G, q_B)% is fixed, we expect about %100q_R% of the independent Exponentials to go into the R bucket, and similarly for G and B. (Even moreso do we expect this if %100 = M% is huge.) And thinking of %100q_R% as still huge, we’re adding up a huge number of i.i.d. Exponentials, so the CLT will imply that the sum will almost surely be very close to %100q_R%. Similarly, the sum in the G bucket will almost surely be very close to %100q_G%, and again for B. But in this case, the proportions will be really close to %q_R/q_G/q_B% I.e., almost surely it seems %(s_R, s_G, s_B)% will be very nearly %(q_R, q_G, q_B)%. But %(q_R, q_G, q_B)% is distributed uniformly on the %3%-simplex, as desired! 1018.2\\. I don’t think it’s exactly correct, but I still hope some variant is…"},{"username":"ryan-odonnell","timestamp":"2009-03-12T03:53:00.000Z","contents":"1018.1\\. Evidence. Well, my first piece of “evidence” is that in these simple probability matters, sometimes things just turn out nicely <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> Here is slightly more rigorous evidence… (I would try to simply give a proof, but I haven’t the time to work on it this evening, so I thought I’d quickly throw it out there.) For my poor brain, let me fix %M = 100% and fix %k = 3%, %[k] = \\{R, G, B\\}% (red, green, blue). We can think of the “outer” equal slices as choosing %p_1 + \\cdots + p_{100} = 1% uniformly, then drawing from the resulting product distribution on %\\{\\ast_1, \\dots, \\ast_{100}\\}^n%. We can think of the “inner” equal slices as choosing %q_R + q_G + q_B = 1% uniformly, then changing each %\\ast_i% to %R% with probability %q_R%, %G% with probability %G%, and %B% with probability %q_B%. If you think about it a minute, you see that the final is string is drawn from a _randomly chosen product distribution_ %s_R +s_G + s_B = 1% on %\\{R,G,B\\}^n%. That’s good! If we could just show that that %(s_R, s_G, s_B)% is indeed distributed as “uniform on the %3%-simplex” we’d be done. So how _is_ it chosen? Well, we think of %(q_R, q_G, q_R)% as being chosen first. Then, think of %p_1, \\dots, p_{100}% as being independent Exponential%(1)%‘s. (Of course, this does not give %100% numbers adding to %1%, but we use these numbers as proportions, rather than probabilities.) Then %(s_R, s_G, s_B)% gets generated by putting each %p_i% into the “R” bucket with probability %q_R%, into the “G” bucket with probability %q_G%, and into the “B” bucket with probability %q_B%. Finally, you add up all the Exponentials in each bucket, and %(s_R, s_G, s_B)% is proportional to these totals. So what is the distribution on the three bucket totals? It shouldn’t be too hard to really work it out, but let me write heuristically here. Once %(q_R, q_G, q_B)% is fixed, we expect about %100q_R% of the independent Exponentials to go into the R bucket, and similarly for G and B. (Even moreso do we expect this if %100 = M% is huge.) And thinking of %100q_R% as still huge, we’re adding up a huge number of i.i.d. Exponentials, so the CLT will imply that the sum will almost surely be very close to %100q_R%. Similarly, the sum in the G bucket will almost surely be very close to %100q_G%, and again for B. But in this case, the proportions will be really close to %q_R/q_G/q_B% I.e., almost surely it seems %(s_R, s_G, s_B)% will be very nearly %(q_R, q_G, q_B)%. But %(q_R, q_G, q_B)% is distributed uniformly on the %3%-simplex, as desired!"},{"username":"ryan-odonnell","timestamp":"2009-03-12T10:00:00.000Z","contents":"1018.2\\. I don’t think it’s exactly correct, but I still hope some variant is…"},{"username":"ryan-odonnell","timestamp":"2009-03-12T10:29:00.000Z","contents":"ENDS of ENDS = ENDS. I think I’ve got it now. Define a distribution on %k%-partitions of %n% called %k%-Equal Non-Degenerate Slices (%k%-ENDS): To get it, take %n% dots in a line, consider the %n-1% gaps between them, and place %k-1% bars in these gaps, uniformly from among the %\\binom{n-1}{k-1}% many possibilities (i.e., “without replacement”). Then the intra-bar blocks of dots correspond to %k% _positive_ integers %m_1, \\dots, m_k% which add up to %n%. Associating block %m_i% to character %i \\in [k]%, we get a distribution on “Non-Degenerate” slices (each character appears at least once). We can extend this to a distribution on %[k]^n% as usual, just by randomly permuting the string. This is actually a kind of convenient variant on equal-slices, because when you’re using equal-slices to pick combinatorial lines, it’s a bit annoying to have to worry about the unlikely case of getting %0% wildcards. Anyway, I think it’s now easy to see that if you do %M%-ENDS, and then you do %k%-ENDS on that (as described in #1018), then you just get %k%-ENDS, exactly. Because all that’s really happening is that you’re picking %M-1% bar-spots uniformly, and then you’re picking %k-1% of those %M-1% bars to be the “final bars”, uniformly. So that’s just the same as picking %k-1% bars uniformly to start with. I think. Once again, it’s late."},{"username":"ryan-odonnell","timestamp":"2009-03-12T10:30:00.000Z","contents":"1019.1\\. “Formula does not parse” above is “(n-1 choose k-1)”. _[Fixed]_"},{"username":"ryan-odonnell","timestamp":"2009-03-12T11:13:00.000Z","contents":"1019.2\\. Some googling reveals [this paper](http://arxiv.org/PS_cache/arxiv/pdf/0901/0901.4444v1.pdf) (among others) which discusses in its Section 2 the proper nomenclature for this stuff, as well as related probability distributions. k-ENDS -> “random composition into k parts”; the consequent distribution on strings is “random ordered partition into k parts”. dots -> “balls”  \nbars -> “walls”"},{"username":"gowers","timestamp":"2009-03-12T14:21:00.000Z","contents":"DHJ(k) Ryan, that all looks like good news — it would be very nice if we could get the technicalities to run smoothly so that they don’t obscure the main ideas, and your comments above give me hope that it will be possible. I’ve got 25 minutes to spare so I’m going to try to explain why I think that DHJ(k) is basically done too. I’m assuming for this that the proof that a line-free set correlates locally with a set of complexity k-2 is going to work out now, which seems likely, as it’s been sketched down to a pretty fine level of detail and you’ve scrutinized the parts that looked most likely to go wrong if anything was going to. In order to explain how I think the rest of the proof will go, what I need to do is explain the difference between the proof for DHJ(k) and [the proof of DHJ(3) outlined on the wiki here](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument). Analogue of Step 1\\. We need to replace 1-sets by what I call %\\{1,2,\\dots,k-2\\}%-sets. These are sets $\\latex \\mathcal{B}$ such that %x\\in\\mathcal{B}% if and only if %(U_1(x),\\dots,U_{k-2}(x))\\in\\mathcal{Z}% for some suitable collection %\\mathcal{Z}% of disjoint (k-2)-tuples of sets and %U_i(x)% stands for the set of places where x takes the value i. To save writing, I’ll call these _simple_ sets. So the next obvious aim is to prove that a simple set can be approximately partitioned into subspaces. Once that’s done, I think it is fair to say that the rest of the proof is exactly as in the k=3 case. The one other thing that needs to be changed in Step 1 is that the appeal to the multidimensional Sperner theorem has to be replaced by an appeal to multidimensional DHJ(k-1), [a proof of which is sketched on the wiki here](http://michaelnielsen.org/polymath1/index.php?title=DHJ%28k%29_implies_multidimensional_DHJ%28k%29). Otherwise, the changes are trivial: 3 becomes k, and %\\mathcal{U}\\otimes[2]^n% becomes our simple set %\\mathcal{B}.% (I admit that I haven’t been through Step 1 line by line checking what I’m saying — that would use up more than my 25 minutes.) I’ve looked through Step 2 and I think all the changes needed are of the trivial variety (changing 3s to ks and that kind of thing). It might be nice at some point to write an abstract version of this argument, since what we’re using seems to be very general — that we can partition %{}[k]^n% into subspaces very conveniently, that the structure of good sets is preserved when we do so, that dense good sets contain lots of subspaces — that kind of thing. Step 3\\. And now we do the same trick as we did in DHJ(3) except that we have to apply Step 2 k-1 times instead of twice. The end result is that a set of complexity k-2 can be almost partitioned into subspaces, and that’s what we’re looking for."},{"username":"anonymousryan-odonnell","timestamp":"2009-03-12T20:46:00.000Z","contents":"Varnavides. Hi Tim, I think the %k%-ary proof that line-free sets correlate with simple sets is now done to about 100% satisfaction in my mind. I’ll try to wikify some details later, but to illustrate the simplicity with which it’s working out, here is a complete proof of Varnavides: Assume we have DHJ(k). Technicality: I want to say that a combinatorial line is (also) “degenerate” if the fixed coordinates do not use each character from %[k]% at least once. Deducing DHJ(k) with this very slightly stronger notion of degeneracy is trivial. Proof of Varnavides: Assume %A \\subseteq [k]^n% has density %\\delta > 0% under the “uniform %k%-composition” distribution (i.e., equal-slices conditioned on the slice having at least one of each character). Let %M = M(\\delta/2)% denote the least integer such that DHJ(k) guarantees the existence of a nondegenerate line in any subset of %[k]^M% with uniform %k%-composition density at least %\\delta/2%. Think of choosing %x \\in [k]^n% as first drawing a random %M%-dimensional subspace %V% from the uniform %M%-composition distribution, and then drawing %x% randomly from %V \\cong [k]^M% according to the uniform %k%-composition distribution. Since %x \\in A% with probability at least %\\delta%, we know that with probability at least %\\delta/2% over the choice of %V%, the %k%-uniform composition density of %A|_V% is at least %\\delta/2%. Call such %V%‘s “good”. By DHJ(k) we know that for any good %V%, there is a (nondegenerate) line inside %A|_V%. Now suppose we choose a random line %L% in $[k]^n$ according to the uniform %(k+1)%-composition distribution. We claim that it is entirely within %A% with probability at least %\\delta/[2 M^{k/2} (k+1)^M]% . To see this, again think of %L% as being drawn by first choosing %V%, and then drawing a random line in %V% according to uniform %(k+1)%-composition. With probability at least %\\delta/2% %V% is good, in which case there is at least one nondegenerate line in %A|_V%. The probability %L% is picked to be it is at least %1/[M^{k/2} (k+1)^M]%, since the least probability under uniform %(k+1)%-composition of %M% is at least this quantity (er, or something like that). 1021.1\\. That was me again, not logged in. I wanted to add that we can complete the whole “line-free implies correlation with simple set” proof exclusively in this uniform %k%-composition measure rather easily, I think. The only thing to really check is that if you choose a %1 - \\epsilon% fraction of the coordinates %S% randomly, do uniform-%k%-composition on %S%, and then independently do uniform-%k%-composition OR uniform-%(k-1)%-composition on %\\overline{S}%, then the overall probability distribution has total variation distance at most %O(\\epsilon \\sqrt{n})% from the usual global uniform %k%-composition distribution. Which is, I’m pretty sure, true."},{"username":"ryan-odonnell","timestamp":"2009-03-12T20:51:00.000Z","contents":"1021.1\\. That was me again, not logged in. I wanted to add that we can complete the whole “line-free implies correlation with simple set” proof exclusively in this uniform %k%-composition measure rather easily, I think. The only thing to really check is that if you choose a %1 - \\epsilon% fraction of the coordinates %S% randomly, do uniform-%k%-composition on %S%, and then independently do uniform-%k%-composition OR uniform-%(k-1)%-composition on %\\overline{S}%, then the overall probability distribution has total variation distance at most %O(\\epsilon \\sqrt{n})% from the usual global uniform %k%-composition distribution. Which is, I’m pretty sure, true."},{"username":"gowers","timestamp":"2009-03-13T02:58:00.000Z","contents":"Shelah While I wait for reaction about how we should go ahead with writing things up (see [this comment](https://gowers.wordpress.com/2009/03/10/polymath1-and-open-collaborative-mathematics/#comment-2814)), I want to continue thinking a bit about whether the proof we have could be Shelah-ized. Terry said in a comment some time ago that he is a big fan of translating things back and forth between combinatorial and ergodic languages. I (in common with just about every mathematician) am a big fan of “completing the square,” by which I mean completing puzzles of the form “A is to B as C is to ??” For example, “corner is to combinatorial line as large grid is to ??” the answer to which turned out, not completely expectedly, to be “%{}[2]^k% for some large %k%“. And that was just one part of a bigger such puzzle: “the Ajtai-Szemerédi proof of the corners theorem is to the triviality that a dense subset of %{}[n]% contains two distinct points as ?? is to the proof of Sperner’s theorem.” So here I would like to think about the puzzle, “The colour-focusing proof of the Hales-Jewett theorem is to Shelah’s proof of the Hales-Jewett theorem as the proof we now have of DHJ(k) is to ??” The reason I am drawn to this is that I once lectured both proofs of the Hales-Jewett theorem and came to a very clear understanding of the relationship between them, which I’ll have to reconstruct, but I remember it as being natural enough that it will be easy to reconstruct. When I have a spare moment, I’ll write something about this on the wiki, but for now let me just say that Shelah’s basic idea can be interpreted as follows: whereas the old proof of HJ(k) used HJ(k-1) to reduce the problem to the almost trivial HJ(2), Shelah’s new proof used HJ(2) to reduce the problem to HJ(k-1). As I say, I’ll justify that at some point, but here I just want to take that thought and have a stab at applying it to DHJ(k) instead. Of course, I’m not sure that it’s possible to do so, but just having the goal in mind does, I think, suggest some questions. For instance, here’s a place in the proof where DHJ(k-1) comes into the proof of DHJ(k). One first passes to a subspace, which we’ll notate as though it is the whole space, such that the density of %\\mathcal{A}% is almost %\\delta% and the density of %\\mathcal{A}% in %{}[k-1]^n% is still positive. Writing %\\mathcal{B}% for %\\mathcal{A}\\cap[k-1]^n%, we use %\\mathcal{B}% to construct a dense set %\\mathcal{C}% of complexity %k-2% that is disjoint from %\\mathcal{A}%. It doesn’t matter too much what all this means: the main point is that once we have a set of complexity %k-2% we are able to hit it with DHJ(k-1). So a natural question, if we are trying to Shelah-ize, is whether we can construct some set of complexity 1 (which means it’s hittable with DHJ(2), aka Sperner) and thereby, somehow, create a fliptop subspace inside which %\\mathcal{A}% is dense. Recall that a fliptop subspace is one inside which if you change any coordinate with value k-1 to a k, then you don’t change whether the point x belongs to the set %\\mathcal{A}%. So if your set is dense in such a subspace (or rather, dense in the %{}[k-1]^n% part of the subspace) then you can hit it with DHJ(k-1) and finish off the proof immediately. Because DHJ(k-1) is used just as the inductive step and not as some part of an iterative procedure for getting to the inductive step, a proof of this kind should be far better quantitatively. In my next comment I shall think about the following question: are there any complexity-1 sets that can be derived from a set %\\mathcal{A}%? For now, I don’t care whether we can actually use them."},{"username":"gowersgowers","timestamp":"2009-03-13T03:40:00.000Z","contents":"Shelah. Continuing on from 1022, a completely simple-minded idea would be to pass to a subspace such that %\\mathcal{A}% is dense in %{}[2]^n% and still had density almost %\\delta%. (I’m just going to assume that that’s fine.) If we write %\\mathcal{B}% for %\\mathcal{A}\\cap[2]^n,% then what set %\\mathcal{C}% can we construct from %\\mathcal{B}%? In the other argument, we looked at all sequences %x% such that, for every %j%, if you change all %k%s to %j%s then you get a sequence in %\\mathcal{B}%. Here, it is clear, we are going to have to do a much more radical collapsing of the sequence. One possibility, which may be nonsense, would be to convert all non-1 coordinates into 2s. In other words, we let %\\mathcal{C}% be the set of all x such that the 1-set of x is equal to the 1-set of some sequence in %\\mathcal{B}%. In the other argument, we then argued that %\\mathcal{A}% is disjoint from %\\mathcal{C}% if it does not contain a combinatorial line. What happens here? Well, bearing in mind that (using the Shelah proof as a guide) we are not trying to go straight for a combinatorial line, but rather for a fliptop subspace, we should perhaps assume as our hypothesis that %\\mathcal{A}% does not contain such a subspace. Turning that round, we would want to prove that if %\\mathcal{A}% intersects %\\mathcal{C}% then it contains a fliptop subspace. (This doesn’t sound true, but let’s see what’s wrong with it first.) For %\\mathcal{A}% to intersect %\\mathcal{C}% we need a sequence %x\\in\\mathcal{A}% such that if you convert all its non-1s into 2s, then you get a sequence of 1s and 2s that also belongs to %\\mathcal{A}%. Why should that be interesting? I don’t know. But suppose instead that we managed to find quite a large subspace S of such points. Then if x belonged to %\\mathcal{A}\\cap S% and had a variable coordinate equal to 2, we would … no, I don’t see where this goes. Part of the problem is that I seem to be going for the fliptop subspace to be contained in %\\mathcal{A}%, whereas what I should be aiming for is merely that %\\mathcal{A}% is _dense_ in the fliptop subspace (which actually seems to be a flipbottom subspace). I’m going to stop this comment here — I hope I’ve demonstrated the flavour of the argument that I think might conceivably exist. 1023.1 I should have added the remark that the set %\\mathcal{C}% has complexity 1."},{"username":"gowers","timestamp":"2009-03-13T03:41:00.000Z","contents":"1023.1 I should have added the remark that the set %\\mathcal{C}% has complexity 1."},{"username":"terence-taoterence-tao","timestamp":"2009-03-13T03:45:00.000Z","contents":"Austin’s proof I’ve written down an informal (and sketchy) combinatorial translation of the k=3 case of Austin’s argument at [http://michaelnielsen.org/polymath1/index.php?title=Austin%27s_proof](http://michaelnielsen.org/polymath1/index.php?title=Austin%27s_proof) It’s actually very much in the style of the proof of the triangle-removal lemma, especially if one takes an analytic perspective rather than a combinatorial one and talks about decomposing functions rather than regularising graphs. In particular, it’s best to avoid thinking in the classical language of vertices and take a more “point-less” approach in the spirit of probability theory or ergodic theory (or operator algebras, for that matter), thus for instance one should think about averages of products of functions rather than counting triangles etc. One reason for shifting one’s thinking this way is that it allows one to think about strong stationarity in a civilised way, whereas this concept becomes really difficult to work with if one insists on holding on to concrete vertices and edges. (In classical combinatorial language, being strongly stationary is like saying that a certain graph is a “statistically equivalent” to the subgraph formed by restricting of its vertex set from a big space such as [3]^n to a randomly chosen subspace; but defining what “statistically equivalent” means precisely is sort of a pain unless one decides to give up on vertices altogether.) It’s also clear that this proof will end up being messier than the pure density-increment proof (though cleaner than a finitisation of the Furstenberg-Katznelson proof), and give worse bounds, in large part because one has to apply Graham-Rothschild (with parameters given in turn from a Szemeredi-type regularity lemma!) to get the initial strong stationarity properties. It looks possible to use density increment arguments as a substitute for Graham-Rothschild, but this is still going to have quite lousy bounds. 1023.1 Information-theoretic approach Hmm, it occurs to me now that the information-theoretic approach to the regularity lemma that I laid out in [http://front.math.ucdavis.edu/math.CO/0504472](http://front.math.ucdavis.edu/math.CO/0504472) (where I “regularise” random variables using Shannon entropy) is actually well suited to this problem, and may lead to a relatively slick (albeit alien-looking) proof. What I may do at some point is begin by showing how by playing around with conditional Shannon entropies of various random variables, one can establish the triangle removal lemma, and I believe that this proof may extend relatively painlessly to the case under consideration. More on this later…"},{"username":"terence-tao","timestamp":"2009-03-13T03:55:00.000Z","contents":"1023.1 Information-theoretic approach Hmm, it occurs to me now that the information-theoretic approach to the regularity lemma that I laid out in [http://front.math.ucdavis.edu/math.CO/0504472](http://front.math.ucdavis.edu/math.CO/0504472) (where I “regularise” random variables using Shannon entropy) is actually well suited to this problem, and may lead to a relatively slick (albeit alien-looking) proof. What I may do at some point is begin by showing how by playing around with conditional Shannon entropies of various random variables, one can establish the triangle removal lemma, and I believe that this proof may extend relatively painlessly to the case under consideration. More on this later…"},{"username":"ryan-odonnellgowers","timestamp":"2009-03-13T03:47:00.000Z","contents":"Always the uniform-ordered-partition distribution. Following up on my comment #1006, I feel somewhat sure that we can work _entirely_ within the uniform-ordered-partition distribution (i.e., “equal-nondegenerate-slices”). I’m going to try to write it down now to be sure, but if it’s correct, it should mean that “DHJ(k)=>Varnavides”, “DHJ(k)=>multidimensional-DHJ(k)”, and hopefully also “line-free sets correlate with simple sets” can all be proved with simple arguments. More precisely, they will be the _same_ simple arguments we have now, but with no annoyances due to all the tedious passing back and forth between various distributions via subspaces. 1025.1 Wow — I very much hope you’re right about this. I suppose the bit that would be hardest (but not obviously impossible) would be “simple sets can be almost partitioned into subspaces”. In the uniform measure you are then done, but in other measures you can’t instantly argue that a set of density %\\alpha% inside the simple set must have density almost %\\alpha% on one of the subspaces. But that could well be just another little task to add to your collection. Now the idea would be to prove that the measure on a simple set can be written as a positive linear combination of the measures on subspaces. If that too can be done cleanly in the UOPD/ENS then we are in fantastic shape."},{"username":"gowers","timestamp":"2009-03-13T03:54:00.000Z","contents":"1025.1 Wow — I very much hope you’re right about this. I suppose the bit that would be hardest (but not obviously impossible) would be “simple sets can be almost partitioned into subspaces”. In the uniform measure you are then done, but in other measures you can’t instantly argue that a set of density %\\alpha% inside the simple set must have density almost %\\alpha% on one of the subspaces. But that could well be just another little task to add to your collection. Now the idea would be to prove that the measure on a simple set can be written as a positive linear combination of the measures on subspaces. If that too can be done cleanly in the UOPD/ENS then we are in fantastic shape."},{"username":"gowersjozsefgowersjozsefjozsef","timestamp":"2009-03-13T04:26:00.000Z","contents":"Does triangle removal imply DHJ(3)? A wild thought this one, especially as it’s close to an idea I had several years ago and couldn’t get to work. But with the benefit of a bad memory (something that is very helpful in mathematics, in my view) I now can’t see what is wrong with it. The idea is sort of ridiculous. Let’s pass to a subspace in which, notating it as %{}[3]^n%, %\\mathcal{A}% has average density at least %\\alpha% in the slices up to %m%, where %m% is much smaller than %n%. Sorry — by that I mean slices where the 1-set and 2-set are of size at most %m%. The hope now is that the disjointness graph becomes dense (because two sets of size %m% are almost always disjoint), so we can just apply the usual triangle-removal lemma. My one reason for not completely dismissing this idea is that when I tried it all those years ago I did not have the concept of equal-slices measure to play with. Unfortunately, it doesn’t become dense. Only the UV-part of it is dense (because it is just the 1-set and the 2-set that I am assuming to be small). Let me ask a slightly different question, which probably has a fairly easy answer, and that answer will probably kill off this little idea. Given %n% points in %{}[3]^N% (where %N% can be as big as you like), how many combinatorial lines can they contain? Well, if you don’t care about the dimension, then the number of lines might be %cn^2% Just take 0-1 sequences as i zeros followed by n-i ones (for every i) and 0-2 sequences where the zeros are followed by two-s. For any pair of such 0-1 sequences there is a third 0-2, giving a line. So this number itself won’t show that your plan won’t work. I don’t understand — large should mean %cn^3% surely? Or perhaps %Cn^2% but with the three bipartite graphs dense. Probably I don’t understand the question. Isn’t it so that two points define the line uniquely. In any case, my example won’t work. As it happened before, I didn’t read your post carefully. It is an extremely busy period for me; heavy teaching, committees, etc. So, I’m just writing those “half baked remarks” as Boris would say. I will try to refrain from that in the future."},{"username":"jozsef","timestamp":"2009-03-13T05:07:00.000Z","contents":"Well, if you don’t care about the dimension, then the number of lines might be %cn^2% Just take 0-1 sequences as i zeros followed by n-i ones (for every i) and 0-2 sequences where the zeros are followed by two-s. For any pair of such 0-1 sequences there is a third 0-2, giving a line. So this number itself won’t show that your plan won’t work."},{"username":"gowers","timestamp":"2009-03-13T05:17:00.000Z","contents":"I don’t understand — large should mean %cn^3% surely? Or perhaps %Cn^2% but with the three bipartite graphs dense."},{"username":"jozsef","timestamp":"2009-03-13T06:10:00.000Z","contents":"Probably I don’t understand the question. Isn’t it so that two points define the line uniquely. In any case, my example won’t work."},{"username":"jozsef","timestamp":"2009-03-13T07:29:00.000Z","contents":"As it happened before, I didn’t read your post carefully. It is an extremely busy period for me; heavy teaching, committees, etc. So, I’m just writing those “half baked remarks” as Boris would say. I will try to refrain from that in the future."},{"username":"ryan-odonnellryan-odonnell","timestamp":"2009-03-13T06:24:00.000Z","contents":"A single distribution? In preparation for trying to justify what I wrote in 1025, I have written [this document](http://www.cs.cmu.edu/~odonnell/ordered-partitions.pdf) explaining the distribution. I’ll wikify it if and only if this scheme works out and proves helpful <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> 1027.1 I wrote some more [here](http://www.cs.cmu.edu/~odonnell/more-ordered-partitions.pdf) but I’ve gotten too sleepy to finish tonight. I’ll try to finish tomorrow and explain it properly in a post, but I have to go out of town tomorrow too…"},{"username":"ryan-odonnell","timestamp":"2009-03-13T11:22:00.000Z","contents":"1027.1 I wrote some more [here](http://www.cs.cmu.edu/~odonnell/more-ordered-partitions.pdf) but I’ve gotten too sleepy to finish tonight. I’ll try to finish tomorrow and explain it properly in a post, but I have to go out of town tomorrow too…"},{"username":"klas-markstrom","timestamp":"2009-03-13T13:30:00.000Z","contents":"1028.  \nI will make my first visible visit on this side of the project by asking one of the stupid questions discussed in the metathread. Regarding the question from 1026\\. As far as I understand any pair of points specify a unique line. if that is the case n points can specify at most Cn^2 lines. Now if we have a set of points and a set of lines where every pair of points specify a unique line we have the beginnings of a projective plane, but a projective plane has the same number of points and lines. So is there another known nice family of combinatorial objects which instead maximises the gap between the number of points and lines?"},{"username":"gowers","timestamp":"2009-03-13T13:59:00.000Z","contents":"1029 Klas, your “stupid” question has exposed the stupidity (without inverted commas) of mine — it’s not what I meant to ask. Let me have another go. The way we prove Sperner using equal-slices measure can be thought of like this. We partition the measure on %{}[2]^n% into measures where the set of combinatorial lines is dense. Each measure is obtained by taking a random permutation of %{}[n]% and taking all intervals in that permuted set, and then any two points with non-zero measure define a combinatorial line. So a vague version of what I am trying to ask is whether we can somehow find dense structures inside %{}[3]^n% and play a similar game, this time exploiting the corners theorem. However, I don’t have a sensible precise version of this question, which perhaps suggests that the answer is no."},{"username":"gowers","timestamp":"2009-03-13T16:47:00.000Z","contents":"Shelah Another idea. Instead of struggling to preserve the density of %\\mathcal{A}% by constructing just one fliptop subspace, perhaps we can partition the whole of (or almost the whole of) %{}[k]^n% into fliptop subspaces. Then in at least one of them %\\mathcal{A}% would have density almost %\\delta% and we would be done by induction. Actually, that’s not quite right unless we’re rather careful, because in order to apply induction we need %\\mathcal{A}% to be dense in the %{}[k-1]^m% part of the fliptop subspace. Let me not worry about this for the time being. Instead, I’d like to think about whether some kind of argument like the one we’ve used for partitioning 1-sets into subspaces could do the job here. I can’t see it in advance so I’ll just try to plunge in. Let m be a medium-sized integer — much smaller than n, but much larger than 1 — and write %{}[k]^n% as %{}[k]^{n-m}\\times[k]^m.% By the pigeonhole principle we can find a subset %\\mathcal{A}_1% of %{}[k]^{n-m}% of density %k^{-m}% such that for every %x\\in\\mathcal{A}_1% the set of %y\\in[k]^m% such that %(x,y)\\in\\mathcal{A}% is the same. Let that set be %\\mathcal{B}_1%, and find a big subspace %S_1% in %{}[k]^m% that is fliptop with respect to %\\mathcal{B}_1%. Then all the subspaces %\\{x\\}\\times S_1% such that %x\\in\\mathcal{B}_1% are fliptop. (They could be disjoint from %\\mathcal{A}%, but we don’t care about this.) Let us include them in our attempted partition and remove them from %{}[k]^n.% Note that we have removed a positive proportion (depending on %m%) of %{}[k]^n% as a result of this. We now partition %{}[k]^n% according to the part that belongs to %{}[k]^m.% The result is a number of subspaces of the form %S_y=\\{(x,y):x\\in[k]^{n-m}\\}.% If %y\\notin S_1,% then we have not thrown away any of %S_y% and we just repeat the first step. However, if %y\\in S_1,% then things are more complicated, because we have thrown away some of %S_y% and are therefore not allowed to use that part in any new cell that we want to add to the attempted partition. At this point I think it helps to imagine that we’ve somehow managed to deal with this problem for several iterations. What position would we expect to be in then? Well, we’d have restricted to some subspace (of codimension %rm,% where r is the number of iterations completed), and we would want to find a big chunk (which would be a union of fliptop subspaces) to remove. Hmm, it doesn’t seem to work, since by this stage I’m needing to find a big orderly chunk of subspaces inside a set over which I have pretty well no control."},{"username":"ryan-odonnellgowers","timestamp":"2009-03-13T19:58:00.000Z","contents":"New distribution. [This writeup](http://www.cs.cmu.edu/~odonnell/new-ordered-partitions) is practically the same, but I think it sets things up the best. It’s a bit long, but that was mainly just to convince _myself_ what I was saying is correct. The gist is this: The basic objects are length-%m% lists (ordered) of nonempty sets, where the sets’ union is %[n]%. Such lists are in obvious correspondence with the set of “nondegenerate” points in %[m]^n% (where a point is degenerate if it does not include each character at least once). The basic operations are: 1\\. %\\Pi% = randomly permute the list. 2\\. %C% = do a random “coagulation”: pick $i \\in [m-1]$ uniformly, and merge the %m%th set into the %i%th set (shrinking the list length to %m-1%). The basic distribution is: Start with the length-%n% list %\\langle \\{1\\}, \\{2\\}, \\dots, \\{n\\} \\rangle%. If we first apply a %\\Pi% operation, and then apply %n - k% consecutive %C% operations, we get a length-%k% list, which corresponds to a point in %[k]^n%. This is our basic distribution on strings. It is (one can show) the “equal-nondegenerate-slices” distribution. Finally, note that if we only did %n - (k+1)% of the %C% operations, we’d get a string %\\lambda \\in [k+1]^n%, which we think of as %([k] \\cup \\{\\star\\})^n%; i.e., a combinatorial line (template). If we were to further apply the %C% operation to this, we’d get one of the %k% points on this line, uniformly at random. And, note that %C \\lambda% is a string in %[k]^n% distributed according to our basic distribution. —- I’m quite sure that at the very least, Varnavides and Multidimensional DHJ(k) become “trivial” now. The proof of line-free sets in %[k]^n% correlating with simple sets was already pretty simple assuming these two tools, but I’m hoping it also gets a paragraph proof now. Of course, I further hope that this helps simplify the “other half” of the proof, namely the density-increment argument. I’d like to think more about it except I’m going to visit family for a few days and will likely not get the chance. Ryan, the link to your write-up isn’t working. I’m looking forward to seeing it …"},{"username":"gowers","timestamp":"2009-03-14T13:41:00.000Z","contents":"Ryan, the link to your write-up isn’t working. I’m looking forward to seeing it …"},{"username":"gowersjozsefgowersjozsefgowers","timestamp":"2009-03-13T20:08:00.000Z","contents":"Shelah/wikification I’ve now put on the wiki a very slight modification of the usual proof of the (colouring) Hales-Jewett theorem, together with Shelah’s proof. The purpose of the exercise is to show just how similar the two proofs really are: one uses HJ(k-1) to get you down to HJ(2), while the other one uses HJ(2), in pretty well exactly the same way, to get you down to HJ(k-1). If I’d been feeling silly enough, I would have interpolated between the two cases and written a general argument for how to use HJ(s) to get you down to HJ(k-s+1), which can quite clearly be done. (Indeed, it’s so clear that it can be done that maybe what’s already there can be thought of as doing it by giving enough examples to make clear what the general case is.) I bothered to do this partly because I find it historically interesting that it took such a long time for Shelah’s proof to be discovered, and I think that presenting the two proofs side by side in that way makes it really quite mysterious. But my main motive is to try to get people interested in the process of Shelahification of the proof we (probably) have of DHJ(k). If the induction can be turned upside down so easily in the colouring version, I will need a pretty convincing argument to persuade me that it cannot be done for the density version. And if it can be done, it would be really good to do it because the bounds would drop from Ackermann to primitive recursive. The proofs are on [the page about the colouring Hales-Jewett theorem](http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem). Isn’t it so that the iterations give you a k-(k-1) fliptop space, then a (k-1)-(k-2) fliptop space in the smaller cube so on? Then if you check any line, it is fliptop for any consecutive pairs, so it is monochromatic. I was wondering if a statistical variant would work or not; Take an almost fliptop subspace (many neighbours have the same colour) and a smaller almost fliptop subspace so on. At the end if at least one line is pairwise fliptop then we are done. Jozsef, I’m not sure if this is what you’re asking, but on the wiki I presented Shelah’s argument in terms of flipbottom subspaces instead of fliptop ones. Tim, I just changed the notation there. I should have followed yours. While I think that I understand Shelah’s proof completely, I’m not sure that I see exactly the limits of the technique. For example, it seems to me that we don’t use that the points of every neighboring pair have the same colour. For example if 11111… is blue and 21111… is red in the first fliptop subspace, it won’t make any trouble later as the line defined by the two elements is outside of the next subspace. In general, every point with coordinate 1 has exactly one “semi-neighbour” which is important in the iteration, where the 1-s are replaced by 2-s. But I don’t see an easy way to consider the important pairs only. Yes, I had a similar thought soon after writing up the wiki page. I modified the usual proof so as to ensure, unnecessarily, that it reduced HJ(k) to HJ(2). But the usual colour-focusing proof doesn’t need that full strength. I therefore suspect that it is possible to modify the Shelah proof so that instead of producing a fliptop subspace you produce a subspace with some weaker property. But I haven’t tried to work out any details, or even to convince myself that it’s definitely possible."},{"username":"jozsef","timestamp":"2009-03-13T20:34:00.000Z","contents":"Isn’t it so that the iterations give you a k-(k-1) fliptop space, then a (k-1)-(k-2) fliptop space in the smaller cube so on? Then if you check any line, it is fliptop for any consecutive pairs, so it is monochromatic. I was wondering if a statistical variant would work or not; Take an almost fliptop subspace (many neighbours have the same colour) and a smaller almost fliptop subspace so on. At the end if at least one line is pairwise fliptop then we are done."},{"username":"gowers","timestamp":"2009-03-13T23:43:00.000Z","contents":"Jozsef, I’m not sure if this is what you’re asking, but on the wiki I presented Shelah’s argument in terms of flipbottom subspaces instead of fliptop ones."},{"username":"jozsef","timestamp":"2009-03-14T00:18:00.000Z","contents":"Tim, I just changed the notation there. I should have followed yours. While I think that I understand Shelah’s proof completely, I’m not sure that I see exactly the limits of the technique. For example, it seems to me that we don’t use that the points of every neighboring pair have the same colour. For example if 11111… is blue and 21111… is red in the first fliptop subspace, it won’t make any trouble later as the line defined by the two elements is outside of the next subspace. In general, every point with coordinate 1 has exactly one “semi-neighbour” which is important in the iteration, where the 1-s are replaced by 2-s. But I don’t see an easy way to consider the important pairs only."},{"username":"gowers","timestamp":"2009-03-14T00:29:00.000Z","contents":"Yes, I had a similar thought soon after writing up the wiki page. I modified the usual proof so as to ensure, unnecessarily, that it reduced HJ(k) to HJ(2). But the usual colour-focusing proof doesn’t need that full strength. I therefore suspect that it is possible to modify the Shelah proof so that instead of producing a fliptop subspace you produce a subspace with some weaker property. But I haven’t tried to work out any details, or even to convince myself that it’s definitely possible."},{"username":"gowersjozsef","timestamp":"2009-03-14T00:15:00.000Z","contents":"Shelah I’m still just throwing out guesses here (and seeing whether I get anywhere with them in real time — no luck so far) and here’s another one. In the argument we have now, we could present it as saying that we try to find a combinatorial line, and if we can’t then we get correlation with a set of complexity k-2\\. If we are going to try to Shelah-ize, then we’re very much hoping to find a fliptop subspace (or perhaps, as Jozsef suggests above, something slightly weaker but still sufficient) where %\\mathcal{A}% is still dense. So perhaps we should plunge in and try to construct such a subspace, but instead of being completely determined to succeed, we should hope that _either_ we will succeed _or_ we will get correlation with a set of complexity 1\\. The first step of what I was trying to do in 1013, following Jozsef in 1011, was as follows. Write %{}[k]^n% as %{}[k]^r\\times[k]^s%, where %s% is much bigger than %r%. For each %y\\in[k]^s,% define %\\mathcal{A}_y% to be %\\{x\\in[k]^r:(x,y)\\in\\mathcal{A}\\}.% We would like to find %y_1% and %y_2% that differ only in a few places where coordinates that were k-1 in %y_1% are k in %y_2%, such that %\\mathcal{A}_{y_1}=\\mathcal{A}_{y_2}.% So far so good, but we would also like %\\mathcal{A}% to have density almost as big as the starting density when restricted to %{}[k]^r\\times L%, where %L% is the unique combinatorial line that contains both %y_1% and %y_2.% Now if we _can’t_ do this, then we potentially get an interesting set inside which %\\mathcal{A}% has reduced density. It’s formed as follows: for each pair %(y_1,y_2)% as above, form the line %L%, and take the union %X% of all those lines. Ignoring for now the fact that the union is not an arithmetic operation, so we can’t actually conclude without extra work that if %\\mathcal{A}% has reduced density on every %{}[k]^r\\times L% that it has reduced density on %{}[k]\\times X,% let’s see whether %X% has any good structure, such as low complexity. A sequence %z\\in[k]^s% belongs to %X% if and only if we can find some %j% and some subset %Z% of the j-set of %x% such that when we overwrite %Z% with (k-1)s or ks, we get two points %y_1% and %y_2% with %\\mathcal{A}_{y_1}=\\mathcal{A}_{y_2}.% Hmm, that doesn’t seem to have low complexity. Back to the drawing board. Just a technical remark; when you cut %[k]^n% into two parts the you can play with the densities. The density of A is c, say. If any %A_x% or %A_y% had density larger than %c+\\epsilon% then we can go there and repeat. That means that the density of x that %A_x% is at least %c-\\epsilon_2% is almost 1, I think."},{"username":"jozsef","timestamp":"2009-03-14T00:47:00.000Z","contents":"Just a technical remark; when you cut %[k]^n% into two parts the you can play with the densities. The density of A is c, say. If any %A_x% or %A_y% had density larger than %c+\\epsilon% then we can go there and repeat. That means that the density of x that %A_x% is at least %c-\\epsilon_2% is almost 1, I think."},{"username":"gowers","timestamp":"2009-03-14T00:48:00.000Z","contents":"Shelah Time for a very vague thought indeed. Maybe the mistake in our thoughts so far about a density version of Shelah’s proof is that we’re still using colourings too much. Is there some “density equivalent” of a fliptop subspace? To get a handle on this question, let us try to focus on the property that’s good about fliptop subspaces: that if you ever find a line with all the points up to k-1 of the same colour, then the whole line must be monochromatic. The obvious density version of that would be that if the first k-1 points belong to %\\mathcal{A}% then so does the kth. One immediate small observation is that for this to be the case, all we need is that if you change some (k-1)s to ks, you don’t go _out_ of %\\mathcal{A}%. We don’t mind if this operation takes you _in_ to %\\mathcal{A}%. But is there perhaps some much more general circumstance under which we can deduce that %\\mathcal{A}% contains a line from the fact that %\\mathcal{A}\\cap[k-1]^n% does? Or perhaps from the fact that %\\mathcal{A}\\cap[k-1]^n% contains _lots_ of combinatorial lines? In the latter case, what is the set of points that %\\mathcal{A}% will be forced to avoid? I have to go now, but at first glance it still doesn’t look as though there are any super-low-complexity sets floating about."},{"username":"terence-tao","timestamp":"2009-03-14T08:47:00.000Z","contents":"Austin’s proof w/o Ramsey theory or density increment I’ve written up a more or less complete proof of DHJ(3) using a (rather abstract) triangle-removal approach, based on Austin’s proof, at [http://michaelnielsen.org/polymath1/index.php?title=Austin%27s_proof_II](http://michaelnielsen.org/polymath1/index.php?title=Austin%27s_proof_II) I found that stationarity is not used as much as I had previously thought, and can in fact be obtained by a simple energy increment argument and the pigeonhole principle rather than more high-powered Ramsey-theoretic tools such as Graham-Rothschild. The argument is a bit tedious though. It follows the approach to triangle removal used for instance in [my reworking of the hypergraph removal lemma](http://arxiv.org/abs/math.CO/0503572), in particular, heavily using lots of conditional expectation computations, rather than the traditional language of cleaning out “bad” cells and then counting both “non-trivial” and “trivial” triangles in the surviving cells. But there are some distracting complications due to the multiplicity of scales, and the presence of some additional random choices (basically, one has to choose some random index sets I to flip from 0 to 2, etc). One has to make the whole argument “relative” or “conditioned” to these random choices, which makes the argument look stranger than it normally does. The bounds seem to be tower-exponential in nature – not so surprising, being based on triangle removal. Amusingly, if one used uniform measure rather than equal-slices measure, one would be forced to make a double tower-exponential, because of the fact that each new scale would have to essentially be the square root of the previous. Hooray for equal slices measure! I’m pretty sure the density-increment argument is going to end up being shorter and simpler than this one, but I’m putting the triangle removal proof here for completeness. I’ll be travelling overseas shortly, and so I’ll probably be contributing very little for the next week or two."},{"username":"gowersgowersrandalljozsef","timestamp":"2009-03-14T16:57:00.000Z","contents":"Graham-Rothschild A quick pair of closely related questions: is the density version of Graham-Rothschild known, and do we now have the tools to prove it (whether or not it is known)? For the benefit of anyone who can’t be bothered to look it up on the wiki, the Graham-Rothschild theorem is like the Hales-Jewett theorem except that the basic objects are combinatorial subspaces of some fixed dimension rather than points. For instance, a special case is that if you colour the lines in %{}[3]^n% with finitely many colours, then there is a combinatorial subspace of dimension m such that all the lines in that subspace have the same colour — provided n is big enough in terms of m and the number of colours. It may be that the question is easy. For instance, suppose we think of a combinatorial line in %{}[k]^n% as a point in %{}[k+1]^n% with coordinates that belong to the set %\\{1,2,\\dots,k,*\\}.% If we then find in our dense set %\\mathcal{A}% of such lines an m-dimensional subspace, what does that translate into? It gives us some wildcard sets %Z_1,\\dots,Z_m,% each of which can be converted into an element of this alphabet, and it also gives us some fixed coordinates. The problem is that the fixed coordinates can be equal to %*%, so we do not end up with all the lines in some combinatorial subspace. At first this looks problematic: given a dense subset of %\\{1,2,\\dots,k,*\\}^n,% we can’t hope to find a subspace inside it with no fixed coordinates equal to %*%, since %\\mathcal{A}% might consist of all points with at least one coordinate equal to %*%. However, that’s not a problem, because our _entire space_ consists of points with at least one %*% (since they are combinatorial lines). So here’s a DHJ(3) variant. Suppose you have a dense subset %\\mathcal{A}% of %{}[3]^n.% Must there be an m-dimensional subspace S such that all the fixed coordinates are equal to 1 or 2, and every point in S that includes at least one 3 is an element of %\\mathcal{A}%? Just realized that the formulation of that last question was a bit careless, since the usual example of a set with roughly equal numbers of all three coordinates is a counterexample to this when m=2\\. So it’s not instantly obvious what a density version of Graham-Rothschild would even say, but I hope that with the help of equal-slices measure one might be able to formulate a decent statement. Generally, these Ramsey theorems are either “projective” (Schur, Hindman, Ramsey), “affine” (van der Waerden, HJ) or “projective/affine” (Graham-Rothschild, Carlson, CST). Nothing with a projective aspect will typically have a “naive” density version, though I don’t know exactly what you are shooting for here. (Presumably you don’t mean something that the set of words having an odd number of 3s would be a counterexample to.) Hi Randall, It is an interesting question if these problems have density versions or not. Let’s consider Schur’s thm. Ben Green proved that if a subset of [n], S, has only a few, %o(n^2)%, solutions to x+y=z in S then one can remove o(n) elements from S to destroy all solutions. This might be viewed as a density version of Schur’s theorem. On the other hand I don’t see a simple way to deduce Schur’s theorem from this density version. I tried it as it might give a bound on the triangle removal lemma, but without success."},{"username":"gowers","timestamp":"2009-03-14T17:28:00.000Z","contents":"Just realized that the formulation of that last question was a bit careless, since the usual example of a set with roughly equal numbers of all three coordinates is a counterexample to this when m=2\\. So it’s not instantly obvious what a density version of Graham-Rothschild would even say, but I hope that with the help of equal-slices measure one might be able to formulate a decent statement."},{"username":"randall","timestamp":"2009-03-15T02:16:00.000Z","contents":"Generally, these Ramsey theorems are either “projective” (Schur, Hindman, Ramsey), “affine” (van der Waerden, HJ) or “projective/affine” (Graham-Rothschild, Carlson, CST). Nothing with a projective aspect will typically have a “naive” density version, though I don’t know exactly what you are shooting for here. (Presumably you don’t mean something that the set of words having an odd number of 3s would be a counterexample to.)"},{"username":"jozsef","timestamp":"2009-03-15T02:58:00.000Z","contents":"Hi Randall, It is an interesting question if these problems have density versions or not. Let’s consider Schur’s thm. Ben Green proved that if a subset of [n], S, has only a few, %o(n^2)%, solutions to x+y=z in S then one can remove o(n) elements from S to destroy all solutions. This might be viewed as a density version of Schur’s theorem. On the other hand I don’t see a simple way to deduce Schur’s theorem from this density version. I tried it as it might give a bound on the triangle removal lemma, but without success."},{"username":"ryan-odonnellryan-odonnell","timestamp":"2009-03-14T21:05:00.000Z","contents":"Uniform ordered partition measure. [Here](http://www.cs.cmu.edu/~odonnell/new-ordered-measure.pdf) is the corrected link from 1031; in other words, add “.pdf” to the broken link. I think this measure will work well with the Graham-Rothschild problem described above; again, I’ll try to write more in a few days when I get back home. 1037.1 Drat, I keep botching it! It’s [here](http://www.cs.cmu.edu/~odonnell/new-ordered-partitions.pdf)."},{"username":"ryan-odonnell","timestamp":"2009-03-14T21:06:00.000Z","contents":"1037.1 Drat, I keep botching it! It’s [here](http://www.cs.cmu.edu/~odonnell/new-ordered-partitions.pdf)."},{"username":"jozsefjozsef","timestamp":"2009-03-14T23:17:00.000Z","contents":"Subspace implies line A simple argument shows that DHJ(k) implies d-dimensional subspaces in %[k]^n% for any dense subset (with large enough n). I remember seeing a wiki article about this, but I was unable to find it this morning. It is also true that a subspace theorem implies DHJ. That looks fairly trivial but let me explain it; I want to show that the following statement is equivalent to DHJ(k): “There is a constant, %c < 1% that for every d there is an n that any c-dense subset of %[k]^n% contains a d-dimensional subspace.”  \nI would like to show the direction that the statement above implieas DHJ(k). The other direction is already wikified.  \nAs before, write %[k]^n% as %[k]^r\\times[k]^s%, where s is much bigger than r. For each %y\\in [k]^s%, define %\\mathcal{A}_y% to be %\\{x\\in[k]^r:(x,y)\\in\\mathcal{A}\\}%.  \nLet Y denote the set of %y\\in [k]^s%, that %\\mathcal{A}_y% is empty. Suppose that %\\mathcal{A}% is large, line-free, and its density is %\\delta =\\Delta-\\epsilon% where %\\Delta% is the limit of density of line-free sets and %\\epsilon < (1-c)\\Delta%. We can also suppose that no %\\mathcal{A}_y% has density much larger than %\\Delta% as that would guarantee a combinatorial line. But then the density of Y is at most 1-c, so there is a c-dense set Z %Z=[k]^s-Y% such that any element is a tail of some elements of %\\mathcal{A}%. For every %y \\in Z% choose an x %x\\in [k]^r:(x,y)\\in\\mathcal{A}%. This x will be the colour of y. It gives a %[k]^r% colouring on Z. By the initial condition Z contains arbitrary large subspaces, so by HJ(k) we get a line in %\\mathcal{A}%. I’ve find Tim’s post about DHJ(k) implies subspace DHJ(k).  \n[http://michaelnielsen.org/polymath1/index.php?title=DHJ(k)_implies_multidimensional_DHJ(k)](http://michaelnielsen.org/polymath1/index.php?title=DHJ(k)_implies_multidimensional_DHJ(k))  \nThe right title of the post above would be “Weak subspace DHJ(k) implies DHJ(k)”."},{"username":"jozsef","timestamp":"2009-03-15T21:06:00.000Z","contents":"I’ve find Tim’s post about DHJ(k) implies subspace DHJ(k).  \n[http://michaelnielsen.org/polymath1/index.php?title=DHJ(k)_implies_multidimensional_DHJ(k)](http://michaelnielsen.org/polymath1/index.php?title=DHJ(k)_implies_multidimensional_DHJ(k))  \nThe right title of the post above would be “Weak subspace DHJ(k) implies DHJ(k)”."},{"username":"gowers","timestamp":"2009-03-15T03:33:00.000Z","contents":"Graham-Rothschild I’ve got a new attempt at a density version of Graham-Rothschild that might have a chance of being true. I realized that equal-slices measure wouldn’t rescue the previous version, since the set of lines with wildcard set of size approximately %\\alpha n% is a counterexample. As I write, I realize that my new attempt fails too. Basically, if you’ve got a 2D subspace then you must have lines with wildcard sets of sizes x, y and x+y, so a density version looks as though it would have to imply a density version of Schur’s theorem, which is of course false. (I realize that I am sort of repeating what people have already said in their replies to 1036.) Maybe one could go down the route Jozsef implicitly suggests and try proving that if %\\mathcal{A}% is a set that contains few m-dimensional subspaces, then one can remove a small number of elements from %\\mathcal{A}% and end up with no m-dimensional subspaces. I’m not sure that I feel like trying this though …"},{"username":"klas-markstrom","timestamp":"2009-03-15T14:52:00.000Z","contents":"1040.  \nSince the density of a subset of [k]^n is quite insensitive to the removal of an element, at least for large n, a set with positive density will contain many lines. How many lines can be guaranteed, as a function of the density and n? From the equal-slices proof of Sperner it is not hard to get a lower bound for DHJ(2) so I’m curious as to what can be said for DJH(3)."},{"username":"jozsefryan-odonnell","timestamp":"2009-03-15T22:33:00.000Z","contents":"Line-free sets correlate locally with complexity-1 sets I would like to go back to one important part of the DHJ(3) proof, to analyze it from a slightly different angle. Let us consider our set %\\mathcal{A}%, a dense subset of %[3]^n%, as a subset of %({\\Bbb Z}/3{\\Bbb Z})^n%. Build a graph on %\\mathcal{A}% as follows; The vertex set is %\\mathcal{A}% and two vertices, a and b, are connected iff for c: a+b+c=0 c is also in %\\mathcal{A}%. If %\\mathcal{A}% was dense then there are many edges in any typical subspace. Now DHJ is equivalent to the statement that there are two connected elements of %\\mathcal{A}% with the same set of 3-s and that one’s set of 1-s contains the 1-s of the other. This model leads us to a density Sperner problem.  \nIt just turned out that I have to go somewhere right now. I will come back in a few (4+) hours. 1042\\. The distribution. Hmm. In fact, it may be even better to view strings in %[k]^n% being generated in the time-opposite way from the one I described. Specifically, equal-slices and its nondegenerate variant are the same as the following Polya-Eggenberger urn process: [Non-degenerate version:] Start with one of the %k!% permutations of the string %123\\cdots k%. Repeat the following %n-k% times: pick a random character in %[k]% with probability proportional to the number of times it appears already in the string. Now insert that character into a random place in the string. [Equal-slices version, I think:] Same, except: a) start with the empty string; b) for the purposes of proportionality, pretend there is a phantom copy of each of the %k% characters. This yet-another viewpoint on the equal-slices distribution helps with making “subspace arguments” (which the uniform distribution was good for): the point is, if you do a %k%-color Polya urn process for %M + n% steps and %n \\gg M%, then the final distribution hardly depends at all on what happened in the first %M% steps. Will write more when I get back in two days."},{"username":"ryan-odonnell","timestamp":"2009-03-16T03:38:00.000Z","contents":"The distribution. Hmm. In fact, it may be even better to view strings in %[k]^n% being generated in the time-opposite way from the one I described. Specifically, equal-slices and its nondegenerate variant are the same as the following Polya-Eggenberger urn process: [Non-degenerate version:] Start with one of the %k!% permutations of the string %123\\cdots k%. Repeat the following %n-k% times: pick a random character in %[k]% with probability proportional to the number of times it appears already in the string. Now insert that character into a random place in the string. [Equal-slices version, I think:] Same, except: a) start with the empty string; b) for the purposes of proportionality, pretend there is a phantom copy of each of the %k% characters. This yet-another viewpoint on the equal-slices distribution helps with making “subspace arguments” (which the uniform distribution was good for): the point is, if you do a %k%-color Polya urn process for %M + n% steps and %n \\gg M%, then the final distribution hardly depends at all on what happened in the first %M% steps. Will write more when I get back in two days."},{"username":"gil-kalai","timestamp":"2009-03-16T03:57:00.000Z","contents":"Density increasing and an analog problem (This is a little off topic) Let me mention a problem which I thought of as analogous to Roth/cap set where the gaps between lower and upper bounds are similarly shocking and the current density increasing arguments cannot help; (It is related to old work of mine with Kahn and Linial and also to some more recent work with Shelah that we did not publish.) you have a subset A of %\\{0,1\\}^n% of density c and you would like to find a combinatorial subcube F of dimension 0.9n so that the **projection** of A to F is of large density say 0.99\\. In other words, we want to find a set of 0.9n coordinates so that for a fraction 0.99 of the vectors supported on this set we can find a continuation on the other coordinates that is in A. (We usually talk here about restriction to a subcube/subspace and not about projections. But traditionally sections and projections are not unrelated.) By a density increasing argument doing it one coordinate at a time it was known from the late 80s that this can be achieved if c is %n^{-\\alpha}% for %\\alpha=1/5% or so. A conjecture by Benny Chor asserts that %c=0.999^n% is good enough! I think it is a little analogous to Roth (or cap set) a few points: 1) The proof is also by a slow density increasing argument (you reduce the dimension by one every time) and there are examples the such an argument cannot be improved. 2) There are some conjectures (by Friedgut and others) which may explain why we can get the density down to %n^{-\\beta}% for every %\\beta% maybe even %\\beta=\\gamma \\log n% but no plans beyond it. 3) There are alarming examples by Ajtai and Linial of Boolean functions descibed by certain random depth 3 circuits (that Ryan already mentioned) that may (or a varian of) give a counter example. It is complicated to check it. I admit that the analogy with density increasing argument for Roth-type problems is not very strong: this problem is about projection to subcubes and there it is about restrictions to subspaces or similar creatures; But there may be some connecion. In particular I would try subsets described by low depth small size circuits (with operations over {0,1,2}) as candidates for counter examples for the most ambitious conjectures regarding Roth and cap sets. (On the positive side maybe more sophisticated density increasing arguments of the kind we talk about here can be used in this problem.)"},{"username":"kristal-cantwell","timestamp":"2009-03-16T07:12:00.000Z","contents":"Graham-Rothschild From 1036: ” A quick pair of closely related questions: is the density version of Graham-Rothschild known, and do we now have the tools to prove it (whether or not it is known)?  \nFor the benefit of anyone who can’t be bothered to look it up on the wiki, the Graham-Rothschild theorem is like the Hales-Jewett theorem except that the basic objects are combinatorial subspaces of some fixed dimension rather than points. For instance, a special case is that if you colour the lines in with finitely many colours, then there is a combinatorial subspace of dimension m such that all the lines in that subspace have the same colour — provided n is big enough in terms of m and the number of colours. ” I doubt there is density version of the Graham-Rothschild theorem. If one fixes a coordinate and deletes all lines that has a constant coordinate at that point then that will only lower the number of lines by a constant factor but it will prevent the formation of any two dimensional space with all its lines monochromatic as in that case the intersection of the moving coordinates of all of the lines is the null set."},{"username":"kristal-cantwell","timestamp":"2009-03-16T07:19:00.000Z","contents":"Density theorems and Ramsey theorems Density theorems are stronger than Ramsey theorems. Any density theorem can be converted to a Ramsey theorem as follows. One sets the density less than 1/r and gets a large enough configuration so that the if the density is greater than 1/r there will be the desired result than for any r coloring there will be one color with density 1/r or more and we will have a monochromatic configuration in that color."},{"username":"kristal-cantwellgowers","timestamp":"2009-03-16T09:49:00.000Z","contents":"Density Schur Theorem I have just remembered the counterexample for a density version of Schur’s theorem. One just takes the odd numbers, they have density 1/2 but since the sum of two odd numbers is even there are no triples a,b,c in the set such that a+b=c. The fact that the density version of Schur’s theorem is false implies that density Graham-Rothschild is false — see 1038."},{"username":"gowers","timestamp":"2009-03-16T11:54:00.000Z","contents":"The fact that the density version of Schur’s theorem is false implies that density Graham-Rothschild is false — see 1038."},{"username":"gowersrandallrandall","timestamp":"2009-03-16T15:36:00.000Z","contents":"Bad news and good news. Bad news: our proof of DHJ(3) is wrong! Good news: it isn’t too hard to fix. But it’s still quite amusing that nobody noticed that [Substep 2.2](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument#Substep_2.2) of the write-up on the wiki of the crucial lemma that a dense 1-set can be almost completely partitioned into subspaces was nonsense as presented. I’ve left the old version there as a cautionary tale. Fortunately, the reason for the mistake was that I had translated the corresponding part of Terry’s argument in a sloppy way — the argument itself was sound. But it did give me a bit of a scare … 1046.1 Good news and polymath…. Well, perhaps it will be somewhat reassuring that the changes you have made seem to be only adding more detail, and that, from the ergodic perspective at least, it was natural to omit said detail; loosely translated (and unless I misunderstand), what you seem not to be checking more carefully is that when you remove the fibers (over some factor) that recur under a subspace, what you have left is still measurable with respect to that subspace. So perhaps the good news is actually that polymath is like math with a net. (In which case the bad news is the danger than everyone involved will internalize that piece of good news to the point where it becomes a problem.) I of course meant (a) “now” to be checking more carefully, and (b) measureable with respect to that “factor”."},{"username":"randall","timestamp":"2009-03-16T22:28:00.000Z","contents":"1046.1 Good news and polymath…. Well, perhaps it will be somewhat reassuring that the changes you have made seem to be only adding more detail, and that, from the ergodic perspective at least, it was natural to omit said detail; loosely translated (and unless I misunderstand), what you seem not to be checking more carefully is that when you remove the fibers (over some factor) that recur under a subspace, what you have left is still measurable with respect to that subspace. So perhaps the good news is actually that polymath is like math with a net. (In which case the bad news is the danger than everyone involved will internalize that piece of good news to the point where it becomes a problem.)"},{"username":"randall","timestamp":"2009-03-16T22:32:00.000Z","contents":"I of course meant (a) “now” to be checking more carefully, and (b) measureable with respect to that “factor”."},{"username":"gowers","timestamp":"2009-03-16T17:09:00.000Z","contents":"Wikification I have now wikified an [abstract version of the iterative argument](http://michaelnielsen.org/polymath1/index.php?title=A_general_partitioning_principle) that was wrong before. From the abstract point of view the property that I was forgetting about was the all-important sigma-algebra property. This version should be fine now and is intended to be sufficiently general to deal with DHJ(k) as well. The argument could be sharpened up a bit towards the end — I ran out of energy."},{"username":"gowers","timestamp":"2009-03-16T17:17:00.000Z","contents":"Metacomment. I’ve created a thread for comments 1050-1099, so this one should draw to a close."},{"username":"sunee","timestamp":"2009-07-02T14:17:00.000Z","contents":"Thank you. best information for me."},{"username":"gowersjozsefgowersjozsefjozsefgil-kalaigil","timestamp":"2009-03-16T21:38:00.000Z","contents":"Shelah I still haven’t given up on the idea of Shelahifying the proof of DHJ(k), but a weakness in my attempts so far is that I have no good story to tell about corners. So in this comment I am going to try to guess how a proof of the corners theorem might go if the induction worked the other way round. In other words, I want an argument that solves the higher-dimensional corners problem with the one-dimensional Szemerédi theorem as its main tool (rather than Szemerédi in one dimension down). It feels rather unlikely that this is possible, but we aren’t going to know if we don’t try. I think the difference should show up only once we get to three dimensions. I’ve been struggling to think of some way that complexity-1 sets might come into the picture (which in this context means Cartesian products %X\\times Y\\times Z\\subset[n]^3%), and the best I’ve come up with so far is this. The conventional proof takes a “dense diagonal,” meaning a plane of constant %x+y+z% that contains many points of the set %A%, and exploits the fact that this set contains many aligned equilateral triangles to obtain a structured set that %A% is forced to avoid. What if we try to do things the other way round? We start with the fact that %A\\subset[n]^3% is dense and try to deduce from that that almost all diagonal planes are sparse. The hope is that the argument works unless %A% correlates with a set of complexity 1. Let’s be more precise about this. If %A% is a 3D-corner-free subset of %[n]^3% and %(x,y,z)\\in A% then for any %d% we cannot have all three points in the aligned equilateral triangle %(x+d,y,z),(x,y+d,z),(x,y,z+d)%. Thus, each point in %A% rules out a large number of triangles in a large number of diagonal planes. What we would like to be able to deduce from that is that the density of %A% in those planes is necessarily small. This suggests the following question, which I have not had time to think about properly. For what collections %\\mathcal{C}% of corners (in %[n]^2% now) is it the case that every dense subset of %[n]^2% contains a corner in %\\mathcal{C}%? A more specific question is this. A set of complexity 1 in %[n]^2% is a set of the form %K=\\{(x,y):x\\in U,y\\in V,x+y\\in W\\}.% Let us say that %\\mathcal{C}% _correlates with_ %K% if there are more corners with vertices in %K% than there should be. Suppose that %\\mathcal{C}% does not correlate with any dense set of complexity 1\\. Does it follow that every dense set has a corner in %\\mathcal{C}%? And can it be proved surprisingly easily? If the answer to both questions were to be yes then we might be in business, but as I said at the beginning this feels a bit over-optimistic. Another point to make is that the above question could need a little adjustment before it becomes sensible. For instance, it might need to talk about more local forms of correlation. Tim, could you please clarify your last question? “%\\mathcal{C}% correlates with if there are more corners with vertices in K than there should be.” (Probably you meant correlates with K.) I can’t see any condition which guarantees that %\\mathcal{C}% is not even empty. Probably I missed something. OK. First, I’ve added the missing %K% — thanks for pointing that out. The idea I had in mind was to assume that %\\mathcal{C}% had density %\\alpha% in the set of all possible corners, that %\\beta% was the density of corners with vertices in %K%, and that the density of such corners that belong to %\\mathcal{C}% was significantly different from %\\alpha\\beta.% I see. This sounds to me like another sparse hypergraph regularity problem. The complexity 1 set is the shadow of %\\mathcal{A}% and we check if %\\mathcal{A}% is quasirandom on its shadow or not.  \nThe complexity 1 set contains many corners (it is a large Cartesian product with small sumset) so we want to conclude that %\\mathcal{A}% contains corners or it is “very irregular”. Isn’t it your modified Ajtai-Szemeredi proof? I have to read your post again, as I lost track of how this relates to a Shelah type argument. Regarding to the first question; There is a very sparse set of corners that every dense subset of %[n]^2% contains one from that set. Using DHJ(4) (I know that we don’t want to use it but anyways) one can show that there is a set of %n^{2.3219...}% corners that every dense set contains a corner from that set. To construct such set, consider [n] as a %m=\\log{n}% dimensional cube and there is a 1-1 correspondence between the points of %[n]^2% and %[4]^m%. Take the triangles defined by combinatorial lines. By DHJ(4) every dense subset contains a corner (even a square) from this set of corners. Maybe one can find an even sparser set of corners but I don’t see how. I also can’t see a reasonable lower bound for the size of such set of corners. It seems to be an interesting problem. Shalahifying DHJ(k) is a noble cause indeed; What about DHJ(3)? does the proof gives inverse ackerman type function for DHJ(3) as well? Is the goal of density 1/log log log … log n just for DHJ(3) out of reach with the new proof? (for a bounded number of logs? for two logs?) Another quick question: what are the best bound known for multidimensional Sperner? (The wiki argument with DHJ(2^k) gives terrible bounds). I recall we discussed it but didnt find it. (Another question: is there an easy way to do search on the union of all threads?)"},{"username":"jozsef","timestamp":"2009-03-16T22:50:00.000Z","contents":"Tim, could you please clarify your last question? “%\\mathcal{C}% correlates with if there are more corners with vertices in K than there should be.” (Probably you meant correlates with K.) I can’t see any condition which guarantees that %\\mathcal{C}% is not even empty. Probably I missed something."},{"username":"gowers","timestamp":"2009-03-16T23:29:00.000Z","contents":"OK. First, I’ve added the missing %K% — thanks for pointing that out. The idea I had in mind was to assume that %\\mathcal{C}% had density %\\alpha% in the set of all possible corners, that %\\beta% was the density of corners with vertices in %K%, and that the density of such corners that belong to %\\mathcal{C}% was significantly different from %\\alpha\\beta.%"},{"username":"jozsef","timestamp":"2009-03-17T00:15:00.000Z","contents":"I see. This sounds to me like another sparse hypergraph regularity problem. The complexity 1 set is the shadow of %\\mathcal{A}% and we check if %\\mathcal{A}% is quasirandom on its shadow or not.  \nThe complexity 1 set contains many corners (it is a large Cartesian product with small sumset) so we want to conclude that %\\mathcal{A}% contains corners or it is “very irregular”. Isn’t it your modified Ajtai-Szemeredi proof? I have to read your post again, as I lost track of how this relates to a Shelah type argument."},{"username":"jozsef","timestamp":"2009-03-17T22:25:00.000Z","contents":"Regarding to the first question; There is a very sparse set of corners that every dense subset of %[n]^2% contains one from that set. Using DHJ(4) (I know that we don’t want to use it but anyways) one can show that there is a set of %n^{2.3219...}% corners that every dense set contains a corner from that set. To construct such set, consider [n] as a %m=\\log{n}% dimensional cube and there is a 1-1 correspondence between the points of %[n]^2% and %[4]^m%. Take the triangles defined by combinatorial lines. By DHJ(4) every dense subset contains a corner (even a square) from this set of corners. Maybe one can find an even sparser set of corners but I don’t see how. I also can’t see a reasonable lower bound for the size of such set of corners. It seems to be an interesting problem."},{"username":"gil-kalai","timestamp":"2009-03-18T01:29:00.000Z","contents":"Shalahifying DHJ(k) is a noble cause indeed; What about DHJ(3)? does the proof gives inverse ackerman type function for DHJ(3) as well? Is the goal of density 1/log log log … log n just for DHJ(3) out of reach with the new proof? (for a bounded number of logs? for two logs?)"},{"username":"gil","timestamp":"2009-03-18T11:42:00.000Z","contents":"Another quick question: what are the best bound known for multidimensional Sperner? (The wiki argument with DHJ(2^k) gives terrible bounds). I recall we discussed it but didnt find it. (Another question: is there an easy way to do search on the union of all threads?)"},{"username":"gowers","timestamp":"2009-03-18T00:19:00.000Z","contents":"Shelah I’m going to have a quick go at the question I asked in 1050 by following the modified Ajtai-Szemerédi proof and seeing what assumptions I need to make about %\\mathcal{C}% to get the argument to work. Actually, I’ve got to go — will do this later."},{"username":"gowersrandall","timestamp":"2009-03-18T04:07:00.000Z","contents":"Shelah OK here goes. The first step was to find a dense diagonal. Do we still want to do that? Well, the point of a dense diagonal was that it gave rise to a large Cartesian product full of forbidden points. But if we’ve only got a certain subset of corners, then we cannot make such a strong conclusion. So what _can_ we conclude? To investigate this, we could see what happens if the set of pairs on some diagonal that belong to corners in our system %\\mathcal{C}% is random. If there are %\\alpha n^2% of these pairs, then they are ruling out a … random set of size %\\alpha n^2%, which isn’t very helpful. Actually, what I’m doing is nonsense because the contrapositive of what I’m trying to show is that a dense “diagonal plane” forces a complexity-1 bias in the set A, which is clearly untrue. So I still don’t have any feel for how, or whether, the proof can be given a Shelah version. 1052.1 This may make absolutely no sense, but what if instead of looking at corners in %Z^3% you look at four terms progressions in %Z%…only you can still look at your dense plane in 3 space, only now your dense set is a set of a special type. Namely, what we really seek is a four term progression in a dense set %E% of %Z%, but we actually look at the set %E'=\\{(x,y,z)\\in Z^3: x+2y+3z\\in E\\}%, which is dense in %Z^3%. It seems that even though you are only forbidding a sparse set with your dense plane, that sparse set may project back to a dense set in %Z% (though I have no idea why that would be, if so, so this is mere speculation)."},{"username":"randall","timestamp":"2009-03-18T04:13:00.000Z","contents":"1052.1 This may make absolutely no sense, but what if instead of looking at corners in %Z^3% you look at four terms progressions in %Z%…only you can still look at your dense plane in 3 space, only now your dense set is a set of a special type. Namely, what we really seek is a four term progression in a dense set %E% of %Z%, but we actually look at the set %E'=\\{(x,y,z)\\in Z^3: x+2y+3z\\in E\\}%, which is dense in %Z^3%. It seems that even though you are only forbidding a sparse set with your dense plane, that sparse set may project back to a dense set in %Z% (though I have no idea why that would be, if so, so this is mere speculation)."},{"username":"randallgowers","timestamp":"2009-03-18T04:09:00.000Z","contents":"Szemeredi’s proof of Roth’s theorem I just looked for the first time at the page about Szemeredi’s combinatorial proof of Roth’s theorem on the wiki. It seems to me that this proof actual resembles the polymath proof of DHJ3 as much as does the Ajtai/Szemeredi proof, if not more. After all it uses this “cube lemma” which seems to be more comparable to DHJ(2) than is Szemeredi’s theorem. Indeed, isn’t my post 1002, which was intended to be a dumbed-downed version of our DHJ3 proof that would be just strong enough to tackle corners, essentially the same proof? (Assuming of course that 1002 is even correct….) I had a similar thought at some stage (though less precise): the fact that he uses combinatorial cubes is certainly suggestive. No time just now to think about your question about 1002, but we should certainly try to clarify this."},{"username":"gowers","timestamp":"2009-03-18T04:29:00.000Z","contents":"I had a similar thought at some stage (though less precise): the fact that he uses combinatorial cubes is certainly suggestive. No time just now to think about your question about 1002, but we should certainly try to clarify this."},{"username":"ryan-odonnellgowersgowersgilryan-odonnellgowers","timestamp":"2009-03-18T05:41:00.000Z","contents":"Simplifying the DHJ(3) proof. I’ll be back on my normal schedule soon, but let me just say I started thinking about the part of the proof wherein it is shown that if %A% has a density increment on a 12-set, then we can find a subspace (with %\\omega_n(1)% coordinates) on which it also has a density increment. Let’s simplify by imagining that %A% has a density increment on a 1-set. By the argument already sketched (based on multidim Sperner), one can show that a 1-set contains not just a combinatorial line but a combinatorial subspace of dimension %\\omega_n(1)%. I think then one can use the new equal-slices perspective to easily show the Varnavides statement, that a 1-set contains a randomly chosen chosen %\\omega_n(1)%-dimensional subspace with constant probability. Perhaps this can substitute for the argument which almost partitions 1-sets into combinatorial subspaces… 1053.1 I think that doesn’t work, for two reasons. First, I don’t see why a 1-set containing lots of combinatorial subspaces gives you a density increase on one of those subspaces. Why couldn’t they all be disjoint from A? More seriously, and this was an important factor in guiding me to the partitioning-into-subspaces perspective, one can use the Hahn-Banach theorem to prove that IF it follows from the fact that A correlates with X that A correlates with a set of type T, then one MUST be able to partition X into sets of type T. (I’ve oversimplified here, and not explained where partitioning almost the whole set comes in — perhaps I’ll wikify this argument some time soon.) 1053.2 I didn’t quite say what I meant there. The Hahn-Banach theorem tells you that you’re basically forced to find at the very least some kind of fractional partition — that is, a covering by subspaces where almost every element is covered almost the same number of times. But I don’t see a probabilistic way of proving that, because different elements can be in very different numbers of subspaces. (I gave up after failing to find a direct probabilistic proof that a Cartesian product in %{}[n]^2% can be evenly covered by grids.) Is this Hahn-Banach argument essentially LP duality? A general ststament will be useful and may ring various combinatorial bells… Gil, I think think so, except that when Tim says “Hahn-Banach” here I’m not sure he actually means “using the Hahn-Banach theorem”. I interpret it somehow as short for “the argument wherein you say that if one can partition X into sets of type T, then it must be that A being dense in X means A is dense in some set of type T”. Or perhaps he does mean, “use the Hahn-Banach Theorem” (i.e., LP duality), I don’t know. My fault for writing in too much haste — sorry, I’ve been rather busy with other things recently (and am just about to go abroad for a few days, so I’ll be going even quieter). I was actually referring to the Hahn-Banach separation theorem, which says that if a point doesn’t belong to a convex body then you can find a linear functional that’s large on the point and small everywhere on the convex body. (I’m oversimplifying slightly, but not too much.) In this case, if the characteristic function f of X can’t be approximated by a suitable positive combination of characteristic functions of subspaces, then you have a convex set to which f does not belong. The functional given by the Hahn-Banach theorem can be used to produce a bounded function that correlates with X but not with any subspace. I’m trying to wikify the details but have kept messing it up. I’m going to work them out on paper and then I’ll get back to the wikification."},{"username":"gowers","timestamp":"2009-03-18T12:10:00.000Z","contents":"1053.1 I think that doesn’t work, for two reasons. First, I don’t see why a 1-set containing lots of combinatorial subspaces gives you a density increase on one of those subspaces. Why couldn’t they all be disjoint from A? More seriously, and this was an important factor in guiding me to the partitioning-into-subspaces perspective, one can use the Hahn-Banach theorem to prove that IF it follows from the fact that A correlates with X that A correlates with a set of type T, then one MUST be able to partition X into sets of type T. (I’ve oversimplified here, and not explained where partitioning almost the whole set comes in — perhaps I’ll wikify this argument some time soon.)"},{"username":"gowers","timestamp":"2009-03-18T12:52:00.000Z","contents":"1053.2 I didn’t quite say what I meant there. The Hahn-Banach theorem tells you that you’re basically forced to find at the very least some kind of fractional partition — that is, a covering by subspaces where almost every element is covered almost the same number of times. But I don’t see a probabilistic way of proving that, because different elements can be in very different numbers of subspaces. (I gave up after failing to find a direct probabilistic proof that a Cartesian product in %{}[n]^2% can be evenly covered by grids.)"},{"username":"gil","timestamp":"2009-03-19T00:29:00.000Z","contents":"Is this Hahn-Banach argument essentially LP duality? A general ststament will be useful and may ring various combinatorial bells…"},{"username":"ryan-odonnell","timestamp":"2009-03-20T06:59:00.000Z","contents":"Gil, I think think so, except that when Tim says “Hahn-Banach” here I’m not sure he actually means “using the Hahn-Banach theorem”. I interpret it somehow as short for “the argument wherein you say that if one can partition X into sets of type T, then it must be that A being dense in X means A is dense in some set of type T”. Or perhaps he does mean, “use the Hahn-Banach Theorem” (i.e., LP duality), I don’t know."},{"username":"gowers","timestamp":"2009-03-20T13:03:00.000Z","contents":"My fault for writing in too much haste — sorry, I’ve been rather busy with other things recently (and am just about to go abroad for a few days, so I’ll be going even quieter). I was actually referring to the Hahn-Banach separation theorem, which says that if a point doesn’t belong to a convex body then you can find a linear functional that’s large on the point and small everywhere on the convex body. (I’m oversimplifying slightly, but not too much.) In this case, if the characteristic function f of X can’t be approximated by a suitable positive combination of characteristic functions of subspaces, then you have a convex set to which f does not belong. The functional given by the Hahn-Banach theorem can be used to produce a bounded function that correlates with X but not with any subspace. I’m trying to wikify the details but have kept messing it up. I’m going to work them out on paper and then I’ll get back to the wikification."},{"username":"ryan-odonnellryan-odonnellryan-odonnellryan-odonnellryan-odonnellgowersgowersryan-odonnellgowers","timestamp":"2009-03-18T19:08:00.000Z","contents":"Simplifying. Hi Tim, perhaps you’re right, but here is an alternative that I think works. By [known](http://www.cs.cmu.edu/~odonnell/more-ordered-partitions.pdf) [arguments](http://michaelnielsen.org/polymath1/index.php?title=DHJ(k)_implies_multidimensional_DHJ(k)) I think we can Varnavides-ise and multidimensional-ise any DHJ-type statement fairly easily, in the equal-slices measure. (The multidimensionalising is in the wiki for uniform measure I think, but I’m pretty sure it’s easy to do under equal-slices.) In particular, we can even Varnavides+multidim-ise any such statement. Hence, e.g., starting with Sperner, it’s not to hard to get an equal-slices VM-DHJ(2). [M = multidim, V = Varnavides.] That’s good, because that’s a key component in pushing up from k = 2 to k = 3\\. In particular, with VM-DHJ(2), one can get DHJ(3) for 1-sets (“Step 1” in the [overall sketch](http://michaelnielsen.org/polymath1/index.php?title=A_second_outline_of_a_density-increment_argument#Step_1:_a_dense_1-set_contains_an_abundance_of_combinatorial_subspaces).) Were we calling this DHJ(1,3) or something? Let’s say we were. I checked (I think) that again, we can easily V-ise and M-ise. (One only needs to check that restrictions of 1-sets to subspaces are still 1-sets.) So we can easily get VM-DHJ(1,3); i.e., a 1-set in %[3]^n% with density %\\delta% has the property that if you pick a random equal-slices %m%-dimensional subspace, it’s entirely within the set with probability %c(\\delta, m) > 0%. Let me now wave my hands and pretend that we can somehow get DHJ(2,3) as well, where I take “DHJ(2,3)” to mean the claim that a dense 12-set in %[3]^n% contains a line. I’m not quite sure how to do this currently, but the jump from 1-sets to 12-sets in the current proof outline is easy, so let’s pretend it’s easy here too. <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> [More honestly, I think we should try to do it by using the argument that appears below.] Again, we can V-ise and M-ise to get VM-DHJ(2,3). Now for the main point of this post: how can we use this to conclude DHJ(3)? If I’m not mistaken (I’m probably mistaken) I think it’s actually easy: As usual, we note that if %A \\subseteq [3]^n% has (equal-slices) density %\\delta%, yet fails to contain a line, then it must [completely avoid a certain 12-set](http://michaelnielsen.org/polymath1/index.php?title=Line-free_sets_correlate_locally_with_complexity-1_sets) we like to call %U \\cap V%. This set %U \\cap V% has equal-slices density %\\eta = \\eta(\\delta)%. Let %S% denote a randomly chosen (equal-slices) %m%-dimensional subspace of %[3]^n%. Note that if we draw %x \\in S% at random (equal-slices), then %x% is overall distributed as equal-slices. Hence: %\\delta = \\Pr_{S,x}[x \\in A] = \\Pr[x \\in A \\mid S \\subseteq U \\cap V] \\Pr[S \\subseteq U \\cap V]%  \n%+ \\Pr[x \\in A \\mid S \\not \\subseteq U \\cap V] \\Pr[S \\not \\subseteq U \\cap V]%. Since %A% avoids %U \\cap V% we have %\\Pr[x \\in A \\mid S \\subseteq U \\cap V] = 0%. But also, by VM-DHJ(2,3), we know that %\\Pr[S \\not \\subseteq U \\cap V] \\leq 1 - c(\\eta, m)% for some tiny but positive %c(\\eta,m)%. Hence the above equation implies %\\delta/(1-c(\\eta,m)) \\leq \\Pr[x \\in A \\mid S \\not \\subseteq U \\cap V]%. Hence there exists a particular subspace %S_0% (not in %U \\cap V%) such %\\Pr_{x \\in S_0}[x \\in A] \\geq \\delta/(1-c(\\eta,m))%, and we have the desired density increment on a (multidimensional) subspace. A small point: Actually, DHJ(1,3) follows almost trivially just from Sperner (as I’m sure we noted back in the 1-200 thread). So you can get to VM-DHJ(1,3) rather quickly. About DHJ(2,3), I’m still thinking… 1054.2\\. DHJ(3) for 12-sets. Oh, it’s [already on the wiki](http://michaelnielsen.org/polymath1/index.php?title=DHJ(1%2C3)#An_easy_version_of_the_problem). Hmm. Are we done? 1054.3\\. Okay, so just to recap. The following statements all refer to equal-nondegenerate-slices measure: (a) DHJ(3) for 12-sets is an easy consequence of Sperner (see 1054.2). (b) I claim that it is not too hard to Varnavides-ise, Multidimensional-ise, and indeed Varna+Multidim-ise DHJ(3) for 12-sets (using known arguments of Tim and Terry I think that are already on the wiki, but which can be streamlined a bit if you work in equal-nondegenerate-slices) (c) Then, assuming VM-DHJ(3) for 12-sets, use the argument following “Now for the main point of this post:” in 1054. If someone is able to agree with (c), I will endeavour to wikify (b) a bit more clearly. 1054.4\\. I’m slightly nervous about the dog-chasing-its-own-tail aspect of the parameters here; you know, getting too small an increment on too small a subspace. Let me think about why it’s okay in the original proof sketch. 1054.5 I see better what you’re saying now. One detail troubles me, which is that the density increase you get on the subspace depends on the dimension of the subspace, which definitely isn’t the case in the other argument, but I haven’t yet decided whether that’s a serious problem or just an interesting detail. 1054.6 The more I think about it, the more I think that it is a problem. E.g. if on passing to an m-dimensional subspace you get a density increase of %2^{-m}% it seems to me that there’s no way of getting the density up to 1. Right, it seems to be a problem. But I kind of like the flavour of this angle; I wonder if there’s a way to make it work… Probably won’t get a chance to think about it today though. It would be great if you could simplify the argument. I’m busy wikifying an argument that places some limits on any proof that can possibly work — I hope it will narrow down and therefore speed up the search."},{"username":"ryan-odonnell","timestamp":"2009-03-18T19:29:00.000Z","contents":"A small point: Actually, DHJ(1,3) follows almost trivially just from Sperner (as I’m sure we noted back in the 1-200 thread). So you can get to VM-DHJ(1,3) rather quickly. About DHJ(2,3), I’m still thinking…"},{"username":"ryan-odonnell","timestamp":"2009-03-18T19:51:00.000Z","contents":"1054.2\\. DHJ(3) for 12-sets. Oh, it’s [already on the wiki](http://michaelnielsen.org/polymath1/index.php?title=DHJ(1%2C3)#An_easy_version_of_the_problem). Hmm. Are we done?"},{"username":"ryan-odonnell","timestamp":"2009-03-18T20:02:00.000Z","contents":"1054.3\\. Okay, so just to recap. The following statements all refer to equal-nondegenerate-slices measure: (a) DHJ(3) for 12-sets is an easy consequence of Sperner (see 1054.2). (b) I claim that it is not too hard to Varnavides-ise, Multidimensional-ise, and indeed Varna+Multidim-ise DHJ(3) for 12-sets (using known arguments of Tim and Terry I think that are already on the wiki, but which can be streamlined a bit if you work in equal-nondegenerate-slices) (c) Then, assuming VM-DHJ(3) for 12-sets, use the argument following “Now for the main point of this post:” in 1054. If someone is able to agree with (c), I will endeavour to wikify (b) a bit more clearly."},{"username":"ryan-odonnell","timestamp":"2009-03-18T21:04:00.000Z","contents":"1054.4\\. I’m slightly nervous about the dog-chasing-its-own-tail aspect of the parameters here; you know, getting too small an increment on too small a subspace. Let me think about why it’s okay in the original proof sketch."},{"username":"gowers","timestamp":"2009-03-18T21:25:00.000Z","contents":"1054.5 I see better what you’re saying now. One detail troubles me, which is that the density increase you get on the subspace depends on the dimension of the subspace, which definitely isn’t the case in the other argument, but I haven’t yet decided whether that’s a serious problem or just an interesting detail."},{"username":"gowers","timestamp":"2009-03-18T21:39:00.000Z","contents":"1054.6 The more I think about it, the more I think that it is a problem. E.g. if on passing to an m-dimensional subspace you get a density increase of %2^{-m}% it seems to me that there’s no way of getting the density up to 1."},{"username":"ryan-odonnell","timestamp":"2009-03-18T22:50:00.000Z","contents":"Right, it seems to be a problem. But I kind of like the flavour of this angle; I wonder if there’s a way to make it work… Probably won’t get a chance to think about it today though."},{"username":"gowers","timestamp":"2009-03-18T22:53:00.000Z","contents":"It would be great if you could simplify the argument. I’m busy wikifying an argument that places some limits on any proof that can possibly work — I hope it will narrow down and therefore speed up the search."},{"username":"ryan-odonnellrandall","timestamp":"2009-03-18T23:47:00.000Z","contents":"Yes, Tim, I’m starting to see what you say, that this “inner increment/partition argument” is necessary. Without thinking about it too carefully, let me ask the following question: in the [general partitioning argument](http://michaelnielsen.org/polymath1/index.php?title=A_general_partitioning_principle), it seems to me that the “Subspace Richness” property tends to follow in general just from the “Line-Having” property, by the standard Multidimensional-ising and Varnavides-ising (perhaps one needs some of the other properties here; for example, I’ve only bothered to convince myself for 1-sets and 12-sets). If so, I guess the overall argument could go (or rather, _is_ going): DHJ(2) (Sperner) => 12-sets in %[3]^n% have Lines => 12-sets in %[3]^n% are Subspace Rich (VM-ise) => 12-sets in %[3]^n% can be partitioned => DHJ(3) (because Line-free sets correlate with 12-sets) Now presumably the argument continues: DHJ(3) => “lower complexity” (??) sets in %[4]^n% have Lines => lower complexity sets in %[4]^n% are Subspace Rich => lower complexity sets in %[4]^n% sets are partitionable => DH(4)… Sorry, I guess this is already more or less on the wiki already and I’m just thinking out loud / catching up. Is this right? I confess I have trouble following these outlines (partly because I just think rather slowly) but I think both proofs of the part in question that we have, Tim’s via almost-partitioning a 1-set into subspaces and Terry’s via dense fibers (really, these are essentially the same proof) proceed via…well…partitioning a 1-set into subspaces. (This is of course a tautology in the case of Tim’s argument, but it’s true of Terry’s as well because the greedy algorithm he uses may in general throw away almost the entire 1-set in question before it locates a suitable set of “maximal density” or drives the density on the remaining part to 1.) More generally, I tend to think Tim is right than any argument that is going to work at all has got to be powerful enough to partition a 1-set into subspaces, so you might as well do exactly that because it’s much easier to follow the proof in that case anyway. (I at least find Tim’s argument easier to follow than Terry’s, despite the fact that Terry’s is closer in spirit to the ergodic heuristic…and also despite the fact that they are essentially the same argument, of course.)"},{"username":"randall","timestamp":"2009-03-19T21:18:00.000Z","contents":"I confess I have trouble following these outlines (partly because I just think rather slowly) but I think both proofs of the part in question that we have, Tim’s via almost-partitioning a 1-set into subspaces and Terry’s via dense fibers (really, these are essentially the same proof) proceed via…well…partitioning a 1-set into subspaces. (This is of course a tautology in the case of Tim’s argument, but it’s true of Terry’s as well because the greedy algorithm he uses may in general throw away almost the entire 1-set in question before it locates a suitable set of “maximal density” or drives the density on the remaining part to 1.) More generally, I tend to think Tim is right than any argument that is going to work at all has got to be powerful enough to partition a 1-set into subspaces, so you might as well do exactly that because it’s much easier to follow the proof in that case anyway. (I at least find Tim’s argument easier to follow than Terry’s, despite the fact that Terry’s is closer in spirit to the ergodic heuristic…and also despite the fact that they are essentially the same argument, of course.)"},{"username":"ryan-odonnellryan-odonnellgowersryan-odonnell","timestamp":"2009-03-20T03:25:00.000Z","contents":"Wikifying. Just wanted to say I’ve started trying to build a [wiki page](http://michaelnielsen.org/polymath1/index.php?title=Proof_of_DHJ%283%29_via_density-increment) devoted to giving the all-details-filled-in version of the density-increment proof of DHJ(3). I’ll report back when I’ve fleshed it out some. Reader, you too should feel free to flesh it out, of course! Ryan, one aspect of the outline you’ve got on the wiki is different, though perhaps not truly different, from how it is at the moment. You deduce that an ij-insensitive set is subspace rich from the fact that it has subspaces, whereas what we have at the moment deduces it from the fact that a dense subset of %{}[2]^n% is subspace rich. In both cases the proof is that we can express the measure as a positive linear combination (plus an error if necessary, but I think you’ve managed to avoid that) of suitable measures on subspaces. The way you do it, you need to add the extra remark that the restriction of an ij-insensitive set to a subspace is still an ij-insensitive set. I haven’t checked, but I think it may be that there is a sort of commutative diagram that allows one to see that the two proofs are exactly the same, in the sense that there’s a one-to-one correspondence between the sets they consider. If that is the case, then I find it slightly more natural to prove the subspace richness in %{}[2]^n% and then lift it up to ij-insensitive sets in %{}[3]^n%. 1056.3\\. Tim: I agree with everything you wrote 100%, and in fact at some point I independently decided that it made more sense to commute those two steps: i.e., first show that dense sets in %[2]^n% are subspace rich, and then pull up to ij-insensitive sets in %[3]^n%. Besides being arguably more natural, doing it this way has a small benefit: If we really really care, we can use the Gunderson-Rodl-Sidorenko argument to get that dense sets in %[2]^n% have large subspaces, and this paper has way better quantitative parameters than the lines->subspaces argument we use in general. (On this point, I should note that the alternative-to-GRS lines->subspaces deduction I put on the wiki a while ago was extremely wrong, so I deleted it. At some point we should see if we can fix it; if GRS can get civilised parameters, hopefully we can also get civilised parameters for lines-in-%[3]^n% -> subspaces-in-%[3]^n%.)"},{"username":"ryan-odonnell","timestamp":"2009-03-20T03:26:00.000Z","contents":"Reader, you too should feel free to flesh it out, of course!"},{"username":"gowers","timestamp":"2009-03-20T21:20:00.000Z","contents":"Ryan, one aspect of the outline you’ve got on the wiki is different, though perhaps not truly different, from how it is at the moment. You deduce that an ij-insensitive set is subspace rich from the fact that it has subspaces, whereas what we have at the moment deduces it from the fact that a dense subset of %{}[2]^n% is subspace rich. In both cases the proof is that we can express the measure as a positive linear combination (plus an error if necessary, but I think you’ve managed to avoid that) of suitable measures on subspaces. The way you do it, you need to add the extra remark that the restriction of an ij-insensitive set to a subspace is still an ij-insensitive set. I haven’t checked, but I think it may be that there is a sort of commutative diagram that allows one to see that the two proofs are exactly the same, in the sense that there’s a one-to-one correspondence between the sets they consider. If that is the case, then I find it slightly more natural to prove the subspace richness in %{}[2]^n% and then lift it up to ij-insensitive sets in %{}[3]^n%."},{"username":"ryan-odonnell","timestamp":"2009-03-21T03:03:00.000Z","contents":"1056.3\\. Tim: I agree with everything you wrote 100%, and in fact at some point I independently decided that it made more sense to commute those two steps: i.e., first show that dense sets in %[2]^n% are subspace rich, and then pull up to ij-insensitive sets in %[3]^n%. Besides being arguably more natural, doing it this way has a small benefit: If we really really care, we can use the Gunderson-Rodl-Sidorenko argument to get that dense sets in %[2]^n% have large subspaces, and this paper has way better quantitative parameters than the lines->subspaces argument we use in general. (On this point, I should note that the alternative-to-GRS lines->subspaces deduction I put on the wiki a while ago was extremely wrong, so I deleted it. At some point we should see if we can fix it; if GRS can get civilised parameters, hopefully we can also get civilised parameters for lines-in-%[3]^n% -> subspaces-in-%[3]^n%.)"},{"username":"ryan-odonnell","timestamp":"2009-03-21T03:05:00.000Z","contents":"Wikifying. Just wanted to report that I’ve gotten about 50% of the way through making the aforementioned [stub article](http://michaelnielsen.org/polymath1/index.php?title=Proof_of_DHJ(3)_via_density-increment) for the fleshed-out proof. It’s currently an orphan page on the wiki; when/if I get to 90%+ I’ll put a link to it on the main page."},{"username":"gil","timestamp":"2009-03-24T14:22:00.000Z","contents":"While following, and cheering the emerging proof on the wiki, and resting a little from the serious efforts that took place here over the last 6 weeks or so (and I hope there will be a follow up math discussion (not just meta discussion) for a couple of weeks even after the success,) can I offer to amuse ourselves with the silliest spin-off problem that was offered in these discussions: (naturally, by me): What is the size of the largest family of subsets of {1,2,…,n} without two sets A and B so that A\\B is twice as large as B\\A. Any slice will do and we can (as pointed out by NK) add to the middle slice also silices of size n/4, n/8 etc. Can we do better?"},{"username":"kristal-cantwell","timestamp":"2009-03-24T21:58:00.000Z","contents":"Hyper-optimistic conjecture Could the Hyper-optimistic conjecture be extended beyond 3, and if so if it is true would it imply DHJ(n) for higher values of n. For example if it is true for 4 would we have DHJ(4)?"},{"username":"gil","timestamp":"2009-03-25T03:05:00.000Z","contents":"hyper-optimism  \nGreat question. I looked at the wiki and did not find it stated beyond 3\\. I also do not see off hand what it will give."},{"username":"kristal-cantwellgowers","timestamp":"2009-03-26T00:00:00.000Z","contents":"Corners theorem Could the corner’s theorem be extended to higher dimensions? Has it been already? I think that such a result might be useful in trying to get DHJ(n)  \nfrom the hyper-optimistic conjecture for values of n greater than three.  \nEven without the corner’s lemma if we could get the grid a+b+c+d=n  \nof quadruples (a,b,c,d) with density delta contains (a+r,b,c,d),(a,b+r,c,d),  \n(a,b,c+r,d),(a,b,c,d+r) that and its extension into n might be useful in a possible proof. The corners theorem is known in all dimensions, either by ergodic theory or as a consequence of hypergraph regularity. (It also follows from DHJ but obviously that’s no help if you want to use it to prove DHJ.) The hyperoptimistic conjecture can be formulated for any k, and I do not know of any counterexamples. The result you talk about with the grid is equivalent to the 3D corners theorem, if I interpret correctly what you are saying. I recently tried to come up with a lower bound for DHJ(3) that would beat the Behrend bound applied to slices, but had absolutely no luck. But I’d be amazed if the hyperoptimistic conjecture turned out to be true …"},{"username":"gowers","timestamp":"2009-03-27T23:27:00.000Z","contents":"The corners theorem is known in all dimensions, either by ergodic theory or as a consequence of hypergraph regularity. (It also follows from DHJ but obviously that’s no help if you want to use it to prove DHJ.) The hyperoptimistic conjecture can be formulated for any k, and I do not know of any counterexamples. The result you talk about with the grid is equivalent to the 3D corners theorem, if I interpret correctly what you are saying. I recently tried to come up with a lower bound for DHJ(3) that would beat the Behrend bound applied to slices, but had absolutely no luck. But I’d be amazed if the hyperoptimistic conjecture turned out to be true …"},{"username":"gowersryan-odonnelljozsefterence-tao","timestamp":"2009-03-27T23:29:00.000Z","contents":"Metacomment. What are people’s views about writing up? Ryan wanted to wait, and I know others feel that there isn’t a big hurry. I myself would very much like to see a complete write-up, and would like to be closely involved in it, but I don’t mind waiting a bit more because I’m pretty busy over the next couple of weeks. I thought I’d ask though. For example, if I were to produce a first attempt at a plan of a paper, would that be welcomed, or would the other potential writers rather I didn’t? Hi Tim. Yes, I did suggest waiting a bit, but it seems like we all collectively have done so, perhaps unintentionally. I too have sorely needed to catch up on some other projects, which won’t be wrapped up till April 2\\. I still hope to finish the wiki outline I started. In any case, I would be fine with the idea of writing beginning in a week or two or three, whenever we get freer. I can contribute some writing, or Tim if you want to, please do so. I’d be happy with a model wherein anybody who felt they had time to write could just notify the others that they’re doing so. Tim, if you or someone manages to put out a full draft at some point, why not? It would be great if Tim could write up the proof. I’m trying to write a toy version of the proof for corners on the Cartesian product AxA, where A is a Hilbert cube. A few weeks ago my first try didn’t go well as I followed the original Ajtai-Szemeredi proof. I knew it quite well and didn’t pay attention to Tim’s modified proof. Now I see that actually the key of the DHJ3 proof is hiding there. So, I advise the interested reader to compare Tim’s version to the original, even if one knows the Ajtai-Szemeredi proof very well. The two wiki entries are:  \n“Ajtai-Szemerédi’s proof of the corners theorem”  \n[http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemer%C3%A9di%27s_proof_of_the_corners_theorem](http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemer%C3%A9di%27s_proof_of_the_corners_theorem) and “A Modification of the Ajtai-Szemerédi argument”  \n[http://michaelnielsen.org/polymath1/index.php?title=Modification_of_the_Ajtai-Szemer%C3%A9di_argument](http://michaelnielsen.org/polymath1/index.php?title=Modification_of_the_Ajtai-Szemer%C3%A9di_argument) I plan to revisit the writeups I have of the triangle removal proof and the Furstenberg-Katznelson proof, and clean them up – right now I get the sense that they’re not really readable by anyone other than myself. But it does seem like the optimal strategy is to move slowly, in case some simplifications crop up that can be taken advantage of. For instance, I found it very useful in those two proofs to play around with substitution maps such as %\\rho_{1 \\to 2}^I: [3]^n \\to [3]^n% (defined as the map that takes a string x in %[3]^n% and changes all the 1s in positions in I to 2s) in order to build lines (e.g. (x, %\\rho_{1 \\to 2}^I(x)%, %\\rho_{1 \\to 3}^I(x)% is a line whenever x has at least one 1 in I). This might help with some of the arguments in the density-increment argument, though I haven’t checked it."},{"username":"ryan-odonnell","timestamp":"2009-03-28T04:20:00.000Z","contents":"Hi Tim. Yes, I did suggest waiting a bit, but it seems like we all collectively have done so, perhaps unintentionally. I too have sorely needed to catch up on some other projects, which won’t be wrapped up till April 2\\. I still hope to finish the wiki outline I started. In any case, I would be fine with the idea of writing beginning in a week or two or three, whenever we get freer. I can contribute some writing, or Tim if you want to, please do so. I’d be happy with a model wherein anybody who felt they had time to write could just notify the others that they’re doing so. Tim, if you or someone manages to put out a full draft at some point, why not?"},{"username":"jozsef","timestamp":"2009-03-30T19:43:00.000Z","contents":"It would be great if Tim could write up the proof. I’m trying to write a toy version of the proof for corners on the Cartesian product AxA, where A is a Hilbert cube. A few weeks ago my first try didn’t go well as I followed the original Ajtai-Szemeredi proof. I knew it quite well and didn’t pay attention to Tim’s modified proof. Now I see that actually the key of the DHJ3 proof is hiding there. So, I advise the interested reader to compare Tim’s version to the original, even if one knows the Ajtai-Szemeredi proof very well. The two wiki entries are:  \n“Ajtai-Szemerédi’s proof of the corners theorem”  \n[http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemer%C3%A9di%27s_proof_of_the_corners_theorem](http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemer%C3%A9di%27s_proof_of_the_corners_theorem) and “A Modification of the Ajtai-Szemerédi argument”  \n[http://michaelnielsen.org/polymath1/index.php?title=Modification_of_the_Ajtai-Szemer%C3%A9di_argument](http://michaelnielsen.org/polymath1/index.php?title=Modification_of_the_Ajtai-Szemer%C3%A9di_argument)"},{"username":"terence-tao","timestamp":"2009-04-03T06:07:00.000Z","contents":"I plan to revisit the writeups I have of the triangle removal proof and the Furstenberg-Katznelson proof, and clean them up – right now I get the sense that they’re not really readable by anyone other than myself. But it does seem like the optimal strategy is to move slowly, in case some simplifications crop up that can be taken advantage of. For instance, I found it very useful in those two proofs to play around with substitution maps such as %\\rho_{1 \\to 2}^I: [3]^n \\to [3]^n% (defined as the map that takes a string x in %[3]^n% and changes all the 1s in positions in I to 2s) in order to build lines (e.g. (x, %\\rho_{1 \\to 2}^I(x)%, %\\rho_{1 \\to 3}^I(x)% is a line whenever x has at least one 1 in I). This might help with some of the arguments in the density-increment argument, though I haven’t checked it."},{"username":"kristal-cantwell","timestamp":"2009-03-29T04:38:00.000Z","contents":"Hyper-optimistic conjecture I think I can get the hyper-optimistic conjecture implies DHJ(k).  \nI have to assume I have DHJ(k) for density is equivalent for DHJ(k)  \nfor equal slices density as in k=3\\. Then the corners theorem makes the  \ndensity of slices a without corners arbitrarily low then the hyper-optimistic conjecture transfers the small size to the equal slices density and the logical equivalence makes the density arbitrarily low. So If I start with  \ndensity epsilon I can find a large enough n such that the corner free space has density much less than epsilon and that means by the hyper-optimistic conjecture the equal slices density is much less than epsilon and hence by logical equivalence we have the regular density must be low and we are done."},{"username":"gil-kalaigil","timestamp":"2009-03-30T16:41:00.000Z","contents":"Finer slice decompositions. There are “finer decompositions” (compared to slices) of %\\{0,1,2\\}^n% that can also plays a role. For example, we can consider vectors woth a ‘0’, b ‘1’ so that the sum of indices of coordinates with value ‘0’ is x, and the sum of indices of coordinates with value 1 is y. If we denote these vectors by %\\Gamma _{a,b;x,y}% we get a finer slice decomposition which can be of interest. hmm, when we consider union of such finer slices %\\Gamma_{a,b,c;x,y,z}% (or perhaps a different forms of slicing) there are several issues that seems relevant. (But they do not suggest that union of such fine slices will give better constructions.) One thing is this: Consider a subset of {1,2,…,m} %\\times% {1,2,…,k} (where km=n) with out 3 terms A P. Can you get examples with higher densities than for {1,2,…,n}? (lets assume k=m for simplicity) (Probably this is known to experts.) Another thing is that there are various extensions of Sperner lemma which show that under more general conditions you get the same bound. One is the 2-part Sperner lemma of Katona. Another is Kleitman’s solution of Littlewood-Offord problem. So if we think its fruitful to regard DHJ(3) as extension of DHJ(2). Is there an analog for Kleitman’s theorem?"},{"username":"gil","timestamp":"2009-03-31T00:16:00.000Z","contents":"hmm, when we consider union of such finer slices %\\Gamma_{a,b,c;x,y,z}% (or perhaps a different forms of slicing) there are several issues that seems relevant. (But they do not suggest that union of such fine slices will give better constructions.) One thing is this: Consider a subset of {1,2,…,m} %\\times% {1,2,…,k} (where km=n) with out 3 terms A P. Can you get examples with higher densities than for {1,2,…,n}? (lets assume k=m for simplicity) (Probably this is known to experts.) Another thing is that there are various extensions of Sperner lemma which show that under more general conditions you get the same bound. One is the 2-part Sperner lemma of Katona. Another is Kleitman’s solution of Littlewood-Offord problem. So if we think its fruitful to regard DHJ(3) as extension of DHJ(2). Is there an analog for Kleitman’s theorem?"},{"username":"kristal-cantwell","timestamp":"2009-03-31T10:22:00.000Z","contents":"Hyper-optimistic conjecture Apparently this is not true for the generalization for 4 see post  \n1185 in the thread [http://terrytao.wordpress.com/2009/03/14/dhj3-1100-1199-density-hales-jewett-type-numbers/](http://terrytao.wordpress.com/2009/03/14/dhj3-1100-1199-density-hales-jewett-type-numbers/) Congratulations to Klas Markström for finding the counterexample for this."},{"username":"gil-kalai","timestamp":"2009-03-31T14:11:00.000Z","contents":"more on the analogy with Sperner We want to find a set avoiding a configuration C inside a large set X. If we can show that for some subset Y of X the maximum density of a C-avoiding subset is at most c|Y| then usually this is good. When there is a group acting like in the cap set problem we can show the same bound for subsets of X that avoid C. But even if there is no group action this can still be useful. For Sperner the basic observation is that if we take Y to be a chain then a line-free subset of Y contains at most 1 element. So the density of line-free subsets of chain is very small and this can be pushed to various proofs. So the question is (and we discussed it more or less in these threads, i just repeat it): is there a good analog for “chains” for DHJ(3). A small configuration (and I do not know if small is in the order of 1 or of 2^n up to poly(n)) on which we can show (or at least conjecture) that the density of line-avoiding subset must be small. (Thinking about combinatorial subspaces is sort of “cheating” and in any case not the analog for the case of Sperner.) Simple questions to state are: Is there any subset Y of {0,1,2}^n on which we can show (more easily) that the maximum density of line-free subsets is small. Is there a subset Y where this maximum density is expected to be smaller than that for the entire {0,1,2}^n?"},{"username":"terence-tao","timestamp":"2009-04-03T06:02:00.000Z","contents":"Metacomment. I gave a talk at UCLA on the three combinatorial proofs (or proof sketches) we have, namely the density increment-based proof, the triangle-removal proof (based on Austin’s ergodic proof), and the finitary Furstenberg-Katznelson proof, with some notes at [http://terrytao.wordpress.com/2009/04/02/polymath1-and-three-new-proofs-of-the-density-hales-jewett-theorem/](http://terrytao.wordpress.com/2009/04/02/polymath1-and-three-new-proofs-of-the-density-hales-jewett-theorem/)"},{"username":"gowers","timestamp":"2009-04-04T21:29:00.000Z","contents":"Writing up. Here’s a possible structure for a write-up of the density-increment argument. I’m planning to make a start on this, but I’ll try to make it as modular as possible so that if people want to then it may be possible, without a major rewrite of the whole thing, (i) to make local changes and (ii) to switch the order about. \\author{Polymath}  \n\\address{The world} [Not, of course, intended to be taken seriously.] \\begin{document} \\title{A new proof of the density Hales-Jewett theorem} \\maketitle \\section{Introduction} Basic definitions and statement of the theorem. History of and motivation for the problem. Discussion of related results, including the corners theorem. \\section{A sketch proof of the corners theorem} A fairly detailed sketch of the modified Ajtai-Szemer\\’edi argument. \\section{Different measures on $[k]^n$.} Definition of equal-slices measure and the P\\’olya urn measure. Motivation for considering these other measures. Proofs that dense sets in any of those two measures or the uniform  \nmeasure can be restricted to subspaces where they retain their  \ndensity in any other of the measures. \\section{Three important lemmas} \\subsection{The multidimensional Sperner theorem.} \\subsection{A dense line-free set correlates locally with a 12-set.} \\subsection{A 23-insensitive set can be almost entirely partitioned into  \nfairly large combinatorial subspaces.} \\section{A proof of the theorem for $k=3$.} \\section{A proof of the general theorem} \\subsection{Uniform DHJ(k-1) implies that an equal-slices-dense  \nsubset of $[k-1]^n$ contains many combinatorial subspaces.} \\subsection{Putting everything together.} \\end{document}"},{"username":"jozsef","timestamp":"2009-04-05T10:55:00.000Z","contents":"Dear Tim, I still don’t have enough time to work out the details of the proof. Next week is the last week of classes here, so I will make some progress after that, I hope. Until then let me ask a question about the second outline of a density-increment argument. I don’t understand the argument after Substep 2.2\\. In each step you keep a %3^m% “portion” of Z and partition the remaining B into %\\sim 3^M% classes. Elements which partitioned into different classes will never be considered together in a subspace again. What I can’t see is that why will be the intersection of the 1-set and the 2 set large in a subspace? Isn’t it possible that only a small subset of the 1-set is element of a subspace %S_i%? Most likely I overlooked something. I will check it tomorrow again."},{"username":"jozsefgowersgowersjozsefjozsefgowersjozsef","timestamp":"2009-04-05T21:00:00.000Z","contents":"(contd.) I still can’t see how to finish the sequence of iterations. The question is that what happens when Substep 2.3 ends. Given this rapidly growing family of partitions. If a partition is sparse then we don’t do anything with it, if dense AND large enough, then we find a subspace in it. What happens at the end when we have many small and dense partition classes? I’m stucked. I’ll get my morning coffee now… I don’t know if this fully answers your question, but when we have a dense set, we don’t just find a subspace in it — we find a whole family of subspaces whose union is dense. Therefore, the number of steps in the iteration is bounded. But I’m answering this question without looking at the proof — I’ll give another answer in a moment. I’ve had another look, and I still don’t quite understand what your difficulty is, so I’ll have to hope that my first reply answers it, or at least prompts you to explain what it is in terms that I understand better. During the iterations if a %B_i% is sparse, the you do nothing with it, if it is dense then you choose %Z_1,\\ldots , Z_m%. The union of them, Z, has size at most M. It works fine until %B_i% has size at least M. What do you do with smaller %B_i%-s? I don’t see that this error term – the number of elements remaining in small %B_i-s% is small. Maybe I didn’t follow the right algorithm; Do we have a good bound on the number of iterations? I think that now I see; For any element, there is a very high probability that after a few iterations this elements in a selected subspace. Right? I still don’t understand, but largely for “local” reasons. For instance, %B_i% is a subset of a subspace %S_i.% Since the number of iterations is bounded, we can ensure that the dimension of %S_i% is much larger than %M% all the time. When you talk about the cardinality of %B_i,% do you actually mean the dimension of %S_i,% and when you say “at least” do you actually mean “less than”? Also, it’s not clear to me whether your problem arises when we almost partition a 1-set, or only when we look at the intersection of a 1-set with a 2-set. Assuming it’s the first, then here is the rough idea of the argument. We begin by choosing %M% coordinates and partitioning %[3]^n% into %3^M% subspaces according to those coordinates. We then choose %Z_1,\\dots,Z_m% inside the %M% chosen coordinates such that for many %x% the subspace %x+\\langle Z_1,\\dots,Z_m\\rangle% is a subset of %\\mathcal{B}.% Then we remove all subspaces of the form %x+\\langle Z_1,\\dots,Z_m\\rangle% from %\\mathcal{B}.% This removes a positive fraction (depending on %m% and the error %\\eta% you want to end up with) of the elements of %\\mathcal{B}.% Also, one can check that after removing these elements, the restriction of the remaining set to any of the %3^M% subspaces %S_i% is still a 1-set. Therefore, we can iterate, except that we do nothing if we have a set of density less than %\\eta/2% in a given subspace. Let us call such sets _discarded_. The total measure of the discarded sets cannot be more than %\\eta/2,% since they are contained in disjoint subspaces inside which they have relative density at most %\\eta/2.% After a bounded number %R% of iterations (depending on %m% and %\\eta%) we have either discarded or put into subspaces all but at most %\\eta/2% of %\\mathcal{B},% and there is always space to do this provided that %RM% is substantially less than %n.% So provided %n% is large enough, the argument works. I’ve written that in the hope that you either agree with it or can point to the bit where I seem to be going wrong. (Even if it is wrong, I have theoretical reasons for believing that some argument like this has to work, at least if Terry’s argument about correlations with 1-sets implying correlations with subspaces was correct, which it seemed to be.) Tim, first of all I agree that the original argument works even for an incomplete partitioning. I don’t think that the argument is incomplete, I’m just slow following it. In this particular case, I think that I didn’t see the good bound on the number of iterations, I though that the fast growing number of partition classes might be a problem."},{"username":"gowers","timestamp":"2009-04-05T21:57:00.000Z","contents":"I don’t know if this fully answers your question, but when we have a dense set, we don’t just find a subspace in it — we find a whole family of subspaces whose union is dense. Therefore, the number of steps in the iteration is bounded. But I’m answering this question without looking at the proof — I’ll give another answer in a moment."},{"username":"gowers","timestamp":"2009-04-05T22:02:00.000Z","contents":"I’ve had another look, and I still don’t quite understand what your difficulty is, so I’ll have to hope that my first reply answers it, or at least prompts you to explain what it is in terms that I understand better."},{"username":"jozsef","timestamp":"2009-04-05T22:28:00.000Z","contents":"During the iterations if a %B_i% is sparse, the you do nothing with it, if it is dense then you choose %Z_1,\\ldots , Z_m%. The union of them, Z, has size at most M. It works fine until %B_i% has size at least M. What do you do with smaller %B_i%-s? I don’t see that this error term – the number of elements remaining in small %B_i-s% is small."},{"username":"jozsef","timestamp":"2009-04-05T22:42:00.000Z","contents":"Maybe I didn’t follow the right algorithm; Do we have a good bound on the number of iterations? I think that now I see; For any element, there is a very high probability that after a few iterations this elements in a selected subspace. Right?"},{"username":"gowers","timestamp":"2009-04-05T22:48:00.000Z","contents":"I still don’t understand, but largely for “local” reasons. For instance, %B_i% is a subset of a subspace %S_i.% Since the number of iterations is bounded, we can ensure that the dimension of %S_i% is much larger than %M% all the time. When you talk about the cardinality of %B_i,% do you actually mean the dimension of %S_i,% and when you say “at least” do you actually mean “less than”? Also, it’s not clear to me whether your problem arises when we almost partition a 1-set, or only when we look at the intersection of a 1-set with a 2-set. Assuming it’s the first, then here is the rough idea of the argument. We begin by choosing %M% coordinates and partitioning %[3]^n% into %3^M% subspaces according to those coordinates. We then choose %Z_1,\\dots,Z_m% inside the %M% chosen coordinates such that for many %x% the subspace %x+\\langle Z_1,\\dots,Z_m\\rangle% is a subset of %\\mathcal{B}.% Then we remove all subspaces of the form %x+\\langle Z_1,\\dots,Z_m\\rangle% from %\\mathcal{B}.% This removes a positive fraction (depending on %m% and the error %\\eta% you want to end up with) of the elements of %\\mathcal{B}.% Also, one can check that after removing these elements, the restriction of the remaining set to any of the %3^M% subspaces %S_i% is still a 1-set. Therefore, we can iterate, except that we do nothing if we have a set of density less than %\\eta/2% in a given subspace. Let us call such sets _discarded_. The total measure of the discarded sets cannot be more than %\\eta/2,% since they are contained in disjoint subspaces inside which they have relative density at most %\\eta/2.% After a bounded number %R% of iterations (depending on %m% and %\\eta%) we have either discarded or put into subspaces all but at most %\\eta/2% of %\\mathcal{B},% and there is always space to do this provided that %RM% is substantially less than %n.% So provided %n% is large enough, the argument works. I’ve written that in the hope that you either agree with it or can point to the bit where I seem to be going wrong. (Even if it is wrong, I have theoretical reasons for believing that some argument like this has to work, at least if Terry’s argument about correlations with 1-sets implying correlations with subspaces was correct, which it seemed to be.)"},{"username":"jozsef","timestamp":"2009-04-05T23:04:00.000Z","contents":"Tim, first of all I agree that the original argument works even for an incomplete partitioning. I don’t think that the argument is incomplete, I’m just slow following it. In this particular case, I think that I didn’t see the good bound on the number of iterations, I though that the fast growing number of partition classes might be a problem."},{"username":"jozsef","timestamp":"2009-04-05T23:35:00.000Z","contents":"So, what you proved is a strong partition in the following sense; For every %c>0, \\epsilon >0% and m, there is a number K, that any c-dense 1-set has a large, %1-\\epsilon%-dense subset which is the disjoint union of (several) translates of K m-dimensional subspaces."},{"username":"ryan-odonnell","timestamp":"2009-04-07T02:23:00.000Z","contents":"Hi Tim, re the outline in 1065: Looks good. One question: It looks like you’ve divided the proof into three main lemmas: multidim-Sperner (more generally, multidim-DHJ(k-1)), line-free set correlating with intersections of ij-insensitive sets, and ij-insensitive sets being partitionable. It seems to me that the Varnavides-version of multidim-Sperner (more generally, multidim-DHJ(k-1)) may as well be considered the basic lemma. Where will this go? Putting it into \\subsection{The multidimensional Sperner theorem} makes sense to me, although then the actual \\section{A proof of the theorem for $k=3$.} might be quite short. On the other hand, if it goes into the proof section itself, then the multidim-Sperner therem subsection will be awfully short (might as well just quote Gunderson-Rodl-Sidorenko). The latter seems less modular to me, so I guess what I’m ultimately suggesting is that \\subsection{The multidimensional Sperner theorem} be more like \\subsection{The Varnavides multidimensional Sperner theorem}. Except that I strongly vote for using a more generic descriptor than “Varnavides”. There’s got to be a catchy word that indicates to the reader that not only do dense sets contain lines, a _random_ line is in there with positive probability. — Also, I’m still of two minds as to whether “Equal-Slices” should be treated as the main distribution, with Polya as a slight variant, or vice versa."},{"username":"ryan-odonnell","timestamp":"2009-04-07T02:34:00.000Z","contents":"Notation and terminology. Pre-writing, it might be helpful to fix our notation and terminology. I know it’s not crucial, but I think getting just the write names and letters can really help a reader. We may also try to keep things switchable via appropriate use of macros and the xspace package in LaTeX (see the first two comments to the post here: [http://terrytao.wordpress.com/2007/06/08/advice-on-mathematical-careers-and-mathematical-writing/](http://terrytao.wordpress.com/2007/06/08/advice-on-mathematical-careers-and-mathematical-writing/)), although I must confess I sometimes find it tough to rigorously stick to using macros. Anyway, here are some questions we might debate: . are sets called $A$ or $\\mathcal{A}$?  \n. what should “Equal-Slices” measure actually be called (e.g., do probabilists already call this something else?)  \n. what “Polya Urn” measure should actually be called? (same question)  \n. what letter to use for each of them? what letter to use for the uniform distribution?  \n. how to denote drawing a random _line_ or a random _subspace_ from them?  \n. [k] = {0, 1, …, k-1} or {1, 2, …, k}?  \n. unify terminology of ij-set, (special) set of complexity t, ij-insensitive set, etc.  \n. questions from the previous post: what short words/phrases can we use to indicate that not only do dense subsets of [3]^n contain lines, they actually contain large-dimensional subspaces, and in fact a random large-dimensional subspace is in with positive probability?"},{"username":"terence-tao","timestamp":"2009-04-10T01:09:00.000Z","contents":"Hi, I transferred Tim’s outline (and Ryan’s questions) to the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Outline_of_first_paper](http://michaelnielsen.org/polymath1/index.php?title=Outline_of_first_paper) I figure that the wiki format might be better for the topmost level of organising the paper, and then we can switch to LaTeX once all the top-level stuff is finalised. Some thoughts on notation: I guess, on balance, that [k]={1,…,k} looks slightly nicer than [k]={0,…,k-1}; the 0-based notation is slightly more “logical”, but we don’t seem to derive any substantial benefit from it. So I’m weakly in favour of {1,…,k}. We can borrow from geometry and use the notation Gr( [k]^n, d ) to denote the d-dimensional subspaces of [k]^n (a “combinatorial Grassmanian”). I don’t know what to call the measures on this space though. Does every measure %\\mu% on %[k]^n% canonically define a measure on %Gr( [k]^n, d )%? It seems to me that one needs some additional parameters to specify such a measure. “A” versus “%{\\mathcal A}%” – I would prefer A, as I want to think of a subset of %[k]^n% as a set of points, rather than a collection of sets (which is what the %{\\mathcal A}% notation suggests to me). The one problem with using A is that we are also likely to be using A for subsets of %[n]^2% in the corners theorem, and if we are going to discuss the reduction of corners to DHJ then there might be a very slight notational clash there. But perhaps this is actually a good thing, since we want to use the corners argument to motivate the DHJ one… As for the last question, what about “Hales-Jewett property” for containing lines, “subspace Hales-Jewett property” for containing subspaces, and “subspace Hales-Jewett-Varnavides property” for containing subspaces with positive probability? (thus, e.g. “dense ij-insensitive sets obey the subspace Hales-Jewett-Varnavides property”)?"},{"username":"ryan-odonnell","timestamp":"2009-04-14T18:26:00.000Z","contents":"I have collected up a small amount of free time and was planning to put some time into the writing-up. I agree that it will be nice to fix the overall outline before starting. Here are some more comments: . I propose continuing this discussion not here but on the [discussion page](http://michaelnielsen.org/polymath1/index.php?title=Talk:Outline_of_first_paper) of the wiki page Terry made for the [paper outline](http://michaelnielsen.org/polymath1/index.php?title=Outline_of_first_paper). I will copy this post to that page too. . Since I will likely never finish it, I put a link to [my old outline of the proof](http://michaelnielsen.org/polymath1/index.php?title=Proof_of_DHJ(3)_via_density-increment) on the wiki’s home page. I kind of like that outline too <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> although of course it’s quite similar to Tim’s. . Changed “author” from “Polymath” to “A. Polymath” (as Terry had suggested a while ago). Reasons: a) Automatic indexing systems probably like authors to have both a “first name” and a “last name”. This will help the author list to get sorted under “P”. b) The next projects could be authored by “B. Polymath”, “C. Polymath”, “ZZ. Polymath”, etc. This would also help get all projects sorted under “P”, yet differentiate the author groups. c) Subtle allusion to A. Nilli. <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> . Address: perhaps could change to the URL of the wiki. [Perhaps we should get a stabler one?] I suppose we’ll need a corresponding address for the paper-journal version though… . I agree with Terry’s votes for [k] = {1, …, k} and %A% rather than %\\mathcal{A}%. . I’ll admit it, I always find the terminology Grassmannian “scary”. If it were me I would just try to refer to “d-dimensional subspaces”. Not sure we need a name for the set <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> . Naming the distributions: I think the phrase “equal slices” is unbelievably catchy, and [this](http://www.google.com/search?q=equal+slices) is cute. On the other hand, it’s hard to deny the officialness of “random [ordered 3-partition](http://en.wikipedia.org/wiki/Ordered_partition_of_a_set)“. I mean, such terms are in Chapter 1 of Stanley <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> Either becomes a bit of a mouthful when you get to its sister distribution: “equal nondegenerate slices” or “ordered weak 3-partition” (“weak” = “parts of size 0 allowed”, apparently). So I don’t know… . For the letter, who knows? I’m kind of against %\\mu% as it sounds like the uniform distribution to me. %\\nu% is a nice letter for measures. Or maybe %\\lambda% (for “line”) or %\\pi% (for “partition”)… . For the last question, I know Varnavides’s name gets attached to this notion, but I must say it is the opposite of evocative for a nonexpert like me. It actually took me a good few hundred blog posts before I understood that “Varnavides = contains random lines w. positive prob.” Guided by Tim’s spirit of trying to make the paper as easy-to-read as possible, how about… “DHJ(3) = Nonnegligible sets in [3]^n contain a line.” Then:  \n“Nonnegligible sets in [3]^n contain a high-dimensional subspace.”  \n“Nonnegligible sets in [3]^n contain a nonnegligible _fraction_ of lines.”  \n“Nonnegligible sets in [3]^n contain a nonnegligible fraction of high-dimensional subspaces.” Wordy, perhaps, but I think it gets the idea across straight away. Also, using the same word “nonnegligible” subtly gets across the key fact that we’re actually talking about the _same_ distribution.. Or does this just sound too weird to an arithmetic combinatorialist’s ears?"},{"username":"gowers","timestamp":"2009-04-14T18:46:00.000Z","contents":"Ryan, I agree with almost all of what you say. My main quibble would be that I prefer “sets of positive density” to “nonnegligible sets”. Of course, that really means “density bounded away from 0 as n tends to infinity” but that kind of shorthand is very standard. So I’d end up with something more like “Sets of positive density in [3]^n contain a positive proportion of all lines/subspaces.” I’ve never liked Varnavides getting his name attached to a very standard averaging argument. My vote would be for “equal-slices” with a comment about how it relates to known terminology. I’m slightly more in favour of %\\mathcal{A}% than you and Terry, but I see arguments on both sides and so am perfectly willing to be outvoted on that one."},{"username":"terence-tao","timestamp":"2009-04-14T22:46:00.000Z","contents":"I modified the outline slightly to reflect the preceding discussion. I also added a sample abstract, but am not seriously committed to it, feel free to modify. I also added some queries, for instance: should we have a section explaining what Polymath is, the history of the project, and/or the names of contributors? I also added a discussion section where we can talk about quantitative bounds, relationships to other proofs of DHJ, and other sundry topics."},{"username":"ryan-odonnell","timestamp":"2009-04-15T01:49:00.000Z","contents":"I committed to giving a talk about this at Microsoft Research in 3 weeks, so I am thinking now more about the structure. A few thoughts that occurred to me after this morning’s work: . It wasn’t too productive to worry much about the name of the distribution, since I invariable referred to it by its letter anyway. I was writing %\\nu^n_k% for the distribution on %[k]^n%. . It was more convenient to think of the equal nondegenerate-slices distribution as %\\nu% and of equal-slices as the “variant”. This was because the nondegenerate version seems to show up more in the _statements_ of results, whereas the pure equal-slices seems to show up more in the grungy technical details of measure arguments. . It wasn’t too important to name the set of d-dimensional subspaces either, because the point of the distribution %\\nu% is that lines are identified with strings in %[4]^n%, and more generally, %d%-dimensional subspaces of %[k]^n% are identified with strings in %[k+d]^n%. . It may be convenient (I was doing this at one point) to write things like $\\latex \\nu_4^n(A) \\geq \\delta’$ to mean, “if one chooses a random line in %[3]^n%, it is entirely contained within %A% with probability at least %\\delta'%“. . I agree that “dense” is better than “nonnegligible”, after all. E.g., on my roughest sketches of slides, I was writing things like: “DHJ(3) = if %A \\subseteq [3]^n% with %\\nu_3(A) \\geq \\delta%, then $\\latex \\nu_4(A) > 0$; i.e., if %A% has constant %\\nu_3%-density then it has positive %\\nu_4%-density.” “DHJ(3) implies if %A \\subseteq [3]^n% with %\\nu_3(A) \\geq \\delta%, then $\\latex \\nu_4(A) \\geq \\delta’ = \\delta'(\\delta)$.” [the Varnavides version] “DHJ(3) implies if %A \\subseteq [3]^n% with %\\nu_3(A) \\geq \\delta%, then $\\latex \\nu_{3+d}(A) \\geq \\delta’ = \\delta'(\\delta,d)$.” [the Varnavides version for %d%-dimensional subspaces, etc.] To be continued…"},{"username":"ryan-odonnell","timestamp":"2009-04-15T01:58:00.000Z","contents":"Here is another structural idea that I was using in my slides draft — which might also be worth considering for the paper version: First, a section on probability distributions and the various technical lemmas. Second, a section showing how to deduce the contains-a-subspace, contains-a-constant-density-of-lines, and contains-a-constant-density-of-subpsaces versions of DHJ from the simple, contains-a-line version. Third, sections on the argument for ij-insensitive sets, partitioning these into subspaces, density increment, etc. The idea of this structure is that it kind of isolates the more “essential” ideas involved in pulling up DHJ(3) from DHJ(2) from some of the more generic stuff. I.e., a sophisticated reader of the paper might skim through the “second” section described above and say, “yeah, yeah, okay, I get the idea from the sketches, this all seems like straightforward tricks and things”, and feel like “nothing has happened yet” after the second section. This I think would be good; this feeling of “nothing has happened yet” is what I’d hope for. Then, when they settled in to read the third (and succeeding?) sections they’d feel like they were properly starting the argument — which is the feeling I’d actually like to convey."},{"username":"ryan-odonnell","timestamp":"2009-04-15T05:39:00.000Z","contents":"As a tiny point: It occurred to me that sequential ordering of initials — A., B., C., Polymath — will never work; it was hard enough to get comments numbered consistently. How would various groups known not to step on each other’s initials? Heck, for all I know the other, small-%n% thread may already have taken A. Polymath. Perhaps the initials could be chosen in a problem-centric way, then; I would of course propose “D.H.J. Polymath”."},{"username":"ryan-odonnell","timestamp":"2009-04-15T05:40:00.000Z","contents":"Also: Terry, the abstract you wrote looks good to me."},{"username":"gowers","timestamp":"2009-04-15T05:43:00.000Z","contents":"Ryan, I definitely like your “nothing has happened yet” approach. At best, we could get the technical stuff down to some easily comprehensible black boxes that would make the later proofs very streamlined, and I think that ideal may be attainable."},{"username":"ryan-odonnell","timestamp":"2009-04-15T20:05:00.000Z","contents":"I hope so too. By the way, it appears as though the blog is down."},{"username":"ryan-odonnell","timestamp":"2009-04-15T20:06:00.000Z","contents":"I hope so too. By the way, it appears as though the blog is down. Or more precisely, [Michael Nielsen’s whole site is down](http://downforeveryoneorjustme.com/http://michaelnielsen.org/)."},{"username":"ryan-odonnell","timestamp":"2009-04-16T00:23:00.000Z","contents":"Seems like the blog just went back up. Also, I understand now a small point that Tim clearly understood a while ago; namely, it is not necessary to show an abundance of d-dimensional subspaces to execute the partitioning step. Rather, that step only uses existence of a d-dimensional subspace. Although one may as well prove abundance at some point, for kicks."},{"username":"ryan-odonnelljozsef","timestamp":"2009-04-16T18:58:00.000Z","contents":"I’ve been thinking about the partitioning argument. Here are a few points: . Seems to me the partitioning is pretty much the center part of the argument. I mean, once everything is set up, the part about a line-free set correlating with an intersection of ij-insensitive sets is, well, no more than 2 or 3 sentences. So partitioning might deserve a section of itself, or even a billing as “the main lemma”. . I don’t currently see a natural way to do it under something other than the uniform product distribution. That would be a bit of a shame if true, since it would be nice if we didn’t have to do *too* much back and forth and back and forth and back with the measures. For example, I was hoping we wouldn’t have to dust [this](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures#Relative_density_version) argument off, but perhaps we will. I want to think about it. . In [Terry’s UCLA talk description of it](http://terrytao.wordpress.com/2009/04/02/polymath1-and-three-new-proofs-of-the-density-hales-jewett-theorem/), he describes the partitioning step as “quotient down to %[2]^n%, then use the greedy algorithm to pick out large-dimensional subspaces”. Picking out large-dim subspaces in %[2]^n% is particularly easy if you’re willing to appeal to [GRS]. Can the actual argument for the partitioning really be made this easy? (I know Terry was short on space & time and so of course could not write out a full argument.) . Does anybody actually know the [GRS] argument cold? (Jozsef?) From some brief glances, it looks like they’re also doing a “partitioning” style argument… Hi Ryan, I think we should partition a 1-set, %{\\cal{A}}\\times [2]^n%, instead of a set system %{\\cal{A}}\\subset [2]^n% like in the [GRS]. Probably that’s what you meant, I just want to emphasize that this partition is quite different (and as I see it is much stronger) from [GRS]. I would put [GRS] as a reference there, and would write the probabilistic argument for the almost perfect tiling. If you wish you can leave this part to me, I wanted to write more about it anyways."},{"username":"jozsef","timestamp":"2009-04-16T20:15:00.000Z","contents":"Hi Ryan, I think we should partition a 1-set, %{\\cal{A}}\\times [2]^n%, instead of a set system %{\\cal{A}}\\subset [2]^n% like in the [GRS]. Probably that’s what you meant, I just want to emphasize that this partition is quite different (and as I see it is much stronger) from [GRS]. I would put [GRS] as a reference there, and would write the probabilistic argument for the almost perfect tiling. If you wish you can leave this part to me, I wanted to write more about it anyways."},{"username":"kristal-cantwell","timestamp":"2009-04-18T02:36:00.000Z","contents":"Outline of a proof of DHJ(3) I have an outline of a proof of DHJ(3) and an outline of its extension to DHJ(n) I am going to post them here to see if anyone sees any gaps or errors. If it works I think it would be simpler then the current proof. Outline of proof of DHJ(3)  \nWe get an 12 insensitive set.  \nWe get a space of density epsilon  \nActually this could be a much smaller constant function of epsilon  \ninside the 12 insensitive set so it inherits the 12 insensitivity.  \nWe get a space in which the only coordinates  \nThat change are 2 and 3 with density delta  \nmuch lower than epsilon.  \nWe do this be getting a subspace of the original  \nspace which projects down to this space if the preimage  \nthat projects down to the space has density epsilon  \nfor each point then we are done otherwise.  \nWe remove the point that has the low density get  \nA slight increase in the density of the remaining spaces  \nAssociated with the preimages in the then take one  \nof the preimages that has the slightly higher density  \nand repeat the process this will result in a density  \nincrement if we cannot repeat the process often  \nso it terminates  \nand we get a set of primages with the desired density  \nfrom which we can get the desired space. Then we use DHJ(2) to get a  \ncombinatorial line of length two in the space.  \nWe use the fact the space is 12 insensitive to  \nextend the line to length 3 and we are done."},{"username":"kristal-cantwell","timestamp":"2009-04-18T02:39:00.000Z","contents":"Outline of proof of DHJ(n) We get an 12 insensitive set.  \nWe get a space of density epsilon  \nActually this could be a much smaller constant function of epsilon  \ninside the 12 insensitive set so it inherits the 12 insensitivity.  \nWe get a space in which the only coordinates  \nThat change are two through n with density delta  \nmuch lower than epsilon.  \nWe do this be getting a subspace of the original  \nspace which projects down to this space if the preimage  \nthat projects down to the space has density epsilon  \nfor each point then we are done otherwise.  \nWe remove the point that has the low density get  \nA slight increase in the density of the remaining spaces  \nassociated with the preimages in the then take one  \nof the preimages that has the slightly higher density  \nand repeat the process this will result in a density  \nincrement if we cannot repeat the process often  \nso it terminates and from the primages at the terminal step we get the desired space. We use DHJ(n-1) to get a Combinatorial line of length n-1 in the space  \nWe use the fact the space is 12 insensitive to  \nextend the line to length n and we are done."},{"username":"gowers","timestamp":"2009-04-18T03:31:00.000Z","contents":"I’m afraid I’d need a lot more detail before I even began to understand that. Just to give a few examples, what do you mean by “We get” at the beginning? What does the second line mean? I’m potentially interested though, because you seem to be going for a Shelah-type approach, which I was attempting to do in some earlier comments. Do you think you could expand on what you have written? Perhaps if it would be on the long side you could do it on the wiki — perhaps just for k=3 to start with."},{"username":"kristal-cantwell","timestamp":"2009-04-19T06:09:00.000Z","contents":"I can’t get the 12 insensitive sets as easily as I thought. The idea won’t work without them. I doubt I can fix this."},{"username":"ryan-odonnell","timestamp":"2009-04-19T07:50:00.000Z","contents":"Outline. I’ve thought for a while and I can’t really see how to carry out the partitioning argument under the equal-slices distribution. For every technical difficulty that I worked around, a new one would pop up, and I was never able to simultaneously handle all of them. (The main problem: what “progress measure” to keep track of, akin to the “total uniform-distribution mass”.) I still hold the intuition that there should be a slick way to do it, but I’m stuck. Perhaps one could turn the whole argument into a “density-increment on an ij-insensitive set” argument rather than a “density-increment on a subspace” argument…? In any case, I therefore reluctantly propose that in the writeup, the uniform distribution be treated as the “main” distribution, with equal-slices as a secondary distribution one needs to slip in and out of to facilitate the argument. I say reluctantly because I feel that the equal-slices distribution on lines is so natural that it may even be that there is some superb (polynomial?!?) relationship between equal-slices point-density in %[3]^n% and equal-slices line-density in %[3]^n%. It would be nice to spend some time thinking about global obstructions. Anyway, I guess that means the outline of the measure-changing tricks is: 1\\. Start with a set of uniform density %\\delta%. 2\\. Argue that one can pass to a subspace where both the %[3]^n%-equal-slices and %[2]^n%-equal-slices density are at least %\\delta - o(1)%. (Or else, one can get a subspace where the %[3]^n%-equal-slices density increments — in which case, one can further pass to a subspace where the uniform density increments.) 3\\. Either find a line, or get an equal-slices density increment inside an intersection of 13-insensitive and 23-insensitive sets. 4\\. Use [this argument](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures#Relative_density_version) to convert this into a uniform-distribution density increment inside an intersection of 13-insensitive and 23-insensitive sets. (One has to note that that argument preserves the property of being an intersection of 13-insensitive and 23-insensitive sets.) 5\\. Run the [partitioning argument](http://michaelnielsen.org/polymath1/index.php?title=A_general_partitioning_principle) under the uniform distribution. Note that that argument doesn’t really need “subspace richness” per se, because it implicitly proves it anyway. (It’s okay, I think, to have %c% to include a %3^{-M}% factor.) I.e., all it needs is existence of d-dimensional subspaces for %[2]^n%-uniform dense sets. 6\\. Thus get a uniform density-increment on a subspace."},{"username":"ryan-odonnell","timestamp":"2009-04-22T08:08:00.000Z","contents":"Hmm. Once more my mind changes. It’s really essential to be in equal-slices mode when doing the line-free-sets-correlate-with-ij-insensitive stuff (so that a random line is in with nonnegligible probability); and even more is this so in the generalization to %[k]^n%. Further, I think I see a non-horrendous way to carry out the partitioning argument under equal-slices. So, I propose going back to “equal-slices is the main measure”. Re partitioning: the argument is written for the uniform distribution, but it seems like it really only needs a _product_ distribution to work. (The key need is the ability to break strings up into pieces and have the global distribution be the product of the two marginal distributions.) Luckily, equal-slices is a mixture of product distributions. (There’ll be a bit of a technicality here probably, where one will want to throw out the product distributions %(p_1, p_2, p_3)% which have a component extremely close to %0% or %1%.) So if you can partition an %ij%-insensitive set into %d%-dimensional subspaces up to tiny error under any product distribution, then you get a “fractional cover” of it by %\\langle% subspace, product distribution %\\rangle% pairs. So you should be able to get a density increment for %A% on some %d%-dimensional subspace under some product distribution. Which you can convert back to a density increase under equal-slices by an easy passing-between-measures argument. seem plausible?"},{"username":"ryan-odonnell","timestamp":"2009-04-23T00:55:00.000Z","contents":"Simplifying the first step in the proof. Recall that in the first step of the proof, one does a little song and dance to arrange things so that %A% has density %\\geq \\delta - o(1)% under %\\nu_3% (equal-nondegenerate-slices on %3% characters) and simultaneously density at least some %f(\\delta) > 0% under %\\nu_2%. (We currently get %f(\\delta) = \\delta - o(1)%, but it would be okay to get, say, %f(\\delta) = \\Theta(\\delta^{3/2})% as we’ll do here.) It requires some [hacking around with total variation distance between product distributions](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures).. Here is a sketch of an arguably more straightforward method. In this sketch I will: a) ignore the distinction between equal-slices and equal-nondegenerate-slices when it’s convenient (this only changes statements by an additive %o_n(1)%); b) ignore the tiny technicality that when one passes to a subspace %[3]^{T}% where %T% is a random set of coordinates, one has to take care of the unlikely event that %|T|% is outrageously smaller than expected. Define a _random restriction_ %(S,\\ell)% as follows: choose %z \\sim \\nu_4^n% and let %S% be the coordinates where %z% has %4%‘s; also choose %\\ell \\in [3]% randomly. The meaning of “restricting according to %(S,\\ell)%” is that we think of forming a string in %[3]^n% by fixing the %S% coordinates to be all %\\ell%‘s and leaving the remaining coordinates %\\overline{S} = [n] \\setminus S% “free”. (Note that %z% only plays a role insofar as it determines the cardinality and location of %S%.) A basic observation aboute qual-(nondegenerate)-slices is that if we draw a random restriction %(S,\\ell)% and then draw the free coordinates from %\\nu_3^{\\overline{S}}%, we precisely get the equal-(n.d.)-slices distribution %\\nu_3^n%. Hence %\\mathbf{E}_{(S,\\ell)}[\\nu_3^{\\overline{S}}(A_{(S,\\ell)})] = \\delta%, and by the usual argument we can in fact ensure that %\\nu_3^{\\overline{S}}(A_{(S,\\ell)}) \\geq \\delta - \\delta^{10}%, say, for “almost all” restrictions %(S,\\ell)%. (If the variance of %\\nu_3^{\\overline{S}}(A_{(S,\\ell)})% is tiny then this holds; otherwise, we must have a density increment for some %(S,\\ell)%.) Consider now drawing a random restriction %(S,\\ell)% and filling in the free coordinates according to %\\nu_{\\{i,j\\}}^{\\overline{S}}%, where %\\{i,j\\} = [3] \\setminus \\{\\ell\\}%. This gives some nonstandard distribution %\\lambda% on %[3]^n%. Claim: %\\lambda(A) \\geq \\Omega(\\delta^{3/2})%. Once we show this claim the argument concludes as follows: For an %\\Omega(\\delta^{3/2})% fraction of %(S,\\ell)% we have %\\nu_{\\{i,j\\}}(A_{(S,\\ell)}) \\geq \\Omega(\\delta^{3/2})%. Hence there exists some %(S,\\ell)% for which both %\\nu_3(A_{(S,\\ell)}) \\geq \\delta - \\delta^{10}% and %\\nu_{\\{i,j\\}}(A_{(S,\\ell)}) \\geq \\Omega(\\delta^{3/2})%, and our mission is accomplished. (Recall that the latter fact, by Sperner, gives us an %i\\ell%-insensitive %\\cap% %j\\ell%-insensitive set of measure %\\Omega(\\delta^3)% which %A% completely avoids.) Proof of Claim: Given a small %\\gamma% we will show that %\\lambda(A) \\geq \\gamma \\delta - 2 \\gamma^3%. This completes the proof by taking %\\gamma = \\Omega(\\delta^{1/2})%. To argue this, let %\\beta_k% denote the distribution on the cardinality of the set of %k%‘s in a draw from %\\nu_k^n%. The distribution %\\lambda% draws %s \\sim \\beta_4%, picks a random set %S% of cardinality %s%, fills it with a random %1%, %2%, or %3%, then fills the remaining coordinates with equal-slices on the other two characters. If we had instead drawn %s \\sim \\beta_3%, the resulting distribution would have exactly been %\\nu_3%. Hence if we have %d\\beta_4 / d \\beta_3 \\geq \\gamma%, we could conclude %d\\lambda / d\\nu_3^n \\geq \\gamma% and hence %\\lambda(A) \\geq \\gamma\\nu_3^n(A) = \\gamma \\delta%. Unfortunately, this is not quite true; in fact, %\\beta_4(s)% can be arbitrarily smaller than %\\beta_3(s)%. However this occurs only for %s% which are very close to %n%. Indeed, one can think of %s \\sim \\beta_k% as placing %k-1% bars independently at random within a row of %n% dots, and taking the length of the first dot-segment thus formed (here I am conflating equal-slices and equal-n.d.-slices). Hence %\\mathbf{Pr}_{\\beta_4}[s]% %= \\mathbf{Pr}[\\text{min of bar1, bar2, bar3} = s]% %\\geq \\mathbf{Pr}[\\text{min of bar1, bar2, bar3} = s \\mid \\text{min of bar1, bar2} = s]% %\\qquad \\cdot \\mathbf{Pr}[\\text{min of bar1, bar2} = s]% %= (1-s/n) \\mathbf{Pr}_{\\beta_3}[s]%. So we have %d\\lambda / d\\nu_3^n \\geq \\gamma% for all %s% with %s/n \\leq 1 - \\gamma%. But we can simply “lose” (additively) the probability contribution from %s/n \\geq 1 - \\gamma% since this occurs with probability only %\\gamma^{k-1}% under %\\beta_k%. A little calculation then gives the claimed %\\lambda(A) \\geq \\gamma \\delta - 2 \\gamma^3%. (Some tiny details skipped.)"},{"username":"ryan-odonnellryan-odonnellryan-odonnell","timestamp":"2009-04-24T20:16:00.000Z","contents":"Simplifying the partitioning argument. It seems to me slightly simpler if we do the partitioning argument via the older notion of “ij-insensitivity on a subset of coordinates”. (“Local ij-insensitivity”, I think Terry called it?) **Definition:** Let %a, b \\in [k]% be distinct characters, and %I \\subseteq [n]%. We say that %A \\subseteq [k]^n% is “%ab%-insensitive on %I%” if the following condition holds: for all %x \\in A%, if %x'% is formed by changing %x_i% from %a% to %b% or %b% to %a% for some %i \\in I%, then %x' \\in A% also. If %I = [n]% we simply say that %A% is “%ab%-insensitive”. True, this introduces an extra definition. But it has the upside of making the argument a little simpler, I think. The reason is that one doesn’t have to carry around pieces of the set B living in different %[k]^n% universes, and one doesn’t have to collect up pieces of the eventual error set. After removing a subspace, one can simply say the new set is ij-insensitive except on some additional m coordinates. As an example justification, [here is the partitioning argument under any product distribution](http://www.cs.cmu.edu/~odonnell/partition-product.pdf), fitting on a page. In fact, having written that, it’s now (finally) clear to me how to directly do the partitioning under equal-slices (if we want to). In the product distribution case, all we _really_ used was that we could find a d-dimensional subspace in B localised to our favourite m coordinates. We can _almost_ do this under equal-slices except that we don’t get to pick m coordinates arbitrarily; they have to be picked randomly. Still, that’s not a big problem; so long as we’ve only “used up” %Tm% insensitive coordinates, there are still %n - Tm% insensitive ones remaining, so a random %m%-subset will fall inside the insensitive coordinates with probability %1 - o_n(1)%. (The benefit we get from using “local ij-sensitivity” is that we can take the equal-slices distribution of %B% _over all coordinates_ to be the progress measure; the probability measure doesn’t keep shifting on us.) Typo: “%V%” towards the end of the above should be “%Tm%“. What the heck? “V” should be “Tm”. But every time I wrap “Tm” in latex-dollar-signs, it changes to a V in the published version. How strange!"},{"username":"ryan-odonnell","timestamp":"2009-04-24T20:18:00.000Z","contents":"Typo: “%V%” towards the end of the above should be “%Tm%“."},{"username":"ryan-odonnell","timestamp":"2009-04-24T20:19:00.000Z","contents":"What the heck? “V” should be “Tm”. But every time I wrap “Tm” in latex-dollar-signs, it changes to a V in the published version. How strange!"},{"username":"ryan-odonnellklas-markstrom","timestamp":"2009-04-25T00:08:00.000Z","contents":"It seems a couple of the wiki pages on generalizations to DHJ(k) have been deleted. Well, not exactly deleted, but blanked out, by mystery IP addresses. See [here](http://michaelnielsen.org/polymath1/index.php?title=DHJ(k)_implies_multidimensional_DHJ(k)&action=history) and [here](http://michaelnielsen.org/polymath1/index.php?title=Line_free_sets_correlate_locally_with_dense_sets_of_complexity_k-2&action=history). I’ve tried to revert them properly, but can we use captchas on edits or block unregistered users from editing? Earlier I have reverted several changes where someone replaced the proper text with a lot of non-functioning URLs."},{"username":"klas-markstrom","timestamp":"2009-04-25T12:15:00.000Z","contents":"Earlier I have reverted several changes where someone replaced the proper text with a lot of non-functioning URLs."},{"username":"ryan-odonnell","timestamp":"2009-05-02T06:12:00.000Z","contents":"I have started creating TeX files for the writeup. We can still discuss the outline on the [page Terry created](http://michaelnielsen.org/polymath1/index.php?title=Outline_of_first_paper). But to get the ball rolling, here is a [new page for the TeX](http://michaelnielsen.org/polymath1/index.php?title=TeX_files_for_first_paper). The idea was proposed by someone in an earlier thread. On this page are wiki-links to 6 pages, named “dhj.tex”, “dhj.sty”, “intro.tex”, etc. When you go to the wiki page, e.g., [measures.tex](http://michaelnielsen.org/polymath1/index.php?title=Measures.tex), don’t try to read it as a wiki page. Just click “edit”, copy the source for the page, paste it into your favourite editor, and save the document as measures.tex. Once you’ve done this for all 6 files, you should be able to run “latex” and “bibtex” to produce a document. If you want to change a file, just go ahead in your TeX editor, and when you’re done, copy/paste it back into the appropriate wiki-page source. It’s extremely bare-bones right now; I didn’t even put in the title etc. yet. Also, more files/sections need to be created. The only substantive thing done so far is writing some of the “measures” section. My goal for this section is just to get down, in the shortest amount of space, the lemmas that “doing uniform on most coordinates and equal-slices on an eps-fraction is O(eps sqrt(n))-close to uniform”, “doing equal-slices on most coordinates and then either uniform or equal-slice on an eps-fraction is O(k eps sqrt(n))-close to uniform”. I got most of the way on this, but didn’t finish."},{"username":"ryan-odonnell","timestamp":"2009-05-04T01:17:00.000Z","contents":"In case anyone is interested, I saved all 9 threads on general-n DHJ (i.e., comments 000–1099 minus the 200’s, 700’s, and 900’s) to one pdf file. It’s here: [http://www.cs.cmu.edu/~odonnell/dhj-polymath-comments-all.pdf](http://www.cs.cmu.edu/~odonnell/dhj-polymath-comments-all.pdf) Warning: it’s a 10MB file (346 pages). Out of curiosity, I also totted up number of posts by poster. I believe there were 9 people with 5+ posts, with counts ranging from 12 up to 338."},{"username":"ryan-odonnellryan-odonnell","timestamp":"2009-05-08T19:20:00.000Z","contents":"Hi all — I just gave a talk about the results and the project at Microsoft Research New England (thanks Jennifer & Christian for having me). I’ll post the slides soonish — I’m giving the talk again at IAS and am not sure if I want to preempt myself. <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> Several good questions arose from the audience, and I was hoping to get people’s opinions: 1\\. [from Adam Kalai]: When the result is published under the Polymath pseudonym, how will participants put their grant acknowledgment lines in the document? 2\\. [from Jim Propp (I think; I’m better with names than faces)]: When will it be done / written up? For the “writing up” part… Besides, “soon, we hope”, should the correct answer to this question have included, in the spirit of the project, “feel free to write some or all of it up yourself! (under the polymath banner of course)”? I guess ~30 more people have now seen the sketch of the proof and could in principle work on the writing. Is this a good or bad idea? As for the “done” part… 3\\. One audience member, Emanuele Viola, came up to me after the talk and mentioned some promising-seeming lemmas from his papers that might possibly help with parts of the current argument. We traded a few emails about this this morning and then both asked ourselves, “wait, should we be posting these to the blog?” I guess we should, although it seems like the blog has been dormant for ~3 weeks. At some point fairly soon, regardless of whether we get the energy to write up the proof (and of course, it would be terrible if we didn’t do this, I think), I think we ought to declare the project “done”. I mean, we can’t block other people from working on DHJ indefinitely… Emanuele requested I point out that the lemmas mentioned (which I refer to again below) are not his, just that his paper has pointers to the literature (and a variant of the lemma)."},{"username":"ryan-odonnell","timestamp":"2009-05-09T22:03:00.000Z","contents":"Emanuele requested I point out that the lemmas mentioned (which I refer to again below) are not his, just that his paper has pointers to the literature (and a variant of the lemma)."},{"username":"gowers","timestamp":"2009-05-08T20:38:00.000Z","contents":"A few immediate thoughts — I hope others will express their views too. 1\\. I’m mercifully spared from having to think about this kind of thing, so I’m not sure what is sensible and what isn’t. My preference, as I have said before, is to keep actual names off the paper. Perhaps it would be in keeping with previous suggestions on this kind of topic to have minimal information on the paper (i.e., just the pseudonym) and maximal information on the Polymath1 wiki (i.e., names of participants, grant details, links to all the blog comments, etc. etc.). 2\\. I think I’d like to do some writing starting in about a week’s time. The way I imagine going about it is trying to write a skeleton paper with bits that can be filled in by whoever has the inclination to do so (which might be me). Of course, the skeleton itself would be open to discussion and change, but perhaps once it’s there the task will seem more manageable and we can get a complete draft. In principle I don’t see why it shouldn’t take more than about two or three days of collective effort. 3\\. I look forward to hearing more about this. 4\\. For myself, once our proof of DHJ is written up, I would be happy to say that all the material on the blog that doesn’t make it into the paper can be freely used by anyone for any purpose. I suppose if someone felt that they had used some insight from the blog in a way that was critical to their result, it might be appropriate if they acknowledged that and gave the appropriate URL. Having said that, there were some questions that would still be interesting to pursue (such as whether not having any combinatorial lines can lead to a global correlation with a 12-insensitive set if you choose the right measure) and I wouldn’t rule out thinking about them. But neither do I think it would be right to claim indefinite ownership of those questions, so I’d go for declaring the project done sooner rather than later."},{"username":"ryan-odonnell","timestamp":"2009-05-09T22:04:00.000Z","contents":"As I mentioned, I had an email discussion with Emanuele after I gave my talk. With his permission, I reproduce it below…"},{"username":"emanuele-viola","timestamp":"2009-05-09T22:08:00.000Z","contents":"There are a few formulations [of the lemma], see Section 3 in my paper “Hardness amplification proofs require majority”. [link on my homepage; including it here seems to cause WordPress to refuse to accept the post] The statement that appears in this paper allows you to conclude that many coordinates are close to uniform. One in fact can prove a stronger version of the lemma in which the conclusion of Lemma 3.2 in my paper holds even if you condition on the values of all the variables in G – {i_1,…,i_q}. This might be the closest to DHJ: it gives you that if you ignore a few coordinates then you have a line (in fact, lines are dense in this projection).  \nThe proof of (all variants) of this lemma is elementary. Earlier I was thinking whether this might be enough to prove DHJ. The “only” thing that can go wrong is that the variables whose indexes are in the complement of G destroy all lines that you found in the good projection, but maybe one can do a recursive argument to bypass this. Also, I would guess some type of recursion is necessary, for else one would get too strong parameters for DHJ? I’d be interested in thinking more about this and knowing your opinion. Recently I have been spending quite a bit of time doing proofs that use the lemma inductively."},{"username":"ryan-odonnell","timestamp":"2009-05-09T22:09:00.000Z","contents":"Hmm, looks powerful Emanuele. > closest to DHJ: it gives you that if you ignore a few coordinates then you  \n> have a line (in fact, lines are dense in this projection). Hmm, let’s see, why is that… [am not used to thinking about drawing  \na random point from A]. Let me write “B” = [n] \\ G (B for bad, G for  \ngood). Just talking out loud here… I mean, suppose we pretend that life is even awesomer than Lemma 3.2:  \nthat a random draw from A is equivalent to:  \n(a) the coordinates [3]^G get chosen totally uniformly  \n(b) then the coordinates in [3]^B get set adversarially based on  \nwhat happened in step (a). Hmm, so I guess it’s like saying, for every string x in [3]^G, there  \nis an extension y in [3]^B such that (y,x) is in A.  \nSo in this awesome world, I guess yeah, you can take any line you want  \nover the G coordinates, and it’s possible to extend it to an  \n“almost-line” in A (“almost-line” meaning that the B-coordinates might  \nbe screwed up). — What about in the not awesome but still cool world of Lemma 3.2\\. Or  \nin your extension of Lemma 3.2\\. The extension (if I have it right)  \nmeans something like:  \n(a) you choose your favorite q coordinates Q in Q  \n(b) an adversary chooses x in [3]^{G \\ Q}.  \n(c) then for almost all strings y in [3]^{Q}  \n(d) there exists a string z in [3]^B  \nsuch that (x,y,z) in A ? So then you’re saying, you can get pretty much any line action  \nhappening you want inside Q, but then it gets turned into an  \n“almost-line” because of stage (d) above? Am I getting it right? Looks kind of powerful."},{"username":"emanuele-viola","timestamp":"2009-05-09T22:11:00.000Z","contents":"> Hmm, so I guess it’s like saying, for every string x in [3]^G, there  \n> is an extension y in [3]^B such that (y,x) is in A. Yeah I think that’s a way to look at it. > What about in the not awesome but still cool world of Lemma 3.2\\. Or  \n> in your extension of Lemma 3.2\\. The extension (if I have it right)  \n> means something like:  \n> (a) you choose your favorite q coordinates Q in Q  \n> (b) an adversary chooses x in [3]^{G \\ Q}.  \n> (c) then for almost all strings y in [3]^{Q}  \n> (d) there exists a string z in [3]^B  \n> such that (x,y,z) in A ? The statement I have in mind is: Let v denote all the variables, then  \nH(v^Q | v^{G \\ Q) > 1 – eps. So this means that E_{w in v^{G \\ Q}[ H(v^Q | v^{G \\ Q) = w ] > 1 – eps, and so with probability >= 1/2 over w in v^{G \\ Q} it is the case that  \nVariationDistance(v^Q | v^{G \\ Q), uniform) So then you’re saying, you can get pretty much any line action  \n> happening you want inside Q, but then it gets turned into an  \n> “almost-line” because of stage (d) above? I think that is correct. Note by the way the bad set will always be |B| >> |Q|."},{"username":"ryan-odonnell","timestamp":"2009-05-09T22:11:00.000Z","contents":"1081.  \n> E_{w in v^{G \\ Q}[ H(v^Q | v^{G \\ Q) = w ] > 1 – eps,  \n>  \n> and so with probability >= 1/2 over w in v^{G \\ Q} it is the case that  \n> VariationDistance(v^Q | v^{G \\ Q), uniform) < 2eps. Hmm. Could one then say that with probability 1 – sqrt(eps), the  \nvariation distance is at most sqrt(eps)? Because that would be kind  \nof cool, no? For almost all strings x in [3]^{G \\ Q}, it holds that  \nfor almost all y in [3]^Q, that (x,y,z) in A for some z in [3]^B?  \nWouldn’t one just get that for almost all strings w in [3]^G, there is  \na z in [3]^B such that (w,z) in A? I also wonder if maybe it might be better to apply this lemma to the  \nset I was calling “L-bar” in the talk. You know, it had density 1 –  \ndelta^2, hence A had density delta + delta^3 inside it. That set  \nL-bar was almost completely arbitrary, except that it was a union of  \ntwo “12-insensitive sets”, so we worked extremely hard to  \nalmost-partition it into copies of [3]^{log log log n}, so that we  \ncould get the delta^3 density increment on one of them, and recurse. But perhaps instead one could somehow just apply the lemma to L-bar (a  \nbit weird, because its density is very close to 1) and get it  \nfractionally covered by uniform distributions on coordinate-sets of  \nsize q… would have to think about it…"},{"username":"emanuele-viola","timestamp":"2009-05-09T22:13:00.000Z","contents":"1082. > Hmm. Could one then say that with probability 1 – sqrt(eps), the  \n> variation distance is at most sqrt(eps)? Sure, I just put 1/2 for example. > Because that would be kind of cool, no? For almost all strings x in  \n> [3]^{G \\ Q}, it holds that for almost all y in [3]^Q, that (x,y,z) in A for  \n> some z in [3]^B?  \n> Wouldn’t one just get that for almost all strings w in [3]^G, there is  \n> a z in [3]^B such that (w,z) in A? But I think x ranges over projections of strings in A, while y is uniform. I.e. we have found a subset Q of coordinates where projections of strings in A to those coordinates are almost uniform. And this is true even for most ways of filling the coordinates in G \\ Q with projections of strings  \nfrom A. Re your last point about almost-partitioning L-bar, I wonder in what sense it is almost partitioned. I am not sure if the lemma would be good for that…"},{"username":"ryan-odonnell","timestamp":"2009-05-09T22:28:00.000Z","contents":"Two more updates: 1\\. The slides I used for my talk at Microsoft are here:  \n[http://www.cs.cmu.edu/~odonnell/slides/dhj.pps](http://www.cs.cmu.edu/~odonnell/slides/dhj.pps) If you don’t have PowerPoint there is a free viewer here: [http://www.microsoft.com/downloads/details.aspx?FamilyId=428D5727-43AB-4F24-90B7-A94784AF71A4&displaylang=en](http://www.microsoft.com/downloads/details.aspx?FamilyId=428D5727-43AB-4F24-90B7-A94784AF71A4&displaylang=en) 2\\. I have finished most details of the measures.tex file. We can now pass from uniform to equal-slices, and we can also pass from equal-slices overall to equal-slices on a restriction. Next step for me will be to write up the partitioning argument under equal-slices, with the “losing insensitive coordinates” method. _Sorry, this went to my moderation queue._"},{"username":"ryan-odonnell","timestamp":"2009-05-10T02:13:00.000Z","contents":"A tiny notational note: as you can see from the slides, I actually went with %[k] = \\{0, 1, \\dots, k-1\\}%. Two reasons I ended up preferring this: a) makes the deduction on Szemeredi from DHJ plainer (“just write numbers in base %k%“) b) computer scientists like %\\{0,1\\}%."},{"username":"gowers","timestamp":"2009-05-10T22:08:00.000Z","contents":"Just to demonstrate that Ryan is not completely alone in the Polymath universe, I decided to write a draft of an introduction. I’ve put it on the wiki in one of the pages that Ryan created. I’m not sure I made any controversial decisions. I too have been coming to the view that we should make $[k]$ be $\\{0,1,\\dots,k-1\\}$ though as it stands I’ve hedged my bets. But actually I think it’s slightly strange even for DHJ(3) to think of the coordinates as belonging to $\\{1,2,3\\}$ rather than $\\{0,1,2\\}$. As Ryan suggested earlier, I put the introduction up in LaTeX, so if anyone wants to edit it they just paste it into a file, edit the file, and replace what was there before by their new version. Or they can edit the page directly (but then they can’t compile it to see what it looks like). I also decided not to use any macros, though it’s conceivable that I did so by accident. Incidentally, I was imagining that the first section after the introduction would contain quite a lot of technical preparation: definition of combinatorial subspaces, discussion of different measures, etc. So I left all that out of the introduction (apart from the definition of a combinatorial line, which was essential)."},{"username":"ryan-odonnell","timestamp":"2009-05-11T05:12:00.000Z","contents":"I have added some more TeX files to the document. It’s not really organized right now, but I wanted to get some math details written and (hopefully) correct. I ended up just writing the partitioning argument under product distributions. It was just proving too awkward to do it all under equal-slices. So I propose we go in-and-out of a product distribution when doing the partitioning. PS: I went back to [k] = {1, 2, …, k}; I guess we should stop switching at some point. <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> My reasoning was that it was common to talk about distributions on [k] as points in the k-dimensional simplex, and one really likes to index %\\mathbb{R}^k% by 1…k, not 0…k-1."},{"username":"ryan-odonnell","timestamp":"2009-05-15T02:31:00.000Z","contents":"I have made some updates to the paper at [http://michaelnielsen.org/polymath1/index.php?title=TeX_files_for_first_paper](http://michaelnielsen.org/polymath1/index.php?title=TeX_files_for_first_paper)"},{"username":"ryan-odonnell","timestamp":"2009-05-16T02:51:00.000Z","contents":"More updates…"},{"username":"jozsefjozsef","timestamp":"2009-05-16T07:21:00.000Z","contents":"I just finished a write up which proves the corner theorem for Cartesian products of cubes. The problem is easier than DHJ(3) and it makes the explanation of the density increment method easier to follow. I hope that this write up will help the reader to understand the key elements of the DHJ(3) proof. It is available at  \n[http://www.math.ubc.ca/~solymosi/cubes3.pdf](http://www.math.ubc.ca/~solymosi/cubes3.pdf)  \nI would be happy to place it somewhere on the wiki sites, but I don’t know how to do that. I’ve found some typos in my note which I corrected but probably there are more of them. Working out the epsilons and deltas explicitly isn’t my strong side. Please let me know if you find more typos."},{"username":"jozsef","timestamp":"2009-05-16T23:31:00.000Z","contents":"I’ve found some typos in my note which I corrected but probably there are more of them. Working out the epsilons and deltas explicitly isn’t my strong side. Please let me know if you find more typos."},{"username":"ryan-odonnell","timestamp":"2009-05-19T02:06:00.000Z","contents":"I gave a talk about the work at IAS today. Afterwards, Boaz Barak and Anup Rao reminded me that DHJ actually has an application in theoretical computer science: Verbitsky used it to give the first known proof of the Parallel Repetition Theorem (albeit with parameters which are essentially useless for applications of Parallel Repetition). The paper is [here](http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V1G-3VSNJNB-9&_user=10&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=18be7d22621e9165d2d9d4497f08f6bf). Raz subsequently gave an effective (combinatorial/probabilistic) proof of the Parallel Repetition Theorem (for “2 players”), which had massive implications in TCS. Raz’s Theorem was subsequently improved by Thomas Holenstein and then Anup Rao. Anup also pointed out to me that Verbitsky’s proof using DHJ works for any number of players, and is indeed the only known proof of Parallel Repetition for more than 2 players. This application motivates the following question about DHJ: Try to show that for _any_ finite alphabet %\\Sigma%, if %A \\subseteq \\Sigma^n% has density %\\delta% and %n% is large enough, then %A% contains “99 percent” of a line. By this we mean that there is a “template” %\\lambda \\in (\\Sigma \\cup \\{\\star\\})^n \\setminus \\Sigma^n% such that for at least 99 percent of the symbols %a \\in \\Sigma% we have %\\lambda^{\\star \\to a} \\in A%. Can we prove this even for 67 percent in place of 99 percent? 51 percent? 50 percent? 1 percent?"},{"username":"gowers","timestamp":"2009-05-19T02:30:00.000Z","contents":"I’m trying to understand that last question, but I don’t yet. If %n% is allowed to depend on %\\Sigma% then the answer is obviously yes with 100 percent, since that’s just DHJ. So it would seem that you are asking for a proof that for every %\\delta% there exists %n% such that for every finite alphabet %\\Sigma%, every subset of %\\Sigma^n% of density at least %\\delta% contains 99 percent of a line. But this cannot be the correct interpretation either, since for any fixed %n% one can just choose %\\Sigma% to be enormous and choose %A% randomly, and by trivial arguments (Chernoff estimates plus a union bound over all lines) %A% will have density no more than %\\delta(1+\\epsilon)% in any combinatorial line. I’m assuming here that I understand correctly what you mean by containing 99 percent of a line: that it means there is a combinatorial line and at least 99 percent of the points in that line belong to %A%."},{"username":"ryan-odonnell","timestamp":"2009-05-19T06:33:00.000Z","contents":"Hi Tim. Er, yes, my question didn’t make sense. But I believe there is a correct question in there — I will try to write it down soon. In the meantime, there are some more updates to the draft here: [http://michaelnielsen.org/polymath1/index.php?title=TeX_files_for_first_paper](http://michaelnielsen.org/polymath1/index.php?title=TeX_files_for_first_paper) including a pdf of the current (extremely messy and less-than-half-finished) state."},{"username":"ryan-odonnell","timestamp":"2009-05-19T18:51:00.000Z","contents":"It seems that some of the observations made about equal(-nondegenerate-)slices distribution may follow from “de Finetti’s Theorem” and extensions; see e.g. [http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf](http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf)"},{"username":"gowersryan-odonnell","timestamp":"2009-05-20T16:21:00.000Z","contents":"A few comments about notation and definition choices in the current draft of the DHJ paper. I don’t want to make any changes without some discussion taking place, but let me give some opinions and questions here. Ryan, I see that you’ve changed from talking about combinatorial lines to talking about line templates and nondegenerate combinatorial lines. I can see that there is some reason for this, but it strikes me as a situation where the logical convenience of this notion (which presumably comes in when one wishes to argue that a random line template has probability almost 1 of actually having some wildcards) is outweighed by the unfriendliness when you compare it with the more geometrical notion of a combinatorial line. I think I would favour defining a combinatorial line as something that has to be nondegenerate, but allowing line templates to be degenerate. That way, the statements of the theorems don’t have to have the unnecessarily distracting nondegeneracy condition (just as, when stating Szemerédi’s theorem, we don’t say that a dense set contains a nondegenerate AP of length k). Actually, that’s all I have to say for now, but I haven’t got very far through the draft yet. Sure, that’s fine."},{"username":"ryan-odonnell","timestamp":"2009-05-21T10:16:00.000Z","contents":"Sure, that’s fine."},{"username":"gowersryan-odonnell","timestamp":"2009-05-21T14:06:00.000Z","contents":"Oh yes, there was another thing — the question of how to fill in the wildcards in the sentence, “The purpose of this paper is to give the first **** proof of [DHJ].” My initial attempt was “fully combinatorial” and you suggested “finitary”. There are problems with both: one can argue that just about anything is combinatorial, and I think that some combination of Tim Austin and Terry ended up with a finitary proof of DHJ that was heavily influenced by ergodic theory. So the question is this: is there some clever choice of words to insert here, or should we rewrite the first paragraph so as to avoid the problem? Would “first elementary proof” do? “Elementary” sounds good to me. It seems justified, given that the most complicated notion in the proof is the distance between probability distributions."},{"username":"ryan-odonnell","timestamp":"2009-05-21T19:53:00.000Z","contents":"“Elementary” sounds good to me. It seems justified, given that the most complicated notion in the proof is the distance between probability distributions."},{"username":"gowers","timestamp":"2009-05-21T15:26:00.000Z","contents":"I’ve now added a short section on Sperner’s theorem to the [TeX files on the wiki](http://michaelnielsen.org/polymath1/index.php?title=TeX_files_for_first_paper), with a view to motivating the concept of equal-slices measure, and because I think it would be very wrong not to have such a section, given how much Sperner’s theorem influenced our thoughts."},{"username":"ryan-odonnellgowers","timestamp":"2009-05-25T09:51:00.000Z","contents":"I’ve made some more updates to the TeX files. I think that most of the technical pieces of the paper are now more-or-less in there. The major technical thing that remains is the “putting all the pieces together” argument. And the major nontechnical thing that remains is — well, everything <span class=\"wp-smiley wp-emoji wp-emoji-smile\" title=\":)\">:)</span> Aside from the introduction, which is in good shape, there is zero explanation or helpfulness in the rest of the paper, and the organisation is haphazard. I’m planning at some point to go through all the technical stuff and pad it out so that the steps are motivated. Not sure when I’ll get the chance to do it though. Anyhow, it’s great to hear that at least some kind of certificate of doneness almost exists: thanks for all the work you’ve been putting in."},{"username":"gowers","timestamp":"2009-05-26T15:56:00.000Z","contents":"I’m planning at some point to go through all the technical stuff and pad it out so that the steps are motivated. Not sure when I’ll get the chance to do it though. Anyhow, it’s great to hear that at least some kind of certificate of doneness almost exists: thanks for all the work you’ve been putting in."},{"username":"jozsef","timestamp":"2009-06-01T14:16:00.000Z","contents":"I extended the note about corners in Cartesian products with a proof of the k=3 case of the DHJ theorem. This write up is (very) elementary; for example it uses equal slices distributions indirectly, without even stating the definition. Here is the link:  \n[http://www.math.ubc.ca/~solymosi/cubes3.pdf](http://www.math.ubc.ca/~solymosi/cubes3.pdf)  \nI will put there the tex file too in case you will find it useful for the write-up. I’m still not sure what would be the way to use our wiki page for that."},{"username":"jozsef","timestamp":"2009-06-03T16:28:00.000Z","contents":"I’m planning to add a few more pages to the note proving the higher dimensional version of the “corners in Cartesian products of cubes” theorem. It will give a surprisingly short and simple proof of Szemeredi’s theorem. The more I use Tim’s partitioning argument the more I am amazed about its simplicity and effectiveness. I would be very interested to learn more about the connections/similarities to the Hahn-Banach theorem. I vaguely recall that Tim promised some explanations …"},{"username":"ryan-odonnell","timestamp":"2009-06-10T22:38:00.000Z","contents":"Hi guys. I got back on the writeup today, but didn’t make much progress (some updates to the tex, though). One issue is that the parameter-situation is a nightmare! Nothing to be done about that, unfortunately. The other issue was a subtle snag that I did not previously consider. The partitioning argument goes through very pleasantly under any product distribution; this is convenient, since we get our first insensitive-density-increment under equal-slices, and this easily gives the same increment under _some_ product distribution (since equal-slices is a mixture of product distributions). The bummer is this, though: If you draw a string from a product distribution pi^n, and condition it on falling in a certain d-dimensional subspace, the conditional distribution on the subspace is _not_ just the pi-product measure. This spoils the remainder of the argument. It _does_ work fine if pi is uniform, and also (I’m pretty sure) if pi is equal-(nondegenerate-)slices. So, again, there are two choices: (1) take the density increment under equal-slices and do [some wangling](http://michaelnielsen.org/polymath1/index.php?title=Passing_between_measures#Relative_density_version) to get it under uniform, then partition, then go back to equal-slices, and iterate; or, (2) get the partitioning to work under equal-slices. I remember taking several cracks at (2) and failing; maybe I’ll take one more crack. It would really be more elegant if we could execute the whole proof under one measure."},{"username":"ryan-odonnell","timestamp":"2009-06-10T22:56:00.000Z","contents":"Actually, there’s a choice (3): try to get the correlation-with-an-insensitive-set part to work with the uniform distribution in some way. Then we could do everything under uniform, which would be extremely attractive. I wonder if that’s possible…"},{"username":"ryan-odonnell","timestamp":"2009-06-12T01:19:00.000Z","contents":"Okay, I’ve made some more additions, and now I believe the writeup contains all the technical material needed to prove DHJ(k). The only part missing is at the end, where one “puts all the pieces together”. In thinking about how to write that, I tried to figure out the actual quantitative bounds we get. As expected, for DHJ(3) we get a simple tower; I believe %2 \\uparrow \\uparrow O(1/\\delta^3)%. For DHJ(k), %k \\geq 4%, the bounds get comically bad. I get hopelessly muddled when Ackermann functions are involved, but I believe the bound is at least as bad as %A(k+1, poly(1/\\delta))%. Perhaps worse, I’m not a hundred per cent sure. The particularly painful part, quantitatively, is where we deduce the d-dimensional version from the 1-dimensional version. Iterating a bound d times is not pretty, especially when d itself needs to be huge. I’ve wondered if we can improve this. Gunderson-Rodl-Sidorenko certainly improves upon our basic iterative method. Note that the _proof_ is really no more complicated for general k; it’s just the actual bounds get laughable for k bigger than 3. — Anyway, before nailing these last details, I think it’s better to make a clarifying pass over the document. There’s actually strictly more technical stuff in there than needed, because I didn’t quite know what pieces would ultimately be necessary, so I added extra stuff in just in case. And now that we know what quantitative bounds we’re shooting for (the reasonable tower for k = 3, anything at all for higher k), we know which factors we need to save and which we don’t care about."},{"username":"ryan-odonnellgowersterence-tao","timestamp":"2009-06-12T02:47:00.000Z","contents":"By the way, I’m not sure why we can’t bill this as the first finitary proof of DHJ. TimA’s new proof is still “infinitary” (it includes that word in the title). I suppose our paper is also infinitary in the very weak sense that it views equal-slices as an infinitary mixture of product distributions. Perhaps we can evade the issue by saying that our paper is the first to give a quantitative bound. The problem was that, if I understand correctly, Terry had some way of discretizing Tim Austin’s argument. But perhaps that argument isn’t sufficiently written to make it a problem to call ours the first finitary proof. We should probably see what Terry thinks about it. I’ve been busy with other things (including the sister polymath project) but I do intend to have a read through the draft here at some point. So far it’s looking pretty good… The notes on the wiki on finitising Austin’s proof are not yet at the level of a rigorous proof, but I believe that it can be made into one with more effort. At any rate I consider this part of the polymath project and so arguing about precedence seems somewhat moot. Once finitised, some quantitative bound is in principle extractable, but it is likely to be far more laughable than anything Ryan is likely to get out of the combinatorial proof (in particular it relies on multiple regularity lemmas with parameters analogous to those in the hypergraph lemma). In any case it is not at the point where it needs to be cited as fact. Perhaps one can bill the current proof as the first proof which is primarily combinatorial in nature? In the draft there is a side note to ask on the relationship between Austin’s proof and the combinatorial one. What Austin did was to take one of the key insights from this project, namely that lack of lines implied correlation with a local 12-insensitive set (or whatever the current terminology is – I haven’t kept up), and found an ergodic-theory analogue of this fact, in a framework which Austin used previously to establish another proof of the multidimensional Szemeredi theorem and a related multiple ergodic theorem (and which, deep down, is a very heavily disguised version of the hypergraph regularity framework, but that is a whole story in itself…). The other ingredients of the proof are different, though; for instance, there is nothing resembling a density increment argument (for much the same reason as the regularity lemma approaches to these sorts of problems don’t need density increment; they have energy increment instead as a substitute). So I think the relation is that one key idea in Austin’s proof was inspired by the discovery of one key idea in the combinatorial proof."},{"username":"gowers","timestamp":"2009-06-12T03:59:00.000Z","contents":"The problem was that, if I understand correctly, Terry had some way of discretizing Tim Austin’s argument. But perhaps that argument isn’t sufficiently written to make it a problem to call ours the first finitary proof. We should probably see what Terry thinks about it."},{"username":"terence-tao","timestamp":"2009-06-12T11:07:00.000Z","contents":"I’ve been busy with other things (including the sister polymath project) but I do intend to have a read through the draft here at some point. So far it’s looking pretty good… The notes on the wiki on finitising Austin’s proof are not yet at the level of a rigorous proof, but I believe that it can be made into one with more effort. At any rate I consider this part of the polymath project and so arguing about precedence seems somewhat moot. Once finitised, some quantitative bound is in principle extractable, but it is likely to be far more laughable than anything Ryan is likely to get out of the combinatorial proof (in particular it relies on multiple regularity lemmas with parameters analogous to those in the hypergraph lemma). In any case it is not at the point where it needs to be cited as fact. Perhaps one can bill the current proof as the first proof which is primarily combinatorial in nature? In the draft there is a side note to ask on the relationship between Austin’s proof and the combinatorial one. What Austin did was to take one of the key insights from this project, namely that lack of lines implied correlation with a local 12-insensitive set (or whatever the current terminology is – I haven’t kept up), and found an ergodic-theory analogue of this fact, in a framework which Austin used previously to establish another proof of the multidimensional Szemeredi theorem and a related multiple ergodic theorem (and which, deep down, is a very heavily disguised version of the hypergraph regularity framework, but that is a whole story in itself…). The other ingredients of the proof are different, though; for instance, there is nothing resembling a density increment argument (for much the same reason as the regularity lemma approaches to these sorts of problems don’t need density increment; they have energy increment instead as a substitute). So I think the relation is that one key idea in Austin’s proof was inspired by the discovery of one key idea in the combinatorial proof."},{"username":"ryan-odonnell","timestamp":"2009-06-13T01:00:00.000Z","contents":"Am working on some introductory material now; a new draft is up."},{"username":"gowers","timestamp":"2009-06-13T16:22:00.000Z","contents":"Ryan, this is just to say thanks for all the terrific work you’ve been putting into this. Right at the moment I am busy with some other write-ups, but at some point I’d like to work through this one and pad it out with a few explanatory remarks and things like that. But I don’t see myself doing that before about August or September. I hope that delay is acceptable. In any case, once a fully detailed argument exists in written form, which it seems that it more or less does now, we will be in a position where we could in principle do what you suggested in an earlier comment and declare the project to be finished. The practical meaning of that would be that anybody could make any use they wanted of the wiki and the blog posts, whether private or public. Another thing I want to do at some point is trawl through the posts and write a document containing problems that we did not solve but that are, in my view, pretty interesting. The reason I want to do that is that I think they would make ideal PhD problems: the best ones are fully motivated (since, even now we have a proof of DHJ, they would still advance our understanding of the problem), unlikely to have been worked on all that hard, but also unlikely to be trivial either. Of course, for such a document to be useful, we would need to say to a potential PhD student that he/she is welcome to work on these problems privately and make use of any relevant blog comments."},{"username":"kevin-obryantryan-odonnellgowers","timestamp":"2009-06-24T09:58:00.000Z","contents":"In the middle of page 2, the map %(a_1, \\ldots ,a_n) \\to \\sum_i (a_i-1)k^{k-1}% is used to note that finding combinatorial lines in %[k]^n% is at least as hard as finding k-term APs in %[k^n]%. The map %(a_1, \\ldots, a_n) \\to \\sum_i a_i% accomplishes the same thing and is substantially more efficient. The common difference is just the number of wildcards. I think you need a bijection between %[k]^n% and %[N]% though; starting with a subset of %[N]%, how would one naturally convert it to a subset of %[k]^n%? The map Kevin suggests works fine for colourings (you just take their inverse images) but isn’t quite so straightforward for density, since the inverse image of a dense subset of %[N]% doesn’t have to be dense, though it’s probably OK if you are using equal-slices measure. I think you can get it to work if you restrict to central slices. But probably using the map we have is easier."},{"username":"ryan-odonnell","timestamp":"2009-06-25T09:34:00.000Z","contents":"I think you need a bijection between %[k]^n% and %[N]% though; starting with a subset of %[N]%, how would one naturally convert it to a subset of %[k]^n%?"},{"username":"gowers","timestamp":"2009-06-25T12:05:00.000Z","contents":"The map Kevin suggests works fine for colourings (you just take their inverse images) but isn’t quite so straightforward for density, since the inverse image of a dense subset of %[N]% doesn’t have to be dense, though it’s probably OK if you are using equal-slices measure. I think you can get it to work if you restrict to central slices. But probably using the map we have is easier."},{"username":"kevin-obryantryan-odonnell","timestamp":"2009-06-24T10:11:00.000Z","contents":"Somewhere, perhaps, the paper should mention the asymptotic lower bound (currently Theorem 1.3 in the second Polymath paper). For each k there is a %C_k>0% such that for every n there is a subset A of %[k]^n% with cardinality %C_k k^n (r_k(\\sqrt{n})/\\sqrt{n})^{k-1}% and no combinatorial lines. Here, %r_k(N)% is the maximum possible size of a subset of %[N]% not containing any %k%-term AP. The proof is only 10 lines or so; it may well have a better home here than in the “small n” paper. Or two homes. Good point, this lower bound should be added to the “New Proof” paper. I think it might be best for it to just quote the bound in the “Low Dimensions” paper."},{"username":"ryan-odonnell","timestamp":"2009-06-25T09:33:00.000Z","contents":"Good point, this lower bound should be added to the “New Proof” paper. I think it might be best for it to just quote the bound in the “Low Dimensions” paper."},{"username":"ryan-odonnellryan-odonnell","timestamp":"2009-06-25T09:48:00.000Z","contents":"Hi all. I have finished a rough draft of the “New Proof” paper. You can see its pdf [here](http://www.cs.cmu.edu/~odonnell/dhj-june-25.pdf). A version without internal “notes-to-self” is [here](http://www.cs.cmu.edu/~odonnell/dhj-draft.pdf). The files are all on the wiki [here](http://michaelnielsen.org/polymath1/index.php?title=TeX_files_for_first_paper) for anyone who wants to work on it. I’m fairly satisfied with it for now and think I will take a break from it. It’s not 100 percent in submittable form yet; there are still many tiny notes in there about editing decisions, references to be added, concluding discussion to be filled in, proofreading to be done, etc. Also, the ultimate draft need not actually resemble this present draft; if people think the paper can be improved with significant rewriting or reorganization, that would be great. Oops, broken link. The second link should be [this](http://www.cs.cmu.edu/~odonnell/papers/dhj-draft.pdf)."},{"username":"ryan-odonnell","timestamp":"2009-06-25T09:50:00.000Z","contents":"Oops, broken link. The second link should be [this](http://www.cs.cmu.edu/~odonnell/papers/dhj-draft.pdf)."},{"username":"jozsef","timestamp":"2009-06-25T12:45:00.000Z","contents":"Metacomment.  \nWhat do you think about starting a new thread on the writeup? Sometimes it takes too long to download this page."},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Moser & Elkin Michael, thanks for the corrected inequalities at 993\\. I plugged them into the linear program, of course, but unfortunately the bounds did not budge, only the extremisers, which means that we chipped away a little bit but not as much as before. I can imagine that there is a law of diminishing returns in play here. Kevin: thanks very much for the precise computation! I put it on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds#Asymptotics](http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds#Asymptotics) but I suppose it could be cleaned up more. (Actually, the same holds for just about everything else on the wiki.)"},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Hyper-optimistic conjecture Kristal and Jason: looks like we’re off to a good start with the human proof of %c^\\mu_3=6%. I copied Kristal’s text to the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Hyper-optimistic_conjecture](http://michaelnielsen.org/polymath1/index.php?title=Hyper-optimistic_conjecture) but presumably we could develop it further. Given how successful the linear programming method based on sphere densities has been for Moser, I think it is worth a shot at working out linear inequalities for slice densities for the HOC; indeed one can view Kristal’s argument as implicitly using these slice densities."},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Genetic algorithm Kareem, I put some of the description of your GA on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Genetic_algorithm](http://michaelnielsen.org/polymath1/index.php?title=Genetic_algorithm) If you had any further commentary (e.g. on the performance of the algorithm, and links to data) that would be great. Do you still think that verification of the line-free property is the bottleneck? One thought I had on this was to break up the lookup table into pieces, and only apply a piece of the table to each organism. The problem with this though is that it may allow organisms with one or two lines to survive for a bit longer than they should."},{"username":"terence-tao","timestamp":"0009-03-20T04:00:00.000Z","contents":"Kakeya Seva, this is a great suggestion! I transcribed your post to the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Kakeya_problem](http://michaelnielsen.org/polymath1/index.php?title=Kakeya_problem) A recent result of Dvir, Kopparty, Saraf, and Sudan show that a Kakeya set in %F_q^n% has size at least %q^n/2^n%, while an earlier example of Dvir (and reproduced in an appendix of a paper by Saraf and Sudan) constructs a Kakeya set of size %2^{n-1} q^n + O(q^{n-1})%. These are asymptotically tight to within a factor of 2 when q is large, but it doesn’t look like it works all that well for small q. So, we have a new low-dimensional problem to chew on: is %k_4=27% (i.e. is it impossible to have a 26-point set in [3]^4 that contains a line in every direction)? I guess a starting point would be to get a human proof of %k_3=13%."},{"username":"kareem-carr","timestamp":"2010-03-20T04:00:00.000Z","contents":"1104. Thanks Terry. I don’t have a lot of time right now but I will add any extra commentary in a few days. I am not completely sure that the line-free property is the bottleneck with respect to the speed of the program but it is probably a big contributor. I will have to do some experimenting. The line-free property causes limitations in several senses. It grows exponentially with n. The largest lookup table I have made is about half a gigabyte. It takes a long time to read into memory. If I only take part of it then I do think that could speed things up tremendously. One could squeeze some more speed up out of this idea by picking a random selection of lines in the lookup table and evaluating a whole generation with the same lines. This reduces cache misses. If it’s not the same group of lines each time then maybe this won’t hurt us too much."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"c^{\\mu}_3=6 I have made some changes to one of the cases, that case is  \nwhere there is one point removed of the form aaa c^{\\mu}_3=6  \nIf we have all  \nThree points of the form xxx removed  \nThen the remaining points have value 7 and  \nWe have covered all lines any set of moving coordinates  \nAnd all constant points equal to one value this leaves  \nThe lines xab a,b not equal. Each point of the set  \nabc covers three of these lines the entire set covers each of these lines  \nthere is no duplication the only alternative is to remove a point  \nabc and cover the lines with points of the form aab which have  \na higher weight and only cover one line each this would lower the weight  \nso the maximum weight occurs when all of abc is omitted along with  \nthe three points xxx and the weight is 6 If we have only two points removed of the form xxx then  \nThe weight is at most 8 say the point not removed is 222  \nThen we must cover the lines xx2 and x22 we have three six such  \nLines and all the xx2 must be covered one at a time by either 112  \nOr 332 the x22 must be covered one at a time by 322 or 122  \nThese points must be removed and the that lowers the weight  \nTo 8 – 3*(2/6) – 3*(2/6) = 6 again we have c^{\\mu} must be less than 6 If 111 is removed  \nthen we must cover all lines of  \nthe form xx2 xx3 and x22 and x33 Look at the pairs of lines such as xx2 and 33x  \none with moving coordinates in two positions and  \na fixed coordinate equal to 2 or 3 say 2  \nthe other with fixed coordinates equal to the other  \nvalue which in this case is 2 so if the fixed coordinate(s)  \nin one point of the pair is 2 in the other they or it will  \nThen we must have one of the points 222  \n112 332 removed to block the first line of the pair  \nand for the second line we can use 333  \n332 331\\. However we do not have the points  \n222 or 333 removed in this case so we must have  \neither 332 or the pair 112 or 331.  \nFor every one of the six point 332 or 223  \nwe will have a similar choice forcing either  \nthe removal of the point itself or the associated  \npair. After these choices have been made more points  \nof the form aab can be added but there must be  \na subset corresponding to one set of the above six choices  \nsince in each case there are only two ways to cover the lines noted.  \nIf we start with the configuration 111 removed all six points with  \ntwo 2’s and one three and two 3’s and one two removed and all points of  \nthe form abc removed then this configuration has weight 6  \nthen we can perform a series of steps which will  \nreach all combinatorial line free configurations. These steps are as follows: 1 Making choices as above and allowing the addition  \nOf all possible abcs 2.Removal of points of the form aab and addition of all possible abc’s 3.Removal of abc It will be shown that with each step the weight decreases or remains the same  \nso the weight is 6 or less  \nThis will give all line free configurations as we must have sub configuration  \ncorresponding to one of the six choices and all we can do is add points  \nof the form aab and take the resulting set with the most possible  \nAbc’s and them remove any arbitrary abcs that we wish to remove. Are the making of the six choices noted above and the addition of  \nany points of the form abc where possible without forming a combinatorial line.  \nAt the start each point of the form abc cannot be added because it has  \ntwo lines which are some permutations of x12 and x13 now  \nlook at the points possibly blocking x12 they are 112 212 and 312 initially  \npoint 312 which is removed could not be added because the two points  \n112 and 212 are not removed as each choice is moved then each of the  \nremoved lines of type 113 covers two lines of the form permutations of x13  \nsimilarly lines of type 133 covers two lines of the form permutations of x13  \nnow each choice to replace a line of the form 332 increase the number of  \npoints removed with two coordinates the same by one thus lowers the weight  \nby one third and blocks four lines of the form x13 or x12 thus after  \nn such choices we have reduced the weight by n/3 and covered 4n such  \nlines since every point of the of the form permutations of 123  \nstarts out with 2 such lines which it is blocking and can only be added  \nwhen they are filled we can only add at most 2n such points which  \nsince they have weight 1/6 at the end of n such steps the weight  \nis unchanged now afterwards if we remove more points of the form  \naab they cover at most two lines of the form xab and thus allow  \nat most two points of the form abc to be added thus the change in weight  \nis at most -1/3 +1/6 +1/6=0 finally afterwards we can remove points  \nof the form abc but that will only lower the weight. Finally we have no points of the form xxx removed but then we will  \nhave a line of the form xxx"},{"username":"jason-dyer","timestamp":"2012-03-20T04:00:00.000Z","contents":"Metacomment: Logo [A tongue-in-cheek suggestion.](http://www.atariage.com/2600/boxes/b_3DTicTacToe_Color_front.jpg)"},{"username":"seva","timestamp":"2012-03-20T04:00:00.000Z","contents":"Here is a sketch of what, hopefully, can be developed into a human proof of %k_3\\ge 13%. The argument depends heavily on the fact that, applying a non-degenerate affine transformation to a Kakeya set, we get a Kakeya set. Let %e_1=(1,0,0),e_2=(0,1,0),e_3=(0,0,1)% be the standard basis of %{\\mathbb F}_3^3%. Suppose that %K\\subset{\\mathbb F}_3^3% is a Kakeya set with %|K|=12%. For each of the %(3^3-1)/2=13% directions in %{\\mathbb F}_3^3% fix a line in this direction inside %K%. We thus have %13% lines, with %3% points on each line, resulting in %39% point-line incidences — and just %12% points; therefore, there is a point in %K%, incident with (at least) four lines. Shifting %K% suitably, we assume that %0\\in K% and %K% contains four lines through %0%. If all lines through %0% in %K% are contained in a subspace of dimension %2%, then %K% actually contains this subspace, and without loss of generality we can assume that the subspace is %{\\rm Sp}\\{e_1,e_2\\}%. Fix a vector in %K% outside this subspace; WLOG, this vector is %e_3%. Thus, %K% consists of %{\\rm Sp}\\{e_1,e_2\\}%, %e_3%, and two more vectors, and the proof is easy to complete in this case showing that such a set cannot contain a line in every direction. Therefore, the four lines through %0% are not contained in a proper subspace. Hence, applying a suitable linear transformation, we can assume that the directions, determined by these lines, are %e_1,e_2,e_3%, and %b%, with some vector %b\\in{\\mathbb F}_3^3% of weight at least %2%. (The weight of a vector for this purpose is the number of non-zero coordinates in its expansion in the standard basis.) That is, %\\{0,\\pm e_1,\\pm e_2,\\pm e_3,\\pm b\\}\\subset K%. If the weight of %b% is %3%, then, changing the signs of the vectors of the standard basis as needed (which is a linear transformation, of course), we can assume %b=e_1+e_2+e_3%; similarly, if the weight of %b% is %2%, then we can assume %b=e_1+e_2%. We thus are left with two cases to consider, as follows. Case (A): %K% consists of %0,\\pm e_1,\\pm e_2,\\pm e_3, \\pm(e_1+e_2+e_3)%, and three more vectors. Case (B): %K% consists of %0,\\pm e_1,\\pm e_2,\\pm e_3,\\pm(e_1+e_2)%, and three more vectors. Hopefully, both these cases can be completed with some moderate effort."},{"username":"terence-tao","timestamp":"0002-03-20T04:00:00.000Z","contents":"Kakeya I realised that I can import some existing arguments in the literature to get some better lower bounds on Kakeya in this low characteristic, high dimensional regime than the (3/2)^n bound from Dvir-Kopparty-Saraf-Sudan. For instance, we can use the “bush” argument. There are %N := (3^n-1)/2% different directions. Take a line in every direction, let E be the union of these lines, and let %\\mu% be the maximum multiplicity of these lines (i.e. the largest number of lines that are concurrent at a point). On the one hand, from double counting we see that E has cardinality at least %3N/\\mu%. On the other hand, by considering the “bush” of lines emanating from a point with multiplicity %\\mu%, we see that E has cardinality at least %2\\mu+1%. If we minimise %\\max(3N/\\mu, 2\\mu+1)% over all possible values of %\\mu% one obtains approximately %\\sqrt{6N} \\approx 3^{(n+1)/2}% as a lower bound of |E|, which is asymptotically better than %(3/2)^n%. Or, we can use the “slices” argument. Let %A, B, C \\subset ({\\Bbb Z}/3{\\Bbb Z})^{n-1}% be the three slices of a Kakeya set E. We can form a graph G between A and B by connecting A and B by an edge if there is a line in E joining A and B. The restricted sumset %\\{a+b: (a,b) \\in G \\}% is essentially C, while the difference set %\\{a-b: (a-b) \\in G \\}% is all of %({\\Bbb Z}/3{\\Bbb Z})^{n-1}%. Using an estimate from [my paper with Nets Katz](http://front.math.ucdavis.edu/math.CO/9906097), we conclude that %3^{n-1} \\leq \\max(|A|,|B|,|C|)^{11/6}%, leading to the bound %|E| \\geq 3^{6(n-1)/11}%, which is asymptotically better still. For an upper bound, I can invert the “slices” idea and use a construction of Imre Ruzsa. Let %A, B \\subset [3]^n% be the set of strings with %n/3+O(\\sqrt{n})% 1’s, %2n/3+O(\\sqrt{n})% 0’s, and no 2’s; let %C \\subset [3]^n% be the set of strings with %2n/3+O(\\sqrt{n})% 2’s, %n/3+O(\\sqrt{n})% 0’s, and no 1’s, and let %E = \\{0\\} \\times A \\cup \\{1\\} \\times B \\cup \\{2\\} \\times C%. From Stirling’s formula we have %|E| = (27/4 + o(1))^{n/3}%. Now I claim that for most %t \\in [3]^{n-1}%, there exists an algebraic line in the direction (1,t). Indeed, typically t will have %n/3+O(\\sqrt{n})% 0s, %n/3+O(\\sqrt{n})% 1s, and %n/3+O(\\sqrt{n})% 2s, thus %t = e + 2f% where e and f are strings with %n/3 + O(\\sqrt{n})% 1s and no 2s, with the 1-sets of e and f being disjoint. One then checks that the line %(0,f), (1,e), (2,2e+2f)% lies in E. This is already a positive fraction of directions in E. One can use the random rotations trick to get the rest of the directions in E (losing a polynomial factor in n). Putting all this together, I think we have %(3^{6/11} + o(1))^n \\leq k_n \\leq ( (27/4)^{1/3} + o(1))^n% or %(1.8207\\ldots+o(1))^n \\leq k_n \\leq (1.88988+o(1))^n% This seems consistent with your conjecture %k_{n+1} \\leq 2k_n + 1%."},{"username":"seva","timestamp":"0005-03-20T04:00:00.000Z","contents":"Kakeya in %F_3^r% Dear Terry, As I see it, your 1107 post essentially solves the problem: the gap between the lower and upper bounds is not that large now, and narrowing it further may be both pointless and tedious (while finding a sharp asymptotic may be hopeless). I would also say that the values of %k_r% for small %r% are not of much interest now that %\\limsup(\\ln k_r)/r is known to hold. A couple of minor remarks. The “bush argument” gives the very same estimate as that indicated in my original post. Since this estimates supersedes the Dvir-Kopparty-Saraf-Sudan bound, I have not mentioned the latter. For the estimate %k_r\\gg 3^{0.51r}% mentioned in my post I used restricted addition and Balog-Szemeredi; so, the “slices argument” is kind of an improvement of what I did. Also, for the slices argument: I don’t quite see why “the restricted sumset is essentially %C%“, but it is contained in %-C%, which is just fine for our purposes; correct?"},{"username":"terence-tao","timestamp":"0008-03-20T04:00:00.000Z","contents":"Kakeya in %F_3^r% Fair enough. It may still be interesting to know the dependence on the field size though. For each prime power q, let %k_{q,r}% be the minimal size of a Kakeya set in %F_q^r%, then we know from submultiplicativity %k_{q,r+s} \\leq k_{q,r} k_{q,s}% that %k_{q,r}^{1/r}% converges to a limit %\\kappa_q%. For instance I think one can show that %\\kappa_q = \\sqrt{2}%, and we now know that %1.82\\ldots \\leq \\kappa_3 \\leq 1.88\\ldots%. What happens for other values of q? Dvir et al. show that %\\kappa_q \\geq q/2% for all q, and of course %\\kappa_q \\leq q%. I don’t know which bound is closer to the truth. Your Balog-Szemeredi argument, by the way, sounds very similar to the original slices argument of Bourgain in which my paper with Nets was based (and the Bourgain paper was in turn inspired by Tim Gowers’ paper on Szemeredi’s theorem, and in particular on his version of the Balog-Szemeredi lemma): [http://www.ams.org/mathscinet-getitem?mr=1692486](http://www.ams.org/mathscinet-getitem?mr=1692486) In our language, the Bourgain argument would give %k_r \\gg 3^{13r/25} = 3^{0.52 r}%. p.s. Yes, “contained in -C” is what I meant by “essentially C”. I got lazy and didn’t want to work out the exact relationship :)"},{"username":"seva","timestamp":"2011-03-20T04:00:00.000Z","contents":"Kakeya in ${\\mathbb F}_3^r$ (in reply to 1109) > Your Balog-Szemeredi argument, by the way, sounds  \n> very similar to the original slices argument of Bourgain Not surprising: the idea to use this approach was actually suggested to me by Bourgain! Also, upon some thinking. It seems that Ruzsa’s argument that you mention is not really about slices; it is about a clever modification of the construction from my initial post. Namely, instead of the set %K% of all vectors in %{\\mathbb F}_3^r% with either %1%, or %2% missing among their coordinates, consider its subset %K_0:=A\\cup B%, where %A% is the set of all those vectors with %r/3+O(\\sqrt r)% coordinates equal to %1% and the rest equal to %0%, and %B% is the set of all those vectors with %2r/3+O(\\sqrt r)% coordinates equal to %2% and the rest equal to %0%. Then %K_0%, being of size just about %(27/4)^{r/3}%, contains lines in positive proportion of directions. No slices!"},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"Moser sets n=4 If a Moser set for n=4 has 5 or more points with three 2’s it must have less than 41 points it must have the following statistics:  \n(7, 15, 14, 5,0), (6, 15, 15, 5,0) I have already proved the case with 6 or more points cannot occur so all I need to do here is get the case n=5\\. We note that there cannot be a point with four 2’s as then we would have 39 points. We have 5 points with exactly three 2’s and one coordinate not equal to 2\\. That gives 5 values not equal to 2 and four coordinates one coordinate must have the coordinate value not equal to 2 from 2 of the five points. One value must be three the other one. We slice at this point and get two cubes with the center point filled which by the n=3 section of the wiki on Moser sets must have 13 points or less. Since there are 5 points with three 2’s the center slice must have the remaining three. Now if we have 41 or more points it must have a center slice equal to 15 points or more. However by the Pareto-optimal statistics in the section n=3 of the wiki we see that a cube with c greater than three can have value at most 15\\. Further it must have the statistics (3,9,3,0) The two side cubes must have statistics 3,6,3,1) or (4,6,2,1) this gives three possible combinations which give the following possible sets of statsitcs: (8, 15, 13, 5,0), (7, 15, 14, 5,0), (6, 15, 15, 5,0)  \nNow the statistics (8, 15, 13, 5,0) violate the following inequality from the section n=4 of the wiki  \n4a + b + c + d is less than or equal to 64 as it equals 65\\. So we are left with:  \n(7, 15, 14, 5,0), (6, 15, 15, 5,0) and we are done."},{"username":"kareem-carr","timestamp":"0002-03-20T04:00:00.000Z","contents":"Terry suggested that these types of visualizations might be more useful. I have made a few. [http://twofoldgaze.wordpress.com/2009/03/15/visualizing-solutions-i/](http://twofoldgaze.wordpress.com/2009/03/15/visualizing-solutions-i/) [http://twofoldgaze.wordpress.com/2009/03/15/visualizing-solutions-ii/](http://twofoldgaze.wordpress.com/2009/03/15/visualizing-solutions-ii/)"},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"4D Moser Kristal, I played around with Michael’s linear inequalities using maple and I think I can rule out d=5 completely for the 41+ point sets using them via a purely human proof (which was found, of course, using maple). On the 3D section of the Wiki, we have the inequalities %4\\alpha_0+4\\alpha_1+3\\alpha_2+\\alpha_3 \\leq 6% (1) %7\\alpha_0+3\\alpha_1+3\\alpha_2+\\alpha_3 \\leq 7% (2) which when averaged on middle slices gives the 4D inequalities %b/8 + c/6 + 3d/8 + e \\leq 6% (3) %7b/32 + c/8 + 3d/8 + e \\leq 7%. (4) Averaging (1) on side slices similarly gives %a/4 + b/8 + c/8 + d/8 \\leq 6%. (5) Computing 9/4 *(3) + (4) + 4*(5) gives %a+b+c+d+e \\leq 89/2 - 23 d / 32 - 9 e / 4% and so if %a+b+c+d+e \\geq 41% then we must have %d \\leq 4%. I got maple to find the integer solutions to %d=4, a+b+c+d+e \\geq 41% that were consistent with all known inequalities. They are: (6,16,15,4,0,0), (7,16,14,4,0,0), (7,17,13,4,0,0), and (7,18,12,4,0,0)."},{"username":"kareem-carr","timestamp":"0004-03-20T04:00:00.000Z","contents":"An animation of extremals of type c’_5: [http://twofoldgaze.wordpress.com/2009/03/16/visualizations-iii/](http://twofoldgaze.wordpress.com/2009/03/16/visualizations-iii/)"},{"username":"kareem-carr","timestamp":"0007-03-20T04:00:00.000Z","contents":"1115. I found a new largest solution for c’_7: (Text)  \n[http://twofoldgaze.wordpress.com/2009/03/16/solution-of-size-989-text/](http://twofoldgaze.wordpress.com/2009/03/16/solution-of-size-989-text/) (Image)  \n[http://twofoldgaze.wordpress.com/2009/03/16/solution-of-size-989-image/](http://twofoldgaze.wordpress.com/2009/03/16/solution-of-size-989-image/)"},{"username":"christian-elsholtz","timestamp":"0002-03-20T04:00:00.000Z","contents":"Kareem’s 989 Moser-example in dimension 7. here is an analysis which perhaps helps improving your programme:  \nI do not exactly know how your programme works, so the analysis is how a programme could have found the 989 set from the 988 set. The 988 solution and the new 989 solution do not differ very much.  \nStart with the 988 solution.  \nDelete {1112223, 1311231}  \nand add {1111233, 1112333, 2312231}  \nand you have the 989 solution. While a complete search to find such an exchange might need about  \n988 times 987/2 steps for the deletion, multiplied with  \n(2187-988)times (2187-989) times (2187-990)/6 steps for the  \naddition, the following speeds it up considerably: Start with a progression-free set.  \nTake a loop over one element to be added. (Here say 988 steps).  \nWrite down all clashes. (those must include of course the new element, so the search may be efficient).  \nIf there are many forget about this case,  \nif there are very few, delete some of them.  \nCheck if all clashes are avoided, and for any pair of two  \nelements list those elements that extend the two to a progression. In this example this works extremely well. Start with the 988 set.  \nadd {1111233}, giving 989 elements.  \nThe set of clashes is built by the five elements.  \n{1111233, 1112223, 1113213, 1211232, 1311231} Choose subsets of 2 of the 4 elements. (of course avoid 1111233 which we  \njust added). If in this way you delete for example {1112223,1311231}, and check for elements which  \nare now not the last point of an arithmetic progression, then you find  \n{1112333, 2312231}. (Well, I may have been lucky with this choice, but the point is that there are not so many cases.) More generally, it seems one can iterate this:  \ninstead of deleting k and adding k+1 elements, which seems to produce many  \ncases, proceed in the above balanced way:  \nadd one, and only use those cases with very few clashes.  \nChoose from these few clashes two (or more) elements to delete,  \nand check which elements can then be added etc."},{"username":"kareem-carr","timestamp":"0003-03-20T04:00:00.000Z","contents":"Christian’s idea sounds like a great idea for a mutation operator. The current one is somewhat dumb, it adds one where it can and doesn’t do any global considerations of the whole solution. I ruled out any kind of global examination of the solution as too slow but now that more computations aren’t yielding dramatically better results, a slow operator doesn’t seem as expensive as it did before. I had been intending to solicit advice on small tricks that people have found useful in generating new solutions. (If Christian or any one else has any ideas please let me know.) Thanks for the idea, Christian. There are an ever growing list of things that need to be done with this project but I will implement this one as soon as I can."},{"username":"micahael-bacon","timestamp":"0004-03-20T04:00:00.000Z","contents":"I can’t contribute directly to these types of developments, but I would be interested in Mr. Tao’s thoughts regarding what I think is the amazing ability of web-based approaches to these and many other math and physics issues. Does this new rapid and multi-personal addressing of issues offer hope that much more rapid progress will be made?"},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"4D Moser extremals The 3D Moser extremals that Michael found (by computer exhaustion of the 2^{27} cases, though I would imagine that we could also verify the resulting linear inequalities by hand if needed) have been very useful in higher D. It would thus be nice to obtain a list of 4D Moser extremals. Even just knowing the e=1 extremals would presumably improve our inequalities somewhat. Unfortunately, direct exhaustion requires 2^{81} cases to check, which is too huge. There is a symmetry group of size %4! \\times 2^4 \\approx 2^9% to cut things down a little bit, and we can of course close off any branch of the exhaustion that contains a line, but this is still not feasible. On the other hand, by collapsing the 81 points of %[3]^4% into the five classes a,b,c,d,e (with 16, 32, 24, 8, 1 points in each) we get a much more tractable integer program, and we can already get quite far just from using the linear inequalities inherited from 3 and lower dimensions. But it doesn’t quite give us everything we need, e.g. it gives an upper bound of 44 off the bat, and additional work is needed to get 43, and we still can’t rule out d=4 41-point sets other than via Klas’s data. So I propose an intermediate regime, in which we identify 1s and 3s but leave the 2s intact. Let’s use x to denote letters that are either 1 or 3, then the 81 points get split up into 16 groups: 2222 (1 point), 222x, 22×2, 2×22, x222 (two points each), 22xx, 2x2x, x22x, 2xx2, x2x2, xx22 (four points each), 2xxx, x2xx, xx2x, xxx2 (eight points each), xxxx (sixteen points). Let c(w) denote the number of points inside a group w, e.g. c(xx22) is the number of points of the form xx22 inside the set, and is thus an integer from 0 to 4. Every combinatorial line in %\\{2,x\\}^4% gives rise to an inequality between the corresponding c’s. For instance, the line %\\{222x, 2xxx\\}% gives rise to the inequality %c(222x)/2 + c(2xxx)/4 \\leq 2% (as can be seen by considering the four lines connecting a 222x point to two 2xxxx points) and more generally we have %c(u)/2^a + c(v)/2^{b-1} \\leq 2% whenever %\\{u,v\\}% is a line where u has a 2’s and v has b 2’s with %b > a%. This is an integer program on 16 variables and 80 line inequalities (plus the upper bound %c(2222) \\leq 1% and the 16 inequalities asserting that all c(w) are non-negative), which looks tractable. One should be able to get a list of Pareto-optimal and extremal c-statistics either by integer-programming or by brute force. (The total number of possible c-statistics is %2 \\times 3^4 \\times 5^6 \\times 9^4 \\times 17 \\approx 2.8 \\times 10^{11}%, which is at the edge of what brute force can deal with, but presumably one can use symmetries to cut things down a bit (e.g. assuming %c(2xxx) \\leq c(x2xx) \\leq c(xx2x) \\leq c(xxx2)% cuts things down by a factor of 13 or so; also, c(xxxx) can be easily maximised after making all the other choices, potentially saving a factor of 17). The e=1 case (when c(2222)=1) should be particularly doable (the total number of possibilities here is now %2^4 \\times 3^6 \\times 5^4 \\times 9 \\approx 6.5 \\times 10^8%, and after using symmetries this should be quite manageable). Once the possible c-statistics are known, the (a,b,c,d,e) statistics can then be recovered (for instance, we may be able to improve the current upper bound of 39 for the size of e=1 4D Moser sets). (Also, the c-statistics also reveal the (a,b,c,d,e) statistics of 4D diagonal cubes in higher dimensions, e.g. xxyyzw, which could lead to further improvement in the 6D and 7D bounds.) Of course, just as the a,b,c,d,e inequalities didn’t fully recover all possible statistics of Moser sets, it could be that the c-inequalities (which are only “averaged” versions of the property of having no combinatorial lines) don’t quite capture the full truth of the situation. But they should do better than the (a,b,c,d,e) inequalities method, and seem within range of a reasonable computer search."},{"username":"kareem-carr","timestamp":"0004-03-20T04:00:00.000Z","contents":"I thought this analysis of the solutions of c_7 might be of interest: [http://twofoldgaze.wordpress.com/2009/03/16/the-structure-of-the-extremal-1302/](http://twofoldgaze.wordpress.com/2009/03/16/the-structure-of-the-extremal-1302/) [http://twofoldgaze.wordpress.com/2009/03/16/the-structure-of-the-extremals-of-1302-ii/](http://twofoldgaze.wordpress.com/2009/03/16/the-structure-of-the-extremals-of-1302-ii/)"},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"c-statistics I should say that perhaps it would be wise to first see if this method can already recover the known extremals in 3D. I can at least do 2D by hand: here we have four integers %0 \\leq c(22) \\leq 1%, %0 \\leq c(2x), c(x2) \\leq 2%, %0 \\leq c(xx) \\leq 4% obeying the inequalities %c(22) + c(2x), c(22) + c(x2) \\leq 2; c(2x)+c(xx), c(x2)+c(xx) \\leq 4; 2c(22) + c(xx) \\leq 4%. By symmetry we may also impose (say) %c(2x) \\leq c(x2)%. The extremals for %(c(22), c(2x), c(x2), c(xx))% are then (0,0,0,4), (0,2,2,2), (1,1,1,2), which correspond exactly to the (a,b,c) extremals (0,0,4), (0,4,2), (1,2,2). Also, every linear c-inequality at a given dimension implies further linear c-inequalities at higher dimensions by suitable averaging over cubes (but we don’t average as much as in the (a,b,c,d,e) case, we only average over permutations of 1 and 3). One could imagine iteratively building up the c-inequalities much as we have been doing for the coarser (a,b,c,d,e) type inequalities. For instance the above 2D extremals show that %c(22) + c(2x) + c(xx), c(22)+c(x2)+c(xx) \\leq 4%, which can be leveraged to higher dimensions (e.g. the first inequality implies that %c(222)+c(2x2)+c(xx2) \\leq 4% and %c(22x)/2+c(2xx)/2+c(xxx)/2 \\leq 4%, and also %c(22)+c(22x)+c(xxx)/2 \\leq 4%, by looking at yz2, yzx, and yyz cubes respectively, where the wildcard x is understood to be 1 or 3 but the other wildcards are unrestricted.) Dear Micahael: these issues are being discussed over at Gowers’ blog at [http://gowers.wordpress.com/2009/03/10/polymath1-and-open-collaborative-mathematics/](http://gowers.wordpress.com/2009/03/10/polymath1-and-open-collaborative-mathematics/)"},{"username":"kristal-cantwell","timestamp":"0005-03-20T04:00:00.000Z","contents":"Alternative to c-statistics One alternative is breaking up points according to their number of 1’s, 2’s and 3’s this would grow as %n^3% as opposed to c-statistics which might be exponential. Also both could be combined in order to get more information."},{"username":"michael-peake","timestamp":"0009-03-20T04:00:00.000Z","contents":"Alternative to c-statistics Kristal, isn’t that just the %\\Gamma% slices we have been using? It only grows as %n^2% because the total number of 1s, 2s and 3s is constant. I am running Terry’s idea for %{}[3]^4% at home, and it looks like it will take a day or two."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"Alternative to c-statistics Yes I think these are the slices we have been using and I agree they are quadratic, %(n^2 + n)/2%."},{"username":"kareem-carr","timestamp":"0004-03-20T04:00:00.000Z","contents":"The next installment in my new series ‘Better know an extremal …’: [http://twofoldgaze.wordpress.com/2009/03/17/the-structure-of-the-extremals-of-1302-iii/](http://twofoldgaze.wordpress.com/2009/03/17/the-structure-of-the-extremals-of-1302-iii/)"},{"username":"michael-peake","timestamp":"0002-03-20T04:00:00.000Z","contents":"c-statistics I ran a search for the sixteen c-statistics (2×22, 22×2, etc)  \nI could only find 65 rules, from the 65 combinatorial lines. My results limit the total possible points for 4D Moser below 73, which is higher than 43\\. Either I have a bug, or there was too little connection between the various c statistics. For the record, the Pareto extremals it found were  \n16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0  \n8,8,8,8,8,4,4,4,4,4,4,2,2,2,2,1  \n12,4,4,4,4,2,2,2,2,2,2,1,1,1,1,0  \n14,2,2,2,2,1,1,1,1,1,1,0,0,0,0,0  \n10,6,6,6,6,3,3,3,3,3,3,1,1,1,1,0  \n15,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0  \n13,3,3,3,3,1,1,1,1,1,1,0,0,0,0,0  \n11,5,5,5,5,2,2,2,2,2,2,1,1,1,1,0  \n9,7,7,7,7,3,3,3,3,3,3,1,1,1,1,0"},{"username":"michael-peake","timestamp":"0004-03-20T04:00:00.000Z","contents":"c-statistics I ran through the 2^27 subsets of the 3D cube, looking for Moser sets. I classified them with the c-statistics. I collected Pareto optimal points. Then, from the extremal points I found a set of twenty linear inequalities satisfied by all Moser sets. They are the same linear inequalities found earlier for the ‘xxxxyyz diagonals’. They are on Sheet 8 of a spreadsheet in the Moser section of the wiki. I don’t yet know their effects in 4D"},{"username":"jason-dyer","timestamp":"0009-03-20T04:00:00.000Z","contents":"Progress on %\\overline{c}^\\mu_6 = 15% What I have typed so far is at [the talk page about Fujimura’s on the wiki](http://michaelnielsen.org/polymath1/index.php?title=Talk:Fujimura%27s_problem). My time is limited (my wife is close to giving birth to our first child) so it may be a while before I can finish all the cases."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"Progress on \\overline{c}^\\mu_6 = 15 “My time is limited (my wife is close to giving birth to our first child)” Congratulations!"},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"c^{\\mu}_4=9 If c^{\\mu}_4 is greater than 9 then the counterexample must occur  \nwhen there is exactly one point of the form aaaa removed. The other three cases are below. If we have all  \nthree points of the form aaaa removed  \nthen the remaining points have value 12 and  \nwe have covered all lines any set of moving coordinates  \nwith all constant points equal to one value this leaves  \nthe lines xxab, xabc and xaab. let us also remove the three sets of the form aabc, a, b, c not equal. These block the remaining lines and the size is now 9\\. Now suppose we try to replace the removed points of the form aabc by points of the form aaab or aabb. Now we must cover the lines of the form xaab each point of the form aabc covers two and has a weight 1/12 each point of the form aabb covers 4 and has a weight 1/6 each point of the form aaab covers three and has a weight of 1/3 and since each element of the form covers two elements of the form xaab that no other element of the set aabc covers and as noted above all lines of the form xaab are covered by the the elements of the three sets of the form aabc we cannot improve on the weight as we can only cover the xaab’s by the same or higher weight lowering the total from 9 so for this case the value is 9. If we have two points removed of the form aaaa then  \nthe weight is at most 13 say the point not removed is 2222  \nthen we must cover the lines xxx2 and x222 we have eight such  \nlines and all six xx22 must be covered one at a time by either 1122  \nor 3322 the four x222 must be covered one at a time by 3222 or 1222  \nthe four xxx2 must be covered by 3332 or 1112  \nthese points must be removed and the that lowers the weight  \nTo 13 – 4*(6/24) – 6*(1/6) – 4(6/24) = 10.  \nWe also have 24 lines of the form xabc which can only be covered  \ntwo at a time by the aabc which have weight 1/12 so we need to cover  \nthese with at least 12 of these which we can do by choosing all points which are permutations of  \n1123 this reduces the number to 9 and  \nagain we have c^{\\mu} must be 9. Finally we have no points of the form aaaa removed but then we will  \nhave a line of the form xxxx."},{"username":"jason-dyer","timestamp":"0003-03-20T04:00:00.000Z","contents":"%\\overline{c}^\\mu_6 = 15% %\\overline{c}^\\mu_6 \\geq 15% from the bound for general n. Note that there are ten extremal solutions to %\\overline{c}^\\mu_3%: Solution I: remove 300, 020, 111, 003  \nSolution II (and 2 rotations): remove 030, 111, 201, 102  \nSolution III (and 2 rotations): remove 030, 021, 210, 102  \nSolution III’ (and 2 rotations): remove 030, 120, 012, 201 Also consider the same triangular lattice with the point 020 removed, making a trapezoid. Solutions based on I-III are: Solution IV: remove 300, 111, 003  \nSolution V: remove 201, 111, 102  \nSolution VI: remove 210, 021, 102  \nSolution VI’: remove 120, 012, 201 The on the 7x7x7 triangular lattice triangle 141-411-114 must have at least one point removed. Remove 141, noting by symmetry any logic that follows will also work for either of the other two points. Suppose we can remove all equilateral triangles on our 7×7×7 triangular lattice with only 12 removals. Here, “top triangle” means the top four rows of the lattice (with 060 at top) and “bottom trapezoid” means the bottom three rows. At least 4 of those removals must come from the top triangle (the solutions of %\\overline{c}^\\mu_3% mentioned above). The bottom trapezoid includes the overlapping trapezoids 600-420-321-303 and 303-123-024-006\\. If the solutions of these trapezoids come from V, VI, or VI’, then 6 points have been removed. Suppose the trapezoid 600-420-321-303 uses the solution IV (by symmetry the same logic will work with the other trapezoid). Then there are 3 disjoint triangles 402-222-204, 213-123-114, and 105-015-006\\. Then 6 points have been removed. Therefore at least six removals must come from the bottom trapezoid. To make a total of 12 removals there must be either:  \nCase A: 4 removals from the top triangle and 8 from the bottom trapezoid.  \nCase B: 5 removals from the top triangle and 7 from the bottom trapezoid.  \nCase C: 6 removals from the top triangle and 6 from the bottom trapezoid. * Suppose case A is true. Because 141 is already removed, the solution to the upper triangle must remove either solution I (remove 060, 330, 033), solution II (remove 060, 231, 132), solution IIb (remove 033, 150, 240) or solution IIc (remove 330, 051, 042) Suppose I is the solution for the upper triangle. Suppose 222 is open. Then 420, 321, 123, and 024 must all be removed. This leaves five disjoint triangles which require removals in the bottom trapezoid (150-600-105, 051-501-006, 222-402-204, 231-411-213, 132-312-114); therefore the bottom trapezoid needs at least 9 removals, but we can only make 8, therefore 222 is closed. Suppose 411 is open. Then 213 and 015 must be removed. This leaves five disjoint triangles such that each triangle must have exactly one removal (420-150-123, 321-051,024, 600-510-501, 402-312-303, 204-114-105), so the remaining point (006) must be open, forcing 501 to be removed. This makes 600 and 510 open, and based on the triangles 600-240-204 and 510-150-114 both 204 and 114 must both be removed, but 204 and 114 are on the same disjoint triangle, contradicting the statement that each triangle must have exactly one removal. So 411 is closed. This leaves six disjoint triangles each which must have at least one removal (420-123-150, 321-024-051, 510-213-240, 312-015-042, 501-204-231, 402-105-132). This forces 600 and 006 to be open. Based on the triangles 006-501-051 and 600-204-240, this forces 501 and 204 to be open. But then there are no removals from the triangle 501-204-231, which is a contradiction. Therefore the solution of the upper triangle cannot be I. Suppose II is the solution for the upper triangle. There are seven disjoint triangles (150-600-105, 051-501-006, 222-402-204, 240-510-213, 042-312-015, 330-420-321, 033-123-024), therefore of the three points remaining in the bottom trapezoid (411, 303, 114) exactly one must be removed. Suppose 411 is removed. Then 114 and 303 are open; 114 open forces 510 to be removed, forcing 213 to be open. 114 and 213 open force 123 to be closed, forcing 024 to be open. 024 open forces 222 to be closed, which forces 204 to be open, which leaves the triangle 213-303-204 open so we have a contradiction. By symmetry 114 also can’t be removed. Therefore we must remove 303\\. This leaves 411 and 114 open, forcing 510 and 015 closed. 510 and 015 closed forces 312 and 213 open, forcing 222 closed. 222 closed forces 402 and 204 open. 402 and 204 open force 501 and 105 closed. 510 and 105 closed force 600 and 006 open, leaving equilateral triangles 600-204-240 and 402-006-042 open. Therefore 303 can’t be removed, and so the solution of the upper triangle can’t be II. Suppose IIb is the solution for the upper triangle. Suppose 024 is open. This forces 420, 321, and 222 closed. Five disjoint triangles remain (510-600-501, 231-411-213, 132-312-114, 123-303-105, 024-204-006) so each must have exactly one point removed, and the remaining points in the lower trapezoid (402, 015) must be open. This forces 312, 411, 510, and 006 closed, which then force the the other points in their disjoint triangles open (600, 501, 231, 213, 132, 114, 204). The triangle 501-231-204 is therefore open, so we have a contradiction, therefore 024 is closed. Suppose 006 is open. This forces 600, 501 and 402 to be closed, and leaves five disjoint triangles (510-420-411, 321-231-222, 312-132-114, 303-213-204, 105-015-006) and so 123 must be open. This forces 222 closed, which forces 321 open, which forces 420 closed, which forced 510 and 411 open, which forces 213 closed, which forces 303 and 204 open, which forces 105 closed, which forces 015 and 006 to be open, leaving an open triangle at 411-015-051\\. Therefore we have a contradiction, so 006 is closed. Given 024 and 006 closed, now note there are six disjoint triangles (600-510-501, 402-312-303, 204-114-105, 420-330-321, 222-132-123, 411-231-213). Therefore the remaining point in the lower trapezoid 015 must be open, forcing 510, 411, and 312 to be closed. Using the disjoint triangles this forces 600, 501, 402, 303 and 213 to be open, which then forces 420 and 321 to be closed. Both 420 and 321 are on the same disjoint triangle, therefore we have a contradiction, so IIb can’t be solution. Note by symmetry, the same logic for IIb will apply for IIc. Therefore case A isn’t true. * Suppose case B is true. The row 330-231-132-033 has ten possible solutions (excluding reflections, which by symmetry will be handled by the same logic): Case Q: open-open-open-open  \nCase R: closed-closed-closed-closed  \nCase S: closed-open-open-open  \nCase T: closed-open-closed-closed  \nCase U: open-closed-closed-closed  \nCase V: closed-open-closed-closed  \nCase W: closed-closed-open-open  \nCase X: closed-open-closed-open  \nCase Y: open-closed-closed-open  \nCase Z: closed-open-open-closed Consider all 10 cases. Note in all these cases we are still assuming 141 is removed. (Case Q) 330, 231, 132, 033 open forces 060, 150, 051, 240, 141, 042 closed, but the upper triangle only allows 5 removals, so case Q forms a contradiction. (Case R) 060-240-042 is left open, so case R forms a contradiction. (Case S) 231, 132, and 033 open force 031 and 042 removed, which means from 240, 150, 061 there must be exactly one removal. Suppose 213 is open. Then 411 and 015 are closed, and five disjoint triangles are left where each triangle must have exactly one removal (600-420-402, 501-321-303, 312-222-213, 204-105-114, 123-024-033). So the two remaining points in the lower trapezoid (510 and 006) must be open. 510 and 006 open force 303 closed, which forces 321 and 501 open, which forces 204 closed, which forces 114 and 103 open, which forces 402 and 312 closed, forcing 222 open, leaving 321-231-222 as an open triangle, so we have a contradiction. Therefore 213 is closed. This leaves six disjoint triangles (600-420-402, 501-231-204, 303-033-006, 411-321-312, 222-132-123, 114-024-015) which each have exactly one removal. Therefore 510 from the lower trapezoid is open. Note since one of 150 and 060 must be open, then one of 114 or 015 must be closed; therefore 024 is open. This forces 123 closed, forcing 222 to be open, forcing 321 to be closed, forcing 411 and 312 open, forcing 421 closed, forcing 600 and 402 open, forcing 303 closed, forcing 006 open, forcing 204 closed, forcing 501 open, leaving the triangle 600-510-501 open and a contradiction. (Case T) 231 is closed, and 330, 132, and 033 open force 060, 150, and 042 closed. Since 141 is already closed we have five removals from the top triangle, so 240 and 051 are open. Suppose 024 is open. This forces 321 and 123 closed, leaving five disjoint triangles (600-510-501, 402-312-303, 204-114-105, 420-240-222, 213-033-015). Therefore 006 and 411 remaining in the lower trapezoid are open, forcing 501, 303 and 015 closed, forcing 600, 510, 312, 402, and 213 open, forcing 222 closed, forcing 420 open, leaving the open triangle 420-510-411, so we have a contradiction. Therefore 024 is closed. There are now six disjoint triangles (510-600-501, 330-420-321, 312-402-303, 132-222-123, 033-213-015, 114-204-105). This leaves 411 and 006 open, which forces 015 and 501 closed, which forces 510 and 213 open, leaving the triangle 240-510-213 open, so we have a contradiction. (Case U) Given the removals 231, 132, 033, and 141, the only possible removal to not leave any equilateral triangles in the upper triangle is 060\\. So 330, 240, 150, 051, and 042 are open. Suppose 420 is open. This forces 321, 222, and 123 closed. This leaves five disjoint triangles (600-330-303, 510-420-411, 204-105-114, 501-051-006, and 312-042-015), so we have a contradiction. Therefore 420 is closed. This leaves six disjoint triangles (600-330-303, 501-051-006, 204-114-105, 510-240-213, 411-321-312, 222-042-024). 015 in the lower trapezoid is therefore open, forcing 312 and 411 closed, but 312 and 411 are on the same disjoint triangle, so we have a contradiction. (Case V) Given the removals 330, 132, and 033, the only possible removal to not leave any equilateral triangles in the upper triangle is 060\\. So 150, 051, 240, 042, and 231 are open. Suppose 420 is open. Then 222 and 123 are closed, and we are left with 6 disjoint triangles (150-600-105, 240-510-213, 231-501-204, 024-114-015, 042-402-006, 411-321-312). This exceeds our limit of 7 removals in the bottom trapezoid, so we have a contradiction. Therefore 420 is closed. Again we are left with the same 6 disjoint triangles (150-600-105, 240-510-213, 231-501-204, 024-114-015, 042-402-006, 411-321-312). Therefore 222, 123, and 303 are open. This forces 321, 024, and 105 to be closed, which then forces 411 and 015 to be open, forming an open triangle at 051-411-015, so we have a contradiction. (Case W) Given the removals 330 and 231 with 132 and 033 open, 132 and 033 open force 042 closed. Given 141 is already closed, that leaves one more removal on the upper triangle, which must be one of 060-150-051, so 240 must be open. Suppose 024 is open. This forces 123 to be closed, and leaves 6 disjoint triangles (600-510-501, 420-240-222, 411-321-312, 402-132-105, 213-033-015, 204-024-006). So 303 and 114 in the lower trapezoid are left open, forcing 312 and 005 to be closed, forcing 204 and 321 to be open, forcing 600 and 501 to be closed. 600 and 501 are on the same disjoint triangle so we have a contradiction. Therefore 024 is closed. Suppose 312 is open. This forces 114 to be closed, and leaevs 5 disjoint triangles (600-510-501, 411-321-312, 303-213-204, 222-132-123, 105-015-006). Therefore 402 in the lower trapezoid is open, which forces 105 to be closed, which forces 015 and 006 to be open, which forces 213 and 303 to be closed, which are on the same disjoint triangle, so we have a contradiction. Therefore 312 is closed. This leaves five disjoint triangles (510-240-213, 501-411-402, 222-132-123, 204-114-105, 303-033-006), and leaves 600, 420, 321, and 015 in the lower trapezoid open. This forces 402 and 213 to be closed, which forces 510 and 411 to be open, leaving an open triangle at 510-420-411, so we have a contradiction. (Case X) 330 and 132 closed and 231 and 033 open imply 051 is closed, and since 141 is closed and we have only one more removal in the upper triangle it must be one of 240, 042, or 060\\. This leaves 150 open. Suppose 321 is open. This forces 222 to be closed and leaves six disjoint triangles (600-420-402, 510-150-114, 411-321-312, 303-213-204, 105-015-006, 123-033-024), leaving 501 in the lower trapezoid open. This forces 204 to be closed, which forces 303 to be open, leaving an open triangle at 501-321-303. So 321 is closed. This leaves six disjoint triangles (600-420-402, 510-150-114, 501-231-204, 312-222-213, 123-033-024, 105-015-006) and leaves 303 and 411 in the lower trapezoid open. This forces 213 and 006 to be closed, and forces 312, 222, 105, and 015 to be open. This forces 600 to be closed and 402 to be open, leaving an open triangle at 402-312-303\\. So we have a contradiction. (Case Y) 330 and 033 are open, 330 and 033 open force 060 to be closed. With 141 closed there can be only one more removal, so suppose 150 is open (and note that if 150 was closed, we can use symmetry considering 051 to be open). Suppose 600 is open. This forces 303 to be closed, and leaves six disjoint triangles (510-150-114, 420-330-321, 411-501-402, 222-312-213, 033-123-024, 015-105-006). So 204 in the lower trapezoid is left open. 600 and 150 open implies 105 is closed, forcing 015 and 006 to be open, forcing 024 and 213 to be closed, forcing 312, 222, and 123 to be open, forcing 042 in the upper triangle to be closed (so 051 and 240 are open). 051 and 015 open force 411 to be closed, which forces 501 to be open and leaves the open triangle 051-501-006. So 600 is closed. This leaves six disjoint triangles (510-150-114, 420-330-321, 411-501-402, 222-312-213, 033-123-024, 015-105-006). 015 is the lower trapezoid is then left open, forcing 312 and 411 to be closed. However, 312 and 411 are on the same disjoint triangle, so we have a contradiction. (Case Z) 330, 141, and 033 are closed, and 231 and 132 are open. Suppose 051 is open (note that if this forms a contradiction, by symmetrical argument we can say both 150 and 051 are closed). Suppose 114 is closed. This leaves six disjoint triangles (600-510-501, 402-312-303, 105-015-006, 411-231-213, 222-132-123, 321-051-024) and so 420 and 204 are open. This forces 501 to be closed, which forces 510 and 600 to be open, which forces 411 and 402 to be closed, which forces 213 and 303 to be open, leaving an open triangle at 213-303-204. So 114 is open. Suppose 213 is closed. This leaves five disjoint triangles (600-420-402, 501-321-303, 411-051-015, 222-132-123, 204-024-006) and in the lower trapezoid 510 and 105 are open. This forces 204 to be closed, forcing 024 and 006 to be open, forcing 015 to be closed, forcing 411 to be open, forcing 420 to be closed, forcing 402 to be open, leaving the open triangle 402-132-105. So both 114 and 213 are open. Therefore 411 and 312 are closed. This leaves five disjoint triangles (600-510-501, 303-213-204, 105-015-006, 222-132-123, 321-051-024). The remaining points in the lower trapezoid (420, 402) are then open, forcing 105 to be closed, forcing 015 and 006 to be open, forcing 024 to be closed. Also note 114 and 213 open force 123 to be closed, which forces 222 to be open, which forces 321 to be closed. 321 and 024 are on the same disjoint triangle, so we have a contradiction. So 150 and 051 are both closed. However, this leaves the triangle 240-042-060 open, so case Z is impossible. Since all ten solutions have been eliminated, case B is impossible. * Suppose case C is true. Suppose the trapezoid 600-420-321-303 used solution IV. There are three disjoint triangles 402-222-204, 213-123-114, and 105-015-006\\. The remainder of the points in the lower trapezoid (420, 321, 510, 501, 402, 312, 024) must be left open. 024 being open forces either 114 or 015 to be removed. Suppose 114 is removed. Then 213 is open, and with 312 open that forces 222 to be removed. Then 204 is open, and with 024 that forces 006 to be removed. So the bottom trapezoid is a removal configuration of 600-411-303-222-114-006, and the rest of the points in the bottom trapezoid are open. All 10 points in the upper triangle form equilateral triangles with bottom trapezoid points, hence 10 removals in the upper triangle would be needed (more than the 6 allowed), so 114 being removed doesn’t work. Suppose 015 is removed. Then 006-024 forces 204 to be removed. Regardless of where the removal in 123-213-114, the points 420, 321, 222, 024, 510, 312, 501, 402, 105, and 006 must be open. This forces upper triangle removals at 330, 231, 042, 060, 051, 132, our remaining 6 removals on the top triangle. However, we have already removed 141, forcing one removal too many, so the trapezoid 600-420-321-303 doesn’t use solution IV. Suppose the trapezoid 600-420-321-303 uses solution VI. The trapezoid 303-123-024-006 can’t be IV (already eliminated by symmetry) or VI’ (leaves the triangle 402-222-204). Suppose the trapezoid 303-123-024-006 is solution VI. The removals from the lower trapezoid are then 420, 501, 312, 123, 204, and 015, leaving the remaining points in the lower trapezoid open. The remaining open points is forces 10 upper triangle removals, so the trapezoid 600-420-321-303 doesn’t use solution VI. Therefore the trapezoid 303-123-024-006 is solution V. The removals from the lower trapezoid are then 420, 510, 312, 204, 114, and 105\\. The remaining points in the lower trapezoid are open, and force 9 upper triangle removals, hence the trapezoid 303-123-024-006 can’t be V, and the solution for 600-420-321-303 can’t be VI. The solution VI’ for the trapezoid 600-420-321-303 can be eliminated by the same logic by symmetry. Therefore it is impossible for the bottom trapezoid to use only 6 removals. We have determined cases A, B, and C to be impossible, therefore it impossible to form a triangle free configuration on the 7x7x7 lattice with only 12 removals. Therefore %\\overline{c}^\\mu_6 = 15%."},{"username":"jason-dyer","timestamp":"0003-03-20T04:00:00.000Z","contents":"%\\overline{c}^\\mu_6 = 15% The proof is [complete](http://michaelnielsen.org/polymath1/index.php?title=Talk:Fujimura%27s_problem). I tried to post it here but I guess it was too large and got caught by the spam filter? It’s definitely a level of complexity over latex \\overline{c}^\\mu_5$."},{"username":"michael-peake","timestamp":"0005-03-20T04:00:00.000Z","contents":"c-statistics The 3D cube has 14 ways to fit in the 4D cube: 4 side-slices 2yzw, 4 middle slices xyzw, and six diagonal slices wwyz. So twenty inequalities for the cube become 280 inequalities for the 4D cube. There is some repetition, so there are only 239 different inequalities. One solution of these inequalities has 44 points: %cxxxx = cxxx2 = cxx22 = 4% and permutations. There were 2150 inequalities for the 5D cube with c-statistics; I have not yet found the maximum for that."},{"username":"jason-dyer","timestamp":"0008-03-20T04:00:00.000Z","contents":"Commentary on %\\overline{c}^\\mu_6 = 15%. The basic proof strategy was to set up removals such that there would be enough disjoint triangles to say that each triangle needed to have one removal. Then any points not in the union of disjoint triangles were forced to be in %\\overline{c}^\\mu_6%, and those points could be used to force a chain of removals leading to an impossibility. Unfortunately the process was not very mechanical, so it’d be hard to come up with a mathematical generalization. Each case was like its own individual puzzle, and I often had to go through multiple bad attempts before I found a set that broke things open. However, there was a technique I used in making solution sketches which may have some useful general properties — look for all the possible disjoint triangle sets on a possible configuration, then pick the “overlay pattern” that was the most useful (possibly switching patterns mid-problem). For example, here’s a simple mathematical property: Suppose some set of points %Q% on the triangular lattice that has a maximal number of disjoint triangles %m% where %m% removals are also sufficient to produce a triangle-free set. Let %F_1,\\dots,F_j% be the triangle-free sets of %Q% (if the solution is unique, %j = 1%). Consider all sets %T_1,\\dots,T_k% where %k% is the number of possible configurations of %m% disjoint triangles in the set %Q%. Then for each %1 \\leq i \\leq k%, %Q \\setminus T_i% must be a subset of %F_1\\cap\\dots\\cap F_j%. (In other words, any points remaining after locating disjoint triangles must be open, considering all of the possible disjoint triangle configurations.) It’s also possible to set up a graph representing where the disjoint triangles overlap. Let me take the case of %\\overline{c}^\\mu_3%. There are two disjoint triangle configurations: A = 030-120-021, B = 210-300-201, C = 012, 102, 003 and D = 030-210-012, E = 120-300-102, F=021-201-003\\. Any removal from A has to remove also remove from either D, E, or F; any removal from B has to remove from either D, E, or F, and any removal from C has to remove from either D, E, or F. This can be represented as a complete bipartite graph between A, B, C and D, E, F. I’m not sure if anything in this will be useful for solving general n, but it might be possible to scrounge some tools to work on low-n cases."},{"username":"michael-peake","timestamp":"0008-03-20T04:00:00.000Z","contents":"Kakeya k_3 > 12 We try to find a set A containing twelve points in F_3^3 that form a Kakeya set. Seva showed in 1106 that we can assume A contains four lines through the origin, including the three axes. (000),(001),(002),(010),(020),(100),(200) Case A: The fourth line is in the xy plane z=0\\. Then seven points are in this plane, leaving five points in the two planes z=1, z=2\\. There are nine directions not parallel to the xy plane; each direction must contain two of the five points, one in the z=1 plane and one in the z=2 plane. This is impossible. Case B: The fourth line is (000),(111),(222)  \nIf there is a sixth point in the z=0 plane, there are only six points left in z=1 and z=2\\. To form nine directions out of the xy plane, we need three points in z=1 and three in z=2\\. We also need none of the nine directions parallel to each other. But (002)-(111) is parallel to (222)-(001), so we can’t get nine non-parallel directions out of the plane. So the z=0 plane only contains points along the axes. Likewise the x=0 and y=0 planes. There are three points left to place, and they all lie in the cube [1,2]^3. To form a (120) line, we need to include (121) and (211). Then, to form a (110) line, we need to include either (112) or (221) Case B1: the extra three points are (121),(211),(112). There is no (112) line. Case B2: the extra three points are (121),(211),(221). There is no (102) or (012) line."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"c^{\\mu}_4=9 This is the final case of the partial proof in 1129. If one point of the form aaaa is removed say 1111  \nthen we must cover all lines of  \nthe form xxx2 xxx3 and xx22 and xx33 and x222 and x333  \nLook at the pairs of lines such as xxx2 and 333x  \none with moving coordinates in three positions and  \na fixed coordinate equal to 2 or 3 say 2  \nthe other with fixed coordinates equal to the other  \nvalue which in this case is 3. Then we must have one of the points 2222  \n1112 3332 removed to block the first line of the pair  \nand for the second line we can use 3333  \n3332 3331\\. However we do not have the points  \n2222 or 3333 removed in this case so we must have  \neither 3332 or the pair 1112 or 3331.  \nFor every one of the eight points 3332 or 2223  \nwe will have a similar choice forcing either  \nthe removal of the point itself or the associated  \npair. After these choices have been made more points  \nof the form aaab can be added but there must be  \na subset corresponding to one set of the above eight choices  \nsince in each case there are only two ways to cover the lines noted.  \nLook at the pairs of lines such as 22xx and xx33  \none with moving coordinates in two positions and  \na two fixed coordinates equal to 2 or 3 say 2  \nthe other with two fixed coordinates equal to the other  \nvalue which in this case is 2 so if the fixed coordinate(s)  \nin one point of the pair is 2 in the other they or it will  \nThen we must have one of the points 2211  \n2222 2233 removed to block the first line of the pair  \nand for the second line we can use 3333  \n2233 1133\\. However we do not have the points  \n2222 or 3333 removed in this case so we must have  \neither 2233 or the pair 1133 or 2211.  \nFor every one of the six points with two threes and two twos  \nwe will have a similar choice forcing either  \nthe removal of the point itself or the associated  \npair. After these choices have been made more points  \nof the form aabb can be added but there must be  \na subset corresponding to one set of the above six choices If we start with the configuration 1111 removed all eight points with three 2’s and one three and three 3’s and one two removed and all points with two 3’s and two 2’s removed and all points of the form aabc removed then this configuration has weight 8  \nthen we can perform a series of steps in order which will  \nreach all combinatorial line free configurations.  \nThese steps are as follows: 1 Making choices as above for elements of the form aaab and allowing the addition  \nof all possible aabc’ss. 2 Making choices as above for elements of the form aabb and allowing the addition  \nof all possible aabc’ss. 3.Removal of points of the form aabb or aaab and addition of all possible aabc’s. 4.Removal of aabc’s. At present none of the aabc can be added  \nthere are lines of the form xx12 and xx13 and x113 and x112  \nwhich each have to blocked to allow their addition What we are going to do is perform the previous steps in order.  \nwe will show that each step eliminates twice as much in weight  \nof elements of the form aabc as it adds of all other forms. To facilitate the proof  \nwe will have to perform the operations in the order listed. However we will  \nbe able to reach every possible configuration. We must have a subconfiguration  \ncorresponding to one set of choices above then we can delete various elements  \nof the form aabb and aaab to get all possible line free sets of the elements of  \nthe form aabb and aaab and then for each set we add the maximum possible number  \nof elements of the form aabc. The point is doing this is that we can only add two units in weight  \nof the form aabc as the lines of the form xabc must be covered and they  \nCan only be covered by one unit in weight of elements of the form abc  \nSo if we have the two to one ratio or better and have to stop at two we will  \nEnd up with a net change of 2-1 which will raise the weight to 9 or less  \nAnd we still have the maximum weight of is 9. When we make a choice deleting a pair and adding say  \n2223 which will give 1113 and 2221 we block three lines  \nof the form xx13 and three of the form xx12 thus allowing  \nthe addition of 6 points of weight 1/12 which give a total  \nchange of weight 1/2 which is exactly twice the net change in  \nweight caused by the addition of 2223 and the removal of 1113  \nof 2221 Now with each substitution there will also be the blocking of lines  \nof the form x113 or x112 however to allow the point say 2113  \nTo be added we must block both x113 and 211x and to do this  \nWe must make choices adding both 2223 and 2333 which unblocks  \nThe line 2xx1 and forbids the addition of the point 2113 thus the increase in  \nmaterial of the form aabc is twice the net change in weight of the other types 1122 can block the 11×2 two at a time but their  \nweight is 1/6 so even if they block two of the 11×2  \nand allow the addition of two 1132’s the weight will  \nremain the same and since each replacement of 2233  \nby 1133 and 2211 has only one element of the form  \n2211 we are done with step one. For the choices of elements  \nof the form aabb if we replace an element of the form  \n2223 by 1113 and 2221 we will be able to block three lines  \nof the form x113 which gets rid of 3 elements of the form  \n2113 but the weight of the addional aaab element created is 1/4 and the weight of the removed elements is the same. So step  \ntwo can not improve the weight. So we are done with the choices mentioned above and move on two steps 3 and 4 deleting individual elements. If we add an element of the form 2233 and delete 1133 and 2211 then it can unblock two lines of the form x113 and two of the form x112 thus allowing the addition of at most 4 points of the form  \n3112 thus giving a net change of -1/4 +4/12 and again the ratio is 2 to one. Then after all the above choices have been made we can delete elements of the form 2223 which will block at most three lines  \nOf the form 2xx3 if created by the above choices  \nAnd the ratio of points added to points deleted is at most one  \nAnd so is less than 1/2 and we are done. The case 3332  \nis handled the same way. If we delete an element of the form 1112 we unblock three lines  \nOf the form xx12 and three of x112 we free at most 6 points with  \nWeight 1/12 giving a total weight of 1/2 the point removed is weight 1/4 so the ratio is 2 to 1 and we are done the case of  \n1113 is similar. If we delete an element of the form 3331 it can unblock three lines  \nof the form xx31 and we free three points for possible addition  \nThe have total weight equal to the weight of 3331 so the ratio is  \nless than 2 to 1\\. The case of an element of the form 2221 is similar. If we delete a point of the form 2233  \nWe don’t unblock any lines so we can’t  \nAdd any points of the form aabc. If we delete a point of the form 1133 we unblock two lines  \nOf the form xx13 and two of x113 and so can add four points  \nWith weight 1/12 which has total 1/3 which is twice the weight  \nOf the point 1133 so the ratio is less than or equal to 2 and we are  \nDone. The case of deleting a point of the form 1122 is handled similarly.  \nSo we are done with step three. If we delete a point of the form aabc the weight is less. So we have gone through all steps and the ratio in them 2 or less and thus in this case the weight is 9 or less and we are done. Since this is the final case we have proved the theorem and the weight is 9."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"c^{\\mu}_4=9 This is the final case of the partial proof in 1129\\. It is divided  \ninto two parts. Here is part one: If one point of the form aaaa is removed say 1111  \nthen we must cover all lines of  \nthe form xxx2 xxx3 and xx22 and xx33 and x222 and x333  \nLook at the pairs of lines such as xxx2 and 333x  \none with moving coordinates in three positions and  \na fixed coordinate equal to 2 or 3 say 2  \nthe other with fixed coordinates equal to the other  \nvalue which in this case is 3. Then we must have one of the points 2222  \n1112 3332 removed to block the first line of the pair  \nand for the second line we can use 3333  \n3332 3331\\. However we do not have the points  \n2222 or 3333 removed in this case so we must have  \neither 3332 or the pair 1112 or 3331.  \nFor every one of the eight points 3332 or 2223  \nwe will have a similar choice forcing either  \nthe removal of the point itself or the associated  \npair. After these choices have been made more points  \nof the form aaab can be added but there must be  \na subset corresponding to one set of the above eight choices  \nsince in each case there are only two ways to cover the lines noted.  \nLook at the pairs of lines such as 22xx and xx33  \none with moving coordinates in two positions and  \na two fixed coordinates equal to 2 or 3 say 2  \nthe other with two fixed coordinates equal to the other  \nvalue which in this case is 2 so if the fixed coordinate(s)  \nin one point of the pair is 2 in the other they or it will  \nThen we must have one of the points 2211  \n2222 2233 removed to block the first line of the pair  \nand for the second line we can use 3333  \n2233 1133\\. However we do not have the points  \n2222 or 3333 removed in this case so we must have  \neither 2233 or the pair 1133 or 2211.  \nFor every one of the six points with two threes and two twos  \nwe will have a similar choice forcing either  \nthe removal of the point itself or the associated  \npair. After these choices have been made more points  \nof the form aabb can be added but there must be  \na subset corresponding to one set of the above six choices If we start with the configuration 1111 removed all eight points with three 2’s and one three and three 3’s and one two removed and all points with two 3’s and two 2’s removed and all points of the form aabc removed then this configuration has weight 8  \nthen we can perform a series of steps in order which will  \nreach all combinatorial line free configurations.  \nThese steps are as follows: 1 Making choices as above for elements of the form aaab and allowing the addition  \nof all possible aabc’ss. 2 Making choices as above for elements of the form aabb and allowing the addition  \nof all possible aabc’ss. 3.Removal of points of the form aabb or aaab and addition of all possible aabc’s. 4.Removal of aabc’s."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"c^{\\mu}_4=9 Part 2 continued form 1133: At present none of the aabc can be added  \nthere are lines of the form xx12 and xx13 and x113 and x112  \nwhich each have to blocked to allow their addition What we are going to do is perform the previous steps in order.  \nwe will show that each step eliminates twice as much in weight  \nof elements of the form aabc as it adds of all other forms. To facilitate the proof  \nwe will have to perform the operations in the order listed. However we will  \nbe able to reach every possible configuration. We must have a subconfiguration  \ncorresponding to one set of choices above then we can delete various elements  \nof the form aabb and aaab to get all possible line free sets of the elements of  \nthe form aabb and aaab and then for each set we add the maximum possible number  \nof elements of the form aabc. The point is doing this is that we can only add two units in weight  \nof the form aabc as the lines of the form xabc must be covered and they  \nCan only be covered by one unit in weight of elements of the form abc  \nSo if we have the two to one ratio or better and have to stop at two we will  \nEnd up with a net change of 2-1 which will raise the weight to 9 or less  \nAnd we still have the maximum weight of is 9. When we make a choice deleting a pair and adding say  \n2223 which will give 1113 and 2221 we block three lines  \nof the form xx13 and three of the form xx12 thus allowing  \nthe addition of 6 points of weight 1/12 which give a total  \nchange of weight 1/2 which is exactly twice the net change in  \nweight caused by the addition of 2223 and the removal of 1113  \nof 2221 Now with each substitution there will also be the blocking of lines  \nof the form x113 or x112 however to allow the point say 2113  \nTo be added we must block both x113 and 211x and to do this  \nWe must make choices adding both 2223 and 2333 which unblocks  \nThe line 2xx1 and forbids the addition of the point 2113 thus the increase in  \nMaterial of the form aabc is twice the net change in weight of the other types 1122 can block the 11×2 two at a time but their  \nweight is 1/6 so even if they block two of the 11×2  \nand allow the addition of two 1132’s the weight will  \nremain the same and since each replacement of 2233  \nby 1133 and 2211 has only one element of the form  \n2211 we are done. For the choices of elements  \nof the form aabb if we replace an element of the form  \n2223 by 1113 and 2221 we will be able to block three lines  \nof the form x113 which gets rid of 3 elements of the form  \n2113 but the weight of the addional aaab element created is 1/4 and the weight of the removed elements is the same. So step  \ntwo can not improve the weight. So we are done with step 2 and move on to step three If we add an element of the form 2233 and delete 1133 and 2211 then it can unblock two lines of the form x113 and two of the form x112 thus allowing the addition of at most 4 points of the form  \n3112 thus giving a net change of -1/4 +4/12 and again the ratio is 2 to one. Then after all the above choices have been made we can delete elements of the form 2223 which will block at most three lines  \nOf the form 2xx3 if created by the above choices  \nAnd the ratio of points added to points deleted is at most one  \nAnd so is less than 1/2 and we are done. The case 3332  \nis handled the same way. If we delete an element of the form 1112 we unblock three lines  \nOf the form xx12 and three of x112 we free at most 6 points with  \nWeight 1/12 giving a total weight of 1/2 the point removed is weight 1/4 so the ratio is 2 to 1 and we are done the case of  \n1113 is similar. If we delete an element of the form 3331 it can unblock three lines  \nof the form xx31 and we free three points for possible addition  \nThe have total weight equal to the weight of 3331 so the ratio is  \nless than 2 to 1\\. The case of an element of the form 2221 is similar. If we delete a point of the form 2233  \nWe don’t unblock any lines so we can’t  \nAdd any points of the form aabc. If we delete a point of the form 1133 we unblock two lines  \nOf the form xx13 and two of x113 and so can add four points  \nWith weight 1/12 which has total 1/3 which is twice the weight  \nOf the point 1133 so the ratio is less than or equal to 2 and we are  \nDone. The case of deleting a point of the form 1122 is handled similarly. So Step 3 is finished. If we delete a point of the form aabc the weight is less. So we have gone through all steps and the ratio in them 2 or less and thus in this case the weight is 9 or less and we are done. Since this is the final case we have proved the theorem."},{"username":"klas-markstrom","timestamp":"0005-03-20T04:00:00.000Z","contents":"Other values of k  \nI have been busy the last few days, with both work and more everyday things like having a cold, but I decided to let the computers to do some new work by expandingto new values of k. Here  \n[http://abel.math.umu.se/~klasm/Data/HJ/](http://abel.math.umu.se/~klasm/Data/HJ/)  \nis a table with the optimum sizes for subsets of [k]^n without combinatorial lines, and the optimal solutions for some cases. (Anyone who can edit the wiki is welcome to add it there) One natural question here is; For which values of n and k is the optimum size (k-1)k^n ? Kareem, maybe your program can improve the lower bounds for the values my program could not determine?"},{"username":"kristal-cantwell","timestamp":"0009-03-20T04:00:00.000Z","contents":"c^{\\mu}_4=9 Sorry about the duplication I put the proof for the final case in and it did not go through then I split it in half and it went through later the original finally went through. I have put the cases 3 and 4 in the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Hyper-optimistic_conjecture](http://michaelnielsen.org/polymath1/index.php?title=Hyper-optimistic_conjecture)"},{"username":"jason-dyer","timestamp":"0007-03-20T04:00:00.000Z","contents":"Dimension 2, any k Regarding the data in Markström.1135, it is simple to show when restricting to dimension two the maximal set size has to be k(k-1). This can be done by removing the diagonal values 11, 22, 33, …, kk. Since they are in disjoint lines this removal is minimal."},{"username":"michael-peake","timestamp":"2011-03-20T04:00:00.000Z","contents":"Dimension 2, any k The k missing points are one per line and one per column.  \nSo their y-coordinates are a shuffle of their x-coordinates.  \nThere are k! rearrangements of the numbers 1 to k.  \nThe k points include a point on the diagonal, so this shuffle is not a derangement. There are k!/e derangements of the numbers 1 to k, so k!(1-1/e) optimal solutions"},{"username":"klas-markstrom","timestamp":"0001-03-20T04:00:00.000Z","contents":"Dimension 3, k at least 3  \nLet S be a latin square of side k on the symbols 1…k, with colour i in position (i,i) ( This is not possible for k=2 ) Let axis one in S correspond to coordinate 1 in [k]^3, axis two to coordinate 2 and interpret the colour in position (i,j) as the third coordinate. Delete the points so defined. The line with three wild cards has now been removed.  \nA line with two wildcards will be missing the point corresponding to the diagonal in S.  \nA line with a single wildcard will be missing a point corresponding to an off diagonal point in S. Something similar should work in higher dimensions if one can find latin cubes etc with the right diagonal properties."},{"username":"michael-peake","timestamp":"0005-03-20T04:00:00.000Z","contents":"%k \\ge d%, k prime Delete those points whose coordinates add up to a multiple of k.  \nEvery combinatorial line has one point deleted, except for the major diagonal of d=k, which has all points deleted."},{"username":"kristal-cantwell","timestamp":"0008-03-20T04:00:00.000Z","contents":"Values near primes Could we extend 1140 to values near primes with only a slight loss? For instance for 100 we delete all values equal to zero mod 101 and 1 mod 101 then all combinatorial lines with d less than 101 will have 100 different values mod 101 and hence will have one deleted value. So we will have a line free set for d equal to 101 or less with density 99/101 or more."},{"username":"kristal-cantwell","timestamp":"0008-03-20T04:00:00.000Z","contents":"All values I am looking for a further extension of 1141 according to R. C. Baker and G. Harman, “The difference between consecutive primes,” Proc. Lond. Math. Soc., series 3, 72 (1996) 261–280\\. MR 96k:11111 Abstract: The main result of the paper is that for all large x, the interval A=[x-x^0.535,x] contains prime numbers. This is from the abstract at [http://primes.utm.edu/references/refs.cgi?long=BH96](http://primes.utm.edu/references/refs.cgi?long=BH96) I found it using: [http://primes.utm.edu/notes/gaps.html](http://primes.utm.edu/notes/gaps.html) So for any number n there is a nearby prime p whose difference from  \nn is x^.535 so then we remove x^.535 +1 values mod p and for those values the density will be (p-x^.535-1)/p  \nof d less than p we will have no combinatorial lines."},{"username":"kristal-cantwell","timestamp":"0009-03-20T04:00:00.000Z","contents":"lower bounds on line free sets I think the results of 1141 can be used to get lower bounds on lines free sets for large n for all values of k. For any k and any n we can find a prime prelatively close to k^n then we remove the first k+1 values mod p then we pick a value then we remove the must k+1 so we only have k+2, 2k+3, etch  \nthe idea is to prevent any two values on a line because two points on a combinatorial line increase by at most k. This has density 1/k so we have  \na line free density of 1/(k+1)."},{"username":"kristal-cantwell","timestamp":"0009-03-20T04:00:00.000Z","contents":"I think the bound in 1142 could possibly be improved. First by getting most of the set concentrated around the point with equal numbers of ones twos and threes or the point with values closes to equality the standard deviation should be something like the square root of n. Then we could apply the near prime with sets c(k^.5 + 1) and get a density of roughly c/k^.5  \nwhich I think will be better than the Behrend-Elkin construction as e^-x will eventually be less than 1/x as x increases without limit and the square root of k will increase without limit."},{"username":"terence-tao","timestamp":"0002-03-20T04:00:00.000Z","contents":"Just a quick note to say that I’m traveling at present and have not been able to reply to all the interesting progress made in the last week or so, but it looks like the project is humming along nicely without my assistance anyway! I’ll try to say something more constructive when I return next week."},{"username":"kevin-obryant","timestamp":"0003-03-20T04:00:00.000Z","contents":"Implication of Behrend/Elkin for %c_n^{\\mu}%. If %n% is sufficiently large, then %c_n^\\mu > n^2 2^{-4\\sqrt{2}\\sqrt{\\log n}+1/2 \\log \\log n -2}%, where the %log%‘s are base 2. I’ve posted the details to the wiki under “Hyper-optimistic conjecture” ([http://michaelnielsen.org/polymath1/index.php?title=Hyper-optimistic_conjecture](http://michaelnielsen.org/polymath1/index.php?title=Hyper-optimistic_conjecture))."},{"username":"kevin-obryant","timestamp":"0003-03-20T04:00:00.000Z","contents":"Gaps between primes If I recall correctly, then the Riemann hypothesis implies that there is always a prime between %n% and %n^{1/2+\\epsilon}% (provided of course that %n% is large enough, while the truth is believed to be that there is a prime between %n% and %n + C (\\log n)^2%, and some people have guessed %C=1%."},{"username":"anonymous","timestamp":"0004-03-20T04:00:00.000Z","contents":"Hyper-optimistic conjecture extended beyond 3 If the Hyper-optimistic conjecture is extended to 4, 5 etc I can prove it for some specific values. Specifically I think I can prove it Hyper-optimistic conjecture for all the values k greater than d where k is prime as in 1140, the key is every incidence is exactly one and every incidence is of minimum weight on the lines consisting of x repeated d times x followed by d-1 a’s not divisible by the prime k, x followed by d-2 a’s not divisible by the prime k etc these lines will have to be blocked somehow and this can only be done at greater or equal cost. So for the extension of this conjecture beyond 3 we have that it holds true for primes p for values less than or equal to p."},{"username":"kristal-cantwell","timestamp":"0004-03-20T04:00:00.000Z","contents":"Hyper-optimistic conjecture extended beyond 3 The previous anonymous comment was by me, I forgot to include my information."},{"username":"kristal-cantwell","timestamp":"0007-03-20T04:00:00.000Z","contents":"Extending Hyper-optimistic conjecture To extend this conjecture we need to define a weight beyond 3  \nwe do this by letting the weight of a point with a ones b twos c threes  \nand d fours to be a!b!c!d!/a+b+c+d! Similarly we can add e for 5 f for6  \nthe general pattern should be clear.  \nWe also need to extend slices thus in the case of k=4 we add a d which is the number of 4’s in the slice similarly we add an e in the case of k=5\\. The extension of the conjecture would that the maximal weight is realized by a set of slices."},{"username":"kareem-carr","timestamp":"0007-03-20T04:00:00.000Z","contents":"1149. Klas, I will look into it. I did not have much time last week but things are opening up now. Kareem"},{"username":"kristal-cantwell","timestamp":"0007-03-20T04:00:00.000Z","contents":"Extending Hyper-optimistic conjecture I am going to try to prove part of the result I tried to prove in 1146, there is at least one mistake. I want to prove that if p is a prime and d is a dimension less than p and the Hyper-optimistic conjecture is extended as in 1148 for p then for the specific values of d  \nless than p the Hyper-optimistic conjecture is true. To try to prove this try we remove all points whose sum is divisible by p, then since permuting the coordinates will not change the sum this is a set of slices we only need to prove it is minimal in weight and it covers all combinatorial lines. So I have it is a set of slices. It covers all combinatorial lines because the line since d is less than p must increase by a number less than p hence all values modulo p will be realized including p and we are done. So I have it covers all combinatorial lines. I still need to prove the weight is minimal though to get the result I want.  \nI thought I could do this in 1146 but what I wrote did not work."},{"username":"klas-markstrom","timestamp":"2011-03-20T04:00:00.000Z","contents":"Gaps between primes. There is a later improvement of the gap result. Baker, R. C.(1-BYU); Harman, G.(4-LNDHB); Pintz, J.(H-AOS)  \nThe difference between consecutive primes. II.  \nProc. London Math. Soc. (3) 83 (2001), no. 3, 532–562\\. Here they find a prime in [x-x^{0.525},x]"},{"username":"jason-dyer","timestamp":"0007-03-20T04:00:00.000Z","contents":"Dimension 2, any k The number of optimal solutions is [this sequence](http://www.research.att.com/~njas/sequences/A002467)."},{"username":"klas-markstrom","timestamp":"2011-03-20T04:00:00.000Z","contents":"Bounds on c_n from Fujmura. Has anyone calculated which lower bounds for c_n we get from the optimal solutions to Fujimura’s problem I constructed a while ago?"},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"Bounds on c_n from Fujmura. I have gone back to the posts 923 through 927 where you gave some of these solutions. I didn’t find any improvement to our current bounds. I think to go over some of these solutions the process will have to be automated since there are a lot of solutions in some cases. Possibly we might be able to improve some of the bounds at higher levels by going through all the cases."},{"username":"klas-markstrom","timestamp":"0001-03-20T04:00:00.000Z","contents":"Bounds on c_n from Fujmura. Yes for the smaller n we might actually already have the optimal values, but for higher n we could possibly get better bounds than we get from the bounds for general n. Does anyone want to volunteer to automate this? I can probably find the full set of optimal solutions to Fujimura’s problem for a few more values of n too."},{"username":"klas-markstrom","timestamp":"0001-03-20T04:00:00.000Z","contents":"Do we have any “hand made” values for small n for the quadruple version of Fujimura’s problem Kristal suggested?"},{"username":"jason-dyer","timestamp":"0004-03-20T04:00:00.000Z","contents":"Higher-dimensional Fujimura for k=4: When n=2, the largest triangle-free set is trivially 2 (remove for example 1000 and 0100). When n=3, the largest triangle-free set is 6 (remove 0200, 1010, 1001, 0011). Proof: One of 2000, 0200, 0020 must be removed — say it is 0200, and note by symmetry the logic that follows will apply if 2000 or 0020 is removed. There are three disjoint triangles (1010-0110-0020, 2000-1100-1001, 0002-0101-0011) which must all have at least one removal, so the minimal number of removals is 4, so we have proven our solution to be optimal."},{"username":"jason-dyer","timestamp":"0004-03-20T04:00:00.000Z","contents":"Higher-dimensional Fujimura Oops, my n=3 is wrong. Higher dimensions are tricky: I left 2000-0020-0002 open. Let me fiddle and come back."},{"username":"jason-dyer","timestamp":"0004-03-20T04:00:00.000Z","contents":"Higher-dimensional Fujimura Do over! When n=3, the largest triangle-free set is 6 (remove 2000, 0200, 0020, 0002). Proof: One of 2000, 0200, 0020 must be removed — say it is 0200, and note by symmetry the logic that follows will apply if 2000 or 0020 is removed. There are three disjoint triangles (1010-0110-0020, 2000-1100-1001, 0002-0101-0011) which must all have at least one removal, so the minimal number of removals is 4, so we have proven our solution to be optimal. Note this not only leaves “upside down” triangles but “lateral” triangles like 1100-1010-0010, but none seem to form a combinatoric line."},{"username":"jason-dyer","timestamp":"0007-03-20T04:00:00.000Z","contents":"Higher-dimensional Fujimura My n are one off from our convention, I of course mean: When k=4 and n=1, the largest triangle-free set is 2.  \nWhen k=4 and n=2, the largest triangle-free set is 6. I believe that when k=4 and n=3, the largest triangle free set is 12, but I do not have a proof yet. The lower bound is 6(n-1), using the exact same pattern on each face of the tetrahedron as general n bound with the k=3 case. We also need a symbol to represent the problem. It’s tempting to use %\\overline{\\overline{c}}^\\mu_n% for k=4 but we might be working with larger k (making the stack rather large)."},{"username":"michael-peake","timestamp":"0008-03-20T04:00:00.000Z","contents":"Lower bounds on line-free sets There are %k^{n-1}% disjoint lines *abcd..m, so the density of removed points must be at least 1/k, and retained points at most (k-1)/k.  \nIf %n \\le p \\le k% then one can get a density of (p-1)/p by deleting points whose coordinates sum to a multiple of p.  \nThe lower bound (p-1)/p approaches (k-1)/k as %k\\rightarrow \\infty%."},{"username":"jason-dyer","timestamp":"0009-03-20T04:00:00.000Z","contents":"Tetrahedron geometry In the k = 3 case of Fujimura things get tricky when n >= 3 because the internal points in the tetrahedral lattice must be considered. I believe my early lower bound was too hasty."},{"username":"jason-dyer","timestamp":"2010-03-20T04:00:00.000Z","contents":"k=4 Fujimura Now I see where my problem has been, we need the tetrahedron-free set to match combinatoric lines, not the triangle-free set. So n=1 would just be one removal and a set size of 3.  \nand n=2 would be two removals (say, 1100 and 0002) and a set size of 8."},{"username":"klas-markstrom","timestamp":"2011-03-20T04:00:00.000Z","contents":"Higher-dimensional Fujimura Jason, are you considering triangles or squares? I’m just wondering if you are looking at the problem Kristal posted at Tim’s blog or a different version?"},{"username":"klas-markstrom","timestamp":"2011-03-20T04:00:00.000Z","contents":"It seems you answered my question while I was writing it."},{"username":"jason-dyer","timestamp":"2011-03-20T04:00:00.000Z","contents":"Fixing k=4 Fujimura Just in case I make any more mistakes I will putting current progress for now [on the talk page of the Wiki](http://michaelnielsen.org/polymath1/index.php?title=Talk:Fujimura%27s_problem). The current values I have now (barring any further mistakes *cough*) are: n = 0, largest set of 1  \nn = 1, largest set of 3  \nn = 2, largest set of 7 I’m using %\\overline{\\overline{c}}^\\mu_n% for now (until we decide on a better notation)."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"HJ(3,3) is greater than 7 For a fixed r and k, the least n needed for the Hales-Jewett theorem to apply is denoted HJ(k,r).  \nSee [http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem](http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem)  \nCurrently we have: HJ(3,1) = 1  \nHJ(3,2) = 4  \nHJ(3,3) > 6 We start with the set formed by removing (0,1,6), (1,0,6), (0,5,2), (5,0,2) , (1,5,1), (5,1,1),(1,6,0), (6,1,0) from D_7.  \nNote that none of (0,1,6), (1,0,6), (0,5,2), (5,0,2) , (1,5,1), (5,1,1),(1,6,0), (6,1,0)contains a point whose coordinate sum is divisible by three we give it color 1  \nwhere (a,b,c) is shorthand for the slice Γ_a,b,c.  \nIt is combinatorial line free from the n=7 section of the upper and lower bounds wiki at  \n[http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds#n.3D7](http://michaelnielsen.org/polymath1/index.php?title=Upper_and_lower_bounds#n.3D7) then we divide the remaining points into all points whose coordinate sum is not equal to 0 mod 9 and  \nall points of (0,1,6), (1,0,6), (0,5,2), (5,0,2) , (1,5,1), (5,1,1),(1,6,0), (6,1,0)whose coordinate sum is equal to 1 mod 3 We give these color 2 then we take all points of (0,1,6), (1,0,6), (0,5,2), (5,0,2) , (1,5,1), (5,1,1),(1,6,0), (6,1,0)whose coordinate sum equal to 2 mod 3 and those points  \nwhose sum is equal to 0 mod 9\\. We give these color 3. Then with this coloring there are nor monochromatic lines. If there are any monochromatic  \nLines with number of moving coordinates not divisible by three in color 2 they must contain point whose coordinate sum is equal to 2 mod three  \nBut there are no such points with color 2\\. If there are any monochromatic lines whose coordinate sum is divisible by three  \nThan they must contain points whose coordinate sum is 0 mod 9 but there are no such points with color 2\\. So there  \nAre no monochromatic combinatorial lines with color 2\\. If there are any monochromatic  \nLines with number of moving coordinates not divisible by three in color 3 they must contain point whose coordinate sum is equal to 1 mod three  \nBut there are no such points with color 3\\. If there are any monochromatic lines whose coordinate sum is divisible by three  \nThan they must contain points whose coordinate sum is not 0 mod 9 but there are no such points with color 3. As already noted color 1 is combinatorial line free and we are done."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"Bounds on c_n from Fujmura. I have been looking at the upper bounds that we have and I think some of them come from lower bounds from Fujimura problem so I think that improvements to this problem could give better bounds."},{"username":"klas-markstrom","timestamp":"0001-03-20T04:00:00.000Z","contents":"Higher-dimensional Fujimura Jason, how about using %\\bar{c}^{\\mu}_{n,k}% where k is the size of the tuples, or the dimension of the Z^k used?  \nThe original Fujimura problem would correpspond to %\\bar{c}^{\\mu}_{n,3}% and the quadruple version to %\\bar{c}^{\\mu}_{n,4}% You get a non-constructive quadratic lower bound for the quadruple problem by taking a random subset of size %c n^2%. If c is not too large the linearity of expectation shows that the expected number of tetrahedrons in such a set is less than one, and so there must be a set of that size with no tetrahedrons."},{"username":"jason-dyer","timestamp":"0005-03-20T04:00:00.000Z","contents":"%\\overline{c}^\\mu_{n,4}% Klas, I have adopted your notation and added your non-constructive bound [to the talk page](http://michaelnielsen.org/polymath1/index.php?title=Talk:Fujimura's_problem). Should this be merged into the regular page or should we be making a separate one for the k > 3 cases?"},{"username":"michael-peake","timestamp":"2010-03-20T04:00:00.000Z","contents":"%\\overline{c}^\\mu_{n,4}% if the coordinates of points (a,b,c,d), then a+2b+3c is a four-term arithmetic progression for any of these tetrahedrons.  \nWe include slices a+2b+3c = constant.  \n[http://arxiv.org/PS_cache/arxiv/pdf/0811/0811.3057v2.pdf](http://arxiv.org/PS_cache/arxiv/pdf/0811/0811.3057v2.pdf) this paper shows that the proportion of slices included can be at least  \n%C\\frac{(log N)^{1/4}}{2^{2\\sqrt{2 log N}}}% for some constant C."},{"username":"klas-markstrom","timestamp":"2010-03-20T04:00:00.000Z","contents":"%\\overline{c}^{\\mu}_{n,k}% Michael, this should give a lower bound for general k as well.  \nLikewise the counting argument gives an upper bound for general k too. It is also possible to use a triangle-free set to build a tetrahedron-free set for a larger value of n, but I’m not sure how efficient such constructions can be made."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"%\\overline{c}^{\\mu}_{n,k}% In general for %\\overline{c}^{\\mu}_{n,k}% the series a + 2b + 3c+…  \ncontinued to k-1 terms will be a k-term arithmetic progression for  \nthe points of any configuration (a+r, b,..), (a,b+r .. which is the generalization  \nof an upward triangle, an upward k-1 dimensional simplex. So as in 1160  \nfor the case k=4  \nwe can get a formula in this case the general formula from the paper  \n[http://arxiv.org/PS_cache/arxiv/pdf/0811/0811.3057v2.pdf](http://arxiv.org/PS_cache/arxiv/pdf/0811/0811.3057v2.pdf)  \nand we get if k is greater than 2^n +1  \n%C\\frac{\\sqrt[2n]{log N}}{2^{n2^{(n-1)/2}\\sqrt[2n]{log N}}}%  \nThis is actually a set of formulas as n will have to be chosen given k  \nit is the greatest integer less than log base 2 of k-1."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"4D Moser If a Moser set has 4 points with 3 2’s and its size is 41  \nor more then it must not have two sets of two points with three twos with  \nthe same c statistic. To prove this assume that it is true then we must have two  \npoints without lost of generality of the form x222 then  \none must have one in the first position the other must  \nhave 3 we can then cut at the first coordinate and get  \nthe side slices must have 13 or less points which means  \nthat the center slice must have 15 or more points and  \nthe remaining two points of the form say 2×22 are in the  \ncenter cube so we can slice and the side squares have  \nThe center spot occupied that means that total number  \nof points in the center cube with one coordinate equal  \nto 2 in the cube and two in the configuration must be  \nAt most 8 two each from the side squares since the center  \nspot is occupied and possibly all 4 from the center square. Then form the Pareto optimal statistics in section n=3  \nof the Moser wiki  \nand the fact the center cube has 15 points and has c=2  \nwe have the statistics of the center  \nslice must be (4,9,2,0) but from the above we have  \nAt most 8 points with one coordinate equal to 2 so we are done."},{"username":"terence-tao","timestamp":"0002-03-20T04:00:00.000Z","contents":"%c_{n,k}% I suppose it is reasonable to call %c_{n,k}% the size of the largest line-free subset of %[k]^n%. Thus %c_n = c_{n,3}%. Sperner’s theorem tells us that %c_{n,2} = \\binom{n}{\\lfloor n/2\\rfloor}%. We have %c_{n,1}=0% so let’s forget about the k=1 case. Dyer.1137, Markstrom.1139 show that %c_{n,k} = (k-1) k^{n-1}% for n=1,2,3 (one needs k at least 3 when n=3), while Peake.1140 shows that this is also the case for n up to k if k is prime. Kristal has various modifications of this in the non-prime case. Here is a naive conjecture: the above are the _only_ values of (n,k) for which one has %c_{n,k} = (k-1) k^{n-1}% (i.e. exactly one point deleted from each row or column), thus if n is at least 4 and k is composite then %c_{n,k}% has to be strictly less than %(k-1) k^{n-1}%. Actually it suffices to check this claim for n=4 since %c_{n+1,k} \\leq k c_{n,k}%. Klas’s tables at [http://abel.math.umu.se/~klasm/Data/HJ/](http://abel.math.umu.se/~klasm/Data/HJ/) indeed show that %c_{4,k} < (k-1) k^3% for k=4,6. When k=3 we obtained this inequality by exploiting the fact that there was a unique 3D line-free set with %(k-1) k^2% points. Unfortunately, as Klas’s tables show, this is no longer the case in higher D, but perhaps we can still understand the 3D extremisers well enough to settle this conjecture (e.g. do they all correspond to Latin squares as in Markstrom.1139?)"},{"username":"terence-tao","timestamp":"0002-03-20T04:00:00.000Z","contents":"HJ(k,r) Kristal, that’s a nice lower bound construction for HJ(3,3), which is better than the existing bound in the literature; I put it on the wiki. One may hope to get some better asymptotic bounds for HJ(n,k) too, by leveraging Behrend sets or something. There is a way to use the Behrend construction to colour [n] with relatively few colours in such a way that there are no monochromatic arithmetic progressions, which should then lift to HJ by the same sorts of methods as in, say, 1170\\. There should also be some existing lower bounds on HJ(n,k) in the literature, though I don’t have them on hand right now…"},{"username":"jason-dyer","timestamp":"0003-03-20T04:00:00.000Z","contents":"%c_{n,k}% In between baby holdings, so I don’t have time to check the lemma on this, but: For proving the n=4 case you cite for all k, wouldn’t it suffice to form some sort of controlled exchange between coordinate positions such that the number of combinatorial lines is never increased? Therefore any configuration hopeful of finding %c_{4,k} = (k-1)k^3% can be shuffled to the diagonal, which is clearly not a line-free set?"},{"username":"kristal-cantwell","timestamp":"0004-03-20T04:00:00.000Z","contents":"HJ(k,r) I found this pre-print: [http://www.math.ucsd.edu/~etressle/hj32.pdf](http://www.math.ucsd.edu/~etressle/hj32.pdf)"},{"username":"jason-dyer","timestamp":"0004-03-20T04:00:00.000Z","contents":"%c_{n,k}% Wait, can’t we just immediately shift all the points to the diagonal and claim that for each point the number of intersecting combinatorial lines has to either stay the same or increase? The maximal intersections are clearly where you have a maximal number of possible wildcards, that is, the largest number of identical coordinates that can be substituted by the * wildcard. Shifting everything immediately to the diagonal does not necessarily cause an increase of intersections in a particular point (since now the blocking of the diagonal is duplicated in every point) but it can’t cause a decrease, either, since the highest possible duplication for each point is 1 line."},{"username":"kristal-cantwell","timestamp":"0004-03-20T04:00:00.000Z","contents":"k=4 Fujimura I think this is 12\\. Taking all slices for which a +2b+3b is one of three different values mod 5 gives this as any arithmetic progression a +kr in this set must have r less than 4, so the four values in the arithmetic progression will have four different values mod 5 so this set of slices will not contain any tetrahedrons and 3/5 of 20 is 12 so we have 12 works."},{"username":"jason-dyer","timestamp":"0006-03-20T04:00:00.000Z","contents":"Metacomment. I have made a new page for [higher-dimensional Fujimura](http://michaelnielsen.org/polymath1/index.php?title=Higher-dimensional_Fujimura) and moved the content from the talk page (although not deleted it yet)."},{"username":"kristal-cantwell","timestamp":"0006-03-20T04:00:00.000Z","contents":"k=4 Fujimura I think this is 12 for k=4, n=3\\. I didn’t give the value of n in the previous post."},{"username":"kristal-cantwell","timestamp":"0007-03-20T04:00:00.000Z","contents":"HJ(p,2) Let p be prime then HJ(p,2) is greater than p-1.  \nWe take p-1 values mod p and let them be one color  \nand let the remaining value take the other color, then  \neach combinatorial line with less than p moving coordinates  \nwill run through all values mod p and hence will not be monochromatic  \nso we are done."},{"username":"kristal-cantwell","timestamp":"0007-03-20T04:00:00.000Z","contents":"HJ(n,2) From post 1149 we get this reference  \nBaker, R. C.(1-BYU); Harman, G.(4-LNDHB); Pintz, J.(H-AOS)  \nThe difference between consecutive primes. II.  \nProc. London Math. Soc. (3) 83 (2001), no. 3, 532–562. Here they find a prime in [x-x^{0.525},x]  \nso HJ(n,2) will be greater than n-n^.525-1 as we can find a prime and  \nuse the above argument namely take p-1 values of the prime in color  \nand the remaining value in a second color and as long as the number  \nof moving coordinates in the combinatorial line is less than p all values  \nwill be realized mod p and we will have the combinatorial line not monochromatic."},{"username":"kristal-cantwell","timestamp":"0008-03-20T04:00:00.000Z","contents":"HJ(n,2) HJ(n,2) is greater than 2n-(2n)^{0.525}. We can find a prime less than 2n greater than 2n-(2n)^{0.525}  \nas in the previous post  \nthen we divide the classes modulo this prime into two nearly equal classes  \nthere may be a disparity of one between them both classes will have  \ncardinality less than n we give color 1 to one class and two to the other, the combinatorial lines smaller than or equal to 2n-(2n)^{0.525}  \nwill realize different values modulo that prime for each point there are n points in a combinatorial line so we must have members of both classes and we are done."},{"username":"kristal-cantwell","timestamp":"0008-03-20T04:00:00.000Z","contents":"HJ(n,m) HJ(n,m) is greater than mn-(mn)^{0.525} We can find a prime less than mn greater than mn-(mn)^{0.525}  \nas in 1179  \nthen we divide the classes modulo this prime into m nearly equal classes  \nthere may be a disparity of one between them All classes will have  \ncardinality less than n we give one color to each class, the combinatorial lines smaller than or equal to mn-(mn)^{0.525}  \nwill realize different values modulo that prime for each point, there are n points in a combinatorial line so we must have members of more than one class and we are done."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"%c_{n,k} = (k-1) k^{n-1}% For n= 35 %c_{35,4} = 34 \\times 35^{3}%  \nTo see this select one value modulo 35 and eliminate it.  \nCombinatorial lines with one, two, three or four moving coordinates will  \nrealize all values modulo 35 as one, two, three, or four are units modulo 35.  \nThis generalizes in the following way:  \n%c_{n,k} = (k-1) k^{n-1}%  \nif k is less than all divisors of n."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"%c_{n,k} = (k-1) k^{n-1}% That should be for n= 35 c_{35,4} = 34*35^{3} _[Corrected – T.]_"},{"username":"klas-markstrom","timestamp":"2010-03-20T04:00:00.000Z","contents":"Fujimura k=4 I’ve had my program running during the weekend and it reports the following values for Fujimura’s problem with k=4 d=2 size 7  \nd=3 size 14  \nd=4 size 24  \nd=5 size 37  \nd=6 size 55  \nRight now it has a lower bound of 78 for d=7 I’ll update my table page and put the solutions there too. Jason, congratulations on your baby!"},{"username":"klas-markstrom","timestamp":"0001-03-20T04:00:00.000Z","contents":"HOC k=4 Unless I have messed something up this is a line free set with weight 7.5 in the n=2, k=4 version of the HOC {{1, 1}, {1, 2}, {1, 3}, {2, 1}, {2, 3}, {2, 4}, {3, 2}, {3, 3}, {3,  \n4}, {4, 1}, {4, 2}, {4, 4}}"},{"username":"ks","timestamp":"0004-03-20T04:00:00.000Z","contents":"HOC: %c_{n,k} \\ge \\bar{c}_{n,k}% I agree with 1185\\. One can still considers the alternate HOC. In the  \noriginal %k=3% case, let %\\bar{c}_n% be the maximal  \nweighted triangle-free subset of %\\Delta_{n}% with %(a,b,c)% weighted by %n!/(a!b!c!)%. Then we have %c_n \\ge \\bar{c}_n% and equality holds for %n \\le 6% (when %c_n% is known). The more natural HOC is perhaps %c_n=\\bar{c}_n% i.e. the optimal line free sets are union of slices  \n(both sides are integers now). For %k=4%, by 1182,1185 we have  \n%c^\\mu_{2,4}=7.5> \\bar{c}^\\mu_{2,4}=7% but we still have  \n%c_{2,4}=\\bar{c}_{2,4}=12% by Klas’s example in 1185 and  \nJason’s example %\\Delta_n-\\{0002,0020,1100\\}%."},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"%c_{n,k}% Well, so much for the naive conjecture… but it does make me curious now to figure out exactly what pairs of n,k admit “saturated” configurations, i.e. line-free sets of size %(k-1) k^n%. Right now the set of such “saturated” pairs includes * (n,1) for all n [this is trivial]  \n* (1,k) for all k >= 1  \n* (2,k) for all k >= 1  \n* (3,k) for all k >= 3  \n* (n,k) whenever n is less than all divisors of k [note there is a notational mismatch in 1182]  \n* not (4,4) or (4,6) So I guess the first unknown cases are (4,8) and then (4,9). I’ve moved the higher-d DHJ numbers data to their own page on the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Higher-dimensional_DHJ_numbers](http://michaelnielsen.org/polymath1/index.php?title=Higher-dimensional_DHJ_numbers) it could still do with some editing."},{"username":"michael-peake","timestamp":"0005-03-20T04:00:00.000Z","contents":"Regarding %c_{4,k}% I split the %k^4% points into five sorts of points: xyzw, xxyz, xxyy, xxxy and xxxx, where different letters represent different digits. Then there are seven sorts of lines: *xyz, *xxy,*xxx, **xy, **xx, ***x, ****. Linear inequalities connect the number of each type of point to each type of line. For example, each xxyz point covers two *xyz lines, two *xxy lines and one **xy line. The maximum number of *xyz lines removed is 4*number of xyzw points + 2*number of xxyz points. If there is a solution with %k^3% missing points, there must be  \n*(k-1)(k-2)(k-3) missing xyzw points  \n*6(k-1)(k-2) missing xxyz points  \n*3(k-1) missing xxyy points  \n*4(k-1) missing xxxy points  \n*1 missing xxxx point."},{"username":"michael-peake","timestamp":"0007-03-20T04:00:00.000Z","contents":"Regarding %c_{4,k}% There is another possible solution to the linear inequalities  \n* k(k-1)(k-5) missing xyzw points  \n* 6k(k-1) missing xxyz points  \n* k missing xxxx points and probably other solutions with values intermediate between this post and the previous post. The difference between the new and old solutions is  \n* 6(k-1) fewer xyzw points  \n* 12(k-1) extra xxyz points  \n* 3(k-1) fewer xxyy points  \n* 4(k-1) fewer xxxy points  \n* k-1 extra xxxx points which hints at k different solutions altogether."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"H(4,2) is greater than 11 We take the slices of the eleven dimensional hypercube  \nof side four in the following way:  \nWe start with the Van der Warden number W(4,2) for which  \nwe have an exact value:  \nW(4,2) =35  \nfrom the Ramsey theory book by Graham Rothschild and Spencer  \nsecond addition  \nWe give the slice (a,b,c,d) the color associated  \nwith a + 2b + 3b+1 then the maximum value is 34  \nWe add one because the coloring in W(4,2) starts with one  \nand we have zero values in our slices.  \nThen we will not have a monochromatic combinatorial line  \nbecause it would correspond to a monochromatic upward  \ntetrahedron (a+r,b,c,d) (a,b+r,c,d) etc. so we are done.  \nThis is an improvement in the old value of HJ(4,2) is greater than 6."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"H(3,3) is greater than 13 We take the slices of the eleven dimensional hypercube  \nof side four in the following way:  \nWe start with the Van der Warden number W(3,3) for which  \nwe have an exact value:  \nW(3,3) =27  \nsame source as 1190  \nWe give the slice (a,b,c,) the color associated  \nwith a + 2b +1 then the maximum value is 27  \nWe add one because the coloring in W(3,3) starts with one  \nand we have zero values in our slices.  \nThen we will not have a monochromatic combinatorial line  \nbecause it would correspond to a monochromatic upward  \ntriangle and a monochromatic upward triangle would lead to  \nan arithmetic progression of lenght three but we have forbidden such a progression by our choice of coloring so we are done.  \nThis is an improvement in the old value of HJ(3,3) is my value of 7 before that there was a computer generated value of 6 in the paper I cited in 1175."},{"username":"kristal-cantwell","timestamp":"2010-03-20T04:00:00.000Z","contents":"H(4,2) is greater than 11 redo I am redoing this because I omitted part of the proof:  \nWe take the slices of the eleven dimensional hypercube  \nof side four in the following way:  \nWe start with the Van der Warden number W(4,2) for which  \nwe have an exact value:  \nW(4,2) =35  \nfrom the Ramsey theory book by Graham Rothschild and Spencer  \nsecond addition  \nWe give the slice (a,b,c,d) the color associated  \nwith a + 2b + 3b+1 then the maximum value is 34  \nWe add one because the coloring in W(4,2) starts with one  \nand we have zero values in our slices.  \nThen we will not have a monochromatic combinatorial line  \nbecause it would correspond to a monochromatic upward  \ntetrahedron (a+r,b,c,d) (a,b+r,c,d) etc. and a monochromatic upward tetrahedron would lead to  \nan arithmetic progression of length four but we have forbidden such a progression by our choice of coloring so we are done.  \nThis is an improvement in the old value of HJ(4,2) is greater than 6."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"H(5,2) is greater than 59 We color the slices of the 59 dimensional hypercube  \nof side four in the following way:  \nWe start with the Van der Warden number W(5,2) for which  \nwe have an exact value:  \nW(5,2) =178  \nfrom the Ramsey theory book by Graham Rothschild and Spencer  \nsecond addition  \nWe give the slice (a,b,c,d) the color associated  \nwith a + 2b + 3b+1 then the maximum value is 178  \nWe add one because the coloring in W(5,2) starts with one  \nand we have zero values in our slices.  \nThen we will not have a monochromatic combinatorial line  \nbecause it would correspond to a monochromatic upward  \ntetrahedron (a+r,b,c,d) (a,b+r,c,d) etc. and a monochromatic upward tetrahedron would lead to  \nan arithmetic progression of length four but we have forbidden such a progression by our choice of coloring so we are done."},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"H(3,4) is greater than 37 We color the slices of the 37 dimensional hypercube  \nof side three in the following way:  \nWe start with the Van der Warden number W(3,4) for which  \nwe have an exact value:  \nW(3,4) =76  \nfrom the Ramsey theory book by Graham Rothschild and Spencer  \nsecond addition  \nWe give the slice (a,b,c,d) the color associated  \nwith a + 2b + 1 then the maximum value is 75  \nWe add one because the coloring in W(5,2) starts with one  \nand we have zero values in our slices.  \nThen we will not have a monochromatic combinatorial line  \nbecause it would correspond to a monochromatic upward  \ntriangle and a monochromatic upward triangle would lead to  \nan arithmetic progression of length three but we have forbidden such a progression by our choice of coloring so we are done."},{"username":"klas-markstrom","timestamp":"2011-03-20T04:00:00.000Z","contents":"HOC k=4 After finding the example for HOC k=4 I decided to modify my program to work for this case too. The example is optimal and I get the values. d=2 weight 7.5 points 12 solutions 8  \nd=3 weight 14.5 points 45 solutions 112  \nd=4 weight 25 points 180 solutions 24 The solutions are here  \n[http://abel.math.umu.se/~klasm/Data/HJ/HOC/](http://abel.math.umu.se/~klasm/Data/HJ/HOC/)"},{"username":"kristal-cantwell","timestamp":"2011-03-20T04:00:00.000Z","contents":"H(n,2) We start with the fact W(p+1, 2) is greater than or equal to p*2^p  \nFrom the book mentioned above  \nthen for any n we an find a prime p less than n greater than n-n^.525  \nthen we get a sequence of length p*2^p free of progressions of length  \np and hence of length progressions of length n we look at the largest integer  \nless than or equal to p*2p/n-1 then subtract one from it that will be the dimension of our slice space we give each slice the coloring associated with  \nthe p*2^p for a + 2b+ 3c .. continued for n terms the maximum value will be less than or equal to p*2^p we will not have an upward n-1 dimensional simplex hence avoid combinatorial lines because if we did we would have a monochromatic arithmetic progression of length p which we have forbidden thus we get a bound H(n,2) is greater than ((n-n^5.35)*2^(n-n.535)/n-1)-2 (the two comes from possible rounding in taking the greatest integer and th subtraction of one)."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"H(n,r) We start with the bound W(n,r) is greater than r^n/erk(1+o(1))  \nwhich is from the website [http://mathworld.wolfram.com/vanderWaerdenNumber.html](http://mathworld.wolfram.com/vanderWaerdenNumber.html) which gives the reference  \nHeule, M. J. H. “Improving the Odds: New Lower Bounds for Van der Waerden Numbers.” March 4, 2008\\. [http://www.st.ewi.tudelft.nl/sat/slides/waerden.pdf](http://www.st.ewi.tudelft.nl/sat/slides/waerden.pdf). then we take that coloring and give it to a +2b+.. continued n-1 times  \nwe can do this if the dimension is less than ((n^r/ern(1+o(1)))/n-1) -2  \nwe then will get no n-1 dimensional upward simplex as then we would  \nhave a monochromatic arithmetic progression which we have forbidden but since we have no monochromatic upward n-1 dimensional simplex we will have no combinatorial lines and so we are done and we have H(n,r) is greater than ((n^r/ern(1+o(1)))/n-1) -2"},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"HJ(n,r) I should have written that not H(n,r) in 1190-1194 and in 1195-6 thus the above should be: HJ(n,r) is greater than ((n^r/ern(1+o(1)))/n-1) -2 HJ(n,2) is greater than ((n-n^5.35)*2^(n-n.535)/n-1)-2 HJ(3,4) is greater than 37 HJ(5,2) is greater than 59 HJ(3,4) is greater than 37 HJ(4,2) is greater than 11 HJ(3,3) is greater than 13"},{"username":"klas-markstrom","timestamp":"2012-03-20T04:00:00.000Z","contents":"Fujimura k=3 n= 13 The computation for k=3 n=13 has finished. The optimal size is 46 and I have added the solutions to my table page. This required some spare time from a linux cluster to be possible so I will not try to extend Fujimura for k=3 unless it turns out to be needed for something specific."},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"%c_n% and generalization beyond 3 We have HJ(3,r) is greater than ((3^r/er3(1+o(1)))/2) -2  \nso we c_n greater than 3^n/r for n is less than ((3^r/er3(1+o(1)))/2) -2  \nand we get c_n is is greater than 3^n/(logn-log (log n) -log 3e(1+(o)1))  \nsimilarly we get in general the maximal number of points in a combinatorial line free cube of side m and dimension n is m^n/(logn-log (log n) -log (me(1+o1))"},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"correction imilarly we get in general the maximal number of points in a combinatorial line free cube of side m and dimension n is m^n/(logn-log (log n) -log (me(1+o1)) should be imilarly we get in general the maximal number of points in a combinatorial line free cube of side m and dimension n is greater than m^n/(logn-log (log n) -log (me(1+o1))"},{"username":"kristal-cantwell","timestamp":"0001-03-20T04:00:00.000Z","contents":"More HJ numbers We have from  \n[http://www.st.ewi.tudelft.nl/sat/waerden.php](http://www.st.ewi.tudelft.nl/sat/waerden.php)  \nW(3,5) is greater than 170 from this we get  \nusing methods similar to the above HJ(3,5) is  \ngreater than 84 this actually makes a difference for values of  \n82,83 and 84 for c_n in the table at [http://spreadsheets.google.com/ccc?key=p5T0SktZY9DsU-uZ1tK7VEg](http://spreadsheets.google.com/ccc?key=p5T0SktZY9DsU-uZ1tK7VEg) as the maximal density must be greater than  \n.2 We have from  \nthe same source  \nW(3,6) is greater than 207  \nso by similar method as above we have  \nHJ(3,6) is greater than 103 again this makes a difference in  \nthe table starting at 82 and continuing through 98 as the maximal density must be greater than 1/6"},{"username":"terence-tao","timestamp":"0003-03-20T04:00:00.000Z","contents":"Looks like the thread has been wrapped up just in time! Of course, we start afresh at the new thread: [https://terrytao.wordpress.com/2009/03/30/dhjk-1200-1299-density-hales-jewett-type-numbers/](https://terrytao.wordpress.com/2009/03/30/dhjk-1200-1299-density-hales-jewett-type-numbers/)"},{"username":"kristal-cantwell","timestamp":"0003-03-20T04:00:00.000Z","contents":"Additions to Wiki I have added some material to the wiki at: [http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem](http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem) n sections 5 through 8\\. It includes material from the end of the 1100 thread through 1196"},{"username":"michael-peake","timestamp":"0008-03-20T04:00:00.000Z","contents":"%c_{4,k}% is saturated for all odd %k \\ge 5% Delete all points xxxx, along with all points xxyz and xyzw whose coordinates add up to a multiple of k. k is allowed to be a multiple of 3."},{"username":"michael-peake","timestamp":"0009-03-20T04:00:00.000Z","contents":"Oops  \nSorry, I think my previous comment is wrong"},{"username":"kristal-cantwell","timestamp":"0009-03-20T04:00:00.000Z","contents":"Additions to Wiki I have added more material to the wiki: Here is a summary of results the proofs are on the site at:  \n[http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem#Improved_bounds_on_HJ.28n.2Cr.29](http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem#Improved_bounds_on_HJ.28n.2Cr.29)  \nin sections 5 through 12 each group below is from one section: HJ(3,3) is greater than 13  \nHJ(3,4) is greater than 37  \nHJ(3,5) is greater than 84  \nThis leads to improvements in the existing bounds for c_n in the spreadsheet for values 82-84 as the density must be greater than 1/r=1/5 for n less than or equal to 84.  \nHJ(3,6) is greater than 103  \nAgain this leads to improvements in the existing bounds for c_n for 82 to 98 again as the density must be greater than 1/r=1/6 for n less than or equal to 103 and the table stops at 98\\. H(4,2) is greater than 11  \nHJ(4,3) is greater than 97  \nHJ(4,4) is greater than 349  \nHJ(4,5) is greater than 751  \nHJ(4,6) is greater than 3259 HJ(5,2) is greater than 59  \nHJ(5,3) is greater than 302  \nHJ(5,4) is greater than 2609  \nHJ(5,5) is greater than 6011  \nHJ(5,6) is greater than 14173 HJ(6,2) is greater than 226  \nHJ(6,3) is greater than 1777  \nHJ(6,4) is greater than 18061  \nHJ(6,5) is greater than 49391  \nHJ(6,6) is greater than 120097 HJ(7,2) is greater than 617  \nHJ(7,3) is greater than 7309  \nHJ(7,4) is greater than 64661 HJ(8,2) is greater than 1069  \nHJ(8,3) is greater than 34057 HJ(9,2) is greater than 3389 HJ(n,r) is greater than ((n^r/ern(1+o(1)))/n-1) -2  \nHJ(3,r) is greater than ((3^r/er3(1+o(1)))/2) -2  \nc_n is is greater than 3^n/(logn-log (log n) -log 3e(1+(o)1))  \nThe maximal number of points in a combinatorial line free cube of side m and dimension n is greater than m^n/(logn-log (log n) -log (me(1+o1)) ."},{"username":"klas-markstrom","timestamp":"2010-03-20T04:00:00.000Z","contents":"Fujimura k=4 n=7 This night my program finished the computation for Fujimur’as problem with k=4 n=7\\. The optimum size is 78, and there are only 48 solutions.  \nI have updated my table page and put the solutions there. I do not plan to extend the k=4 case further unless further values are needed."},{"username":"michael-peake","timestamp":"0001-03-20T04:00:00.000Z","contents":"Query on HJ(k,r) Kristal, I am having trouble with the limit for %c_n%.  \nThe formula given for the van der Waerden number W(r,k) is %\\frac{r^k}{erk}(1+o(1))%, where r is the number of colours and k the size of the cube. k is also the length of the arithmetic progression. For DHJ(3), k=3, so $W(r,3) = \\frac{r^2}{3e}(1+o(1))$. Then we can set the dimension to be %n = r^2/6e + o(r^2)% and keep 1/r of the points. The total number of points kept is %3^n/r%, which is %O(3^n/\\sqrt{n})% or %3^{n-log(n)/2}%. This is smaller than the old limit of %3^{n-\\sqrt{log(n)}}%."},{"username":"klas-markstrom","timestamp":"2012-03-20T04:00:00.000Z","contents":"HOC Is there some viable alternative to the HOC? For k=2 to original HOC is true.  \nFor k=3 it holds for the few values of n where we know the involved values. For k=4 it fails for n=2, weight 7.5 and Fujimura has has 7 points, but the number of points in the weighted problem is the same as in the unweighted maximum For k=4 and n=3 the weight is greater than the size of Fujimura solution, and the number of points is smaller than in the maximum unweighted set.  \nThe same things holds for n=4"},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"HJ(k,r) I made a mistake in this section of the wiki, I switched r and k  \nin the exponent I have repaired this I no longer can improve the  \nexisting asymptotic to c_n so I have deleted that material."},{"username":"kristal-cantwell","timestamp":"2012-03-20T04:00:00.000Z","contents":"HOC If the in the HOC equality is replaced a bounding constant factor then I think you can still get DHJ(k) from the weakened HOC The corners theorem and the combinatorial line free state forces the number of slices to be small  \nthe constant factor forces the set in equal slices measure to be small and then equivalence of measures makes the density of the points with ordinary weighting small and we are done."},{"username":"d-eppstein","timestamp":"0001-03-20T04:00:00.000Z","contents":"I’m coming to this very late, and probably this is overly naive and already long since considered and discarded, but: a permutation can be thought of as a machine for turning slices into {0,1}-colorings of [n]: the coloring is formed by coloring a prefix of the permutation with 0 and a suffix with 1, and the slice tells you how long to make the prefix and suffix. This machine has the property that any two colorings picked out by the same permutation form a combinatorial line in [2]^n. And the equal-slices measure of a set of colorings is just the expected number of colorings picked out by a random permutation. So I was wondering, in connection with the hyper-optimistic conjecture, whether one could define some sort of combinatorial machine for turning slices into {0,1,2}-colorings of [n], with the property that for any three slices forming an equilateral triangle, the three colorings picked out by the machine for those slices form a combinatorial line. If such machines existed, we could hope for a proof of the hyperoptimistic conjecture in which the equal-slices measure is the expected number of colorings picked out by a random machine. Sadly, machines of this type do not seem to exist. At least, I tried placing {0,1,2}-colorings of a two-element set into a triangle of the six possible slices a two-element set, and the constraint that equilateral triangles form lines quickly led to inconsistencies. With this avenue of attack on the hyper-optimistic conjecture blocked off, I’m wondering what other approaches might seem more promising. I looked on the wiki but all I saw were computer calculations; I can imagine a counterexample to the hyper-optimistic conjecture being found in this way but that seems unlikely to lead to a proof."},{"username":"jason-dyer","timestamp":"0002-03-20T04:00:00.000Z","contents":"Coloring machine David, welcome! I vaguely considered what you are speaking of [at this comment](https://terrytao.wordpress.com/2009/03/14/dhj3-1100-1199-density-hales-jewett-type-numbers/#comment-37021). I haven’t had time to think about it further, though. There’s a definite feel of some sort of “automatic chain” going on based on disjoint triangles that could be used to formulate a sensible coloring."},{"username":"terence-tao","timestamp":"0004-03-20T04:00:00.000Z","contents":"Coloring machine Hi David! What you propose sounds similar to what Gowers proposed [back in 331](http://gowers.wordpress.com/2009/02/06/dhj-the-triangle-removal-approach/#comment-1938) and then [discounted in 332](http://gowers.wordpress.com/2009/02/06/dhj-the-triangle-removal-approach/#comment-1940) for more or less the same reason you did. However, this approach did lead to what I called “Cartesian semi-products”, which were basically upper triangular grids in the cube such that any triangle with _one edge on the diagonal_ formed a combinatorial line; see e.g. my [comment 451](http://gowers.wordpress.com/2009/02/08/dhj-quasirandomness-and-obstructions-to-uniformity/#comment-2108). Some vestige of this idea eventually got incorporated into the crucial observation “if a set has no combinatorial lines, then it correlates with a local complexity 1 set” which was finally proven in [Gowers’ comment 820](http://gowers.wordpress.com/2009/02/23/brief-review-of-polymath1/#comment-2431). And this is a cornerstone of one of our current proofs of DHJ. So, basically, your idea is what did lead (after about four weeks of work, and many twists and turns) to a full solution of the problem :-) The breakdown of the hyper-optimistic conjecture in k=4 does strongly suggest that the k=3 HOC, if true, is going to be true for “fluky” reasons rather than because there is an elegant proof of the fact. The version of HOC with a constant loss may of course still be true, but cannot be proven or disproven numerically, and I’m not sure there is much more one can say about that version of the conjecture at this stage."},{"username":"d-eppstein","timestamp":"0005-03-20T04:00:00.000Z","contents":"Coloring machine Ok, define a “coloring machine” to be a function M that maps slices in Δn to colorings in [3]^n, but now rather than requiring that all equilateral triangles are mapped by M to lines, we only hope that many of them are. Let a(M) be the cardinality of the largest subset of Δn that avoids all of the triangles mapped by M to lines (obviously this is greater than or equal to the largest triangle-free set). Then the equal-slices measure of any line-free set must be at most a(M). The proof idea is pretty much the same as for Lubell’s proof of Sperner. The equal-slices measure is the expected number of elements of the line-free set that are picked out by a machine M’ formed by applying a random permutation to M, but any individual one of these machines can only pick out a(M) of these elements else the elements it picks out would include a line. That is, even if this idea is hopeless for getting to the hyper-optimistic conjecture, maybe it can at least give a nontrivial upper bound on equal-slices measure, if only we could find an infinite family of M’s with small a’s. Of course, you do already have a nontrivial upper bound on equal-slices measure: it is o(|Δn|) because ([the Wiki says](http://michaelnielsen.org/polymath1/index.php?title=Equal-slices_measure)) that statement is the same as DHJ(3)."},{"username":"kristal-cantwell","timestamp":"0008-03-20T04:00:00.000Z","contents":"HJ(3,r) I have added the new bound: HJ(3,r) is greater than r^{clnr}/2-1 to the wiki at  \n[http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem#Improved_bounds_on_HJ.283.2Cr.29](http://michaelnielsen.org/polymath1/index.php?title=Coloring_Hales-Jewett_theorem#Improved_bounds_on_HJ.283.2Cr.29)  \nmore is on it there. As far as I know this is the first greater than polynomial bound for  \nHJ(3,r)"},{"username":"gil-kalai","timestamp":"2012-04-20T04:00:00.000Z","contents":"all sort of things Let me repeat some little comments (from the 1000 thread) which are perhaps more appropriate on this thread. The first is that it can be fruitful to examine slightly finer slice structures (perhaps in order to eliminate the hyper optimistic conj already for k=3). For example: Let %\\Gamma_{a,b,c; x,y,z}% be all 0-1-2 vectors with a 0’s b 1’s and c 2’s where x is the sum of indices of the 0 entries y the sum of indices of 1 entries and z the sum of entriez of z vectors. (Such slices are of interest for binary vectors; you can have further refinements by presecribing the sum of squares of 0-indices etc;) union of slices where the sets of vectors (b+2x,y+2z) avoides 3-term AP look like avoiding combinatorial lines. But I do not know if such examples are potentially useful. The other comment related to David’s is that in the DHJ(2) case (it is still an interesting issue if DHJ(2) is a good analog for DHJ(3)) there are sets which are line-ample – with a lot of lines. Namely, the chains. Now for DHJ(3) I am not aware of sets which have a “lot of lines” (by a lot we really want more than for combinatorial subspaces). A concrete problem is this: for Sperner a maximal chain is a subset on which the density of a line-free subsubset is at most 1/(n+1). (And of course, there is no subset which is better). For %{0,1,2}^n% what is the subset X so that the maximum density %c_X% in X of a line avoiding subsubset Y is minimum. Are there such X for which %c_X% is smaller or even substantially smaller than for %X= \\{0,1,2\\}^n?%."},{"username":"jozsef","timestamp":"0008-04-20T04:00:00.000Z","contents":"HJ(3,r) re. 1212\\. Kristal, I think that this is a standard argument. If one can r-colour the integers from 0 to 2d avoiding monochromatic 3-term arithmetic progressions, then it implies HJ(3.r)>d; just colour every point of %\\{0,1,2\\}^d% with the colour assigned to the sum of its coordinates. So, %HJ(3,r)\\geq W(3,r)/2%. You used a+2b+1, but it is the same."},{"username":"jozsef","timestamp":"0009-04-20T04:00:00.000Z","contents":"HJ(3,r) (contd.) Maybe one can use a 1-1 map between %[3]^n% and a subset of integers to get a new bound on HJ(3,r) or DHJ3\\. We can consider the coordinates of points in %[3]^n% as exponents of the prime factorization of an integer. If we can find a large set of integers without a 3-term GEOMETRIC progression, that would give a line-free subset in %[3]^n%. It is different from Kristal’s construction and it might give a better bound (but I’m not sure)."},{"username":"michael-peake","timestamp":"0009-04-20T04:00:00.000Z","contents":"Regarding %c_{5,k}% Following from 1188 and 1189, any coordinate-line-free solution of the five-dimensional k-cube, with %k^4% missing points, must have the following missing points of each type. p(vwxyz) = 24a + (k-1)(k-2)(k-3)(k-4)  \np(wwxyz) = -60a + 10(k-1)(k-2)(k-3)  \np(xxyyz) = 30a + 15(k-1)(k-2)  \np(xxxyz) = 20a + 10(k-1)(k-2)  \np(xxxyy) = -10a + 10(k-1)  \np(xxxxy) = -5a + 5(k-1)  \np(xxxxx) = a+1  \n(a=0..k-1) Again, the number of missing points on the main diagonal (xxxxx) can be anything from 1 to k, but then the number of each other type is fixed. I don’t know why the formulae are so neat; the theory of Young tableaux and Schur polynomials must have something to do with it."},{"username":"gil","timestamp":"2010-04-20T04:00:00.000Z","contents":"re 1214 part B  \nDear Jozsef, This looks very interesting! but are there any bounds known on the density of sets avoiding 3-terms geometric progression? (also when you say it is different than Krystal’s example which example do you refer to?)"},{"username":"terence-tao","timestamp":"2010-04-20T04:00:00.000Z","contents":"Coloring machines I tried playing around a bit with David’s suggestion to find maps %\\phi: \\Delta_r \\to [3]^n% from a triangle %\\Delta_r := \\{ (a,b,c) \\in {\\Bbb Z}_+^3: a+b+c=r\\}% into %[3]^n% which map as many equilateral triangles into %[3]^n% as possible. Currently, we are using embeddings such as %\\phi( a,b,c ) = 0^a 1^b 2^c% (taking r=n, say); this maps (a+r,b,c), (a,b+r,c), (a,b,c+r) to a line as long as b=0 (thus the equilateral triangle has to have one side on the boundary of %\\Delta_r%). Another thing to do is to take %r = 2^n-1%, and map %\\phi(a,b,c) = w% whenever a (resp. b, c) is the n-bit binary string which has 1s at the places where w is 0 (resp. 1, 2) and 0s elsewhere. This map is not defined on all of %\\Delta_r%, but only maps a sort of [Sierpinski triangle](http://en.wikipedia.org/wiki/Sierpinski_triangle) in %\\Delta_r% to %[3]^n%, in which exactly one of the i^th bits of a, b, c is one for each i. Any line in %[3]^n% maps back to an equilateral triangle in %\\Delta_r%. But this basically is just reformulating DHJ(3) as a problem about avoiding a sort of Sierpinski gasket of equilateral triangles (a little reminiscent of the [IP-Szemeredi theorem](http://michaelnielsen.org/polymath1/index.php?title=IP-Szemer%C3%A9di_theorem) or [strong Roth theorem](http://michaelnielsen.org/polymath1/index.php?title=Tidy_problem_page#Strong_Roth_theorem))."},{"username":"d-eppstein","timestamp":"2011-04-20T04:00:00.000Z","contents":"Ternary error correction While thinking more about what kinds of bounds the coloring machine idea might be able to prove, I ran into a different question that may be heading a bit too far off-topic, but I thought I’d mention it here anyway. A ternary error-correcting code is just a subset of [3]^n such that no two colorings differ only in a single position. The maximum size of an error correcting code is just [3]^{n-1}, or equivalently the maximum density is 1/3 — just take all colorings whose sum is 0 mod 3\\. But what is the maximum equal-slices density? A lower bound of 1/3 on maximum equal-slices density is trivial: take the three sets of colorings whose sum is i mod 3 for i=0,1,2, and choose whichever of them has largest density. But this is not in general optimal. A better lower bound for some n is to take a maximum independent set in Δ_n and use all colorings within those slices. For n=2 this gives equal-slices density 1/2, and for n=4 it gives 2/5, but for n=3 it doesn’t do any better than the trivial 1/3 lower bound. An upper bound on the maximum equal-slices density is the value of a fractional relaxation of the maximum independent set: assign real numbers to Δ_n such that no two adjacent numbers add to more than one, and take the average of these numbers. I think the optimal solution is the assignment of 1/2 to each slice, so the maximum equal-slices density should be at most 1/2\\. For n=2 this matches the lower bound but for higher n it is different. My suspicion is that the lower bound is tight and the upper bound isn’t. Is this known? Any hope of a proof?"},{"username":"jozsef","timestamp":"2011-04-20T04:00:00.000Z","contents":"Re 126\\. Geometric progressions Dear Gil, I was thinking about how to get a lower bound on HJ(3,r) which is different from Kristal’s in post 1212\\. I don’t know good lower bounds on monochromatic-3-term-geometric-progression (3GP) avoiding colourings, but I will think about it. In fact, we don’t need to avoid all geometric progressions, just the ones where %a% and %d% are relative primes where the 3GP is written as %a, ad, ad^2%."},{"username":"jason-dyer","timestamp":"2012-04-20T04:00:00.000Z","contents":"Fujimura and divisibility I was working on k=3 Fujimura a little, and while I haven’t had much luck, I wanted to note the divisibility of n seems to be a major factor in the layout of any potential disjoint triangles. For example, when n mod 2 = 1, one can lay out triangles of side length 1 all the way to the edges of the lattice (for example when n = 3, the triangles 030-120-021, 210-300-201, 012-102-003) whereas when n mod 2 = 0 there is an “extra row”. This means the increase in disjoint triangles to a n mod 2 = 1 case is larger than to a n mod 2 = 0 case."},{"username":"d-eppstein","timestamp":"0002-04-20T04:00:00.000Z","contents":"Minor bugfix: that should be “error detection” and “error-detecting code” not “error correction” etc — correction requires greater Hamming distance than two."},{"username":"jason-dyer","timestamp":"0002-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover The triangular lattice of Fujimura’s problem can be covered by disjoint triangles as long as n=2*3^(i-1)-1 (where i >= 1). Esentially, 6 copies of the previous cover are arranged to make the new cover, and the points not included can then be covered. It’s somewhat Sierpinski-ish."},{"username":"d-eppstein","timestamp":"0003-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover. Jason: see [my C++ code for finding covers by disjoint triangles](http://www.ics.uci.edu/~eppstein/0xDE/x3c.cc). Obvious necessary conditions for a cover to exist are that the triangular lattice have a number of points divisible by three, and that the bottom edge of the triangle have evenly many points. That is, the number of points on the bottom edge must be 0 or 2 (mod 6). My code finds solutions for all small n satisfying those conditions, with numbers of points on the bottom edge equal to 2, 6, 8, 12, 14, and after that I got tired of waiting for an answer. Your construction allows any solution to be tripled, so 18 and 24 are also solvable; the next unknown case is 20."},{"username":"d-eppstein","timestamp":"0005-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover. If you have a solution with a triangle with n points on the bottom edge, and in addition your solution has the property that it uses the three side-length-1 triangles from the three corners, then Jason’s idea of arranging 6 copies and then covering the remaining points can be used in such a way that the 6 copies overlap only on these side-length-1 triangles. The resulting cover has 3n-4 points on the bottom edge (and again uses three side-length-1 corner triangles). The solutions for 6 and 8 do have this property of using the three small corner triangles, and lead to solutions for 14 and 20\\. As I said, 18 and 24 are solvable using Jason’s version of the tripling construction. The same idea with order-6 overlapping corner regions allows the side-length-14 solution to turned into a solution with side length 3*14 – 2*6 = 30\\. So the next unknown case is 26."},{"username":"jason-dyer","timestamp":"0006-04-20T04:00:00.000Z","contents":"Alternate slice notation Kalai.1213 discusses an alternate slice notation %\\Gamma_{a,b,c;x,y,z}%. To begin with, Gil, are you really meaning this? x is the sum of indices of the 1 entries  \ny is the sum of indices of the 2 entries  \nz is the sum of indices of the 3 entries So for example the string 321 would have x = 2, y = 1, and z = 0\\. Equal slice notation would have a = 1, b = 1, and c = 1. Given that, I’m not sure how useful it is combining the equal-slice and index-slice together; on %c_3% for example it gives one point for each slice, which isn’t terribly useful. It seems better to form a separate slice type notation, say, %\\Psi_{x,y,z}%, and then if at any point we need equal-slice and index-slice together we can use intersection etc. as needed. With %c_3% then each %\\Psi_{x,y,z}% is a combinatorial line. For example %\\Psi_{2,1,0}% is the line 121-221-321 intersecting the equal slices %\\Gamma_{2,1,0}%, %\\Gamma_{1,2,0}%, and %\\Gamma_{1,1,1}%."},{"username":"jason-dyer","timestamp":"0008-04-20T04:00:00.000Z","contents":"Alternate slice notation The %\\Psi_{x,y,z}% I just mentioned gives combinatorial lines with a single wildcard in the first position; it is therefore useful to be able to number indices arbitrarily, so for example %\\Psi^{[2,0,1]}_{x,y,z}% would give a single wildcard in the second position. Note also %\\Psi^{[1,1,1]}_{x,y,z}% is then simply equal slices, so it’s probably best to keep with our original notation of Gamma, so the original index-slice notation would be %\\Gamma^{[0,1,2]}_{x,y,z}% (Or whatever alternate way other than my bracket-thing people want to do this.)"},{"username":"gil-kalai","timestamp":"0009-04-20T04:00:00.000Z","contents":"1225.  \nDear Jason, I suppose this is what I meant; It is ok to think separately on slices of various types. (I did not understand your notation %\\Gamma_{x,y,x}^{a,b,c}%.) I did not understand what is %\\Psi_{x,y,z}% in 1223.  \nif the sum of indices of the locations for ‘1’ is ‘2’ why do you get there 121 and 221?"},{"username":"jason-dyer","timestamp":"0009-04-20T04:00:00.000Z","contents":"Alternate slice notation Maybe I’m misunderstanding, but: 121 (in equal slices it would be a = 2, b = 1, c = 0)  \n1s: 0 + 2 = 2 = x  \n2s: 1 = 1 = y  \n3s: 0 = z 221 (in equal slices a = 1, b = 2, c = 0)  \n1s: 2 = 2 = x  \n2s: 0 + 1 = 1 = y  \n3s: 0 = z 321 (in equal slices a = 1, b = 1, c = 1)  \n1s: 2 = 2 = x  \n2s: 1 = 1 = y  \n3s: 0 = z"},{"username":"gil-kalai","timestamp":"2010-04-20T04:00:00.000Z","contents":"Ok, i suppose I started with 1 (not with 0) it makes no difference if you insist your sets will be from the same slice with the two definitions together. But starting with ‘1’ will not allow even the coarse slices to have lines. Now the individual slices in the index sum definitions are smaller.  \n(Actually a single slice in the ususal sense is at most %3^n/n% and not %3^n/\\sqrt n% as the wiki suggests, right?)  \nEach slice in the new sense have %3^n/n^3% (when you just take the sum of indices) and if you insist on a slice both in terms of the sums of indices and in terms of number of coordinates of each type (and then you can start with 0) you get slices of at most %3^n/n^4%. Now we want to gather many such slices to avoid a combinatorial line. So it looks that if the indices of the slices are (a,b,c,x,y,z,) we should not include 6-tuples avoiding 3 term a. p. simultaniusly in both b+2c and x+2y The tragedy is that it seems that unions of such slices will give you examples in the same ball park as the examples we already have. but I am not sure about it. Also if it is in the same ball park asymptotically it can lead to better examples than the hyper optimistic ones. We may be able to explore the situation for union of slices in the different sense and in the refined sense for small but not terribly small n’s. the question is related to the maximum sizes of subsets of a rectangle {1,2,…,a} times {1,2,…,b} not having a 3-term A.P. I suppose the asnwer is in the same ball park as for {1,2,…,ab} but I am not sure about it and I suppose experts would know."},{"username":"jason-dyer","timestamp":"0001-04-20T04:00:00.000Z","contents":"Alternate slice notation Just to clarify on my notation, %\\Gamma^{0,1,2}% would mean the index starts at 0, and %\\Gamma^{1,2,3}% would mean the index starts at 1. So to compare: 221 with %\\Gamma^{0,1,2}%: 1s: 2 = x  \n2s: 0 + 1 = 1 = y  \n3s: 0 = z 221 with %\\Gamma^{1,2,3}%: 1s: 3 = x  \n2s: 1 + 2 = 3 = y  \n3s: 0 = x Equal slices is %\\Gamma^{1,1,1}%, giving equal weight to each position in the string. I think it worthwhile to compare the different possible slices using this notation."},{"username":"kristal-cantwell","timestamp":"0002-04-20T04:00:00.000Z","contents":"Maximal single slice I think that the maximal single slice is one with all coordinates equal if the dimension is divisible by three, or one of three slices of the same size  \nusing stirling’s approximation  \n[http://mathworld.wolfram.com/StirlingsApproximation.html](http://mathworld.wolfram.com/StirlingsApproximation.html) gives roughly  \n3^n/n. Apparently the division is by a power approximately one factor of square root of n greater each time we increase k by one and look for the maximum center slice of a cube of dimension n and side k."},{"username":"klas-markstrom","timestamp":"2012-04-20T04:00:00.000Z","contents":"HOC k=3 I have now used a different integer programming solver, the cplex solver, in order to determine the value of %c_{6,3}^{\\mu}%.  \nThe program found a solutions quite quickly and after a bit over twelve hours the program had reduced the upper bound to the same value. The result is %c_{n,3}^{\\mu}=15%. With this program I can currently not get complete sets of solutions but the solution the program did find is here  \n[http://abel.math.umu.se/~klasm/solutions-n=6-k=3-HOC-6](http://abel.math.umu.se/~klasm/solutions-n=6-k=3-HOC-6)"},{"username":"jason-dyer","timestamp":"0005-04-20T04:00:00.000Z","contents":"Alternate slice notation Quick note: when strings get larger the above notation is obviously impractical and not very general, so it’s probably best to have %Gamma^1% mean Gil’s indexing starting at 1 and %Gamma^0% be my own starting at 0."},{"username":"jason-dyer","timestamp":"2010-04-20T04:00:00.000Z","contents":"Alternate slice notation visual comparison Just for fun, here’s [3]^2 with four different slice types: [![](https://numberwarrior.files.wordpress.com/2009/04/sampleslice.gif?w=490)](http://numberwarrior.files.wordpress.com/2009/04/sampleslice.gif)"},{"username":"kareem-carr","timestamp":"0007-04-20T04:00:00.000Z","contents":"1232. I wanted to mentioned before it slips my mind that I expanded my GA to strings in [4]^n, [5]^n, [6]^n and [7]^n. I haven’t had the time to try very hard but I did make an improvement in the lowerbound of [6]^3 of 1031\\. Things keep coming up but I will try to remember to exhibit the solution. If I don’t and someone is curious about this, please make a post on my blog and remind me or something like that. Cheers."},{"username":"kristal-cantwell","timestamp":"0007-04-20T04:00:00.000Z","contents":"GA lower bound 1031 Possibly could that be the Moser set 6^4 has a lower bound of 1031? 6^3 is 216."},{"username":"mats-granvik","timestamp":"0008-04-20T04:00:00.000Z","contents":"1234. Dear Terence Tao, It appears that except for the first term, sequence A156989 in the oeis  \nis given by the expression A105659(A101976) + A000027 – 2.  \nThat is: 0, 2, 6, 18, 52, 150, 450"},{"username":"kareem-carr","timestamp":"0008-04-20T04:00:00.000Z","contents":"1235. Thanks Kristal. Yes I meant [6]^4."},{"username":"jason-dyer","timestamp":"0009-04-20T04:00:00.000Z","contents":"OEIS sequence Mats, I get a sequence starting 0, 4, 17, 42, 123 . . . 1*1+1-2 = 0  \n2*2+2-2 = 4  \n4*4+3-2 = 17  \n5*8+4-2 = 42 Am I reading your expression wrong?"},{"username":"jason-dyer","timestamp":"0009-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover David, it might also be useful to have the same information about near-covers, that is, covering %|\\Delta_n|-1% and %|\\Delta_n|-2%. At the least constructive proofs of the covers seem to leave the door open for upper bound improvement, although I’m still noodling with that."},{"username":"kristal-cantwell","timestamp":"2012-04-20T04:00:00.000Z","contents":"Colorings avoiding geometric lines Let M(n,r) be defined as the least dimension of an n-cube such that any r-coloring of it has a monochromatic line. We will show M(3,2) =3.  \nIt is greater than 2 since we can color the square of side three as follows 221  \n112  \n212 To see that it is three must show that every two coloring of the three dimensional cube of side 3 must have a monochromatic geometric line. We start by noting that one of the colors must have the center point then we subtract the Pareto optimal statistics from the total number of points in each class then we get a set of possible statistics that are the only statistics except for those which we get by adding points which correspond to the original statistics with one central point with points removed. We start with (8,12,6,1) and subtract (3,6,3,1), (4,4,3,1) and (4,6,2,1) getting (5,6,3,0), (4,8,3,0) and (4,6,4,0). Of these only (4,6,4,0) is an extremiser. Now it has two four points with two 2’s. So it must have a pair with the same c-statistic slice on the coordinate of this pair which is not equal to two and we get two side slices which are squares with the central spot occupied. Without loss of generality let the first coordinate be the one with two points with the same c-statistic. No suppose that there is a second pair of points with two 2’s with the same c-statistic then in the center slice the center and the two points with two 2’s will be empty and hence form a line in the other color and this gives a contradiction so we must have the two remaining points having different c-statistics. Now without loss of generality let the two remaining points with two twos that are in the center slice be (2,2,1) and (2,1,2).  \nThen look at the points (1,1,1), (3,1,3), and (3,3,1) only one can be in the set because of possible monochromatic lines formed by (2,2,1), (2,1,1) and (3,2,2). Similarly of the points (3,1,1), (1,1,3) and (1,3,1) can contain only one element in the set because of possible monochromatic lines formed by (2,2,1), (2,1,1) and (1,2,2). Since we must have four points with no twos by the above we must have the points (1,3,3) and (3,3,3) in the set. This will exclude the point (2,3,3).  \nSince the point (2,3,3) is excluded and the center slice must contain 4 points the point (2,1,1) must be excluded or it will eliminate all possible remaining points due to possible lines. Because of a possible line with (1,3,3) and (1,2,2) the point (1,1,1) must be excluded. Similarly because of a possible line with (3,3,3) and (3,2,2) the point (3,1,1) must be excluded. But now with (1,1,1), (2,1,1) and (3,1,1) excluded by the above they must be in the other color where they form a monochromatic line and we are done and M(3,2) =3."},{"username":"mats-granvik","timestamp":"2010-04-20T04:00:00.000Z","contents":"Re:1236. Jason, the expression with the parentheses means that A101976 is used as an index to A105659\\. In other words lookup the A101976:th number in sequence A105659, then add A000027 and subtract two. Doing it backwards as I originally did it, you can lookup 2 5 16 49 146 445 without separating commas in the search. The position of those numbers are given by sequence A101976."},{"username":"gil-kalai","timestamp":"2011-04-20T04:00:00.000Z","contents":"re: David’s 1218  \nThere are some studies of the “diatance distribution” for error correcting codes, and for linear codes this is equivalent to asking about the densities in every slice. (In particular, about the equal slice measure.) In fact, the linear programming method is based on solving linear programs for the entire distance distribution. (I am not so sure if talking about slices is so natural in the non linear case.) For example, the conjecture that for (say linear) error-correcting codes that correct a linear number of errors the Gilbert-Varshamov bound is asymptotically optimal, can be strenghtened to Gilbert Varshamov being asymptotically optimal for every slice (and the density in a slice with %bn% ‘1’s will thus be %2^{-H(a)n}% where %an% is the minimal distance. (Nati linial and I studied such distance distributions in an old paper of ours.) Anyway, I think trying to relate questions discussed here with error-correcting codes would be very nice. But such a connection is, so far, far. Also it is worth mentioning that for Sperner’s theorem there is a complete description (sharper than the LYM inequalities) of the possible “f-vectors” namely the number of sets of every possible size. (and this follows from the Kruskal-Katona’s theorem). But it would be ultra optimistic to hope for something like this in DHJ(3)."},{"username":"kristal-cantwell","timestamp":"2012-04-20T04:00:00.000Z","contents":"4D Moser If a Moser set has 4 points with 3 2’s and its size is 41  \nor more then it must not have any set of two points with three twos with  \nthe same c statistic. We have already shown that if a Moser set has 4 points with 3 2’s and its size is 41  \nor more then it must not have two sets of two points with three twos with  \nthe same c statistic. See post 1171 in [https://terrytao.wordpress.com/2009/03/14/dhj3-1100-1199-density-hales-jewett-type-numbers/](https://terrytao.wordpress.com/2009/03/14/dhj3-1100-1199-density-hales-jewett-type-numbers/) We start by assuming we have such a pair and as noted above only one pair the we slice on that the coordinate of that pair say i getting two slide slices with 13 or less points that means that the center slice must have 15 point or more it cannot have sixteen as it has at least one point with three twos in the configuration which becomes a point with two 2’s in the central slice and prevents the only realization of sixteen points from occurring since it has no points with two 2’s. Further we see from the Pareto optimal statistics for Moser sets for n=3 at the Moser wiki we see its distribution must be (4,9,2) we slice the center cube along one of the coordinates j in which there is a point with three twos then one outside slice without loss of generality say j=1 with the point with three twos must have that point at its center and hence have a maximum of 5 points and at most two points with three twos. the center slice has no center point and one point with three twos(we know it is not in the other outside slice because we have only one set of two points with the same c statistic and they are outside the center cube. Because of this the center slice can have at most 4 and it can contain at most three points with two twos, so the remaining slice j =3 must have all four points with two twos. Now the two side slices since they have thirteen points must have statistics (3,6,3,1) or (4,6,2,1) each. So each of the diagonals connecting points with one two in the slice overall must have at least one point and that means that if we cut each of the side cubes along the same coordinate we cut the center cube we get there are a total four points with one two in the two side slices and if they are divided 2, 2\\. We can pick two pairs one point in a pair in each slice such there is a correspondence between the pairs of the points in that the coordinates j are equal. If they are divided 1,3 again there is a correspondence between the one point and one of the three such that the coordinates except for j are equal. Now in the case of division 2-2 or 1-3 in the above we get a contradiction as follows be we have the above correspondences plus another one in the cube j=1 since it has its center spot occupied combined they give there are one or two points with the same center coordinates except for i in the cube j equals three but the center square of j=3 contains all its points with two two’s which block all lines between points with identical coordinates except for j and i equal to three and we get a line so we must have a four zero split, and since that split must have four points in each side cube and the center slice contains of j=3 contains all of it line so must have at most four of these point and the cube j=1 contains its center point and contains at most four of these points, these points must lie on either the squares i=1,j=3 and i=3,j=1 or i=1,j=1 and i=3,j=3 but then they will form a diagonal cube with i=2,j=2 which contains one point with three twos and this forms a line with one of the pairs of these four points on the diagonal cube and we are done."},{"username":"d-eppstein","timestamp":"0001-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover Ok, let P(k,S) denote the statement “the triangular lattice with k points per side admits a partition into disjoint equilateral triangles, such that, for each number k’ in set S, the three corner sublattices with k’ points per side are also partitioned into equilateral triangles”. We’ve seen  \n(1) P(k,S) => P(3k,S u {k}) [Jason Dyer 1220] and  \n(2) P(k,S) and j in S => P(3k-2j,S u {k}) [me 1222]. But we also have  \n(3) P(k,S) => P(3k+2,S u {k}).  \nTo see this, in a (3k+2)-lattice, place six copies of the P(k,S) partition, three in each corner and three centered on each side. There are two points left over on each side edge of the (3k+2)-lattice that form an equilateral triangle with a point near the middle of the lattice. The remaining points form three regions that are translates of each other in an equilateral pattern and can be covered with congruent equilateral triangles having one point per region. The cover for the 8-lattice is an example of this construction. So we have  \nP(2,Ø) — obvious  \nP(6,{2}) — by (1) from P(2,Ø)  \nP(8,{2}) — by (3) from P(2,Ø)  \nP(14,{2,6}) — by (2) from P(6,{2})  \nP(20,{2,6}) — by (3) from P(6,{2})  \nP(20,{2,8}) — by (2) from P(8,{2})  \nP(24,{2,8}) — by (1) from P(8,{2})  \nP(26,{2,8}) — by (3) from P(8,{2})  \nP(30,{2,6,14}) — by (2) from P(14,{2,6})  \n36: NO SOLUTION KNOWN  \nP(38,{2,6,14}) — by (2) from P(14,{2,6})  \nP(42,{2,6,14}) — by (1) from P(14,{2,6})  \nP(44,{2,6,14}) — by (3) from P(14,{2,6})  \nP(44,{2,8,20}) — by (2) from P(20,{2,8})  \nP(48,{2,6,20}) — by (2) from P(20,{2,6})  \n50: NO SOLUTION KNOWN  \n54: NO SOLUTION KNOWN  \nP(56,{2,6,20}) — by (2) from P(20,{2,6})  \nP(56,{2,8,20}) — by (2) from P(20,{2,8})  \nP(60,{2,6,20}) — by (1) from P(20,{2,6})  \nP(60,{2,8,20}) — by (1) from P(20,{2,8})  \nP(62,{2,6,20}) — by (3) from P(20,{2,6})  \nP(62,{2,8,20}) — by (3) from P(20,{2,8}) It’s starting to look likely that there’s a solution whenever the side length is 0 or 2 mod 6."},{"username":"d-eppstein","timestamp":"0004-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover I forgot that my program had already found a solution for 12\\. Tripling that gives 36\\. So the first unknown sizes are 50 and 56. Also, in the 3n+2 construction, there’s no need to treat the six leftover side points and the three center points as distinct from the three congruent regions. It’s simpler just to think of this as a partition of the (3n+2)-triangular lattice into six n-lattices and three upside-down (n+1)-lattices."},{"username":"d-eppstein","timestamp":"0004-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover Ok, after noticing several more erroneous omissions in the list above, I wrote a program to run the various lattice-tripling patterns for me. Among the possible triangular lattice sizes (congruent to 0 or 2 mod 6) up to 1000, it only fails to find a solution for 32, 66, 96, 192, 288, 572, and 578. If the size-12 solution can have size-1 equilateral triangles in all four of its corners (that is, in the previous notation, P(12,{2}) — I haven’t checked, but this should be within reach of hand search or the search algorithm I posted earlier), then the only size without a known solution up to 5000 would be 66\\. So it seems that these tripling rules are almost enough to solve all possible lattice sizes."},{"username":"klas-markstrom","timestamp":"0001-04-20T04:00:00.000Z","contents":"HJ k>3 I have had the programs running during the weekend and they have improved the bounds for two pairs of values. %c_{4,6}\\geq 1051%  \n%c_{5,4}\\geq 708% I have updated my table page and the solutions are available there too."},{"username":"jason-dyer","timestamp":"0007-04-20T04:00:00.000Z","contents":"Metacomment. The sequence primary to Granvik.1234 is A105659, which was sent in to the OEIS by Sascha Kurz. [Sascha Kurz’s research page](http://www.wm.uni-bayreuth.de/index.php?id=206) has quite a few papers about integral point sets, although I haven’t found the sequence in question. I have emailed the author to hopefully get further info. I have no idea why integral triangles would be linked to combinatorial lines in such a manner."},{"username":"jonathan-vos-post","timestamp":"2011-04-20T04:00:00.000Z","contents":"I withdrew my submission 24 hours earlier of A156989 in favor of Terry Tao’s submission, because I have 2130 sequences there already, and the world of mathematics would benefit from more directly from Dr. Tao. OEIS is a tremendously valuable collaborationware. I now consider it, among other things, a kind of alpha test of Polymath1."},{"username":"d-eppstein","timestamp":"2011-04-20T04:00:00.000Z","contents":"Fujimura disjoint triangle cover Theorem: for all n congruent to 0 or 2 mod 6, the triangular lattice with n points per side can be partitioned into (upwards oriented) triples of points forming the vertices of equilateral triangles. Proof: We show more strongly by induction that such a partition exists in which the outer three points in each corner of the lattice are each covered by triangles of the partition. As a base case, it’s trivial for n=2. When n=0 or 6 mod 18, the result follows by partitioning the lattice into six upwards-oriented sublattices of size n/3 and three downwards-oriented sublattices of size n/3 – 1, per [1220]. The upwards sublattices can be partitioned by induction and the downwards sublattices are translates of each other in an equilateral triangle and together can be partitioned into congruent triangles with one vertex per downwards sublattice. Similarly, when n = 2 or 8 mod 18, the result follows by partitioning the lattice into six upwards-oriented sublattices of size n/3 and three downwards-oriented sublattices of size n/3 + 1, per [1242]. When n = 14 mod 18, the result follows by covering the lattice by six upwards-oriented sublattices of size (n+4)/3 that overlap only on their corners, with six downwards-oriented sublatices of size (n+4)/3 – 5 in an equilateral triangle pattern, per [1222]. When n = 12 mod 18, the result follows by placing three upwards-oriented sublattices of size (n+6)/3 centered on each side of the lattice, overlapping in a size-2 sublattice in the center of the lattice, and placing three upwards-oriented sublattices of size (n+6)/3 – 4 in the corners of the lattice, leaving three downwards-oriented sublattices of size (n+6)/3 – 3 in an equilateral triangle pattern. This covers all possible values mod 18, so the result is proven."},{"username":"d-eppstein","timestamp":"2011-04-20T04:00:00.000Z","contents":"Follow-up to previous comment: there’s a gap in the proof. The claimed decomposition for 12 mod 18 actually works for 14 mod 18\\. If you try it for 12 mod 18 the three side-centered sublattices will have too large an area of overlap."},{"username":"michael-peake","timestamp":"2010-04-20T04:00:00.000Z","contents":"Fujimura triangles If you have a solution for n, you can get a solution for 2n+2.  \nFirst cover the 2n+2 grid with the smallest triangles.  \nThat leaves an n grid of twice the size. That together with the previous post gives a solution for 30 mod 36."},{"username":"mats-granvik","timestamp":"0007-04-20T04:00:00.000Z","contents":"1249. What I said here in 1234 and 1239 does not hold. The next term with this method would have been 1473 which is outside the bounds for $latex $."},{"username":"mats-granvik","timestamp":"0007-04-20T04:00:00.000Z","contents":"1249.  \nOops, I got the latex wrong I meant c7."},{"username":"kristal-cantwell","timestamp":"2012-04-20T04:00:00.000Z","contents":"Density Moser theorems implies DH(k) for all k If we take a point from n^k and map it to 2^k points in 2n^k as follows f(p) is the set of points such that the ith coordinate value is either the ith value of p or 2k-p. This will give 2^k possible points for each point p and thus preserve density. If there is a geometric line in the 2k set it corresponds to a combinatorial line in the k set as pairs of the form 2k-i and i which are the two possible ith points of a geometric line are images of i and so from a geometric line we get a combinatorial line in the original. This is a generalization for a specific result involving 3 and 6 already known. We can use this as follows if we have a proof that if a cube of side n has density epsilon and there is a dimension high enough that it has a geometric line and this is true for all n Call this DM(n), we can get DHJ(n) for all n. To see this suppose we do not have DHJ(k) for a specific k then we can use the above mapping which preserves density to get a Moser set with side 2k with epsilon for all dimensions n such that there is no geometric line and this contradicts DM(n) for the specific values epsilon and 2k. This could be used to possibly simplify proofs of DHJ(k) and its consequences."},{"username":"michael-peake","timestamp":"0005-04-20T04:00:00.000Z","contents":"Fujimura, size 12 Referring to 1244, here is a solution to size 12 with a small triangle in each corner:  \na,  \naa,  \nbhj,  \nbbjj,  \nchlhq,  \ncckorv,  \ndglnlvv,  \neggoqotq,  \neekprkurx,  \ndimdswuuxx,  \nfiinsstnytz,  \nffmpmwpwyyzz."},{"username":"michael-peake","timestamp":"0008-04-20T04:00:00.000Z","contents":"Fujimura, disjoint triangles Proof that an equilateral triangle of sides 6n, 6n+2 can be split into equilateral triangles. Further, they can be covered so that the three corners contain length-2 triangles. Further, if the side is more than 204, they can be covered so the three corners contain separate length-6 triangles. Take David’s three rules from 1242.  \n(1) P(k,S) => P(3k,S u {k})  \n(2) P(k,S) and j in S => P(3k-2j,S u {k})  \n(3) P(k,S) => P(3k+2,S u {k}).  \nAdd a fourth rule from 1248:  \n(4) P(k,S) => P(2k+2, 2 U (2S+2) ) Let Q(k) be the property P(k,{2,6}).  \nWe can do induction if n >34.  \nSuppose Q(6n) and Q(6n+2) are true  \nthen Q(6n) -> Q(18n-4),Q(18n-12),Q(18n),Q(18n+2)  \nand Q(6n+2) -> Q(18n+2),Q(18n-6),Q(18n+6),Q(18n+8)  \n-> Q(18n+m) for m=-6,-4,0,2,6,8  \n-> Q(6p) and Q(6p+2) are true for p=3n-1, 3n, 3n+1 All that remains now is to establish P(k,S) for k below 204, and Q(6k),Q(6k+2) for k between 35 and 104\\. My Matlab routine, based on the four rules, said it was true."},{"username":"klas-markstrom","timestamp":"0009-04-20T04:00:00.000Z","contents":"HJ I decided try out a variation of the earlier constructions for large sets without combinatorial lines. Consider all sets of the following type: A point %x% is in %S_A% if the sum of the coordinates of %x% belongs to %A%.  \nThe construction considering the divisors of k is a special case of this kind of set. It is easy to modify the integer programs to sets of this kind. The results are here  \n[http://abel.math.umu.se/~klasm/Data/HJ/](http://abel.math.umu.se/~klasm/Data/HJ/)  \nThe numbers of optimal solutions are quite small. The optimal set of this type for (5,4) actually improved the lower bound for %c_{5,4}% from 708 to 712."},{"username":"kristal-cantwell","timestamp":"0001-04-20T04:00:00.000Z","contents":"Colorings avoiding geometric lines Let M(n,r) be defined as the least dimension of an n-cube such that any r-coloring of it has a monochromatic line.  \nWe will show: M(2k,r) is greater than or equal to HJ(k,r) and  \nM(2k+1,r) is greater than or equal to HJ(k,r). If we have a r-coloring of k^n  \nwhich is combinatorial line free then  \nwe give the color each point of 2k (a,b,c..  \nto each point (a or 2k-a, b or 2k-b and  \nget a geometric line free coloring as a geometric  \nline would force a combinatorial line in the orginal coloring  \nso M(2k,r) is greater than or equal to HJ(k,r). Similarly  \nif we have a r-coloring of k^n  \nwhich is combinatorial line free then  \nwe give the color each point of 2k (a,b,c..  \nto each point (a or 2k-a+1, b or 2k-b+1 and  \nget a geometric line free coloring as a geometric  \nline would force a combinatorial line in the orginal coloring  \nso M(2k+1,r) is greater than or equal to HJ(k,r)."},{"username":"kristal-cantwell","timestamp":"2011-04-20T04:00:00.000Z","contents":"Hyper-optimistic conjecture for Moser sets If we look at the question of whether the weight of a set that is geometric line free is equal to  \nthe greatest number of slices in a geometric line free set we get  \nthe hyper-optimistic conjecture for Moser sets. For 2 the question is trivially true  \nsince we are allowed only one point and there are single points that are slices. For 3 and 4 since there are two slice sets that contain geometric lines the number  \nof slices in a geometric set is linear and quadratic respectively for a density of c/n  \nin the slice space and this will transfer over to a maximum density of all sets composed of slices of I think  \n1/n^.5 and this in turn should I think give colorings of less than 1/n^.5 must have a monochromatic line.  \nBeyond 3 and 4 the colorings start getting better at 5\\. Can these numbers  \nbe improved by some sort of nonslice division of points. Or for that matter is there a conterexample to the this conjecture for Moser sets."},{"username":"kristal-cantwell","timestamp":"0009-04-20T04:00:00.000Z","contents":"Hyper-optimistic conjecture for Moser sets correction In the above the lower bound for the density for moser sets n=3 and 4 deri ed from slices should be something like  \n1/n^5 but with a factor of log n as well. That will also affect the colorings by a factor of log n as well."},{"username":"michael-peake","timestamp":"0003-04-20T04:00:00.000Z","contents":"Regarding %c_{4,5}% Following my 1189, I have found a saturated solution of %c_{4,5}% in which all five points of the major diagonal are among the 125 deleted points. I think that makes it different from the other solutions. I have put it in the wiki. I can’t see how to generalize it yet. Also, regarding the dissection of the Fujimura triangle into disjoint triangles, there are 118 solutions when n=8, and millions when n=12\\. So my proof that involved triangles with n>600 was probably overkill."},{"username":"michael-peake","timestamp":"0004-04-20T04:00:00.000Z","contents":"1257. The solution for %c_{4,5}% that I put in the wiki was just a disguised version of the solution we already have: Replace the digits (0,1,2,3,4) by (3,4,0,2,1), then for all 125 points (x,y,z,w), x+y+z+2w is a multiple of 5."},{"username":"kristal-cantwell","timestamp":"2011-04-20T04:00:00.000Z","contents":"Lower bound for density of combinatorial line free k cube of side n From 1170 we get a formula for slice desity based on  \nlower bounds for sets without k term geometric progressions  \nand we get if k is greater than 2^n +1  \nC\\frac{\\sqrt[2n]{log N}}{2^{n2^{(n-1)/2}\\sqrt[2n]{log N}}} Now if we restrict ourselves to several standard deviations from the mean  \nthen all the slices will be roughly the same size  \nas the limit of (1+1/c(n^.5))^c(n^.5) is roughly e^-c  \nthe density is roughly the same there will be a factor of 1/2^5 on the constant multiplying the exponent 2^c(log n)^.5  \nalso in the original I think N had to be k*n+1 since the values for which the arithmetic series are excluded go from 0 to kn and here that range would be roughly c(n^.5)"},{"username":"kristal-cantwell","timestamp":"2012-04-20T04:00:00.000Z","contents":"Correction and further remarks It should be side k and dimension N  \nn is used in the formula  \nthe argument has already been used in the wiki to  \nget the asymptote for c_n based on a set of slices  \nclose to even distribution by a multiple of square root  \nof n so here I am extending this argument for cases greater than three  \nusing the most recent bound for k arithmetic progression free sets  \nfrom [http://arxiv.org/PS_cache/arxiv/pdf/0811/0811.3057v2.pdf](http://arxiv.org/PS_cache/arxiv/pdf/0811/0811.3057v2.pdf) and we get if k is greater than 2^n +1 C\\frac{\\sqrt[2n]{log k(cN^.5)+1}}{2^{n2^{(n-1)/2}\\sqrt[2n]{log K(cN^5}+1} where here capital N is dimension and n is an integer entirely separate  \nfrom N dependent of k"},{"username":"kristal-cantwell","timestamp":"2011-04-20T04:00:00.000Z","contents":"4D Moser If a 4D Moser set has more than 40 points and 4 points with three  \n2’s it must have the following statistics: (6,16,15,4,0,0) We have from post 1113 that only the following statistics are possible: (6,16,15,4,0), (7,16,14,4,0), (7,17,13,4,0), and (7,18,12,4,0). and from post 1241 If a Moser set has 4 points with 3 2’s and its size is 41  \nor more then it must not have any set of two points with three twos with  \nthe same c statistic. We look at the number of points with one two each one must appear in exactly one of the four possible middles cubes from the four possible coordinate slices, hence if the number of these points is greater than 16 then there must be one such middle cube with five of these points  \nthen by 1241 we know that there must be three points with three twos in the cube as well as otherwise there would be two with the same c statistic so we check the pareto optimizers from the wiki and find there is only one satisfying 5 something 3 something and that is (5,4,3,0)  \nso the center has 12 points and distribution (5,4,3,0) and the cube with  \nthe remaining point with three twos has thirteen points so the remaining cube must have 16 points and hence distribution (4,12,0,0) and the distribution of the other side cube since has thirteen points must be  \n(3,6,3,1) or (4,6,2,1) but this gives a total of 5 + 12+ 6 points with one two  \nwhich is two many for any of the possible sets of statistics so the only  \ndistribution that is possible is the one with 16 points with one two  \nnamely (6,16,15,4,0) and we are done."},{"username":"kristal-cantwell","timestamp":"2011-04-20T04:00:00.000Z","contents":"4D Moser A 4D Moser set with 4 points with three threes has 40 points or less.  \nBy the reasoning of the above post each slice of a Moser set must have 4 points or less with one 2in the center cube. Since from the above we must have 16 of these points and each point only appears in one coordinate slice the division must be exactly 4 4 4 4. No extremal slice can be 16 if so it has distribution (4,12,0,0) by the Pareto optimizers and hence since by the above the distribution of points with one two must include exactly 4 in the center slice we have a total of 16 and hence none in the other extremal set which contains its center point then by looking at the Pareto optimizers we see that we have to remove 6 points from a thirteen point or four points from a 11 point set in order to realize this and this gives at most 8 points then we have 8 + 16 points for the two side slices and we need 17 points in the center slice which cannot be realized. So we have the side slice without the center point must have 15 points or less. Next we show that the center slice must have 14 points or less. By the above it must have 4 points with one two but this gives a distribution  \nof (4 x y z) note the points with one two in this slice count as points with no 2 in the internal statistics of the cube. The center slice must have y = to three because we have already established that there are no two points with the same c-statistic so we must have statistics (4 x 3 0)  \nwhich looking at the Pareto optimizers must have 14 points or less.  \n(I will continue this in the next post .)"},{"username":"kristal-cantwell","timestamp":"2012-04-20T04:00:00.000Z","contents":"4D Moser set Continued from 1260: Now we recall that now two points with three twos can have the same c-statistic then we can assume without loss of generality that all such points have the coordinate not equal to 2 equal to one. Then we look at the points  \n(1 1 3 2) (1 3 1 2) (3 1 1 2) Only one of these can be in the Moser set because otherwise a line will be formed with the points with three 2’s under the above assumption. Now that means that under one of the coordinate cuts must have a diagonal connecting two points with one two. empty and that means that it can have at most 5 points with one two since each of the other diagonals have at most one point. That implies that the there must be at most 12 points in this cube since the other points have at most 14 and 15 points we must have the distribution 12 14  \n15 or we will have less than 41 points. Then the statistics of the  \ncube must be (3 5 3 1) or ( 4 5 2 1) and the other side cube  \nmust have statistics ( 3 9 3 1) (4 9 2 1) or (4 11 0 0)  \nor (3 12 0 0) but since we have the statistics (6 15 16 4 0) we can only have the choice (3 5 3 1) and the choices (3 9 3 1) and (3 12 0 0)  \nor we would have more than 6 points with no 2’s and we can discard  \nthe possiblity (3 12 0 0) because it would give more than 16 points with one 2\\. So we must have (3 5 3 1) and (3 9 3 1) and this gives center slice  \n(2 9 3 0) but this has less than four points with two twos and will force another slice to have more than 4 and so we are done."},{"username":"kristal-cantwell","timestamp":"2012-04-20T04:00:00.000Z","contents":"Correction The first sentence should read recall that no two points with three twos can have the same c-statistic."},{"username":"terence-tao","timestamp":"0002-04-20T04:00:00.000Z","contents":"1262. Kristal, this is great! Looks like we are on track to produce a human proof of the %c'_5=124% theorem, though if I recall correctly we used Klas’s data in other places as well than the %d \\leq 3% bound for 41+ Moser sets. Also, my reduction of the statistics used Michael’s data for the n=3 Moser sets, which was a brute force search over the %2^{27}% possibilities if I recall correctly. But that is at least a computation that is easily reproduced, and can probably be proven by human methods also."},{"username":"terence-tao","timestamp":"0005-04-20T04:00:00.000Z","contents":"p.s. if you could transcribe the argument to the wiki, that would be great also. At some point I’ll also describe the argument in 1113 in more detail, to help complete the proof."},{"username":"kristal-cantwell","timestamp":"2011-04-20T04:00:00.000Z","contents":"4D Moser set I put all my posts and 1113 on the Moser wiki at the end of the section n=4."},{"username":"jason-dyer","timestamp":"0008-04-20T04:00:00.000Z","contents":"Limited HOC Do we need the complete HOC to get DHJ(3)? I haven’t had much time still but I’ve been fiddling with the disjoint triangle proofs and it seems like configurations can get quite different depending on the divisibility of the side length. I theorize it might be possible to find an extremely specific case (to generate a random and non-serious example, where (n+1) mod 128 = 41) where Fujimura is not only tractable but constructible, and the construction can then be used to resolve both ends of the equation."},{"username":"kristal-cantwell","timestamp":"0009-04-20T04:00:00.000Z","contents":"Limited HOC As I understand it we do not need equality but bounding by a constant factor would be enough for the cases n=3 and higher."},{"username":"michael-peake","timestamp":"0005-04-20T04:00:00.000Z","contents":"4D Moser set Kristal, I was going through your proof of the 4D Moser set, and I don’t think you have eliminated the statistic (7,16,14,4,0). In 1259 you show that b=16, and in 1261 you use that a=6. I have edited your proof a little in the Wiki; I hope you don’t mind."},{"username":"kristal-cantwell","timestamp":"2010-04-20T04:00:00.000Z","contents":"4D Moser set It looks like I made a mistake hear and I need to  \ndo case (7 16 14 4 0)  \nHere it is: Recall we must have the distribution 12 14  \n15 or we will have less than 41 points. Also recall that the statistics of the side cube with its center point must be (3 5 3 1) or ( 4 5 2 1) then the statistics of the other side cube  \nmust be ( 3 9 3 0) (4 9 2 0) or (4 11 0 0)  \nor (3 12 0 0)  \nSince the statistics are  \n(7,16,14,4,0) we must have one of the following four pairs  \n(3 5 3 1) and (4 9 2 0)  \n(3 5 3 1) and (4 11 0 0)  \n( 4 5 2 1) and (3 9 3 0)  \n( 4 5 2 1) and (3 12 0 0) (3 5 3 1) and (4 9 2 0)  \nforces the middle slice to have statistics (0 2 9 3)  \nwhich has less than 4 points with one two  \nhence forces another slice to have more than four which gives  \na contradiction (3 5 3 1) and (4 11 0 0)  \nforces the middle slice to have statistics (0 0 11 3)  \nwhich has less than 4 points with one two  \nhence forces another slice to have more which gives  \na contradiction ( 4 5 2 1) and (3 9 3 0)  \nforces the middle slice to have statistics (0 2 9 3)  \nwhich has less than 4 points with one two  \nhence forces another slice to have more which gives  \na contradiction ( 4 5 2 1) and (3 12 0 0)  \nhas 17 points with one two which contradicts the  \nstatistics of the entire set.  \nSo none of the cases work and we are done."},{"username":"kristal-cantwell","timestamp":"0001-05-20T04:00:00.000Z","contents":"4D Moser Set I have added 1267 to the wiki at [http://michaelnielsen.org/polymath1/index.php?title=Moser%27s_cube_problem](http://michaelnielsen.org/polymath1/index.php?title=Moser%27s_cube_problem) at the end of the section n=4 Thank you to Michael Peake for finding the error and editing  \nthe proof."},{"username":"klas-markstrom","timestamp":"0006-05-20T04:00:00.000Z","contents":"Metacomment:  \nGiven that we have not added much new material for the last month or so I have a metaquestion: What are the current plans for writing something up based on the threads here? Will it be part of the paper already being written or a second one? We have a number of results on both asymptotics and small (n,k). In the outline for the write up that has begun at the wiki, bounds are mentioned as a possible part of the Discussion section."},{"username":"terence-tao","timestamp":"0008-05-20T04:00:00.000Z","contents":"Ah, yes, I should be getting back to this project at some point. I was kind of delaying things hoping for the paper writing project on the other side to advance a bit first, but that seems to be slow also. Right now, it looks like the plan is to have two papers – one for the new proofs of DHJ(3), and the other for the small (n,k) results. My feeling is that we don’t need to put every single result we’ve established into the small(n,k) paper; there are some which definitely have a preliminary feel to them, and in any case there’s no reason why we can’t write a subsequent paper should there be further significant advances, and in the meantime we have the wiki. The obvious things to put in, I think, are the c_6 and c’_5 computations, the numerics for higher n, the asymptotic counterexamples we have, and the results we have for the n,k DHJ numbers, including the breakdown of the HOC. It looks like we’re close to a human proof of c’_5; it would be nice to have both in there. That would already be a reasonably-sized paper with a number of non-trivial results, I think. Once we get some consensus on what to put in, I can try to draft an outline of the paper similar to what one has for the other paper, and then we can discuss individual sections, standardise the notation (e.g. is [3] equal to {0,1,2} or {1,2,3}? Not a life-or-death issue, but one which needs to be settled at some point.)"},{"username":"klas-markstrom","timestamp":"0005-05-20T04:00:00.000Z","contents":"I think the suggested material should make a good paper. This will be emphasising the original “growing n” direction of the problem. I am away for about a week now but once I get back home I should have some time to devote to this project again."},{"username":"kristal-cantwell","timestamp":"2010-05-20T04:00:00.000Z","contents":"I also agree that the suggested material looks good for a paper. I would prefer {1,2,3} as I am used to seeing 2 as the center point. I will be looking at the problem of making the proof of c_5′ a human proof."},{"username":"jason-dyer","timestamp":"0006-05-20T04:00:00.000Z","contents":"I also vote for {1,2,3}. I think Fujimura deserves a paper on its own but we don’t know enough about it yet to consider one. Separated from DHJ, allowing r < 0 should be considered, and I think it’d be productive to study all the various restrictions on r (only r = 1, 0 < r < 3, etc.) I also now believe the HOC is false for even DHJ(3), and it’d be great if we could find a counterexample before the paper goes up, but that might be out of our computational reach."},{"username":"jason-dyer","timestamp":"0006-05-20T04:00:00.000Z","contents":"Second thought: would it be productive to make a “spin-off” thread for a Fujimura-only polymath? (I’d be happy to make one but my blog traffic is about 30 times less than here, so I doubt we’d get much of the same cross-interaction we’ve had.)"},{"username":"jason-dyer","timestamp":"2011-05-20T04:00:00.000Z","contents":"Fujimura when r can only equal 1 r=1 is the case of considering only the triangles of sidelength 1. Consider each row r of the triangular lattice. Let the terms of a particular row be i_0, i_1, …. , i_r.  \nWhen r mod 3 = 0, then the terms i_k where k mod 3 = 1 should be removed.  \nWhen r mod 3 = 1, then the terms i_k where k mod 3 = 0 should be removed.  \nWhen r mod 3 = 2, then the terms i_k where k mod 3 = 2 should be removed. This leaves maximal sets of 0, 2, 4, 6, 10, 14, 18, 24, … which curiously enough matches with [A128422](http://www.research.att.com/~njas/sequences/A128422) which is the projective plane crossing number of %K_{4,n}%. There’s a closed form of %f(n) = \\lfloor \\frac{n}{3} \\rfloor(2n-3(1+\\lfloor \\frac{n}{3} \\rfloor))% for A128422 which means the closed form for the sequence above is just %f(n+3)% of the same."},{"username":"jason-dyer","timestamp":"2011-05-20T04:00:00.000Z","contents":"(Oops, unfortunate variable name clash in 1269\\. r row is different from other r. Sorry about that!)"},{"username":"kevin-obryant","timestamp":"2010-05-20T04:00:00.000Z","contents":"It occurs to me that it could be beneficial to build line-free sets directly from Behrend-ish arguments, rather than from the Behrend bound on %r_3(n)%. Here’s what I have in mind, but I haven’t worked out the numbers. Take %u% in %T^d = [-1/2,1/2)^d% uniformly, and take %v% in %(T^d)^n% uniformly. Let %\\phi% be the map from $[3]^n={0,1,2}^n \\to T^d$ that takes %x% to %u + v \\cdot x \\mod 1%. The useful thing is that this map takes any combinatorial line to an arithmetic progression in %T^d%. If we restrict our attention to those %x% that map into the box %[-1/4,1/4)^d% then we avoid wrap around progressions in %T^d%, and so we can think of %[-1/4,1/4)^d% as a subset of %R^d%. If we further restrict our attention to those %x% that map into a thin annulus, then we are only left with progressions with very small difference. Since %u% was chosen uniformly, we can estimate the number of %x \\in [3]^n% that map into the annulus and into the box: the proportion is just the volume of that shape. Letting %d% be the 0-1 vector with 1’s where the wild-cards are, we get a difference in %R^d% of %d \\cdot v%, which must be uniformly distributed mod 1 by the choice of %v%, and must be small because the annulus is thin. We can bound how many times that happens by the volume of a small ball, and throw out one point for each time. This is basically just rehashing the Green-Wolf construction, but using a map from [3]^n instead of a map from [1,k] and then taking slices. I don’t know what this gives…and I’d prefer to have a construction that includes this and the slices one as special cases, and *then* optimize the parameters. I’ll work out what this gives next week (after the CANT conference here in New York), but if anybody sees a better map or family of maps to look at please post!"},{"username":"michael","timestamp":"0004-02-20T05:00:00.000Z","contents":"Guys, we forgot to do combinatorial planes ;) There are $(latex 5^n-2*4^n+3^n)/2$ combinatorial planes. Let %S% be a set that intersects all 2-dimensional combinatorial planes of %[3]^n%. Let %P(n)% be the size of the smallest such set. This is %3^n% minus the sets we have been considering. Just for the sake of writing this down, we have %P(1)=0, P(2)=1%. Then %P(3) = 3% for example %111,222,333%. %9 \\leq P(4) \\leq 11% as there are nine disjoint planes, and the following eleven points cut all planes: %1111,2222,3333,1123,1231,1312,2132,2321,2213,3111,3222% %27 \\leq P(5) \\leq 39 as there are 27 disjoint planes, and the following 39 points cover all planes: %latex 11123 11132 11233 11312 11331 12121 12211$ %12223 12232 12322 13131 13222 13313 21113% %21131 21222 21311 22112 22231 22323 23122% %23212 23221 23233 23332 31112 31211 31221% %31233 31323 32133 32222 32313 32331 33111% %33123 33213 33321 33332%"}]}